{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task3\n",
    "\n",
    "Index_X = FSR_for_coord\n",
    "\n",
    "Index_y = x_coord, y_coord\n",
    "\n",
    "Data = Splited by Time\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-07-19_02-32-25/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-07-19_02-32-25\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "0.6551"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.ANN'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    imputer = trial.suggest_categorical('imputer', ['sklearn.impute.SimpleImputer'])\n",
    "    if imputer == 'sklearn.impute.SimpleImputer':\n",
    "        trial.suggest_categorical('imputer_args/strategy', [\n",
    "            'mean',\n",
    "            'median',\n",
    "        ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': 'FSR_for_coord',\n",
    "        'index_y': ['x_coord', 'y_coord'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_time'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-19 02:32:25,666] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 02:32:27,846\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-07-19 02:32:29,232\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-19 02:51:54</td></tr>\n",
       "<tr><td>Running for: </td><td>00:19:24.84        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.1/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=100<br>Bracket: Iter 64.000: -0.6563670002671669 | Iter 32.000: -0.657598025585568 | Iter 16.000: -0.6601799324694015 | Iter 8.000: -0.6650387005288745 | Iter 4.000: -0.6723997729072466 | Iter 2.000: -0.6867657823741797 | Iter 1.000: -0.7038196995217894<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>imputer             </th><th>imputer_args/strateg\n",
       "y       </th><th>index_X      </th><th>index_y             </th><th>model        </th><th style=\"text-align: right;\">    model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">  model_args/num_layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">     mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_01ccd703</td><td>TERMINATED</td><td>172.26.215.93:374463</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3040</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00603681 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       32.8727  </td><td style=\"text-align: right;\">0.655106</td><td style=\"text-align: right;\">0.346257</td><td style=\"text-align: right;\">0.096689 </td></tr>\n",
       "<tr><td>FSR_Trainable_5f091b2b</td><td>TERMINATED</td><td>172.26.215.93:374536</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3a80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.83215e-05</td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.853106</td><td style=\"text-align: right;\">2.72647 </td><td style=\"text-align: right;\">2.41543 </td><td style=\"text-align: right;\">0.374158 </td></tr>\n",
       "<tr><td>FSR_Trainable_0bd8e5da</td><td>TERMINATED</td><td>172.26.215.93:374709</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d880</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0123096  </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        4.44098 </td><td style=\"text-align: right;\">0.719601</td><td style=\"text-align: right;\">0.395933</td><td style=\"text-align: right;\">0.110779 </td></tr>\n",
       "<tr><td>FSR_Trainable_7b106491</td><td>TERMINATED</td><td>172.26.215.93:374885</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__a980</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0621451  </td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.602421</td><td style=\"text-align: right;\">0.904122</td><td style=\"text-align: right;\">0.490035</td><td style=\"text-align: right;\">0.110675 </td></tr>\n",
       "<tr><td>FSR_Trainable_0e31f13a</td><td>TERMINATED</td><td>172.26.215.93:375206</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__eb80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00926397 </td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.881248</td><td style=\"text-align: right;\">0.754306</td><td style=\"text-align: right;\">0.426689</td><td style=\"text-align: right;\">0.110005 </td></tr>\n",
       "<tr><td>FSR_Trainable_896925d9</td><td>TERMINATED</td><td>172.26.215.93:375433</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9900</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00649975 </td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.40069 </td><td style=\"text-align: right;\">0.715144</td><td style=\"text-align: right;\">0.445488</td><td style=\"text-align: right;\">0.108265 </td></tr>\n",
       "<tr><td>FSR_Trainable_9d1f6fd8</td><td>TERMINATED</td><td>172.26.215.93:375656</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5300</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000125944</td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        4.08348 </td><td style=\"text-align: right;\">0.710793</td><td style=\"text-align: right;\">0.34891 </td><td style=\"text-align: right;\">0.0939858</td></tr>\n",
       "<tr><td>FSR_Trainable_08791bfe</td><td>TERMINATED</td><td>172.26.215.93:375888</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8cc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0254255  </td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.676482</td><td style=\"text-align: right;\">0.775819</td><td style=\"text-align: right;\">0.504217</td><td style=\"text-align: right;\">0.124197 </td></tr>\n",
       "<tr><td>FSR_Trainable_ae8bf4d3</td><td>TERMINATED</td><td>172.26.215.93:376113</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c3c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000352055</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        6.70063 </td><td style=\"text-align: right;\">0.708348</td><td style=\"text-align: right;\">0.356931</td><td style=\"text-align: right;\">0.0948239</td></tr>\n",
       "<tr><td>FSR_Trainable_eae8aca6</td><td>TERMINATED</td><td>172.26.215.93:376205</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6340</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000252741</td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.774517</td><td style=\"text-align: right;\">2.58393 </td><td style=\"text-align: right;\">2.22169 </td><td style=\"text-align: right;\">0.377169 </td></tr>\n",
       "<tr><td>FSR_Trainable_fd5b9379</td><td>TERMINATED</td><td>172.26.215.93:376512</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c100</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0525884  </td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.666875</td><td style=\"text-align: right;\">0.836945</td><td style=\"text-align: right;\">0.550215</td><td style=\"text-align: right;\">0.116554 </td></tr>\n",
       "<tr><td>FSR_Trainable_d661622e</td><td>TERMINATED</td><td>172.26.215.93:376746</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e040</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00219616 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       21.3933  </td><td style=\"text-align: right;\">0.695682</td><td style=\"text-align: right;\">0.338602</td><td style=\"text-align: right;\">0.099574 </td></tr>\n",
       "<tr><td>FSR_Trainable_ea541971</td><td>TERMINATED</td><td>172.26.215.93:376835</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b940</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00173857 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.30795 </td><td style=\"text-align: right;\">0.718553</td><td style=\"text-align: right;\">0.390476</td><td style=\"text-align: right;\">0.112792 </td></tr>\n",
       "<tr><td>FSR_Trainable_aef894a5</td><td>TERMINATED</td><td>172.26.215.93:377011</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b700</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0017399  </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       25.3254  </td><td style=\"text-align: right;\">0.67505 </td><td style=\"text-align: right;\">0.359279</td><td style=\"text-align: right;\">0.0954795</td></tr>\n",
       "<tr><td>FSR_Trainable_46ab530f</td><td>TERMINATED</td><td>172.26.215.93:377319</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7580</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00142791 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        5.28649 </td><td style=\"text-align: right;\">0.770429</td><td style=\"text-align: right;\">0.451844</td><td style=\"text-align: right;\">0.116283 </td></tr>\n",
       "<tr><td>FSR_Trainable_c982cc1d</td><td>TERMINATED</td><td>172.26.215.93:377559</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b400</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00369763 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.50874 </td><td style=\"text-align: right;\">0.780096</td><td style=\"text-align: right;\">0.433413</td><td style=\"text-align: right;\">0.107517 </td></tr>\n",
       "<tr><td>FSR_Trainable_e59b43e6</td><td>TERMINATED</td><td>172.26.215.93:377768</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c200</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00326175 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       17.8389  </td><td style=\"text-align: right;\">0.691198</td><td style=\"text-align: right;\">0.338262</td><td style=\"text-align: right;\">0.100093 </td></tr>\n",
       "<tr><td>FSR_Trainable_cf3426ef</td><td>TERMINATED</td><td>172.26.215.93:377993</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3c40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000647633</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       20.3554  </td><td style=\"text-align: right;\">0.668986</td><td style=\"text-align: right;\">0.35426 </td><td style=\"text-align: right;\">0.093928 </td></tr>\n",
       "<tr><td>FSR_Trainable_0649225d</td><td>TERMINATED</td><td>172.26.215.93:378231</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ce80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000803567</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       21.7116  </td><td style=\"text-align: right;\">0.669828</td><td style=\"text-align: right;\">0.356255</td><td style=\"text-align: right;\">0.0947116</td></tr>\n",
       "<tr><td>FSR_Trainable_dd656807</td><td>TERMINATED</td><td>172.26.215.93:378444</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__dd80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00102017 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       22.6743  </td><td style=\"text-align: right;\">0.66963 </td><td style=\"text-align: right;\">0.35745 </td><td style=\"text-align: right;\">0.0950322</td></tr>\n",
       "<tr><td>FSR_Trainable_1e24860f</td><td>TERMINATED</td><td>172.26.215.93:378672</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e400</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000843353</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       42.5696  </td><td style=\"text-align: right;\">0.667412</td><td style=\"text-align: right;\">0.352629</td><td style=\"text-align: right;\">0.0936227</td></tr>\n",
       "<tr><td>FSR_Trainable_54a1fa11</td><td>TERMINATED</td><td>172.26.215.93:378901</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d300</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000756759</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       65.6268  </td><td style=\"text-align: right;\">0.656158</td><td style=\"text-align: right;\">0.344522</td><td style=\"text-align: right;\">0.096818 </td></tr>\n",
       "<tr><td>FSR_Trainable_3801c760</td><td>TERMINATED</td><td>172.26.215.93:379129</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c300</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0116766  </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        7.53604 </td><td style=\"text-align: right;\">0.672626</td><td style=\"text-align: right;\">0.361704</td><td style=\"text-align: right;\">0.0953728</td></tr>\n",
       "<tr><td>FSR_Trainable_8d4fea50</td><td>TERMINATED</td><td>172.26.215.93:379360</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5000</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0274824  </td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.697224</td><td style=\"text-align: right;\">0.746679</td><td style=\"text-align: right;\">0.43607 </td><td style=\"text-align: right;\">0.116359 </td></tr>\n",
       "<tr><td>FSR_Trainable_7853eed4</td><td>TERMINATED</td><td>172.26.215.93:379587</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8500</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00077974 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       43.0951  </td><td style=\"text-align: right;\">0.667355</td><td style=\"text-align: right;\">0.354434</td><td style=\"text-align: right;\">0.0944315</td></tr>\n",
       "<tr><td>FSR_Trainable_4ad33e7a</td><td>TERMINATED</td><td>172.26.215.93:379819</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8540</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00080698 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       21.6808  </td><td style=\"text-align: right;\">0.66946 </td><td style=\"text-align: right;\">0.355376</td><td style=\"text-align: right;\">0.0941366</td></tr>\n",
       "<tr><td>FSR_Trainable_94b05c37</td><td>TERMINATED</td><td>172.26.215.93:380046</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ed00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000454148</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.70587 </td><td style=\"text-align: right;\">0.69856 </td><td style=\"text-align: right;\">0.36339 </td><td style=\"text-align: right;\">0.0990351</td></tr>\n",
       "<tr><td>FSR_Trainable_cf0e6cd8</td><td>TERMINATED</td><td>172.26.215.93:380288</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5200</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000453201</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.64663 </td><td style=\"text-align: right;\">0.699661</td><td style=\"text-align: right;\">0.364227</td><td style=\"text-align: right;\">0.0993799</td></tr>\n",
       "<tr><td>FSR_Trainable_529edf46</td><td>TERMINATED</td><td>172.26.215.93:380504</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3380</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0045265  </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.93832 </td><td style=\"text-align: right;\">0.685845</td><td style=\"text-align: right;\">0.37406 </td><td style=\"text-align: right;\">0.0970363</td></tr>\n",
       "<tr><td>FSR_Trainable_6cd25336</td><td>TERMINATED</td><td>172.26.215.93:380739</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6440</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00430864 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.35155 </td><td style=\"text-align: right;\">0.714374</td><td style=\"text-align: right;\">0.367035</td><td style=\"text-align: right;\">0.0931276</td></tr>\n",
       "<tr><td>FSR_Trainable_83ace7ba</td><td>TERMINATED</td><td>172.26.215.93:380987</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__aa80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000168234</td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.875372</td><td style=\"text-align: right;\">0.730907</td><td style=\"text-align: right;\">0.400627</td><td style=\"text-align: right;\">0.108258 </td></tr>\n",
       "<tr><td>FSR_Trainable_d1ef26ad</td><td>TERMINATED</td><td>172.26.215.93:381206</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b6c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000106807</td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       66.47    </td><td style=\"text-align: right;\">0.655997</td><td style=\"text-align: right;\">0.345129</td><td style=\"text-align: right;\">0.0960426</td></tr>\n",
       "<tr><td>FSR_Trainable_e10c5432</td><td>TERMINATED</td><td>172.26.215.93:381290</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ac80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        8.13517e-05</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       99.0447  </td><td style=\"text-align: right;\">0.655676</td><td style=\"text-align: right;\">0.345298</td><td style=\"text-align: right;\">0.0968428</td></tr>\n",
       "<tr><td>FSR_Trainable_fd46a171</td><td>TERMINATED</td><td>172.26.215.93:381472</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9d40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        8.95622e-05</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       99.7043  </td><td style=\"text-align: right;\">0.655475</td><td style=\"text-align: right;\">0.345186</td><td style=\"text-align: right;\">0.0967741</td></tr>\n",
       "<tr><td>FSR_Trainable_ab1ebf30</td><td>TERMINATED</td><td>172.26.215.93:381775</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0980</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        5.04143e-05</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.56403 </td><td style=\"text-align: right;\">0.691594</td><td style=\"text-align: right;\">0.347151</td><td style=\"text-align: right;\">0.102418 </td></tr>\n",
       "<tr><td>FSR_Trainable_65506096</td><td>TERMINATED</td><td>172.26.215.93:382015</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0b00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        7.07409e-05</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       95.5101  </td><td style=\"text-align: right;\">0.655403</td><td style=\"text-align: right;\">0.345576</td><td style=\"text-align: right;\">0.0969083</td></tr>\n",
       "<tr><td>FSR_Trainable_2daa8b50</td><td>TERMINATED</td><td>172.26.215.93:382299</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4600</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0012187  </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       45.1965  </td><td style=\"text-align: right;\">0.656272</td><td style=\"text-align: right;\">0.344541</td><td style=\"text-align: right;\">0.0969524</td></tr>\n",
       "<tr><td>FSR_Trainable_017adcb8</td><td>TERMINATED</td><td>172.26.215.93:382558</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1640</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.40994e-05</td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.651536</td><td style=\"text-align: right;\">0.781922</td><td style=\"text-align: right;\">0.446322</td><td style=\"text-align: right;\">0.116572 </td></tr>\n",
       "<tr><td>FSR_Trainable_229c32a7</td><td>TERMINATED</td><td>172.26.215.93:382757</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5c40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.12885e-05</td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.868828</td><td style=\"text-align: right;\">0.765131</td><td style=\"text-align: right;\">0.415666</td><td style=\"text-align: right;\">0.114258 </td></tr>\n",
       "<tr><td>FSR_Trainable_0e42846b</td><td>TERMINATED</td><td>172.26.215.93:382987</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__09c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        3.34951e-05</td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.786042</td><td style=\"text-align: right;\">0.731992</td><td style=\"text-align: right;\">0.397781</td><td style=\"text-align: right;\">0.116592 </td></tr>\n",
       "<tr><td>FSR_Trainable_0c1880fb</td><td>TERMINATED</td><td>172.26.215.93:383232</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5640</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        2.99958e-05</td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.31956 </td><td style=\"text-align: right;\">0.728985</td><td style=\"text-align: right;\">0.387535</td><td style=\"text-align: right;\">0.102735 </td></tr>\n",
       "<tr><td>FSR_Trainable_ce58f49b</td><td>TERMINATED</td><td>172.26.215.93:383324</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6c00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        6.80199e-05</td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.27642 </td><td style=\"text-align: right;\">2.60557 </td><td style=\"text-align: right;\">2.27391 </td><td style=\"text-align: right;\">0.343254 </td></tr>\n",
       "<tr><td>FSR_Trainable_5fdfe2cb</td><td>TERMINATED</td><td>172.26.215.93:383636</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6a00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        9.50257e-05</td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.19288 </td><td style=\"text-align: right;\">1.8881  </td><td style=\"text-align: right;\">1.62449 </td><td style=\"text-align: right;\">0.276258 </td></tr>\n",
       "<tr><td>FSR_Trainable_dec4e142</td><td>TERMINATED</td><td>172.26.215.93:383728</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__48c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000102228</td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       72.9069  </td><td style=\"text-align: right;\">0.655703</td><td style=\"text-align: right;\">0.345351</td><td style=\"text-align: right;\">0.0962731</td></tr>\n",
       "<tr><td>FSR_Trainable_a43e2e9e</td><td>TERMINATED</td><td>172.26.215.93:384039</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8f40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000142758</td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       30.5624  </td><td style=\"text-align: right;\">0.660389</td><td style=\"text-align: right;\">0.343817</td><td style=\"text-align: right;\">0.0969635</td></tr>\n",
       "<tr><td>FSR_Trainable_29e8b656</td><td>TERMINATED</td><td>172.26.215.93:384128</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d580</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000173282</td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       32.1414  </td><td style=\"text-align: right;\">0.661727</td><td style=\"text-align: right;\">0.344971</td><td style=\"text-align: right;\">0.0966991</td></tr>\n",
       "<tr><td>FSR_Trainable_bef25036</td><td>TERMINATED</td><td>172.26.215.93:384433</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__fc40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000189651</td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       32.063   </td><td style=\"text-align: right;\">0.660609</td><td style=\"text-align: right;\">0.345233</td><td style=\"text-align: right;\">0.0967874</td></tr>\n",
       "<tr><td>FSR_Trainable_db3cb1ad</td><td>TERMINATED</td><td>172.26.215.93:384706</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__18c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000164   </td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       28.8779  </td><td style=\"text-align: right;\">0.662055</td><td style=\"text-align: right;\">0.3434  </td><td style=\"text-align: right;\">0.0962685</td></tr>\n",
       "<tr><td>FSR_Trainable_d9117331</td><td>TERMINATED</td><td>172.26.215.93:384909</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5800</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000284599</td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.729169</td><td style=\"text-align: right;\">2.96946 </td><td style=\"text-align: right;\">2.48042 </td><td style=\"text-align: right;\">0.406442 </td></tr>\n",
       "<tr><td>FSR_Trainable_1adea1b7</td><td>TERMINATED</td><td>172.26.215.93:385137</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5340</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        9.96476e-05</td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.766668</td><td style=\"text-align: right;\">2.81721 </td><td style=\"text-align: right;\">2.46254 </td><td style=\"text-align: right;\">0.387683 </td></tr>\n",
       "<tr><td>FSR_Trainable_aeafd544</td><td>TERMINATED</td><td>172.26.215.93:385364</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__fec0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        9.73373e-05</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.4254  </td><td style=\"text-align: right;\">0.702341</td><td style=\"text-align: right;\">0.362104</td><td style=\"text-align: right;\">0.100485 </td></tr>\n",
       "<tr><td>FSR_Trainable_eac9ef26</td><td>TERMINATED</td><td>172.26.215.93:385605</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7980</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        2.40871e-05</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.918453</td><td style=\"text-align: right;\">0.715168</td><td style=\"text-align: right;\">0.410968</td><td style=\"text-align: right;\">0.117483 </td></tr>\n",
       "<tr><td>FSR_Trainable_5c949558</td><td>TERMINATED</td><td>172.26.215.93:385699</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4280</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        6.20168e-05</td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       37.533   </td><td style=\"text-align: right;\">0.656367</td><td style=\"text-align: right;\">0.342934</td><td style=\"text-align: right;\">0.0958196</td></tr>\n",
       "<tr><td>FSR_Trainable_bdc8e398</td><td>TERMINATED</td><td>172.26.215.93:386013</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__cf80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        5.32551e-05</td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.13562 </td><td style=\"text-align: right;\">0.692419</td><td style=\"text-align: right;\">0.351942</td><td style=\"text-align: right;\">0.104147 </td></tr>\n",
       "<tr><td>FSR_Trainable_40f53e56</td><td>TERMINATED</td><td>172.26.215.93:386100</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7f40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        6.02636e-05</td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.35163 </td><td style=\"text-align: right;\">0.69191 </td><td style=\"text-align: right;\">0.345269</td><td style=\"text-align: right;\">0.102878 </td></tr>\n",
       "<tr><td>FSR_Trainable_61069d94</td><td>TERMINATED</td><td>172.26.215.93:386411</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6580</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        6.78613e-05</td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.838179</td><td style=\"text-align: right;\">0.735685</td><td style=\"text-align: right;\">0.390536</td><td style=\"text-align: right;\">0.107596 </td></tr>\n",
       "<tr><td>FSR_Trainable_3a991208</td><td>TERMINATED</td><td>172.26.215.93:386644</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ab00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        3.59927e-05</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.636961</td><td style=\"text-align: right;\">0.745584</td><td style=\"text-align: right;\">0.408197</td><td style=\"text-align: right;\">0.115124 </td></tr>\n",
       "<tr><td>FSR_Trainable_b1620c1c</td><td>TERMINATED</td><td>172.26.215.93:386867</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8ec0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000231237</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.1893  </td><td style=\"text-align: right;\">0.703456</td><td style=\"text-align: right;\">0.363869</td><td style=\"text-align: right;\">0.108826 </td></tr>\n",
       "<tr><td>FSR_Trainable_597ecf06</td><td>TERMINATED</td><td>172.26.215.93:387092</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d340</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000243251</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.97991 </td><td style=\"text-align: right;\">0.783768</td><td style=\"text-align: right;\">0.483073</td><td style=\"text-align: right;\">0.127717 </td></tr>\n",
       "<tr><td>FSR_Trainable_49d36a14</td><td>TERMINATED</td><td>172.26.215.93:387325</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7a80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000122035</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.852699</td><td style=\"text-align: right;\">0.883636</td><td style=\"text-align: right;\">0.51249 </td><td style=\"text-align: right;\">0.129176 </td></tr>\n",
       "<tr><td>FSR_Trainable_dba85aa4</td><td>TERMINATED</td><td>172.26.215.93:387412</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d3c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00011358 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.80924 </td><td style=\"text-align: right;\">0.691273</td><td style=\"text-align: right;\">0.351507</td><td style=\"text-align: right;\">0.0994895</td></tr>\n",
       "<tr><td>FSR_Trainable_431d77fc</td><td>TERMINATED</td><td>172.26.215.93:387590</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f980</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.20173e-05</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.9429  </td><td style=\"text-align: right;\">0.69297 </td><td style=\"text-align: right;\">0.356781</td><td style=\"text-align: right;\">0.103496 </td></tr>\n",
       "<tr><td>FSR_Trainable_6835396b</td><td>TERMINATED</td><td>172.26.215.93:387775</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2740</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000332755</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.87347 </td><td style=\"text-align: right;\">0.699026</td><td style=\"text-align: right;\">0.374019</td><td style=\"text-align: right;\">0.108212 </td></tr>\n",
       "<tr><td>FSR_Trainable_78aa2cbe</td><td>TERMINATED</td><td>172.26.215.93:388089</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__de40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000329077</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.06703 </td><td style=\"text-align: right;\">0.713872</td><td style=\"text-align: right;\">0.385513</td><td style=\"text-align: right;\">0.102441 </td></tr>\n",
       "<tr><td>FSR_Trainable_36cddab1</td><td>TERMINATED</td><td>172.26.215.93:388184</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ce00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        4.38677e-05</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.02898 </td><td style=\"text-align: right;\">0.698243</td><td style=\"text-align: right;\">0.351394</td><td style=\"text-align: right;\">0.0996444</td></tr>\n",
       "<tr><td>FSR_Trainable_e51283d5</td><td>TERMINATED</td><td>172.26.215.93:388497</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0980</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        4.33252e-05</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.04267 </td><td style=\"text-align: right;\">0.695293</td><td style=\"text-align: right;\">0.342885</td><td style=\"text-align: right;\">0.101561 </td></tr>\n",
       "<tr><td>FSR_Trainable_3448c238</td><td>TERMINATED</td><td>172.26.215.93:388592</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9980</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        8.37773e-05</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.64528 </td><td style=\"text-align: right;\">0.705054</td><td style=\"text-align: right;\">0.379969</td><td style=\"text-align: right;\">0.105495 </td></tr>\n",
       "<tr><td>FSR_Trainable_063e70f4</td><td>TERMINATED</td><td>172.26.215.93:388904</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e280</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        9.00063e-05</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.68498 </td><td style=\"text-align: right;\">0.702904</td><td style=\"text-align: right;\">0.370457</td><td style=\"text-align: right;\">0.106853 </td></tr>\n",
       "<tr><td>FSR_Trainable_621022c5</td><td>TERMINATED</td><td>172.26.215.93:388997</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2140</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        8.07604e-05</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.953424</td><td style=\"text-align: right;\">0.712201</td><td style=\"text-align: right;\">0.386376</td><td style=\"text-align: right;\">0.106749 </td></tr>\n",
       "<tr><td>FSR_Trainable_8795d3e2</td><td>TERMINATED</td><td>172.26.215.93:389313</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0a80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000568139</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.63872 </td><td style=\"text-align: right;\">0.699815</td><td style=\"text-align: right;\">0.353035</td><td style=\"text-align: right;\">0.105966 </td></tr>\n",
       "<tr><td>FSR_Trainable_b8149bd6</td><td>TERMINATED</td><td>172.26.215.93:389403</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3800</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000501998</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       57.4704  </td><td style=\"text-align: right;\">0.658346</td><td style=\"text-align: right;\">0.343935</td><td style=\"text-align: right;\">0.0973615</td></tr>\n",
       "<tr><td>FSR_Trainable_56248280</td><td>TERMINATED</td><td>172.26.215.93:389714</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__bf00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00248652 </td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.05091 </td><td style=\"text-align: right;\">0.7683  </td><td style=\"text-align: right;\">0.421174</td><td style=\"text-align: right;\">0.113704 </td></tr>\n",
       "<tr><td>FSR_Trainable_4de2513e</td><td>TERMINATED</td><td>172.26.215.93:389802</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0480</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00242236 </td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.10641 </td><td style=\"text-align: right;\">0.785747</td><td style=\"text-align: right;\">0.425266</td><td style=\"text-align: right;\">0.123047 </td></tr>\n",
       "<tr><td>FSR_Trainable_b7ee8305</td><td>TERMINATED</td><td>172.26.215.93:390109</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f1c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00124463 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       47.4233  </td><td style=\"text-align: right;\">0.656486</td><td style=\"text-align: right;\">0.344374</td><td style=\"text-align: right;\">0.0968478</td></tr>\n",
       "<tr><td>FSR_Trainable_93bf26d0</td><td>TERMINATED</td><td>172.26.215.93:390336</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e500</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00148831 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       47.1471  </td><td style=\"text-align: right;\">0.65736 </td><td style=\"text-align: right;\">0.34458 </td><td style=\"text-align: right;\">0.0968502</td></tr>\n",
       "<tr><td>FSR_Trainable_f8c708a6</td><td>TERMINATED</td><td>172.26.215.93:390554</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5c00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0010839  </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       67.6971  </td><td style=\"text-align: right;\">0.656014</td><td style=\"text-align: right;\">0.344976</td><td style=\"text-align: right;\">0.0969407</td></tr>\n",
       "<tr><td>FSR_Trainable_67300bae</td><td>TERMINATED</td><td>172.26.215.93:390828</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1080</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000144875</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.02049 </td><td style=\"text-align: right;\">0.707784</td><td style=\"text-align: right;\">0.365885</td><td style=\"text-align: right;\">0.104426 </td></tr>\n",
       "<tr><td>FSR_Trainable_e264846f</td><td>TERMINATED</td><td>172.26.215.93:391035</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3540</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00115636 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       40.6192  </td><td style=\"text-align: right;\">0.656568</td><td style=\"text-align: right;\">0.345091</td><td style=\"text-align: right;\">0.097006 </td></tr>\n",
       "<tr><td>FSR_Trainable_785536af</td><td>TERMINATED</td><td>172.26.215.93:391249</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0d00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00775419 </td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.05145 </td><td style=\"text-align: right;\">0.734952</td><td style=\"text-align: right;\">0.462592</td><td style=\"text-align: right;\">0.120539 </td></tr>\n",
       "<tr><td>FSR_Trainable_c7f330f8</td><td>TERMINATED</td><td>172.26.215.93:391499</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d400</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00569315 </td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.756469</td><td style=\"text-align: right;\">0.766038</td><td style=\"text-align: right;\">0.482504</td><td style=\"text-align: right;\">0.124513 </td></tr>\n",
       "<tr><td>FSR_Trainable_7a4ce919</td><td>TERMINATED</td><td>172.26.215.93:391726</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ccc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000406341</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.9872  </td><td style=\"text-align: right;\">0.691955</td><td style=\"text-align: right;\">0.343923</td><td style=\"text-align: right;\">0.0994491</td></tr>\n",
       "<tr><td>FSR_Trainable_1cc2942e</td><td>TERMINATED</td><td>172.26.215.93:391952</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f280</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000404915</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       83.7771  </td><td style=\"text-align: right;\">0.656031</td><td style=\"text-align: right;\">0.344702</td><td style=\"text-align: right;\">0.0967195</td></tr>\n",
       "<tr><td>FSR_Trainable_5c455f49</td><td>TERMINATED</td><td>172.26.215.93:392196</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ba40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000216657</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      102.291   </td><td style=\"text-align: right;\">0.65606 </td><td style=\"text-align: right;\">0.345147</td><td style=\"text-align: right;\">0.0970032</td></tr>\n",
       "<tr><td>FSR_Trainable_294a0cde</td><td>TERMINATED</td><td>172.26.215.93:392422</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3f00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00183992 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       47.2096  </td><td style=\"text-align: right;\">0.657842</td><td style=\"text-align: right;\">0.345436</td><td style=\"text-align: right;\">0.0971221</td></tr>\n",
       "<tr><td>FSR_Trainable_a64e431a</td><td>TERMINATED</td><td>172.26.215.93:392638</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4780</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000707345</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       74.5138  </td><td style=\"text-align: right;\">0.655748</td><td style=\"text-align: right;\">0.345438</td><td style=\"text-align: right;\">0.096507 </td></tr>\n",
       "<tr><td>FSR_Trainable_20b5734f</td><td>TERMINATED</td><td>172.26.215.93:392939</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__fc00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000926594</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       64.9176  </td><td style=\"text-align: right;\">0.655735</td><td style=\"text-align: right;\">0.344746</td><td style=\"text-align: right;\">0.0965722</td></tr>\n",
       "<tr><td>FSR_Trainable_97704ed9</td><td>TERMINATED</td><td>172.26.215.93:393177</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2d00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000784478</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        3.17806 </td><td style=\"text-align: right;\">0.698823</td><td style=\"text-align: right;\">0.374219</td><td style=\"text-align: right;\">0.104908 </td></tr>\n",
       "<tr><td>FSR_Trainable_7ee5543c</td><td>TERMINATED</td><td>172.26.215.93:393393</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2600</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000963335</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.35182 </td><td style=\"text-align: right;\">0.710591</td><td style=\"text-align: right;\">0.393678</td><td style=\"text-align: right;\">0.103407 </td></tr>\n",
       "<tr><td>FSR_Trainable_10611099</td><td>TERMINATED</td><td>172.26.215.93:393614</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b2c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000244349</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       56.5165  </td><td style=\"text-align: right;\">0.65647 </td><td style=\"text-align: right;\">0.344784</td><td style=\"text-align: right;\">0.0968387</td></tr>\n",
       "<tr><td>FSR_Trainable_5d250a68</td><td>TERMINATED</td><td>172.26.215.93:393851</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5500</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000127929</td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.09715 </td><td style=\"text-align: right;\">0.784845</td><td style=\"text-align: right;\">0.413008</td><td style=\"text-align: right;\">0.104639 </td></tr>\n",
       "<tr><td>FSR_Trainable_0c5f1635</td><td>TERMINATED</td><td>172.26.215.93:394066</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__11c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00013    </td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.09936 </td><td style=\"text-align: right;\">0.765337</td><td style=\"text-align: right;\">0.386592</td><td style=\"text-align: right;\">0.0963793</td></tr>\n",
       "<tr><td>FSR_Trainable_9cbfaa57</td><td>TERMINATED</td><td>172.26.215.93:394292</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6d80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000137435</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.860733</td><td style=\"text-align: right;\">0.709273</td><td style=\"text-align: right;\">0.382043</td><td style=\"text-align: right;\">0.117381 </td></tr>\n",
       "<tr><td>FSR_Trainable_8ccec9f9</td><td>TERMINATED</td><td>172.26.215.93:394530</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c380</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00057991 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       52.3667  </td><td style=\"text-align: right;\">0.65717 </td><td style=\"text-align: right;\">0.345406</td><td style=\"text-align: right;\">0.0971051</td></tr>\n",
       "<tr><td>FSR_Trainable_a0692d97</td><td>TERMINATED</td><td>172.26.215.93:394759</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2200</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000528967</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       79.2482  </td><td style=\"text-align: right;\">0.656134</td><td style=\"text-align: right;\">0.344456</td><td style=\"text-align: right;\">0.0968717</td></tr>\n",
       "<tr><td>FSR_Trainable_715970c4</td><td>TERMINATED</td><td>172.26.215.93:394975</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8ac0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000629374</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       46.6452  </td><td style=\"text-align: right;\">0.656511</td><td style=\"text-align: right;\">0.344525</td><td style=\"text-align: right;\">0.0967564</td></tr>\n",
       "<tr><td>FSR_Trainable_a9b7fd49</td><td>TERMINATED</td><td>172.26.215.93:395195</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__fc40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000583463</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       47.9561  </td><td style=\"text-align: right;\">0.656568</td><td style=\"text-align: right;\">0.344769</td><td style=\"text-align: right;\">0.0969165</td></tr>\n",
       "<tr><td>FSR_Trainable_25190e32</td><td>TERMINATED</td><td>172.26.215.93:395489</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3780</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000696149</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       46.9664  </td><td style=\"text-align: right;\">0.656846</td><td style=\"text-align: right;\">0.344438</td><td style=\"text-align: right;\">0.0969086</td></tr>\n",
       "<tr><td>FSR_Trainable_28a0fe27</td><td>TERMINATED</td><td>172.26.215.93:395727</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__22c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000194342</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.03575 </td><td style=\"text-align: right;\">0.71935 </td><td style=\"text-align: right;\">0.405994</td><td style=\"text-align: right;\">0.119801 </td></tr>\n",
       "<tr><td>FSR_Trainable_e5963266</td><td>TERMINATED</td><td>172.26.215.93:395927</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3940</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000188691</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.07522 </td><td style=\"text-align: right;\">0.709065</td><td style=\"text-align: right;\">0.378114</td><td style=\"text-align: right;\">0.10465  </td></tr>\n",
       "<tr><td>FSR_Trainable_b093e976</td><td>TERMINATED</td><td>172.26.215.93:396165</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4b10</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8380</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00039343 </td><td>sklearn.preproc_4a50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.717163</td><td style=\"text-align: right;\">0.725892</td><td style=\"text-align: right;\">0.422656</td><td style=\"text-align: right;\">0.123482 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 02:32:29,267\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">     mape</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_017adcb8</td><td>2023-07-19_02-39-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.446322</td><td style=\"text-align: right;\">0.116572 </td><td>172.26.215.93</td><td style=\"text-align: right;\">382558</td><td style=\"text-align: right;\">0.781922</td><td style=\"text-align: right;\">            0.651536</td><td style=\"text-align: right;\">          0.651536</td><td style=\"text-align: right;\">      0.651536</td><td style=\"text-align: right;\"> 1689701963</td><td style=\"text-align: right;\">                   1</td><td>017adcb8  </td></tr>\n",
       "<tr><td>FSR_Trainable_01ccd703</td><td>2023-07-19_02-33-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.346257</td><td style=\"text-align: right;\">0.096689 </td><td>172.26.215.93</td><td style=\"text-align: right;\">374463</td><td style=\"text-align: right;\">0.655106</td><td style=\"text-align: right;\">           32.8727  </td><td style=\"text-align: right;\">          0.25926 </td><td style=\"text-align: right;\">     32.8727  </td><td style=\"text-align: right;\"> 1689701599</td><td style=\"text-align: right;\">                 100</td><td>01ccd703  </td></tr>\n",
       "<tr><td>FSR_Trainable_063e70f4</td><td>2023-07-19_02-44-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.370457</td><td style=\"text-align: right;\">0.106853 </td><td>172.26.215.93</td><td style=\"text-align: right;\">388904</td><td style=\"text-align: right;\">0.702904</td><td style=\"text-align: right;\">            1.68498 </td><td style=\"text-align: right;\">          0.658058</td><td style=\"text-align: right;\">      1.68498 </td><td style=\"text-align: right;\"> 1689702248</td><td style=\"text-align: right;\">                   2</td><td>063e70f4  </td></tr>\n",
       "<tr><td>FSR_Trainable_0649225d</td><td>2023-07-19_02-35-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.356255</td><td style=\"text-align: right;\">0.0947116</td><td>172.26.215.93</td><td style=\"text-align: right;\">378231</td><td style=\"text-align: right;\">0.669828</td><td style=\"text-align: right;\">           21.7116  </td><td style=\"text-align: right;\">          0.633955</td><td style=\"text-align: right;\">     21.7116  </td><td style=\"text-align: right;\"> 1689701730</td><td style=\"text-align: right;\">                  32</td><td>0649225d  </td></tr>\n",
       "<tr><td>FSR_Trainable_08791bfe</td><td>2023-07-19_02-33-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.504217</td><td style=\"text-align: right;\">0.124197 </td><td>172.26.215.93</td><td style=\"text-align: right;\">375888</td><td style=\"text-align: right;\">0.775819</td><td style=\"text-align: right;\">            0.676482</td><td style=\"text-align: right;\">          0.676482</td><td style=\"text-align: right;\">      0.676482</td><td style=\"text-align: right;\"> 1689701608</td><td style=\"text-align: right;\">                   1</td><td>08791bfe  </td></tr>\n",
       "<tr><td>FSR_Trainable_0bd8e5da</td><td>2023-07-19_02-32-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.395933</td><td style=\"text-align: right;\">0.110779 </td><td>172.26.215.93</td><td style=\"text-align: right;\">374709</td><td style=\"text-align: right;\">0.719601</td><td style=\"text-align: right;\">            4.44098 </td><td style=\"text-align: right;\">          0.54688 </td><td style=\"text-align: right;\">      4.44098 </td><td style=\"text-align: right;\"> 1689701575</td><td style=\"text-align: right;\">                   8</td><td>0bd8e5da  </td></tr>\n",
       "<tr><td>FSR_Trainable_0c1880fb</td><td>2023-07-19_02-39-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.387535</td><td style=\"text-align: right;\">0.102735 </td><td>172.26.215.93</td><td style=\"text-align: right;\">383232</td><td style=\"text-align: right;\">0.728985</td><td style=\"text-align: right;\">            1.31956 </td><td style=\"text-align: right;\">          1.31956 </td><td style=\"text-align: right;\">      1.31956 </td><td style=\"text-align: right;\"> 1689701996</td><td style=\"text-align: right;\">                   1</td><td>0c1880fb  </td></tr>\n",
       "<tr><td>FSR_Trainable_0c5f1635</td><td>2023-07-19_02-49-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.386592</td><td style=\"text-align: right;\">0.0963793</td><td>172.26.215.93</td><td style=\"text-align: right;\">394066</td><td style=\"text-align: right;\">0.765337</td><td style=\"text-align: right;\">            1.09936 </td><td style=\"text-align: right;\">          1.09936 </td><td style=\"text-align: right;\">      1.09936 </td><td style=\"text-align: right;\"> 1689702562</td><td style=\"text-align: right;\">                   1</td><td>0c5f1635  </td></tr>\n",
       "<tr><td>FSR_Trainable_0e31f13a</td><td>2023-07-19_02-33-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.426689</td><td style=\"text-align: right;\">0.110005 </td><td>172.26.215.93</td><td style=\"text-align: right;\">375206</td><td style=\"text-align: right;\">0.754306</td><td style=\"text-align: right;\">            0.881248</td><td style=\"text-align: right;\">          0.406422</td><td style=\"text-align: right;\">      0.881248</td><td style=\"text-align: right;\"> 1689701586</td><td style=\"text-align: right;\">                   2</td><td>0e31f13a  </td></tr>\n",
       "<tr><td>FSR_Trainable_0e42846b</td><td>2023-07-19_02-39-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.397781</td><td style=\"text-align: right;\">0.116592 </td><td>172.26.215.93</td><td style=\"text-align: right;\">382987</td><td style=\"text-align: right;\">0.731992</td><td style=\"text-align: right;\">            0.786042</td><td style=\"text-align: right;\">          0.786042</td><td style=\"text-align: right;\">      0.786042</td><td style=\"text-align: right;\"> 1689701987</td><td style=\"text-align: right;\">                   1</td><td>0e42846b  </td></tr>\n",
       "<tr><td>FSR_Trainable_10611099</td><td>2023-07-19_02-50-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.344784</td><td style=\"text-align: right;\">0.0968387</td><td>172.26.215.93</td><td style=\"text-align: right;\">393614</td><td style=\"text-align: right;\">0.65647 </td><td style=\"text-align: right;\">           56.5165  </td><td style=\"text-align: right;\">          1.05669 </td><td style=\"text-align: right;\">     56.5165  </td><td style=\"text-align: right;\"> 1689702607</td><td style=\"text-align: right;\">                  64</td><td>10611099  </td></tr>\n",
       "<tr><td>FSR_Trainable_1adea1b7</td><td>2023-07-19_02-41-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.46254 </td><td style=\"text-align: right;\">0.387683 </td><td>172.26.215.93</td><td style=\"text-align: right;\">385137</td><td style=\"text-align: right;\">2.81721 </td><td style=\"text-align: right;\">            0.766668</td><td style=\"text-align: right;\">          0.766668</td><td style=\"text-align: right;\">      0.766668</td><td style=\"text-align: right;\"> 1689702098</td><td style=\"text-align: right;\">                   1</td><td>1adea1b7  </td></tr>\n",
       "<tr><td>FSR_Trainable_1cc2942e</td><td>2023-07-19_02-48-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.344702</td><td style=\"text-align: right;\">0.0967195</td><td>172.26.215.93</td><td style=\"text-align: right;\">391952</td><td style=\"text-align: right;\">0.656031</td><td style=\"text-align: right;\">           83.7771  </td><td style=\"text-align: right;\">          0.853068</td><td style=\"text-align: right;\">     83.7771  </td><td style=\"text-align: right;\"> 1689702500</td><td style=\"text-align: right;\">                 100</td><td>1cc2942e  </td></tr>\n",
       "<tr><td>FSR_Trainable_1e24860f</td><td>2023-07-19_02-36-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.352629</td><td style=\"text-align: right;\">0.0936227</td><td>172.26.215.93</td><td style=\"text-align: right;\">378672</td><td style=\"text-align: right;\">0.667412</td><td style=\"text-align: right;\">           42.5696  </td><td style=\"text-align: right;\">          0.67317 </td><td style=\"text-align: right;\">     42.5696  </td><td style=\"text-align: right;\"> 1689701775</td><td style=\"text-align: right;\">                  64</td><td>1e24860f  </td></tr>\n",
       "<tr><td>FSR_Trainable_20b5734f</td><td>2023-07-19_02-49-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.344746</td><td style=\"text-align: right;\">0.0965722</td><td>172.26.215.93</td><td style=\"text-align: right;\">392939</td><td style=\"text-align: right;\">0.655735</td><td style=\"text-align: right;\">           64.9176  </td><td style=\"text-align: right;\">          0.484788</td><td style=\"text-align: right;\">     64.9176  </td><td style=\"text-align: right;\"> 1689702571</td><td style=\"text-align: right;\">                 100</td><td>20b5734f  </td></tr>\n",
       "<tr><td>FSR_Trainable_229c32a7</td><td>2023-07-19_02-39-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.415666</td><td style=\"text-align: right;\">0.114258 </td><td>172.26.215.93</td><td style=\"text-align: right;\">382757</td><td style=\"text-align: right;\">0.765131</td><td style=\"text-align: right;\">            0.868828</td><td style=\"text-align: right;\">          0.868828</td><td style=\"text-align: right;\">      0.868828</td><td style=\"text-align: right;\"> 1689701977</td><td style=\"text-align: right;\">                   1</td><td>229c32a7  </td></tr>\n",
       "<tr><td>FSR_Trainable_25190e32</td><td>2023-07-19_02-51-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.344438</td><td style=\"text-align: right;\">0.0969086</td><td>172.26.215.93</td><td style=\"text-align: right;\">395489</td><td style=\"text-align: right;\">0.656846</td><td style=\"text-align: right;\">           46.9664  </td><td style=\"text-align: right;\">          0.432914</td><td style=\"text-align: right;\">     46.9664  </td><td style=\"text-align: right;\"> 1689702713</td><td style=\"text-align: right;\">                  64</td><td>25190e32  </td></tr>\n",
       "<tr><td>FSR_Trainable_28a0fe27</td><td>2023-07-19_02-51-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.405994</td><td style=\"text-align: right;\">0.119801 </td><td>172.26.215.93</td><td style=\"text-align: right;\">395727</td><td style=\"text-align: right;\">0.71935 </td><td style=\"text-align: right;\">            1.03575 </td><td style=\"text-align: right;\">          1.03575 </td><td style=\"text-align: right;\">      1.03575 </td><td style=\"text-align: right;\"> 1689702677</td><td style=\"text-align: right;\">                   1</td><td>28a0fe27  </td></tr>\n",
       "<tr><td>FSR_Trainable_294a0cde</td><td>2023-07-19_02-47-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.345436</td><td style=\"text-align: right;\">0.0971221</td><td>172.26.215.93</td><td style=\"text-align: right;\">392422</td><td style=\"text-align: right;\">0.657842</td><td style=\"text-align: right;\">           47.2096  </td><td style=\"text-align: right;\">          0.684603</td><td style=\"text-align: right;\">     47.2096  </td><td style=\"text-align: right;\"> 1689702479</td><td style=\"text-align: right;\">                  64</td><td>294a0cde  </td></tr>\n",
       "<tr><td>FSR_Trainable_29e8b656</td><td>2023-07-19_02-41-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.344971</td><td style=\"text-align: right;\">0.0966991</td><td>172.26.215.93</td><td style=\"text-align: right;\">384128</td><td style=\"text-align: right;\">0.661727</td><td style=\"text-align: right;\">           32.1414  </td><td style=\"text-align: right;\">          0.8758  </td><td style=\"text-align: right;\">     32.1414  </td><td style=\"text-align: right;\"> 1689702070</td><td style=\"text-align: right;\">                  32</td><td>29e8b656  </td></tr>\n",
       "<tr><td>FSR_Trainable_2daa8b50</td><td>2023-07-19_02-39-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.344541</td><td style=\"text-align: right;\">0.0969524</td><td>172.26.215.93</td><td style=\"text-align: right;\">382299</td><td style=\"text-align: right;\">0.656272</td><td style=\"text-align: right;\">           45.1965  </td><td style=\"text-align: right;\">          0.724908</td><td style=\"text-align: right;\">     45.1965  </td><td style=\"text-align: right;\"> 1689701980</td><td style=\"text-align: right;\">                  64</td><td>2daa8b50  </td></tr>\n",
       "<tr><td>FSR_Trainable_3448c238</td><td>2023-07-19_02-44-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.379969</td><td style=\"text-align: right;\">0.105495 </td><td>172.26.215.93</td><td style=\"text-align: right;\">388592</td><td style=\"text-align: right;\">0.705054</td><td style=\"text-align: right;\">            1.64528 </td><td style=\"text-align: right;\">          0.573491</td><td style=\"text-align: right;\">      1.64528 </td><td style=\"text-align: right;\"> 1689702240</td><td style=\"text-align: right;\">                   2</td><td>3448c238  </td></tr>\n",
       "<tr><td>FSR_Trainable_36cddab1</td><td>2023-07-19_02-43-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.351394</td><td style=\"text-align: right;\">0.0996444</td><td>172.26.215.93</td><td style=\"text-align: right;\">388184</td><td style=\"text-align: right;\">0.698243</td><td style=\"text-align: right;\">            2.02898 </td><td style=\"text-align: right;\">          0.791646</td><td style=\"text-align: right;\">      2.02898 </td><td style=\"text-align: right;\"> 1689702225</td><td style=\"text-align: right;\">                   2</td><td>36cddab1  </td></tr>\n",
       "<tr><td>FSR_Trainable_3801c760</td><td>2023-07-19_02-35-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.361704</td><td style=\"text-align: right;\">0.0953728</td><td>172.26.215.93</td><td style=\"text-align: right;\">379129</td><td style=\"text-align: right;\">0.672626</td><td style=\"text-align: right;\">            7.53604 </td><td style=\"text-align: right;\">          0.39872 </td><td style=\"text-align: right;\">      7.53604 </td><td style=\"text-align: right;\"> 1689701754</td><td style=\"text-align: right;\">                  16</td><td>3801c760  </td></tr>\n",
       "<tr><td>FSR_Trainable_3a991208</td><td>2023-07-19_02-42-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.408197</td><td style=\"text-align: right;\">0.115124 </td><td>172.26.215.93</td><td style=\"text-align: right;\">386644</td><td style=\"text-align: right;\">0.745584</td><td style=\"text-align: right;\">            0.636961</td><td style=\"text-align: right;\">          0.636961</td><td style=\"text-align: right;\">      0.636961</td><td style=\"text-align: right;\"> 1689702161</td><td style=\"text-align: right;\">                   1</td><td>3a991208  </td></tr>\n",
       "<tr><td>FSR_Trainable_40f53e56</td><td>2023-07-19_02-42-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.345269</td><td style=\"text-align: right;\">0.102878 </td><td>172.26.215.93</td><td style=\"text-align: right;\">386100</td><td style=\"text-align: right;\">0.69191 </td><td style=\"text-align: right;\">            2.35163 </td><td style=\"text-align: right;\">          1.07225 </td><td style=\"text-align: right;\">      2.35163 </td><td style=\"text-align: right;\"> 1689702144</td><td style=\"text-align: right;\">                   2</td><td>40f53e56  </td></tr>\n",
       "<tr><td>FSR_Trainable_431d77fc</td><td>2023-07-19_02-43-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.356781</td><td style=\"text-align: right;\">0.103496 </td><td>172.26.215.93</td><td style=\"text-align: right;\">387590</td><td style=\"text-align: right;\">0.69297 </td><td style=\"text-align: right;\">            1.9429  </td><td style=\"text-align: right;\">          0.794903</td><td style=\"text-align: right;\">      1.9429  </td><td style=\"text-align: right;\"> 1689702203</td><td style=\"text-align: right;\">                   2</td><td>431d77fc  </td></tr>\n",
       "<tr><td>FSR_Trainable_46ab530f</td><td>2023-07-19_02-34-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.451844</td><td style=\"text-align: right;\">0.116283 </td><td>172.26.215.93</td><td style=\"text-align: right;\">377319</td><td style=\"text-align: right;\">0.770429</td><td style=\"text-align: right;\">            5.28649 </td><td style=\"text-align: right;\">          1.09049 </td><td style=\"text-align: right;\">      5.28649 </td><td style=\"text-align: right;\"> 1689701672</td><td style=\"text-align: right;\">                   4</td><td>46ab530f  </td></tr>\n",
       "<tr><td>FSR_Trainable_49d36a14</td><td>2023-07-19_02-43-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.51249 </td><td style=\"text-align: right;\">0.129176 </td><td>172.26.215.93</td><td style=\"text-align: right;\">387325</td><td style=\"text-align: right;\">0.883636</td><td style=\"text-align: right;\">            0.852699</td><td style=\"text-align: right;\">          0.852699</td><td style=\"text-align: right;\">      0.852699</td><td style=\"text-align: right;\"> 1689702187</td><td style=\"text-align: right;\">                   1</td><td>49d36a14  </td></tr>\n",
       "<tr><td>FSR_Trainable_4ad33e7a</td><td>2023-07-19_02-36-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.355376</td><td style=\"text-align: right;\">0.0941366</td><td>172.26.215.93</td><td style=\"text-align: right;\">379819</td><td style=\"text-align: right;\">0.66946 </td><td style=\"text-align: right;\">           21.6808  </td><td style=\"text-align: right;\">          0.781743</td><td style=\"text-align: right;\">     21.6808  </td><td style=\"text-align: right;\"> 1689701802</td><td style=\"text-align: right;\">                  32</td><td>4ad33e7a  </td></tr>\n",
       "<tr><td>FSR_Trainable_4de2513e</td><td>2023-07-19_02-44-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.425266</td><td style=\"text-align: right;\">0.123047 </td><td>172.26.215.93</td><td style=\"text-align: right;\">389802</td><td style=\"text-align: right;\">0.785747</td><td style=\"text-align: right;\">            1.10641 </td><td style=\"text-align: right;\">          1.10641 </td><td style=\"text-align: right;\">      1.10641 </td><td style=\"text-align: right;\"> 1689702284</td><td style=\"text-align: right;\">                   1</td><td>4de2513e  </td></tr>\n",
       "<tr><td>FSR_Trainable_529edf46</td><td>2023-07-19_02-36-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.37406 </td><td style=\"text-align: right;\">0.0970363</td><td>172.26.215.93</td><td style=\"text-align: right;\">380504</td><td style=\"text-align: right;\">0.685845</td><td style=\"text-align: right;\">            2.93832 </td><td style=\"text-align: right;\">          0.651515</td><td style=\"text-align: right;\">      2.93832 </td><td style=\"text-align: right;\"> 1689701817</td><td style=\"text-align: right;\">                   4</td><td>529edf46  </td></tr>\n",
       "<tr><td>FSR_Trainable_54a1fa11</td><td>2023-07-19_02-36-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.344522</td><td style=\"text-align: right;\">0.096818 </td><td>172.26.215.93</td><td style=\"text-align: right;\">378901</td><td style=\"text-align: right;\">0.656158</td><td style=\"text-align: right;\">           65.6268  </td><td style=\"text-align: right;\">          0.7768  </td><td style=\"text-align: right;\">     65.6268  </td><td style=\"text-align: right;\"> 1689701816</td><td style=\"text-align: right;\">                 100</td><td>54a1fa11  </td></tr>\n",
       "<tr><td>FSR_Trainable_56248280</td><td>2023-07-19_02-44-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.421174</td><td style=\"text-align: right;\">0.113704 </td><td>172.26.215.93</td><td style=\"text-align: right;\">389714</td><td style=\"text-align: right;\">0.7683  </td><td style=\"text-align: right;\">            1.05091 </td><td style=\"text-align: right;\">          1.05091 </td><td style=\"text-align: right;\">      1.05091 </td><td style=\"text-align: right;\"> 1689702276</td><td style=\"text-align: right;\">                   1</td><td>56248280  </td></tr>\n",
       "<tr><td>FSR_Trainable_597ecf06</td><td>2023-07-19_02-42-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.483073</td><td style=\"text-align: right;\">0.127717 </td><td>172.26.215.93</td><td style=\"text-align: right;\">387092</td><td style=\"text-align: right;\">0.783768</td><td style=\"text-align: right;\">            0.97991 </td><td style=\"text-align: right;\">          0.97991 </td><td style=\"text-align: right;\">      0.97991 </td><td style=\"text-align: right;\"> 1689702179</td><td style=\"text-align: right;\">                   1</td><td>597ecf06  </td></tr>\n",
       "<tr><td>FSR_Trainable_5c455f49</td><td>2023-07-19_02-48-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.345147</td><td style=\"text-align: right;\">0.0970032</td><td>172.26.215.93</td><td style=\"text-align: right;\">392196</td><td style=\"text-align: right;\">0.65606 </td><td style=\"text-align: right;\">          102.291   </td><td style=\"text-align: right;\">          1.08566 </td><td style=\"text-align: right;\">    102.291   </td><td style=\"text-align: right;\"> 1689702529</td><td style=\"text-align: right;\">                 100</td><td>5c455f49  </td></tr>\n",
       "<tr><td>FSR_Trainable_5c949558</td><td>2023-07-19_02-42-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.342934</td><td style=\"text-align: right;\">0.0958196</td><td>172.26.215.93</td><td style=\"text-align: right;\">385699</td><td style=\"text-align: right;\">0.656367</td><td style=\"text-align: right;\">           37.533   </td><td style=\"text-align: right;\">          0.740839</td><td style=\"text-align: right;\">     37.533   </td><td style=\"text-align: right;\"> 1689702173</td><td style=\"text-align: right;\">                  64</td><td>5c949558  </td></tr>\n",
       "<tr><td>FSR_Trainable_5d250a68</td><td>2023-07-19_02-49-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.413008</td><td style=\"text-align: right;\">0.104639 </td><td>172.26.215.93</td><td style=\"text-align: right;\">393851</td><td style=\"text-align: right;\">0.784845</td><td style=\"text-align: right;\">            1.09715 </td><td style=\"text-align: right;\">          1.09715 </td><td style=\"text-align: right;\">      1.09715 </td><td style=\"text-align: right;\"> 1689702551</td><td style=\"text-align: right;\">                   1</td><td>5d250a68  </td></tr>\n",
       "<tr><td>FSR_Trainable_5f091b2b</td><td>2023-07-19_02-32-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.41543 </td><td style=\"text-align: right;\">0.374158 </td><td>172.26.215.93</td><td style=\"text-align: right;\">374536</td><td style=\"text-align: right;\">2.72647 </td><td style=\"text-align: right;\">            0.853106</td><td style=\"text-align: right;\">          0.853106</td><td style=\"text-align: right;\">      0.853106</td><td style=\"text-align: right;\"> 1689701563</td><td style=\"text-align: right;\">                   1</td><td>5f091b2b  </td></tr>\n",
       "<tr><td>FSR_Trainable_5fdfe2cb</td><td>2023-07-19_02-40-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">1.62449 </td><td style=\"text-align: right;\">0.276258 </td><td>172.26.215.93</td><td style=\"text-align: right;\">383636</td><td style=\"text-align: right;\">1.8881  </td><td style=\"text-align: right;\">            1.19288 </td><td style=\"text-align: right;\">          1.19288 </td><td style=\"text-align: right;\">      1.19288 </td><td style=\"text-align: right;\"> 1689702011</td><td style=\"text-align: right;\">                   1</td><td>5fdfe2cb  </td></tr>\n",
       "<tr><td>FSR_Trainable_61069d94</td><td>2023-07-19_02-42-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.390536</td><td style=\"text-align: right;\">0.107596 </td><td>172.26.215.93</td><td style=\"text-align: right;\">386411</td><td style=\"text-align: right;\">0.735685</td><td style=\"text-align: right;\">            0.838179</td><td style=\"text-align: right;\">          0.838179</td><td style=\"text-align: right;\">      0.838179</td><td style=\"text-align: right;\"> 1689702151</td><td style=\"text-align: right;\">                   1</td><td>61069d94  </td></tr>\n",
       "<tr><td>FSR_Trainable_621022c5</td><td>2023-07-19_02-44-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.386376</td><td style=\"text-align: right;\">0.106749 </td><td>172.26.215.93</td><td style=\"text-align: right;\">388997</td><td style=\"text-align: right;\">0.712201</td><td style=\"text-align: right;\">            0.953424</td><td style=\"text-align: right;\">          0.953424</td><td style=\"text-align: right;\">      0.953424</td><td style=\"text-align: right;\"> 1689702252</td><td style=\"text-align: right;\">                   1</td><td>621022c5  </td></tr>\n",
       "<tr><td>FSR_Trainable_65506096</td><td>2023-07-19_02-39-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.345576</td><td style=\"text-align: right;\">0.0969083</td><td>172.26.215.93</td><td style=\"text-align: right;\">382015</td><td style=\"text-align: right;\">0.655403</td><td style=\"text-align: right;\">           95.5101  </td><td style=\"text-align: right;\">          0.91714 </td><td style=\"text-align: right;\">     95.5101  </td><td style=\"text-align: right;\"> 1689701987</td><td style=\"text-align: right;\">                 100</td><td>65506096  </td></tr>\n",
       "<tr><td>FSR_Trainable_67300bae</td><td>2023-07-19_02-45-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.365885</td><td style=\"text-align: right;\">0.104426 </td><td>172.26.215.93</td><td style=\"text-align: right;\">390828</td><td style=\"text-align: right;\">0.707784</td><td style=\"text-align: right;\">            1.02049 </td><td style=\"text-align: right;\">          1.02049 </td><td style=\"text-align: right;\">      1.02049 </td><td style=\"text-align: right;\"> 1689702347</td><td style=\"text-align: right;\">                   1</td><td>67300bae  </td></tr>\n",
       "<tr><td>FSR_Trainable_6835396b</td><td>2023-07-19_02-43-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.374019</td><td style=\"text-align: right;\">0.108212 </td><td>172.26.215.93</td><td style=\"text-align: right;\">387775</td><td style=\"text-align: right;\">0.699026</td><td style=\"text-align: right;\">            1.87347 </td><td style=\"text-align: right;\">          0.67296 </td><td style=\"text-align: right;\">      1.87347 </td><td style=\"text-align: right;\"> 1689702210</td><td style=\"text-align: right;\">                   2</td><td>6835396b  </td></tr>\n",
       "<tr><td>FSR_Trainable_6cd25336</td><td>2023-07-19_02-37-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.367035</td><td style=\"text-align: right;\">0.0931276</td><td>172.26.215.93</td><td style=\"text-align: right;\">380739</td><td style=\"text-align: right;\">0.714374</td><td style=\"text-align: right;\">            1.35155 </td><td style=\"text-align: right;\">          1.35155 </td><td style=\"text-align: right;\">      1.35155 </td><td style=\"text-align: right;\"> 1689701823</td><td style=\"text-align: right;\">                   1</td><td>6cd25336  </td></tr>\n",
       "<tr><td>FSR_Trainable_715970c4</td><td>2023-07-19_02-51-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.344525</td><td style=\"text-align: right;\">0.0967564</td><td>172.26.215.93</td><td style=\"text-align: right;\">394975</td><td style=\"text-align: right;\">0.656511</td><td style=\"text-align: right;\">           46.6452  </td><td style=\"text-align: right;\">          0.777535</td><td style=\"text-align: right;\">     46.6452  </td><td style=\"text-align: right;\"> 1689702662</td><td style=\"text-align: right;\">                  64</td><td>715970c4  </td></tr>\n",
       "<tr><td>FSR_Trainable_7853eed4</td><td>2023-07-19_02-36-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.354434</td><td style=\"text-align: right;\">0.0944315</td><td>172.26.215.93</td><td style=\"text-align: right;\">379587</td><td style=\"text-align: right;\">0.667355</td><td style=\"text-align: right;\">           43.0951  </td><td style=\"text-align: right;\">          0.509515</td><td style=\"text-align: right;\">     43.0951  </td><td style=\"text-align: right;\"> 1689701819</td><td style=\"text-align: right;\">                  64</td><td>7853eed4  </td></tr>\n",
       "<tr><td>FSR_Trainable_785536af</td><td>2023-07-19_02-46-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.462592</td><td style=\"text-align: right;\">0.120539 </td><td>172.26.215.93</td><td style=\"text-align: right;\">391249</td><td style=\"text-align: right;\">0.734952</td><td style=\"text-align: right;\">            1.05145 </td><td style=\"text-align: right;\">          1.05145 </td><td style=\"text-align: right;\">      1.05145 </td><td style=\"text-align: right;\"> 1689702371</td><td style=\"text-align: right;\">                   1</td><td>785536af  </td></tr>\n",
       "<tr><td>FSR_Trainable_78aa2cbe</td><td>2023-07-19_02-43-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.385513</td><td style=\"text-align: right;\">0.102441 </td><td>172.26.215.93</td><td style=\"text-align: right;\">388089</td><td style=\"text-align: right;\">0.713872</td><td style=\"text-align: right;\">            1.06703 </td><td style=\"text-align: right;\">          1.06703 </td><td style=\"text-align: right;\">      1.06703 </td><td style=\"text-align: right;\"> 1689702216</td><td style=\"text-align: right;\">                   1</td><td>78aa2cbe  </td></tr>\n",
       "<tr><td>FSR_Trainable_7a4ce919</td><td>2023-07-19_02-46-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.343923</td><td style=\"text-align: right;\">0.0994491</td><td>172.26.215.93</td><td style=\"text-align: right;\">391726</td><td style=\"text-align: right;\">0.691955</td><td style=\"text-align: right;\">            1.9872  </td><td style=\"text-align: right;\">          0.968548</td><td style=\"text-align: right;\">      1.9872  </td><td style=\"text-align: right;\"> 1689702396</td><td style=\"text-align: right;\">                   2</td><td>7a4ce919  </td></tr>\n",
       "<tr><td>FSR_Trainable_7b106491</td><td>2023-07-19_02-32-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.490035</td><td style=\"text-align: right;\">0.110675 </td><td>172.26.215.93</td><td style=\"text-align: right;\">374885</td><td style=\"text-align: right;\">0.904122</td><td style=\"text-align: right;\">            0.602421</td><td style=\"text-align: right;\">          0.602421</td><td style=\"text-align: right;\">      0.602421</td><td style=\"text-align: right;\"> 1689701576</td><td style=\"text-align: right;\">                   1</td><td>7b106491  </td></tr>\n",
       "<tr><td>FSR_Trainable_7ee5543c</td><td>2023-07-19_02-48-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.393678</td><td style=\"text-align: right;\">0.103407 </td><td>172.26.215.93</td><td style=\"text-align: right;\">393393</td><td style=\"text-align: right;\">0.710591</td><td style=\"text-align: right;\">            1.35182 </td><td style=\"text-align: right;\">          1.35182 </td><td style=\"text-align: right;\">      1.35182 </td><td style=\"text-align: right;\"> 1689702529</td><td style=\"text-align: right;\">                   1</td><td>7ee5543c  </td></tr>\n",
       "<tr><td>FSR_Trainable_83ace7ba</td><td>2023-07-19_02-37-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.400627</td><td style=\"text-align: right;\">0.108258 </td><td>172.26.215.93</td><td style=\"text-align: right;\">380987</td><td style=\"text-align: right;\">0.730907</td><td style=\"text-align: right;\">            0.875372</td><td style=\"text-align: right;\">          0.875372</td><td style=\"text-align: right;\">      0.875372</td><td style=\"text-align: right;\"> 1689701830</td><td style=\"text-align: right;\">                   1</td><td>83ace7ba  </td></tr>\n",
       "<tr><td>FSR_Trainable_8795d3e2</td><td>2023-07-19_02-44-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.353035</td><td style=\"text-align: right;\">0.105966 </td><td>172.26.215.93</td><td style=\"text-align: right;\">389313</td><td style=\"text-align: right;\">0.699815</td><td style=\"text-align: right;\">            1.63872 </td><td style=\"text-align: right;\">          0.637337</td><td style=\"text-align: right;\">      1.63872 </td><td style=\"text-align: right;\"> 1689702263</td><td style=\"text-align: right;\">                   2</td><td>8795d3e2  </td></tr>\n",
       "<tr><td>FSR_Trainable_896925d9</td><td>2023-07-19_02-33-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.445488</td><td style=\"text-align: right;\">0.108265 </td><td>172.26.215.93</td><td style=\"text-align: right;\">375433</td><td style=\"text-align: right;\">0.715144</td><td style=\"text-align: right;\">            2.40069 </td><td style=\"text-align: right;\">          0.619383</td><td style=\"text-align: right;\">      2.40069 </td><td style=\"text-align: right;\"> 1689701595</td><td style=\"text-align: right;\">                   4</td><td>896925d9  </td></tr>\n",
       "<tr><td>FSR_Trainable_8ccec9f9</td><td>2023-07-19_02-50-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.345406</td><td style=\"text-align: right;\">0.0971051</td><td>172.26.215.93</td><td style=\"text-align: right;\">394530</td><td style=\"text-align: right;\">0.65717 </td><td style=\"text-align: right;\">           52.3667  </td><td style=\"text-align: right;\">          0.895331</td><td style=\"text-align: right;\">     52.3667  </td><td style=\"text-align: right;\"> 1689702645</td><td style=\"text-align: right;\">                  64</td><td>8ccec9f9  </td></tr>\n",
       "<tr><td>FSR_Trainable_8d4fea50</td><td>2023-07-19_02-35-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.43607 </td><td style=\"text-align: right;\">0.116359 </td><td>172.26.215.93</td><td style=\"text-align: right;\">379360</td><td style=\"text-align: right;\">0.746679</td><td style=\"text-align: right;\">            0.697224</td><td style=\"text-align: right;\">          0.697224</td><td style=\"text-align: right;\">      0.697224</td><td style=\"text-align: right;\"> 1689701755</td><td style=\"text-align: right;\">                   1</td><td>8d4fea50  </td></tr>\n",
       "<tr><td>FSR_Trainable_93bf26d0</td><td>2023-07-19_02-45-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.34458 </td><td style=\"text-align: right;\">0.0968502</td><td>172.26.215.93</td><td style=\"text-align: right;\">390336</td><td style=\"text-align: right;\">0.65736 </td><td style=\"text-align: right;\">           47.1471  </td><td style=\"text-align: right;\">          0.542269</td><td style=\"text-align: right;\">     47.1471  </td><td style=\"text-align: right;\"> 1689702359</td><td style=\"text-align: right;\">                  64</td><td>93bf26d0  </td></tr>\n",
       "<tr><td>FSR_Trainable_94b05c37</td><td>2023-07-19_02-36-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.36339 </td><td style=\"text-align: right;\">0.0990351</td><td>172.26.215.93</td><td style=\"text-align: right;\">380046</td><td style=\"text-align: right;\">0.69856 </td><td style=\"text-align: right;\">            1.70587 </td><td style=\"text-align: right;\">          0.78353 </td><td style=\"text-align: right;\">      1.70587 </td><td style=\"text-align: right;\"> 1689701791</td><td style=\"text-align: right;\">                   2</td><td>94b05c37  </td></tr>\n",
       "<tr><td>FSR_Trainable_97704ed9</td><td>2023-07-19_02-48-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.374219</td><td style=\"text-align: right;\">0.104908 </td><td>172.26.215.93</td><td style=\"text-align: right;\">393177</td><td style=\"text-align: right;\">0.698823</td><td style=\"text-align: right;\">            3.17806 </td><td style=\"text-align: right;\">          1.37206 </td><td style=\"text-align: right;\">      3.17806 </td><td style=\"text-align: right;\"> 1689702517</td><td style=\"text-align: right;\">                   2</td><td>97704ed9  </td></tr>\n",
       "<tr><td>FSR_Trainable_9cbfaa57</td><td>2023-07-19_02-49-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.382043</td><td style=\"text-align: right;\">0.117381 </td><td>172.26.215.93</td><td style=\"text-align: right;\">394292</td><td style=\"text-align: right;\">0.709273</td><td style=\"text-align: right;\">            0.860733</td><td style=\"text-align: right;\">          0.860733</td><td style=\"text-align: right;\">      0.860733</td><td style=\"text-align: right;\"> 1689702573</td><td style=\"text-align: right;\">                   1</td><td>9cbfaa57  </td></tr>\n",
       "<tr><td>FSR_Trainable_9d1f6fd8</td><td>2023-07-19_02-33-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.34891 </td><td style=\"text-align: right;\">0.0939858</td><td>172.26.215.93</td><td style=\"text-align: right;\">375656</td><td style=\"text-align: right;\">0.710793</td><td style=\"text-align: right;\">            4.08348 </td><td style=\"text-align: right;\">          0.382412</td><td style=\"text-align: right;\">      4.08348 </td><td style=\"text-align: right;\"> 1689701605</td><td style=\"text-align: right;\">                   8</td><td>9d1f6fd8  </td></tr>\n",
       "<tr><td>FSR_Trainable_a0692d97</td><td>2023-07-19_02-51-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.344456</td><td style=\"text-align: right;\">0.0968717</td><td>172.26.215.93</td><td style=\"text-align: right;\">394759</td><td style=\"text-align: right;\">0.656134</td><td style=\"text-align: right;\">           79.2482  </td><td style=\"text-align: right;\">          0.658954</td><td style=\"text-align: right;\">     79.2482  </td><td style=\"text-align: right;\"> 1689702686</td><td style=\"text-align: right;\">                 100</td><td>a0692d97  </td></tr>\n",
       "<tr><td>FSR_Trainable_a43e2e9e</td><td>2023-07-19_02-41-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.343817</td><td style=\"text-align: right;\">0.0969635</td><td>172.26.215.93</td><td style=\"text-align: right;\">384039</td><td style=\"text-align: right;\">0.660389</td><td style=\"text-align: right;\">           30.5624  </td><td style=\"text-align: right;\">          0.860975</td><td style=\"text-align: right;\">     30.5624  </td><td style=\"text-align: right;\"> 1689702063</td><td style=\"text-align: right;\">                  32</td><td>a43e2e9e  </td></tr>\n",
       "<tr><td>FSR_Trainable_a64e431a</td><td>2023-07-19_02-48-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.345438</td><td style=\"text-align: right;\">0.096507 </td><td>172.26.215.93</td><td style=\"text-align: right;\">392638</td><td style=\"text-align: right;\">0.655748</td><td style=\"text-align: right;\">           74.5138  </td><td style=\"text-align: right;\">          0.654203</td><td style=\"text-align: right;\">     74.5138  </td><td style=\"text-align: right;\"> 1689702522</td><td style=\"text-align: right;\">                 100</td><td>a64e431a  </td></tr>\n",
       "<tr><td>FSR_Trainable_a9b7fd49</td><td>2023-07-19_02-51-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.344769</td><td style=\"text-align: right;\">0.0969165</td><td>172.26.215.93</td><td style=\"text-align: right;\">395195</td><td style=\"text-align: right;\">0.656568</td><td style=\"text-align: right;\">           47.9561  </td><td style=\"text-align: right;\">          0.795601</td><td style=\"text-align: right;\">     47.9561  </td><td style=\"text-align: right;\"> 1689702675</td><td style=\"text-align: right;\">                  64</td><td>a9b7fd49  </td></tr>\n",
       "<tr><td>FSR_Trainable_ab1ebf30</td><td>2023-07-19_02-37-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.347151</td><td style=\"text-align: right;\">0.102418 </td><td>172.26.215.93</td><td style=\"text-align: right;\">381775</td><td style=\"text-align: right;\">0.691594</td><td style=\"text-align: right;\">            2.56403 </td><td style=\"text-align: right;\">          1.08703 </td><td style=\"text-align: right;\">      2.56403 </td><td style=\"text-align: right;\"> 1689701869</td><td style=\"text-align: right;\">                   2</td><td>ab1ebf30  </td></tr>\n",
       "<tr><td>FSR_Trainable_ae8bf4d3</td><td>2023-07-19_02-33-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.356931</td><td style=\"text-align: right;\">0.0948239</td><td>172.26.215.93</td><td style=\"text-align: right;\">376113</td><td style=\"text-align: right;\">0.708348</td><td style=\"text-align: right;\">            6.70063 </td><td style=\"text-align: right;\">          0.387024</td><td style=\"text-align: right;\">      6.70063 </td><td style=\"text-align: right;\"> 1689701623</td><td style=\"text-align: right;\">                  16</td><td>ae8bf4d3  </td></tr>\n",
       "<tr><td>FSR_Trainable_aeafd544</td><td>2023-07-19_02-41-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.362104</td><td style=\"text-align: right;\">0.100485 </td><td>172.26.215.93</td><td style=\"text-align: right;\">385364</td><td style=\"text-align: right;\">0.702341</td><td style=\"text-align: right;\">            1.4254  </td><td style=\"text-align: right;\">          0.592105</td><td style=\"text-align: right;\">      1.4254  </td><td style=\"text-align: right;\"> 1689702112</td><td style=\"text-align: right;\">                   2</td><td>aeafd544  </td></tr>\n",
       "<tr><td>FSR_Trainable_aef894a5</td><td>2023-07-19_02-34-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.359279</td><td style=\"text-align: right;\">0.0954795</td><td>172.26.215.93</td><td style=\"text-align: right;\">377011</td><td style=\"text-align: right;\">0.67505 </td><td style=\"text-align: right;\">           25.3254  </td><td style=\"text-align: right;\">          0.451183</td><td style=\"text-align: right;\">     25.3254  </td><td style=\"text-align: right;\"> 1689701682</td><td style=\"text-align: right;\">                  32</td><td>aef894a5  </td></tr>\n",
       "<tr><td>FSR_Trainable_b093e976</td><td>2023-07-19_02-51-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.422656</td><td style=\"text-align: right;\">0.123482 </td><td>172.26.215.93</td><td style=\"text-align: right;\">396165</td><td style=\"text-align: right;\">0.725892</td><td style=\"text-align: right;\">            0.717163</td><td style=\"text-align: right;\">          0.717163</td><td style=\"text-align: right;\">      0.717163</td><td style=\"text-align: right;\"> 1689702702</td><td style=\"text-align: right;\">                   1</td><td>b093e976  </td></tr>\n",
       "<tr><td>FSR_Trainable_b1620c1c</td><td>2023-07-19_02-42-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.363869</td><td style=\"text-align: right;\">0.108826 </td><td>172.26.215.93</td><td style=\"text-align: right;\">386867</td><td style=\"text-align: right;\">0.703456</td><td style=\"text-align: right;\">            1.1893  </td><td style=\"text-align: right;\">          0.549465</td><td style=\"text-align: right;\">      1.1893  </td><td style=\"text-align: right;\"> 1689702173</td><td style=\"text-align: right;\">                   2</td><td>b1620c1c  </td></tr>\n",
       "<tr><td>FSR_Trainable_b7ee8305</td><td>2023-07-19_02-45-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.344374</td><td style=\"text-align: right;\">0.0968478</td><td>172.26.215.93</td><td style=\"text-align: right;\">390109</td><td style=\"text-align: right;\">0.656486</td><td style=\"text-align: right;\">           47.4233  </td><td style=\"text-align: right;\">          0.743045</td><td style=\"text-align: right;\">     47.4233  </td><td style=\"text-align: right;\"> 1689702350</td><td style=\"text-align: right;\">                  64</td><td>b7ee8305  </td></tr>\n",
       "<tr><td>FSR_Trainable_b8149bd6</td><td>2023-07-19_02-45-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.343935</td><td style=\"text-align: right;\">0.0973615</td><td>172.26.215.93</td><td style=\"text-align: right;\">389403</td><td style=\"text-align: right;\">0.658346</td><td style=\"text-align: right;\">           57.4704  </td><td style=\"text-align: right;\">          1.15178 </td><td style=\"text-align: right;\">     57.4704  </td><td style=\"text-align: right;\"> 1689702335</td><td style=\"text-align: right;\">                  64</td><td>b8149bd6  </td></tr>\n",
       "<tr><td>FSR_Trainable_bdc8e398</td><td>2023-07-19_02-42-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.351942</td><td style=\"text-align: right;\">0.104147 </td><td>172.26.215.93</td><td style=\"text-align: right;\">386013</td><td style=\"text-align: right;\">0.692419</td><td style=\"text-align: right;\">            2.13562 </td><td style=\"text-align: right;\">          0.986618</td><td style=\"text-align: right;\">      2.13562 </td><td style=\"text-align: right;\"> 1689702137</td><td style=\"text-align: right;\">                   2</td><td>bdc8e398  </td></tr>\n",
       "<tr><td>FSR_Trainable_bef25036</td><td>2023-07-19_02-41-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.345233</td><td style=\"text-align: right;\">0.0967874</td><td>172.26.215.93</td><td style=\"text-align: right;\">384433</td><td style=\"text-align: right;\">0.660609</td><td style=\"text-align: right;\">           32.063   </td><td style=\"text-align: right;\">          1.16154 </td><td style=\"text-align: right;\">     32.063   </td><td style=\"text-align: right;\"> 1689702082</td><td style=\"text-align: right;\">                  32</td><td>bef25036  </td></tr>\n",
       "<tr><td>FSR_Trainable_c7f330f8</td><td>2023-07-19_02-46-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.482504</td><td style=\"text-align: right;\">0.124513 </td><td>172.26.215.93</td><td style=\"text-align: right;\">391499</td><td style=\"text-align: right;\">0.766038</td><td style=\"text-align: right;\">            0.756469</td><td style=\"text-align: right;\">          0.756469</td><td style=\"text-align: right;\">      0.756469</td><td style=\"text-align: right;\"> 1689702382</td><td style=\"text-align: right;\">                   1</td><td>c7f330f8  </td></tr>\n",
       "<tr><td>FSR_Trainable_c982cc1d</td><td>2023-07-19_02-34-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.433413</td><td style=\"text-align: right;\">0.107517 </td><td>172.26.215.93</td><td style=\"text-align: right;\">377559</td><td style=\"text-align: right;\">0.780096</td><td style=\"text-align: right;\">            1.50874 </td><td style=\"text-align: right;\">          1.50874 </td><td style=\"text-align: right;\">      1.50874 </td><td style=\"text-align: right;\"> 1689701678</td><td style=\"text-align: right;\">                   1</td><td>c982cc1d  </td></tr>\n",
       "<tr><td>FSR_Trainable_ce58f49b</td><td>2023-07-19_02-40-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.27391 </td><td style=\"text-align: right;\">0.343254 </td><td>172.26.215.93</td><td style=\"text-align: right;\">383324</td><td style=\"text-align: right;\">2.60557 </td><td style=\"text-align: right;\">            1.27642 </td><td style=\"text-align: right;\">          1.27642 </td><td style=\"text-align: right;\">      1.27642 </td><td style=\"text-align: right;\"> 1689702003</td><td style=\"text-align: right;\">                   1</td><td>ce58f49b  </td></tr>\n",
       "<tr><td>FSR_Trainable_cf0e6cd8</td><td>2023-07-19_02-36-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.364227</td><td style=\"text-align: right;\">0.0993799</td><td>172.26.215.93</td><td style=\"text-align: right;\">380288</td><td style=\"text-align: right;\">0.699661</td><td style=\"text-align: right;\">            1.64663 </td><td style=\"text-align: right;\">          0.746523</td><td style=\"text-align: right;\">      1.64663 </td><td style=\"text-align: right;\"> 1689701805</td><td style=\"text-align: right;\">                   2</td><td>cf0e6cd8  </td></tr>\n",
       "<tr><td>FSR_Trainable_cf3426ef</td><td>2023-07-19_02-35-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.35426 </td><td style=\"text-align: right;\">0.093928 </td><td>172.26.215.93</td><td style=\"text-align: right;\">377993</td><td style=\"text-align: right;\">0.668986</td><td style=\"text-align: right;\">           20.3554  </td><td style=\"text-align: right;\">          0.818384</td><td style=\"text-align: right;\">     20.3554  </td><td style=\"text-align: right;\"> 1689701717</td><td style=\"text-align: right;\">                  32</td><td>cf3426ef  </td></tr>\n",
       "<tr><td>FSR_Trainable_d1ef26ad</td><td>2023-07-19_02-38-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.345129</td><td style=\"text-align: right;\">0.0960426</td><td>172.26.215.93</td><td style=\"text-align: right;\">381206</td><td style=\"text-align: right;\">0.655997</td><td style=\"text-align: right;\">           66.47    </td><td style=\"text-align: right;\">          1.02195 </td><td style=\"text-align: right;\">     66.47    </td><td style=\"text-align: right;\"> 1689701914</td><td style=\"text-align: right;\">                 100</td><td>d1ef26ad  </td></tr>\n",
       "<tr><td>FSR_Trainable_d661622e</td><td>2023-07-19_02-34-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.338602</td><td style=\"text-align: right;\">0.099574 </td><td>172.26.215.93</td><td style=\"text-align: right;\">376746</td><td style=\"text-align: right;\">0.695682</td><td style=\"text-align: right;\">           21.3933  </td><td style=\"text-align: right;\">          1.53098 </td><td style=\"text-align: right;\">     21.3933  </td><td style=\"text-align: right;\"> 1689701663</td><td style=\"text-align: right;\">                  16</td><td>d661622e  </td></tr>\n",
       "<tr><td>FSR_Trainable_d9117331</td><td>2023-07-19_02-41-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.48042 </td><td style=\"text-align: right;\">0.406442 </td><td>172.26.215.93</td><td style=\"text-align: right;\">384909</td><td style=\"text-align: right;\">2.96946 </td><td style=\"text-align: right;\">            0.729169</td><td style=\"text-align: right;\">          0.729169</td><td style=\"text-align: right;\">      0.729169</td><td style=\"text-align: right;\"> 1689702087</td><td style=\"text-align: right;\">                   1</td><td>d9117331  </td></tr>\n",
       "<tr><td>FSR_Trainable_db3cb1ad</td><td>2023-07-19_02-41-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.3434  </td><td style=\"text-align: right;\">0.0962685</td><td>172.26.215.93</td><td style=\"text-align: right;\">384706</td><td style=\"text-align: right;\">0.662055</td><td style=\"text-align: right;\">           28.8779  </td><td style=\"text-align: right;\">          0.839348</td><td style=\"text-align: right;\">     28.8779  </td><td style=\"text-align: right;\"> 1689702109</td><td style=\"text-align: right;\">                  32</td><td>db3cb1ad  </td></tr>\n",
       "<tr><td>FSR_Trainable_dba85aa4</td><td>2023-07-19_02-43-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.351507</td><td style=\"text-align: right;\">0.0994895</td><td>172.26.215.93</td><td style=\"text-align: right;\">387412</td><td style=\"text-align: right;\">0.691273</td><td style=\"text-align: right;\">            1.80924 </td><td style=\"text-align: right;\">          0.670407</td><td style=\"text-align: right;\">      1.80924 </td><td style=\"text-align: right;\"> 1689702197</td><td style=\"text-align: right;\">                   2</td><td>dba85aa4  </td></tr>\n",
       "<tr><td>FSR_Trainable_dd656807</td><td>2023-07-19_02-35-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.35745 </td><td style=\"text-align: right;\">0.0950322</td><td>172.26.215.93</td><td style=\"text-align: right;\">378444</td><td style=\"text-align: right;\">0.66963 </td><td style=\"text-align: right;\">           22.6743  </td><td style=\"text-align: right;\">          0.614438</td><td style=\"text-align: right;\">     22.6743  </td><td style=\"text-align: right;\"> 1689701740</td><td style=\"text-align: right;\">                  32</td><td>dd656807  </td></tr>\n",
       "<tr><td>FSR_Trainable_dec4e142</td><td>2023-07-19_02-41-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.345351</td><td style=\"text-align: right;\">0.0962731</td><td>172.26.215.93</td><td style=\"text-align: right;\">383728</td><td style=\"text-align: right;\">0.655703</td><td style=\"text-align: right;\">           72.9069  </td><td style=\"text-align: right;\">          0.867707</td><td style=\"text-align: right;\">     72.9069  </td><td style=\"text-align: right;\"> 1689702103</td><td style=\"text-align: right;\">                 100</td><td>dec4e142  </td></tr>\n",
       "<tr><td>FSR_Trainable_e10c5432</td><td>2023-07-19_02-39-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.345298</td><td style=\"text-align: right;\">0.0968428</td><td>172.26.215.93</td><td style=\"text-align: right;\">381290</td><td style=\"text-align: right;\">0.655676</td><td style=\"text-align: right;\">           99.0447  </td><td style=\"text-align: right;\">          0.850187</td><td style=\"text-align: right;\">     99.0447  </td><td style=\"text-align: right;\"> 1689701952</td><td style=\"text-align: right;\">                 100</td><td>e10c5432  </td></tr>\n",
       "<tr><td>FSR_Trainable_e264846f</td><td>2023-07-19_02-46-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.345091</td><td style=\"text-align: right;\">0.097006 </td><td>172.26.215.93</td><td style=\"text-align: right;\">391035</td><td style=\"text-align: right;\">0.656568</td><td style=\"text-align: right;\">           40.6192  </td><td style=\"text-align: right;\">          0.643   </td><td style=\"text-align: right;\">     40.6192  </td><td style=\"text-align: right;\"> 1689702411</td><td style=\"text-align: right;\">                  64</td><td>e264846f  </td></tr>\n",
       "<tr><td>FSR_Trainable_e51283d5</td><td>2023-07-19_02-43-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.342885</td><td style=\"text-align: right;\">0.101561 </td><td>172.26.215.93</td><td style=\"text-align: right;\">388497</td><td style=\"text-align: right;\">0.695293</td><td style=\"text-align: right;\">            2.04267 </td><td style=\"text-align: right;\">          0.734357</td><td style=\"text-align: right;\">      2.04267 </td><td style=\"text-align: right;\"> 1689702233</td><td style=\"text-align: right;\">                   2</td><td>e51283d5  </td></tr>\n",
       "<tr><td>FSR_Trainable_e5963266</td><td>2023-07-19_02-51-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.378114</td><td style=\"text-align: right;\">0.10465  </td><td>172.26.215.93</td><td style=\"text-align: right;\">395927</td><td style=\"text-align: right;\">0.709065</td><td style=\"text-align: right;\">            1.07522 </td><td style=\"text-align: right;\">          1.07522 </td><td style=\"text-align: right;\">      1.07522 </td><td style=\"text-align: right;\"> 1689702690</td><td style=\"text-align: right;\">                   1</td><td>e5963266  </td></tr>\n",
       "<tr><td>FSR_Trainable_e59b43e6</td><td>2023-07-19_02-35-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.338262</td><td style=\"text-align: right;\">0.100093 </td><td>172.26.215.93</td><td style=\"text-align: right;\">377768</td><td style=\"text-align: right;\">0.691198</td><td style=\"text-align: right;\">           17.8389  </td><td style=\"text-align: right;\">          1.30036 </td><td style=\"text-align: right;\">     17.8389  </td><td style=\"text-align: right;\"> 1689701705</td><td style=\"text-align: right;\">                  16</td><td>e59b43e6  </td></tr>\n",
       "<tr><td>FSR_Trainable_ea541971</td><td>2023-07-19_02-34-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.390476</td><td style=\"text-align: right;\">0.112792 </td><td>172.26.215.93</td><td style=\"text-align: right;\">376835</td><td style=\"text-align: right;\">0.718553</td><td style=\"text-align: right;\">            1.30795 </td><td style=\"text-align: right;\">          0.685347</td><td style=\"text-align: right;\">      1.30795 </td><td style=\"text-align: right;\"> 1689701647</td><td style=\"text-align: right;\">                   2</td><td>ea541971  </td></tr>\n",
       "<tr><td>FSR_Trainable_eac9ef26</td><td>2023-07-19_02-41-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.410968</td><td style=\"text-align: right;\">0.117483 </td><td>172.26.215.93</td><td style=\"text-align: right;\">385605</td><td style=\"text-align: right;\">0.715168</td><td style=\"text-align: right;\">            0.918453</td><td style=\"text-align: right;\">          0.918453</td><td style=\"text-align: right;\">      0.918453</td><td style=\"text-align: right;\"> 1689702118</td><td style=\"text-align: right;\">                   1</td><td>eac9ef26  </td></tr>\n",
       "<tr><td>FSR_Trainable_eae8aca6</td><td>2023-07-19_02-33-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.22169 </td><td style=\"text-align: right;\">0.377169 </td><td>172.26.215.93</td><td style=\"text-align: right;\">376205</td><td style=\"text-align: right;\">2.58393 </td><td style=\"text-align: right;\">            0.774517</td><td style=\"text-align: right;\">          0.774517</td><td style=\"text-align: right;\">      0.774517</td><td style=\"text-align: right;\"> 1689701620</td><td style=\"text-align: right;\">                   1</td><td>eae8aca6  </td></tr>\n",
       "<tr><td>FSR_Trainable_f8c708a6</td><td>2023-07-19_02-46-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.344976</td><td style=\"text-align: right;\">0.0969407</td><td>172.26.215.93</td><td style=\"text-align: right;\">390554</td><td style=\"text-align: right;\">0.656014</td><td style=\"text-align: right;\">           67.6971  </td><td style=\"text-align: right;\">          0.516741</td><td style=\"text-align: right;\">     67.6971  </td><td style=\"text-align: right;\"> 1689702399</td><td style=\"text-align: right;\">                 100</td><td>f8c708a6  </td></tr>\n",
       "<tr><td>FSR_Trainable_fd46a171</td><td>2023-07-19_02-39-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.345186</td><td style=\"text-align: right;\">0.0967741</td><td>172.26.215.93</td><td style=\"text-align: right;\">381472</td><td style=\"text-align: right;\">0.655475</td><td style=\"text-align: right;\">           99.7043  </td><td style=\"text-align: right;\">          0.779933</td><td style=\"text-align: right;\">     99.7043  </td><td style=\"text-align: right;\"> 1689701960</td><td style=\"text-align: right;\">                 100</td><td>fd46a171  </td></tr>\n",
       "<tr><td>FSR_Trainable_fd5b9379</td><td>2023-07-19_02-33-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.550215</td><td style=\"text-align: right;\">0.116554 </td><td>172.26.215.93</td><td style=\"text-align: right;\">376512</td><td style=\"text-align: right;\">0.836945</td><td style=\"text-align: right;\">            0.666875</td><td style=\"text-align: right;\">          0.666875</td><td style=\"text-align: right;\">      0.666875</td><td style=\"text-align: right;\"> 1689701627</td><td style=\"text-align: right;\">                   1</td><td>fd5b9379  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_01ccd703_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_02-32-29/wandb/run-20230719_023240-01ccd703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: Syncing run FSR_Trainable_01ccd703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/01ccd703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_5f091b2b_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_02-32-35/wandb/run-20230719_023247-5f091b2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: Syncing run FSR_Trainable_5f091b2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5f091b2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb:                      mae 2.41543\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb:                     mape 0.37416\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb:                     rmse 2.72647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb:       time_since_restore 0.85311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb:         time_this_iter_s 0.85311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb:             time_total_s 0.85311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb:                timestamp 1689701563\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: 🚀 View run FSR_Trainable_5f091b2b at: https://wandb.ai/seokjin/FSR-prediction/runs/5f091b2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374708)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023247-5f091b2b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_0bd8e5da_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_02-32-42/wandb/run-20230719_023254-0bd8e5da\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: Syncing run FSR_Trainable_0bd8e5da\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0bd8e5da\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb:                      mae ▁▄▄▅▄▄▂█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb:                     mape ▁▄▃▅▄▃▄█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb:                     rmse ▁▁▃▃▆▆▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb:         time_this_iter_s █▃▂▃▁▁▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb:                timestamp ▁▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb:                      mae 0.39593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb:                     mape 0.11078\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb:                     rmse 0.7196\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb:       time_since_restore 4.44098\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb:         time_this_iter_s 0.54688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb:             time_total_s 4.44098\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb:                timestamp 1689701575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: 🚀 View run FSR_Trainable_0bd8e5da at: https://wandb.ai/seokjin/FSR-prediction/runs/0bd8e5da\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374884)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023254-0bd8e5da/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_7b106491_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_02-32-49/wandb/run-20230719_023300-7b106491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Syncing run FSR_Trainable_7b106491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7b106491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023300-7b106491/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023300-7b106491/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023300-7b106491/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023300-7b106491/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023300-7b106491/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023300-7b106491/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023300-7b106491/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023300-7b106491/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023300-7b106491/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023300-7b106491/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023300-7b106491/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023300-7b106491/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023300-7b106491/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375072)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023300-7b106491/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_0e31f13a_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_02-32-55/wandb/run-20230719_023308-0e31f13a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: Syncing run FSR_Trainable_0e31f13a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0e31f13a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb:                      mae 0.42669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb:                     mape 0.11\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb:                     rmse 0.75431\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb:       time_since_restore 0.88125\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb:         time_this_iter_s 0.40642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb:             time_total_s 0.88125\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb:                timestamp 1689701586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: 🚀 View run FSR_Trainable_0e31f13a at: https://wandb.ai/seokjin/FSR-prediction/runs/0e31f13a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023308-0e31f13a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_896925d9_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_02-33-03/wandb/run-20230719_023316-896925d9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: Syncing run FSR_Trainable_896925d9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/896925d9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb:                      mae ▁██▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb:                     mape █▆▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb:                     rmse █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb:         time_this_iter_s ▃▁█▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb:                      mae 0.44549\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb:                     mape 0.10827\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb:                     rmse 0.71514\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb:       time_since_restore 2.40069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb:         time_this_iter_s 0.61938\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb:             time_total_s 2.40069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb:                timestamp 1689701595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: 🚀 View run FSR_Trainable_896925d9 at: https://wandb.ai/seokjin/FSR-prediction/runs/896925d9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375525)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023316-896925d9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023316-896925d9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb:                      mae █▆▄▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb:                     mape █▆▅▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb:                     rmse █▆▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb:         time_this_iter_s █▃▃▃▅▃▂▂▁▁▁▄▃▂▂▃▃▂▂▂▂▂▁▁▁▃▂▂▁▂▁▁▁▃▄▂▃▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_9d1f6fd8_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_02-33-11/wandb/run-20230719_023324-9d1f6fd8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: Syncing run FSR_Trainable_9d1f6fd8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9d1f6fd8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=374535)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:33:29,741\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.492 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:33:29,743\tWARNING util.py:315 -- The `process_trial_result` operation took 1.496 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:33:29,746\tWARNING util.py:315 -- Processing trial results took 1.498 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:33:29,747\tWARNING util.py:315 -- The `process_trial_result` operation took 1.499 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb:                      mae █▆▅▄▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb:                     mape █▆▅▄▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb:                     rmse █▆▅▄▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb:         time_this_iter_s █▃▂▃▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb:                timestamp ▁▅▅▆▆▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb:                      mae 0.34891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb:                     mape 0.09399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb:                     rmse 0.71079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb:       time_since_restore 4.08348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb:         time_this_iter_s 0.38241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb:             time_total_s 4.08348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb:                timestamp 1689701605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: 🚀 View run FSR_Trainable_9d1f6fd8 at: https://wandb.ai/seokjin/FSR-prediction/runs/9d1f6fd8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023324-9d1f6fd8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_08791bfe_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_02-33-20/wandb/run-20230719_023331-08791bfe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Syncing run FSR_Trainable_08791bfe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/08791bfe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375754)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:33:36,412\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.673 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:33:36,416\tWARNING util.py:315 -- The `process_trial_result` operation took 1.678 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:33:36,418\tWARNING util.py:315 -- Processing trial results took 1.680 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:33:36,420\tWARNING util.py:315 -- The `process_trial_result` operation took 1.682 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023331-08791bfe/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023331-08791bfe/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023331-08791bfe/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023331-08791bfe/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023331-08791bfe/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023331-08791bfe/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023331-08791bfe/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023331-08791bfe/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023331-08791bfe/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023331-08791bfe/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023331-08791bfe/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023331-08791bfe/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023331-08791bfe/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023331-08791bfe/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_ae8bf4d3_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_02-33-27/wandb/run-20230719_023338-ae8bf4d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: Syncing run FSR_Trainable_ae8bf4d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ae8bf4d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=375985)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 02:33:42,198\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.702 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:33:42,200\tWARNING util.py:315 -- The `process_trial_result` operation took 1.705 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:33:42,202\tWARNING util.py:315 -- Processing trial results took 1.708 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:33:42,203\tWARNING util.py:315 -- The `process_trial_result` operation took 1.709 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb:                      mae █▇▆▅▅▄▄▃▃▃▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb:                     mape █▇▆▅▄▄▃▃▃▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb:                     rmse █▇▆▅▅▄▄▄▃▃▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb:       time_since_restore ▁▂▂▂▃▃▄▄▄▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb:         time_this_iter_s █▃▂▂▂▁▁▂▁▁▆▅▃▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb:             time_total_s ▁▂▂▂▃▃▄▄▄▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb:                timestamp ▁▃▃▃▄▄▄▅▅▅▆▆▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb:                      mae 0.35693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb:                     mape 0.09482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb:                     rmse 0.70835\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb:       time_since_restore 6.70063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb:         time_this_iter_s 0.38702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb:             time_total_s 6.70063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb:                timestamp 1689701623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: 🚀 View run FSR_Trainable_ae8bf4d3 at: https://wandb.ai/seokjin/FSR-prediction/runs/ae8bf4d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376204)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023338-ae8bf4d3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023344-eae8aca6/logs\n",
      "2023-07-19 02:33:50,050\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.458 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:33:50,056\tWARNING util.py:315 -- The `process_trial_result` operation took 2.464 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:33:50,059\tWARNING util.py:315 -- Processing trial results took 2.467 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:33:50,064\tWARNING util.py:315 -- The `process_trial_result` operation took 2.472 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_fd5b9379_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-33-39/wandb/run-20230719_023352-fd5b9379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: Syncing run FSR_Trainable_fd5b9379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd5b9379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd5b9379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd5b9379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd5b9379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd5b9379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd5b9379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd5b9379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd5b9379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd5b9379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd5b9379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd5b9379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd5b9379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd5b9379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd5b9379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376377)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd5b9379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb:                      mae 0.55021\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb:                     mape 0.11655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb:                     rmse 0.83694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb:       time_since_restore 0.66688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb:         time_this_iter_s 0.66688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb:             time_total_s 0.66688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb:                timestamp 1689701627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: 🚀 View run FSR_Trainable_fd5b9379 at: https://wandb.ai/seokjin/FSR-prediction/runs/fd5b9379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023352-fd5b9379/logs\n",
      "2023-07-19 02:34:00,708\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.867 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:34:00,710\tWARNING util.py:315 -- The `process_trial_result` operation took 1.870 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:34:00,712\tWARNING util.py:315 -- Processing trial results took 1.872 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:34:00,713\tWARNING util.py:315 -- The `process_trial_result` operation took 1.873 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376605)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_d661622e_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-33-46/wandb/run-20230719_023402-d661622e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: Syncing run FSR_Trainable_d661622e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d661622e\n",
      "2023-07-19 02:34:06,903\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.150 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:34:06,905\tWARNING util.py:315 -- The `process_trial_result` operation took 2.154 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:34:06,907\tWARNING util.py:315 -- Processing trial results took 2.156 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:34:06,909\tWARNING util.py:315 -- The `process_trial_result` operation took 2.158 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_ea541971_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-33-56/wandb/run-20230719_023409-ea541971\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: Syncing run FSR_Trainable_ea541971\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ea541971\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:34:14,897\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.228 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:34:14,906\tWARNING util.py:315 -- The `process_trial_result` operation took 2.238 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:34:14,908\tWARNING util.py:315 -- Processing trial results took 2.240 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:34:14,910\tWARNING util.py:315 -- The `process_trial_result` operation took 2.242 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb:                      mae 0.39048\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb:                     mape 0.11279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb:                     rmse 0.71855\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb:       time_since_restore 1.30795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb:         time_this_iter_s 0.68535\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb:             time_total_s 1.30795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb:                timestamp 1689701647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: 🚀 View run FSR_Trainable_ea541971 at: https://wandb.ai/seokjin/FSR-prediction/runs/ea541971\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377010)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023409-ea541971/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_aef894a5_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-34-04/wandb/run-20230719_023418-aef894a5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: Syncing run FSR_Trainable_aef894a5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/aef894a5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb:                      mae █▇▇▆▅▅▄▄▃▃▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb:                     mape █▇▆▆▅▄▄▃▃▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb:                     rmse █▇▇▆▆▅▅▄▄▄▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▃▄▄▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb:         time_this_iter_s █▂▁▁▂▃▃▁▂▃▆▆▆▄▇▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▃▃▃▄▄▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb:                timestamp ▁▂▂▂▃▃▄▄▄▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb:                      mae 0.3386\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb:                     mape 0.09957\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb:                     rmse 0.69568\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb:       time_since_restore 21.39326\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb:         time_this_iter_s 1.53098\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb:             time_total_s 21.39326\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb:                timestamp 1689701663\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: 🚀 View run FSR_Trainable_d661622e at: https://wandb.ai/seokjin/FSR-prediction/runs/d661622e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=376834)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023402-d661622e/logs\n",
      "2023-07-19 02:34:29,434\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.915 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:34:29,440\tWARNING util.py:315 -- The `process_trial_result` operation took 1.921 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:34:29,441\tWARNING util.py:315 -- Processing trial results took 1.923 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:34:29,443\tWARNING util.py:315 -- The `process_trial_result` operation took 1.924 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_46ab530f_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-34-11/wandb/run-20230719_023432-46ab530f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: Syncing run FSR_Trainable_46ab530f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/46ab530f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb:                      mae ▄▁▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb:                     mape ▁▂▄█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb:                     rmse ▃▁▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb:         time_this_iter_s █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb:                      mae 0.45184\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb:                     mape 0.11628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb:                     rmse 0.77043\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb:       time_since_restore 5.28649\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb:         time_this_iter_s 1.09049\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb:             time_total_s 5.28649\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb:                timestamp 1689701672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: 🚀 View run FSR_Trainable_46ab530f at: https://wandb.ai/seokjin/FSR-prediction/runs/46ab530f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377420)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023432-46ab530f/logs\n",
      "2023-07-19 02:34:39,884\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.878 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:34:39,890\tWARNING util.py:315 -- The `process_trial_result` operation took 1.884 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:34:39,892\tWARNING util.py:315 -- Processing trial results took 1.886 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:34:39,894\tWARNING util.py:315 -- The `process_trial_result` operation took 1.888 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_c982cc1d_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-34-25/wandb/run-20230719_023442-c982cc1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Syncing run FSR_Trainable_c982cc1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c982cc1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb:                      mae ▂█▆▇▆▅▄▅▅▅▅▄▄▄▄▄▅▅▅▅▅▅▄▄▃▃▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb:                     mape ▂▆▆▇▇▇███████▇▇▆▆▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb:                     rmse █▅▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb:         time_this_iter_s █▆█▇▄▅█▆▅▄▃▃█▆▅▃▃▃▃▂▁▂▃▃▁▃▃▄▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb:                timestamp ▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb:                      mae 0.35928\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb:                     mape 0.09548\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb:                     rmse 0.67505\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb:       time_since_restore 25.32536\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb:         time_this_iter_s 0.45118\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb:             time_total_s 25.32536\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb:                timestamp 1689701682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: 🚀 View run FSR_Trainable_aef894a5 at: https://wandb.ai/seokjin/FSR-prediction/runs/aef894a5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377187)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023418-aef894a5/logs\n",
      "2023-07-19 02:34:47,031\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.641 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:34:47,037\tWARNING util.py:315 -- The `process_trial_result` operation took 1.647 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:34:47,038\tWARNING util.py:315 -- Processing trial results took 1.649 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:34:47,040\tWARNING util.py:315 -- The `process_trial_result` operation took 1.651 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_e59b43e6_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-34-36/wandb/run-20230719_023448-e59b43e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: Syncing run FSR_Trainable_e59b43e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e59b43e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e59b43e6\n",
      "2023-07-19 02:34:54,531\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.997 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:34:54,534\tWARNING util.py:315 -- The `process_trial_result` operation took 2.001 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:34:54,535\tWARNING util.py:315 -- Processing trial results took 2.002 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:34:54,536\tWARNING util.py:315 -- The `process_trial_result` operation took 2.003 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_cf3426ef_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-34-43/wandb/run-20230719_023457-cf3426ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: Syncing run FSR_Trainable_cf3426ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cf3426ef\n",
      "2023-07-19 02:35:04,483\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.877 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:35:04,487\tWARNING util.py:315 -- The `process_trial_result` operation took 1.882 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:35:04,488\tWARNING util.py:315 -- Processing trial results took 1.884 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:35:04,490\tWARNING util.py:315 -- The `process_trial_result` operation took 1.886 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_0649225d_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-34-51/wandb/run-20230719_023507-0649225d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: Syncing run FSR_Trainable_0649225d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0649225d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb:                      mae █▇▆▅▅▄▃▃▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb:                     mape █▇▆▆▅▄▄▃▃▃▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb:                     rmse █▇▇▇▆▆▅▅▅▄▄▃▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb:         time_this_iter_s █▃▁▃▂▁▆▄▄▂▃▂▂▃▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb:                timestamp ▁▂▂▃▃▃▄▅▅▅▆▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb:                      mae 0.33826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb:                     mape 0.10009\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb:                     rmse 0.6912\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb:       time_since_restore 17.83886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb:         time_this_iter_s 1.30036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb:             time_total_s 17.83886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb:                timestamp 1689701705\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: 🚀 View run FSR_Trainable_e59b43e6 at: https://wandb.ai/seokjin/FSR-prediction/runs/e59b43e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=377877)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023448-e59b43e6/logs\n",
      "2023-07-19 02:35:14,085\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.738 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:35:14,088\tWARNING util.py:315 -- The `process_trial_result` operation took 1.743 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:35:14,091\tWARNING util.py:315 -- Processing trial results took 1.745 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:35:14,093\tWARNING util.py:315 -- The `process_trial_result` operation took 1.747 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_dd656807_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-35-01/wandb/run-20230719_023517-dd656807\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: Syncing run FSR_Trainable_dd656807\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/dd656807\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb:                      mae █▁▂▁▂▃▄▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▅▅▅▅▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb:                     mape ▁▂▄▅▆▇▇███████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb:                     rmse █▆▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb:         time_this_iter_s ▄▃▃▃▂▂▂▃▁▁▂▂▂▄▄▄▄▂▂▁▂▁▁▂▂▁▂▃▃▇█▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb:                      mae 0.35426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb:                     mape 0.09393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb:                     rmse 0.66899\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb:       time_since_restore 20.35544\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb:         time_this_iter_s 0.81838\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb:             time_total_s 20.35544\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb:                timestamp 1689701717\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: 🚀 View run FSR_Trainable_cf3426ef at: https://wandb.ai/seokjin/FSR-prediction/runs/cf3426ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378102)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023457-cf3426ef/logs\n",
      "2023-07-19 02:35:26,629\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.516 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:35:26,634\tWARNING util.py:315 -- The `process_trial_result` operation took 2.521 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:35:26,636\tWARNING util.py:315 -- Processing trial results took 2.523 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:35:26,638\tWARNING util.py:315 -- The `process_trial_result` operation took 2.525 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_1e24860f_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-35-11/wandb/run-20230719_023529-1e24860f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: Syncing run FSR_Trainable_1e24860f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1e24860f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb:                      mae █▃▁▂▃▄▅▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb:                     mape █▅▃▃▃▄▄▄▄▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb:                     rmse █▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb:         time_this_iter_s ▅▃▄▃▂▂▂▁▁▁▂▂▁▅▃▇█▅▆▃▃▄▂▁▂▃▅▆▄▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb:                      mae 0.35625\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb:                     mape 0.09471\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb:                     rmse 0.66983\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb:       time_since_restore 21.71163\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb:         time_this_iter_s 0.63395\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb:             time_total_s 21.71163\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb:                timestamp 1689701730\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: 🚀 View run FSR_Trainable_0649225d at: https://wandb.ai/seokjin/FSR-prediction/runs/0649225d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378314)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023507-0649225d/logs\n",
      "2023-07-19 02:35:37,309\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.732 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:35:37,310\tWARNING util.py:315 -- The `process_trial_result` operation took 1.735 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:35:37,313\tWARNING util.py:315 -- Processing trial results took 1.738 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:35:37,315\tWARNING util.py:315 -- The `process_trial_result` operation took 1.740 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_54a1fa11_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-35-23/wandb/run-20230719_023540-54a1fa11\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: Syncing run FSR_Trainable_54a1fa11\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/54a1fa11\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb:                      mae █▄▅▅▄▄▃▃▂▂▂▂▂▂▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb:                     mape ▅▅▇██▇▇▇▇▇▆▆▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb:                     rmse █▆▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb:         time_this_iter_s ▄▂▆█▄▆▃▃▃▂▁▂▃▄▅▆▃▃▂▃▂▅▁▂▁▁▂▄▄▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb:                timestamp ▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb:                      mae 0.35745\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb:                     mape 0.09503\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb:                     rmse 0.66963\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb:       time_since_restore 22.67427\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb:         time_this_iter_s 0.61444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb:             time_total_s 22.67427\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb:                timestamp 1689701740\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: 🚀 View run FSR_Trainable_dd656807 at: https://wandb.ai/seokjin/FSR-prediction/runs/dd656807\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378542)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023517-dd656807/logs\n",
      "2023-07-19 02:35:47,302\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.967 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:35:47,304\tWARNING util.py:315 -- The `process_trial_result` operation took 1.971 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:35:47,306\tWARNING util.py:315 -- Processing trial results took 1.973 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:35:47,310\tWARNING util.py:315 -- The `process_trial_result` operation took 1.977 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_3801c760_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-35-34/wandb/run-20230719_023550-3801c760\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: Syncing run FSR_Trainable_3801c760\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3801c760\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:35:57,699\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.830 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:35:57,704\tWARNING util.py:315 -- The `process_trial_result` operation took 1.835 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:35:57,706\tWARNING util.py:315 -- Processing trial results took 1.837 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:35:57,707\tWARNING util.py:315 -- The `process_trial_result` operation took 1.838 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb:                      mae █▁▂▂▃▄▄▄▅▅▅▅▅▅▅▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb:                     mape █▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb:                     rmse █▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb:       time_since_restore ▁▁▂▃▃▄▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb:         time_this_iter_s █▃█▄▄▄▃▂▃▄▃▃▂▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb:             time_total_s ▁▁▂▃▃▄▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb:                timestamp ▁▃▃▃▄▄▅▅▆▆▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb:                      mae 0.3617\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb:                     mape 0.09537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb:                     rmse 0.67263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb:       time_since_restore 7.53604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb:         time_this_iter_s 0.39872\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb:             time_total_s 7.53604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb:                timestamp 1689701754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: 🚀 View run FSR_Trainable_3801c760 at: https://wandb.ai/seokjin/FSR-prediction/runs/3801c760\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379232)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023550-3801c760/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_8d4fea50_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-35-44/wandb/run-20230719_023600-8d4fea50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: Syncing run FSR_Trainable_8d4fea50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8d4fea50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb:                      mae 0.43607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb:                     mape 0.11636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb:                     rmse 0.74668\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb:       time_since_restore 0.69722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb:         time_this_iter_s 0.69722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb:             time_total_s 0.69722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb:                timestamp 1689701755\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: 🚀 View run FSR_Trainable_8d4fea50 at: https://wandb.ai/seokjin/FSR-prediction/runs/8d4fea50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379455)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023600-8d4fea50/logs\n",
      "2023-07-19 02:36:08,447\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.843 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:36:08,450\tWARNING util.py:315 -- The `process_trial_result` operation took 1.847 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:36:08,452\tWARNING util.py:315 -- Processing trial results took 1.849 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:36:08,453\tWARNING util.py:315 -- The `process_trial_result` operation took 1.850 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_7853eed4_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-35-55/wandb/run-20230719_023611-7853eed4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: Syncing run FSR_Trainable_7853eed4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7853eed4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb:                      mae ▄▃███▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb:                     mape ▃▅▇████▇▇▇▆▆▅▅▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb:                     rmse █▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb:         time_this_iter_s ▆█▄▃▃▂▂▁▁▃▄▄▁▂▃▃▂▄▅▄▃▃▃▃▃▅▄▄▄▅▂▃▂▇▅▆▃▆▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb:                      mae 0.35263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb:                     mape 0.09362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb:                     rmse 0.66741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb:       time_since_restore 42.56959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb:         time_this_iter_s 0.67317\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb:             time_total_s 42.56959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb:                timestamp 1689701775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: 🚀 View run FSR_Trainable_1e24860f at: https://wandb.ai/seokjin/FSR-prediction/runs/1e24860f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=378772)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023529-1e24860f/logs\n",
      "2023-07-19 02:36:19,793\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.901 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:36:19,797\tWARNING util.py:315 -- The `process_trial_result` operation took 1.905 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:36:19,799\tWARNING util.py:315 -- Processing trial results took 1.908 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:36:19,800\tWARNING util.py:315 -- The `process_trial_result` operation took 1.909 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_4ad33e7a_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-36-05/wandb/run-20230719_023622-4ad33e7a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: Syncing run FSR_Trainable_4ad33e7a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4ad33e7a\n",
      "2023-07-19 02:36:30,907\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.881 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:36:30,910\tWARNING util.py:315 -- The `process_trial_result` operation took 1.885 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:36:30,912\tWARNING util.py:315 -- Processing trial results took 1.888 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:36:30,915\tWARNING util.py:315 -- The `process_trial_result` operation took 1.890 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_94b05c37_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-36-17/wandb/run-20230719_023633-94b05c37\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: Syncing run FSR_Trainable_94b05c37\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/94b05c37\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb:                      mae 0.36339\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb:                     mape 0.09904\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb:                     rmse 0.69856\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb:       time_since_restore 1.70587\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb:         time_this_iter_s 0.78353\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb:             time_total_s 1.70587\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb:                timestamp 1689701791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: 🚀 View run FSR_Trainable_94b05c37 at: https://wandb.ai/seokjin/FSR-prediction/runs/94b05c37\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380135)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023633-94b05c37/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:36:45,041\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.131 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:36:45,045\tWARNING util.py:315 -- The `process_trial_result` operation took 2.135 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:36:45,046\tWARNING util.py:315 -- Processing trial results took 2.136 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:36:45,048\tWARNING util.py:315 -- The `process_trial_result` operation took 2.138 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb:                      mae █▆▅▅▅▅▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb:                     mape ▇▆███▇▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb:                     rmse █▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb:         time_this_iter_s █▄▅▆▁▂▄▂▂▂▂▃▂▅▄▆▃▄▁▃▃▃▁▃▂▂▂▁▂▁▁▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb:                timestamp ▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb:                      mae 0.35538\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb:                     mape 0.09414\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb:                     rmse 0.66946\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb:       time_since_restore 21.68078\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb:         time_this_iter_s 0.78174\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb:             time_total_s 21.68078\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb:                timestamp 1689701802\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: 🚀 View run FSR_Trainable_4ad33e7a at: https://wandb.ai/seokjin/FSR-prediction/runs/4ad33e7a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379911)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023622-4ad33e7a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_cf0e6cd8_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-36-28/wandb/run-20230719_023647-cf0e6cd8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Syncing run FSR_Trainable_cf0e6cd8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cf0e6cd8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023647-cf0e6cd8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023647-cf0e6cd8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023647-cf0e6cd8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023647-cf0e6cd8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023647-cf0e6cd8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023647-cf0e6cd8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023647-cf0e6cd8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023647-cf0e6cd8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023647-cf0e6cd8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023647-cf0e6cd8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023647-cf0e6cd8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023647-cf0e6cd8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023647-cf0e6cd8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380368)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023647-cf0e6cd8/logs\n",
      "2023-07-19 02:36:55,308\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.032 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:36:55,311\tWARNING util.py:315 -- The `process_trial_result` operation took 2.036 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:36:55,312\tWARNING util.py:315 -- Processing trial results took 2.037 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:36:55,314\tWARNING util.py:315 -- The `process_trial_result` operation took 2.039 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380602)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380602)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380602)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380602)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380602)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_529edf46_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-36-42/wandb/run-20230719_023658-529edf46\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380602)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380602)\u001b[0m wandb: Syncing run FSR_Trainable_529edf46\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380602)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380602)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/529edf46\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb:                      mae █▅▄▃▃▂▂▂▁▁▁▁▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb:                     mape █▆▅▅▅▅▅▅▅▅▄▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb:                     rmse █▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb:         time_this_iter_s █▆▁▃▃▂▆▄▃▂▃▃▂▂▂▂▄█▄▃▄█▇▂▄▃▇▇▄▄▅▄▄▄▅▇▃▂▃▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb:                      mae 0.34452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb:                     mape 0.09682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb:                     rmse 0.65616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb:       time_since_restore 65.62682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb:         time_this_iter_s 0.7768\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb:             time_total_s 65.62682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb:                timestamp 1689701816\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: 🚀 View run FSR_Trainable_54a1fa11 at: https://wandb.ai/seokjin/FSR-prediction/runs/54a1fa11\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023540-54a1fa11/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379001)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: Waiting for W&B process to finish... (success).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380602)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380602)\u001b[0m wandb:                      mae █▃▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380602)\u001b[0m wandb:                     mape █▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380602)\u001b[0m wandb:                     rmse █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380602)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380602)\u001b[0m wandb:         time_this_iter_s █▃▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380602)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380602)\u001b[0m wandb:                timestamp ▁▆▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380602)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb:                      mae █▆▆▆▆▅▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb:                     mape █▇█▇▇▆▆▅▅▄▄▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb:                     rmse █▆▄▄▄▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb:         time_this_iter_s █▅▆▃▅▃▃▄▃▄▄▃▄▃▃▃▃▅▄▄▃▄▄▃▃▃▃▄▄▄▂▂▂▂▁▃▅▅▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2023-07-19 02:37:04,676\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.647 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:37:04,678\tWARNING util.py:315 -- The `process_trial_result` operation took 1.650 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:37:04,680\tWARNING util.py:315 -- Processing trial results took 1.651 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:37:04,682\tWARNING util.py:315 -- The `process_trial_result` operation took 1.654 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: iterations_since_restore 64\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb:                      mae 0.35443\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb:                     mape 0.09443\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb:                     rmse 0.66736\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb:       time_since_restore 43.09509\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb:         time_this_iter_s 0.50951\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb:             time_total_s 43.09509\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb:                timestamp 1689701819\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb:       training_iteration 64\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: 🚀 View run FSR_Trainable_7853eed4 at: https://wandb.ai/seokjin/FSR-prediction/runs/7853eed4\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023611-7853eed4/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=379683)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_6cd25336_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-36-52/wandb/run-20230719_023706-6cd25336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Syncing run FSR_Trainable_6cd25336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6cd25336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023706-6cd25336/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023706-6cd25336/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023706-6cd25336/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023706-6cd25336/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023706-6cd25336/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023706-6cd25336/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023706-6cd25336/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023706-6cd25336/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023706-6cd25336/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023706-6cd25336/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023706-6cd25336/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023706-6cd25336/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023706-6cd25336/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023706-6cd25336/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023706-6cd25336/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=380845)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 02:37:12,572\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.939 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:37:12,574\tWARNING util.py:315 -- The `process_trial_result` operation took 1.942 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:37:12,579\tWARNING util.py:315 -- Processing trial results took 1.946 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:37:12,580\tWARNING util.py:315 -- The `process_trial_result` operation took 1.948 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_83ace7ba_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-37-01/wandb/run-20230719_023714-83ace7ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: Syncing run FSR_Trainable_83ace7ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/83ace7ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:37:19,914\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.812 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:37:19,918\tWARNING util.py:315 -- The `process_trial_result` operation took 1.817 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:37:19,920\tWARNING util.py:315 -- Processing trial results took 1.819 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:37:19,922\tWARNING util.py:315 -- The `process_trial_result` operation took 1.821 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb:                      mae 0.40063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb:                     mape 0.10826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb:                     rmse 0.73091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb:       time_since_restore 0.87537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb:         time_this_iter_s 0.87537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb:             time_total_s 0.87537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb:                timestamp 1689701830\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: 🚀 View run FSR_Trainable_83ace7ba at: https://wandb.ai/seokjin/FSR-prediction/runs/83ace7ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381073)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023714-83ace7ba/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_d1ef26ad_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-37-09/wandb/run-20230719_023722-d1ef26ad\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: Syncing run FSR_Trainable_d1ef26ad\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d1ef26ad\n",
      "2023-07-19 02:37:26,393\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.798 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:37:26,396\tWARNING util.py:315 -- The `process_trial_result` operation took 1.801 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:37:26,399\tWARNING util.py:315 -- Processing trial results took 1.804 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:37:26,401\tWARNING util.py:315 -- The `process_trial_result` operation took 1.806 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_e10c5432_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-37-17/wandb/run-20230719_023729-e10c5432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: Syncing run FSR_Trainable_e10c5432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e10c5432\n",
      "2023-07-19 02:37:33,889\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.713 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:37:33,891\tWARNING util.py:315 -- The `process_trial_result` operation took 1.716 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:37:33,892\tWARNING util.py:315 -- Processing trial results took 1.717 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:37:33,893\tWARNING util.py:315 -- The `process_trial_result` operation took 1.718 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_fd46a171_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-37-23/wandb/run-20230719_023736-fd46a171\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: Syncing run FSR_Trainable_fd46a171\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd46a171\n",
      "2023-07-19 02:37:48,597\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.984 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:37:48,599\tWARNING util.py:315 -- The `process_trial_result` operation took 1.988 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:37:48,602\tWARNING util.py:315 -- Processing trial results took 1.990 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:37:48,603\tWARNING util.py:315 -- The `process_trial_result` operation took 1.992 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_ab1ebf30_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-37-31/wandb/run-20230719_023751-ab1ebf30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: Syncing run FSR_Trainable_ab1ebf30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ab1ebf30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb:                      mae 0.34715\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb:                     mape 0.10242\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb:                     rmse 0.69159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb:       time_since_restore 2.56403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb:         time_this_iter_s 1.08703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb:             time_total_s 2.56403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb:                timestamp 1689701869\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: 🚀 View run FSR_Trainable_ab1ebf30 at: https://wandb.ai/seokjin/FSR-prediction/runs/ab1ebf30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381862)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023751-ab1ebf30/logs\n",
      "2023-07-19 02:38:03,627\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.726 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:38:03,630\tWARNING util.py:315 -- The `process_trial_result` operation took 1.730 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:38:03,631\tWARNING util.py:315 -- Processing trial results took 1.731 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:38:03,632\tWARNING util.py:315 -- The `process_trial_result` operation took 1.732 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_65506096_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-37-45/wandb/run-20230719_023806-65506096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: Syncing run FSR_Trainable_65506096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/65506096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb:                      mae █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb:                     mape █▆▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb:                     rmse █▅▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb:       time_since_restore ▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb:         time_this_iter_s ▆▂▁▁▃▂▂▂▅▄▃█▄▅▆▅▄▃▅▃▃▃▄▄▄▃▅▇▃▅▃▃▃▃▄▃▃▄▄▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb:             time_total_s ▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb:                      mae 0.34513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb:                     mape 0.09604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb:                     rmse 0.656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb:       time_since_restore 66.47004\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb:         time_this_iter_s 1.02195\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb:             time_total_s 66.47004\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb:                timestamp 1689701914\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: 🚀 View run FSR_Trainable_d1ef26ad at: https://wandb.ai/seokjin/FSR-prediction/runs/d1ef26ad\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381289)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023722-d1ef26ad/logs\n",
      "2023-07-19 02:38:50,381\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.605 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:38:50,384\tWARNING util.py:315 -- The `process_trial_result` operation took 2.610 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:38:50,387\tWARNING util.py:315 -- Processing trial results took 2.613 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:38:50,389\tWARNING util.py:315 -- The `process_trial_result` operation took 2.615 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_2daa8b50_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-38-00/wandb/run-20230719_023854-2daa8b50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: Syncing run FSR_Trainable_2daa8b50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2daa8b50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb:                      mae █▁▄▃▃▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb:                     mape █▅▇▆▆▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb:                     rmse █▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb:         time_this_iter_s ▃▂▁▃▃▃▄▆▄▂▂▂▂▄▃▃▃▃▃▂▂▂▃▂▂▂▃▄▄▅▃▆█▄▄▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb:                      mae 0.3453\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb:                     mape 0.09684\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb:                     rmse 0.65568\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb:       time_since_restore 99.04473\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb:         time_this_iter_s 0.85019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb:             time_total_s 99.04473\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb:                timestamp 1689701952\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: 🚀 View run FSR_Trainable_e10c5432 at: https://wandb.ai/seokjin/FSR-prediction/runs/e10c5432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381471)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023729-e10c5432/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb:                      mae █▂▁▃▂▂▂▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb:                     mape █▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb:                     rmse █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb:         time_this_iter_s ▄▂▇▅▅▄▂▂▁▁▄▂▂▃▂▄▁▂▂▂▂▄▂▃▄▃▄▂▅█▃▄▁▁▃▁▁▄▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb:                      mae 0.34519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb:                     mape 0.09677\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb:                     rmse 0.65548\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb:       time_since_restore 99.70432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb:         time_this_iter_s 0.77993\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb:             time_total_s 99.70432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb:                timestamp 1689701960\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: 🚀 View run FSR_Trainable_fd46a171 at: https://wandb.ai/seokjin/FSR-prediction/runs/fd46a171\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=381645)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023736-fd46a171/logs\n",
      "2023-07-19 02:39:27,338\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.400 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:39:27,345\tWARNING util.py:315 -- The `process_trial_result` operation took 3.408 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:39:27,349\tWARNING util.py:315 -- Processing trial results took 3.411 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:39:27,351\tWARNING util.py:315 -- The `process_trial_result` operation took 3.414 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_017adcb8_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-38-46/wandb/run-20230719_023930-017adcb8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: Syncing run FSR_Trainable_017adcb8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/017adcb8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb:                      mae 0.44632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb:                     mape 0.11657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb:                     rmse 0.78192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb:       time_since_restore 0.65154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb:         time_this_iter_s 0.65154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb:             time_total_s 0.65154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb:                timestamp 1689701963\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: 🚀 View run FSR_Trainable_017adcb8 at: https://wandb.ai/seokjin/FSR-prediction/runs/017adcb8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382624)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023930-017adcb8/logs\n",
      "2023-07-19 02:39:39,154\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.106 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:39:39,158\tWARNING util.py:315 -- The `process_trial_result` operation took 2.111 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:39:39,160\tWARNING util.py:315 -- Processing trial results took 2.113 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:39:39,162\tWARNING util.py:315 -- The `process_trial_result` operation took 2.115 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_229c32a7_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-39-23/wandb/run-20230719_023942-229c32a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb: Syncing run FSR_Trainable_229c32a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/229c32a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb:                      mae ▇█▇▆▄▄▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb:                     mape █▇▆▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb:                     rmse █▅▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb:         time_this_iter_s ▅█▆▅█▅▄▅▄▂▃▂▂▃▄▄▃▂▃▂▄▆▂▆▂▂▂▁▁▂▆▅▂▃▃▃▂▂▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb:                      mae 0.34454\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb:                     mape 0.09695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb:                     rmse 0.65627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb:       time_since_restore 45.19647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb:         time_this_iter_s 0.72491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb:             time_total_s 45.19647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb:                timestamp 1689701980\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: 🚀 View run FSR_Trainable_2daa8b50 at: https://wandb.ai/seokjin/FSR-prediction/runs/2daa8b50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382362)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023854-2daa8b50/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023942-229c32a7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:39:49,452\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.059 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:39:49,457\tWARNING util.py:315 -- The `process_trial_result` operation took 2.065 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:39:49,459\tWARNING util.py:315 -- Processing trial results took 2.067 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:39:49,461\tWARNING util.py:315 -- The `process_trial_result` operation took 2.069 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382852)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb:                      mae █▃▃▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb:                     mape █▆▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb:                     rmse █▅▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb:         time_this_iter_s ▅▄▃▄▄▃▂▃▃▃▂▃▃▃▅▄▅▄▇█▅▄▂▂▃▂▂▄▂▂▁▁▅▂▂▂▃▁▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023806-65506096/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023806-65506096/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: iterations_since_restore 100\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb:                      mae 0.34558\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb:                     mape 0.09691\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb:                     rmse 0.6554\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb:       time_since_restore 95.51008\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb:         time_this_iter_s 0.91714\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb:             time_total_s 95.51008\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb:                timestamp 1689701987\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb:       training_iteration 100\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: 🚀 View run FSR_Trainable_65506096 at: https://wandb.ai/seokjin/FSR-prediction/runs/65506096\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=382096)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023806-65506096/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_0e42846b_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-39-36/wandb/run-20230719_023951-0e42846b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: Syncing run FSR_Trainable_0e42846b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0e42846b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:39:58,104\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.047 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:39:58,112\tWARNING util.py:315 -- The `process_trial_result` operation took 2.056 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:39:58,115\tWARNING util.py:315 -- Processing trial results took 2.060 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:39:58,117\tWARNING util.py:315 -- The `process_trial_result` operation took 2.061 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb:                      mae 0.39778\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb:                     mape 0.11659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb:                     rmse 0.73199\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb:       time_since_restore 0.78604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb:         time_this_iter_s 0.78604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb:             time_total_s 0.78604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb:                timestamp 1689701987\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: 🚀 View run FSR_Trainable_0e42846b at: https://wandb.ai/seokjin/FSR-prediction/runs/0e42846b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383091)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_023951-0e42846b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_0c1880fb_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-39-46/wandb/run-20230719_024000-0c1880fb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: Syncing run FSR_Trainable_0c1880fb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0c1880fb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:40:04,913\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.531 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:40:04,918\tWARNING util.py:315 -- The `process_trial_result` operation took 1.537 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:40:04,919\tWARNING util.py:315 -- Processing trial results took 1.538 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:40:04,921\tWARNING util.py:315 -- The `process_trial_result` operation took 1.540 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb:                      mae 0.38754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb:                     mape 0.10274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb:                     rmse 0.72898\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb:       time_since_restore 1.31956\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb:         time_this_iter_s 1.31956\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb:             time_total_s 1.31956\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb:                timestamp 1689701996\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: 🚀 View run FSR_Trainable_0c1880fb at: https://wandb.ai/seokjin/FSR-prediction/runs/0c1880fb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383323)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024000-0c1880fb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_ce58f49b_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-39-54/wandb/run-20230719_024007-ce58f49b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Syncing run FSR_Trainable_ce58f49b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ce58f49b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024007-ce58f49b/logs\n",
      "2023-07-19 02:40:13,216\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.877 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:40:13,220\tWARNING util.py:315 -- The `process_trial_result` operation took 1.882 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:40:13,222\tWARNING util.py:315 -- Processing trial results took 1.884 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:40:13,228\tWARNING util.py:315 -- The `process_trial_result` operation took 1.889 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_5fdfe2cb_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-40-02/wandb/run-20230719_024015-5fdfe2cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: Syncing run FSR_Trainable_5fdfe2cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5fdfe2cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:40:19,314\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.696 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:40:19,315\tWARNING util.py:315 -- The `process_trial_result` operation took 1.698 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:40:19,318\tWARNING util.py:315 -- Processing trial results took 1.701 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:40:19,320\tWARNING util.py:315 -- The `process_trial_result` operation took 1.703 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb:                      mae 1.62449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb:                     mape 0.27626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb:                     rmse 1.8881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb:       time_since_restore 1.19288\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb:         time_this_iter_s 1.19288\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb:             time_total_s 1.19288\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb:                timestamp 1689702011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: 🚀 View run FSR_Trainable_5fdfe2cb at: https://wandb.ai/seokjin/FSR-prediction/runs/5fdfe2cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383727)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024015-5fdfe2cb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_dec4e142_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-40-10/wandb/run-20230719_024021-dec4e142\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: Syncing run FSR_Trainable_dec4e142\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/dec4e142\n",
      "2023-07-19 02:40:29,096\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.179 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:40:29,101\tWARNING util.py:315 -- The `process_trial_result` operation took 2.184 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:40:29,103\tWARNING util.py:315 -- Processing trial results took 2.186 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:40:29,105\tWARNING util.py:315 -- The `process_trial_result` operation took 2.188 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_a43e2e9e_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-40-16/wandb/run-20230719_024031-a43e2e9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: Syncing run FSR_Trainable_a43e2e9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a43e2e9e\n",
      "2023-07-19 02:40:37,199\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.810 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:40:37,202\tWARNING util.py:315 -- The `process_trial_result` operation took 1.814 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:40:37,203\tWARNING util.py:315 -- Processing trial results took 1.815 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:40:37,204\tWARNING util.py:315 -- The `process_trial_result` operation took 1.816 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_29e8b656_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-40-25/wandb/run-20230719_024043-29e8b656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: Syncing run FSR_Trainable_29e8b656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/29e8b656\n",
      "2023-07-19 02:40:49,909\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.903 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:40:49,913\tWARNING util.py:315 -- The `process_trial_result` operation took 1.907 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:40:49,914\tWARNING util.py:315 -- Processing trial results took 1.909 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:40:49,915\tWARNING util.py:315 -- The `process_trial_result` operation took 1.910 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_bef25036_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-40-34/wandb/run-20230719_024053-bef25036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: Syncing run FSR_Trainable_bef25036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/bef25036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb:                      mae █▆▆▄▄▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb:                     mape █▆▅▅▅▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb:                     rmse █▆▅▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb:         time_this_iter_s ▅▂▃▂▂▄▁█▅▆▃▃▃▃▃▄▄▆█▅▇▄▄▇▄█▄▃▃▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb:                      mae 0.34382\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb:                     mape 0.09696\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb:                     rmse 0.66039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb:       time_since_restore 30.56238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb:         time_this_iter_s 0.86097\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb:             time_total_s 30.56238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb:                timestamp 1689702063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: 🚀 View run FSR_Trainable_a43e2e9e at: https://wandb.ai/seokjin/FSR-prediction/runs/a43e2e9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384127)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024031-a43e2e9e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb:                      mae ▄█▅▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb:                     mape ▇█▆▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb:                     rmse █▇▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb:         time_this_iter_s █▃▆▃▂▁▃▃▃▅▆▄▃▆▃▃█▃▇▂▂▃▂▂▃▁▃▄▃▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb:                      mae 0.34497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb:                     mape 0.0967\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb:                     rmse 0.66173\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb:       time_since_restore 32.14144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb:         time_this_iter_s 0.8758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb:             time_total_s 32.14144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb:                timestamp 1689702070\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: 🚀 View run FSR_Trainable_29e8b656 at: https://wandb.ai/seokjin/FSR-prediction/runs/29e8b656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024043-29e8b656/logs\n",
      "2023-07-19 02:41:17,129\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.014 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:41:17,131\tWARNING util.py:315 -- The `process_trial_result` operation took 2.017 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:41:17,132\tWARNING util.py:315 -- Processing trial results took 2.018 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:41:17,133\tWARNING util.py:315 -- The `process_trial_result` operation took 2.019 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_db3cb1ad_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-40-46/wandb/run-20230719_024120-db3cb1ad\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb: Syncing run FSR_Trainable_db3cb1ad\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/db3cb1ad\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb:                      mae ▇▇█▇▆▇▇▆▆▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb:                     mape █▇▇▆▅▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb:                     rmse █▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb:         time_this_iter_s █▄▆▄▄▆▅▇▄▃▃▃▃▄▃▄▅▄▄▃▃▃▂▃▁▄▆▆▅▅▅▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb:                      mae 0.34523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb:                     mape 0.09679\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb:                     rmse 0.66061\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb:       time_since_restore 32.06296\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb:         time_this_iter_s 1.16154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb:             time_total_s 32.06296\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb:                timestamp 1689702082\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: 🚀 View run FSR_Trainable_bef25036 at: https://wandb.ai/seokjin/FSR-prediction/runs/bef25036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384520)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024053-bef25036/logs\n",
      "2023-07-19 02:41:29,691\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.263 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:41:29,692\tWARNING util.py:315 -- The `process_trial_result` operation took 2.265 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:41:29,696\tWARNING util.py:315 -- Processing trial results took 2.268 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:41:29,698\tWARNING util.py:315 -- The `process_trial_result` operation took 2.271 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_d9117331_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-41-13/wandb/run-20230719_024132-d9117331\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: Syncing run FSR_Trainable_d9117331\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d9117331\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb:                      mae 2.48042\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb:                     mape 0.40644\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb:                     rmse 2.96946\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb:       time_since_restore 0.72917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb:         time_this_iter_s 0.72917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb:             time_total_s 0.72917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb:                timestamp 1689702087\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: 🚀 View run FSR_Trainable_d9117331 at: https://wandb.ai/seokjin/FSR-prediction/runs/d9117331\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385003)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024132-d9117331/logs\n",
      "2023-07-19 02:41:41,062\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.273 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:41:41,069\tWARNING util.py:315 -- The `process_trial_result` operation took 2.281 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:41:41,070\tWARNING util.py:315 -- Processing trial results took 2.282 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:41:41,072\tWARNING util.py:315 -- The `process_trial_result` operation took 2.285 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_1adea1b7_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-41-26/wandb/run-20230719_024144-1adea1b7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Syncing run FSR_Trainable_1adea1b7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1adea1b7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb:                      mae █▆▄▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb:                     mape █▇▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb:                     rmse █▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb:         time_this_iter_s █▃▂▃▁▂▂▃▄▅▄▄▃▄▅▄▅▅▆▃▄▄▃▄▄▃▂▂▅▄▅▂▂▅▃▂▃▂▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb:                      mae 0.34535\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb:                     mape 0.09627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb:                     rmse 0.6557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb:       time_since_restore 72.90694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb:         time_this_iter_s 0.86771\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb:             time_total_s 72.90694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb:                timestamp 1689702103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: 🚀 View run FSR_Trainable_dec4e142 at: https://wandb.ai/seokjin/FSR-prediction/runs/dec4e142\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=383906)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024021-dec4e142/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb:       training_iteration ▁\n",
      "2023-07-19 02:41:51,544\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.962 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:41:51,547\tWARNING util.py:315 -- The `process_trial_result` operation took 1.966 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:41:51,548\tWARNING util.py:315 -- Processing trial results took 1.967 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:41:51,549\tWARNING util.py:315 -- The `process_trial_result` operation took 1.968 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb:                      mae █▆█▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb:                     mape █▅▇▇▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb:                     rmse █▆▆▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb:         time_this_iter_s █▆█▆▆▇▄▃▃▃▅▅▅▄▅▃▄▃▃▃▃▆█▆▄▁▂▃▂▁▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb:                      mae 0.3434\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb:                     mape 0.09627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb:                     rmse 0.66206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb:       time_since_restore 28.87789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb:         time_this_iter_s 0.83935\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb:             time_total_s 28.87789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb:                timestamp 1689702109\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_aeafd544_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-41-38/wandb/run-20230719_024153-aeafd544\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Syncing run FSR_Trainable_aeafd544\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/aeafd544\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb: 🚀 View run FSR_Trainable_db3cb1ad at: https://wandb.ai/seokjin/FSR-prediction/runs/db3cb1ad\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=384774)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024120-db3cb1ad/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2023-07-19 02:42:00,223\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.989 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:42:00,228\tWARNING util.py:315 -- The `process_trial_result` operation took 1.994 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:42:00,230\tWARNING util.py:315 -- Processing trial results took 1.996 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:42:00,233\tWARNING util.py:315 -- The `process_trial_result` operation took 1.999 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_eac9ef26_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-41-48/wandb/run-20230719_024202-eac9ef26\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: Syncing run FSR_Trainable_eac9ef26\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/eac9ef26\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:42:06,560\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.643 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:42:06,562\tWARNING util.py:315 -- The `process_trial_result` operation took 1.646 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:42:06,564\tWARNING util.py:315 -- Processing trial results took 1.647 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:42:06,565\tWARNING util.py:315 -- The `process_trial_result` operation took 1.648 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb:                      mae 0.41097\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb:                     mape 0.11748\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb:                     rmse 0.71517\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb:       time_since_restore 0.91845\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb:         time_this_iter_s 0.91845\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb:             time_total_s 0.91845\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb:                timestamp 1689702118\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: 🚀 View run FSR_Trainable_eac9ef26 at: https://wandb.ai/seokjin/FSR-prediction/runs/eac9ef26\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385698)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024202-eac9ef26/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_5c949558_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-41-57/wandb/run-20230719_024209-5c949558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: Syncing run FSR_Trainable_5c949558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5c949558\n",
      "2023-07-19 02:42:16,121\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.993 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:42:16,124\tWARNING util.py:315 -- The `process_trial_result` operation took 1.998 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:42:16,126\tWARNING util.py:315 -- Processing trial results took 2.000 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:42:16,128\tWARNING util.py:315 -- The `process_trial_result` operation took 2.003 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_bdc8e398_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-42-03/wandb/run-20230719_024219-bdc8e398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: Syncing run FSR_Trainable_bdc8e398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/bdc8e398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:42:23,666\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.785 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:42:23,668\tWARNING util.py:315 -- The `process_trial_result` operation took 1.788 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:42:23,670\tWARNING util.py:315 -- Processing trial results took 1.790 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:42:23,672\tWARNING util.py:315 -- The `process_trial_result` operation took 1.792 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb:                      mae 0.35194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb:                     mape 0.10415\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb:                     rmse 0.69242\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb:       time_since_restore 2.13562\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb:         time_this_iter_s 0.98662\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb:             time_total_s 2.13562\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb:                timestamp 1689702137\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: 🚀 View run FSR_Trainable_bdc8e398 at: https://wandb.ai/seokjin/FSR-prediction/runs/bdc8e398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386099)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024219-bdc8e398/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_40f53e56_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-42-13/wandb/run-20230719_024227-40f53e56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: Syncing run FSR_Trainable_40f53e56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/40f53e56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb:                      mae 0.34527\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb:                     mape 0.10288\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb:                     rmse 0.69191\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb:       time_since_restore 2.35163\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb:         time_this_iter_s 1.07225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb:             time_total_s 2.35163\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb:                timestamp 1689702144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: 🚀 View run FSR_Trainable_40f53e56 at: https://wandb.ai/seokjin/FSR-prediction/runs/40f53e56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386276)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024227-40f53e56/logs\n",
      "2023-07-19 02:42:33,516\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.032 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:42:33,519\tWARNING util.py:315 -- The `process_trial_result` operation took 2.036 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:42:33,522\tWARNING util.py:315 -- Processing trial results took 2.039 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:42:33,524\tWARNING util.py:315 -- The `process_trial_result` operation took 2.041 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_61069d94_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-42-20/wandb/run-20230719_024236-61069d94\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: Syncing run FSR_Trainable_61069d94\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/61069d94\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb:                      mae 0.39054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb:                     mape 0.1076\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb:                     rmse 0.73569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb:       time_since_restore 0.83818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb:         time_this_iter_s 0.83818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb:             time_total_s 0.83818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb:                timestamp 1689702151\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: 🚀 View run FSR_Trainable_61069d94 at: https://wandb.ai/seokjin/FSR-prediction/runs/61069d94\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386508)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024236-61069d94/logs\n",
      "2023-07-19 02:42:43,334\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.182 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:42:43,337\tWARNING util.py:315 -- The `process_trial_result` operation took 2.185 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:42:43,339\tWARNING util.py:315 -- Processing trial results took 2.187 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:42:43,340\tWARNING util.py:315 -- The `process_trial_result` operation took 2.188 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_3a991208_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-42-30/wandb/run-20230719_024246-3a991208\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: Syncing run FSR_Trainable_3a991208\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3a991208\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb:                      mae 0.4082\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb:                     mape 0.11512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb:                     rmse 0.74558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb:       time_since_restore 0.63696\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb:         time_this_iter_s 0.63696\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb:             time_total_s 0.63696\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb:                timestamp 1689702161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: 🚀 View run FSR_Trainable_3a991208 at: https://wandb.ai/seokjin/FSR-prediction/runs/3a991208\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386733)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024246-3a991208/logs\n",
      "2023-07-19 02:42:53,010\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.066 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:42:53,013\tWARNING util.py:315 -- The `process_trial_result` operation took 2.069 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:42:53,015\tWARNING util.py:315 -- Processing trial results took 2.071 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:42:53,016\tWARNING util.py:315 -- The `process_trial_result` operation took 2.072 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_b1620c1c_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-42-40/wandb/run-20230719_024255-b1620c1c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Syncing run FSR_Trainable_b1620c1c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b1620c1c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb:                      mae █▆▄▄▃▃▂▂▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb:                     mape █▇▆▆▅▄▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb:                     rmse █▇▅▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb:         time_this_iter_s █▂▂▂▁▂▁▂▂▄▄▃▂▂▄▄▄▁▁▄▃▁▂▄▃▂▃▂▂▂▁▄▂▂▂▂▄▂▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb:                      mae 0.34293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb:                     mape 0.09582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb:                     rmse 0.65637\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb:       time_since_restore 37.53301\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb:         time_this_iter_s 0.74084\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb:             time_total_s 37.53301\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb:                timestamp 1689702173\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: 🚀 View run FSR_Trainable_5c949558 at: https://wandb.ai/seokjin/FSR-prediction/runs/5c949558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=385879)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024209-5c949558/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb:       training_iteration ▁█\n",
      "2023-07-19 02:43:01,817\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.859 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:43:01,821\tWARNING util.py:315 -- The `process_trial_result` operation took 1.865 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:43:01,823\tWARNING util.py:315 -- Processing trial results took 1.866 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:43:01,824\tWARNING util.py:315 -- The `process_trial_result` operation took 1.868 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=386959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_597ecf06_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-42-50/wandb/run-20230719_024304-597ecf06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: Syncing run FSR_Trainable_597ecf06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/597ecf06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb:                      mae 0.48307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb:                     mape 0.12772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb:                     rmse 0.78377\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb:       time_since_restore 0.97991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb:         time_this_iter_s 0.97991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb:             time_total_s 0.97991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb:                timestamp 1689702179\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: 🚀 View run FSR_Trainable_597ecf06 at: https://wandb.ai/seokjin/FSR-prediction/runs/597ecf06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024304-597ecf06/logs\n",
      "2023-07-19 02:43:10,050\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.058 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:43:10,056\tWARNING util.py:315 -- The `process_trial_result` operation took 2.065 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:43:10,060\tWARNING util.py:315 -- Processing trial results took 2.069 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:43:10,062\tWARNING util.py:315 -- The `process_trial_result` operation took 2.071 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387192)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_49d36a14_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-42-59/wandb/run-20230719_024312-49d36a14\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: Syncing run FSR_Trainable_49d36a14\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/49d36a14\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:43:16,389\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.866 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:43:16,395\tWARNING util.py:315 -- The `process_trial_result` operation took 1.873 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:43:16,396\tWARNING util.py:315 -- Processing trial results took 1.874 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:43:16,397\tWARNING util.py:315 -- The `process_trial_result` operation took 1.876 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb:                      mae 0.51249\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb:                     mape 0.12918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb:                     rmse 0.88364\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb:       time_since_restore 0.8527\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb:         time_this_iter_s 0.8527\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb:             time_total_s 0.8527\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb:                timestamp 1689702187\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: 🚀 View run FSR_Trainable_49d36a14 at: https://wandb.ai/seokjin/FSR-prediction/runs/49d36a14\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024312-49d36a14/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387411)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_dba85aa4_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-43-07/wandb/run-20230719_024318-dba85aa4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: Syncing run FSR_Trainable_dba85aa4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/dba85aa4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:43:23,050\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.992 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:43:23,055\tWARNING util.py:315 -- The `process_trial_result` operation took 1.997 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:43:23,056\tWARNING util.py:315 -- Processing trial results took 1.998 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:43:23,057\tWARNING util.py:315 -- The `process_trial_result` operation took 1.999 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb:                      mae 0.35151\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb:                     mape 0.09949\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb:                     rmse 0.69127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb:       time_since_restore 1.80924\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb:         time_this_iter_s 0.67041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb:             time_total_s 1.80924\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb:                timestamp 1689702197\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: 🚀 View run FSR_Trainable_dba85aa4 at: https://wandb.ai/seokjin/FSR-prediction/runs/dba85aa4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387589)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024318-dba85aa4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_431d77fc_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-43-13/wandb/run-20230719_024325-431d77fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: Syncing run FSR_Trainable_431d77fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/431d77fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:43:29,517\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.698 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:43:29,520\tWARNING util.py:315 -- The `process_trial_result` operation took 1.703 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:43:29,523\tWARNING util.py:315 -- Processing trial results took 1.705 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:43:29,525\tWARNING util.py:315 -- The `process_trial_result` operation took 1.707 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb:                      mae 0.35678\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb:                     mape 0.1035\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb:                     rmse 0.69297\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb:       time_since_restore 1.9429\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb:         time_this_iter_s 0.7949\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb:             time_total_s 1.9429\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb:                timestamp 1689702203\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: 🚀 View run FSR_Trainable_431d77fc at: https://wandb.ai/seokjin/FSR-prediction/runs/431d77fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387774)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024325-431d77fc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_6835396b_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-43-19/wandb/run-20230719_024332-6835396b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Syncing run FSR_Trainable_6835396b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6835396b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=387940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024332-6835396b/logs\n",
      "2023-07-19 02:43:38,263\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.071 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:43:38,267\tWARNING util.py:315 -- The `process_trial_result` operation took 2.076 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:43:38,269\tWARNING util.py:315 -- Processing trial results took 2.078 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:43:38,270\tWARNING util.py:315 -- The `process_trial_result` operation took 2.079 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_78aa2cbe_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-43-26/wandb/run-20230719_024340-78aa2cbe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: Syncing run FSR_Trainable_78aa2cbe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/78aa2cbe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:43:44,355\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.693 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:43:44,363\tWARNING util.py:315 -- The `process_trial_result` operation took 1.703 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:43:44,366\tWARNING util.py:315 -- Processing trial results took 1.705 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:43:44,369\tWARNING util.py:315 -- The `process_trial_result` operation took 1.708 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb:                      mae 0.38551\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb:                     mape 0.10244\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb:                     rmse 0.71387\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb:       time_since_restore 1.06703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb:         time_this_iter_s 1.06703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb:             time_total_s 1.06703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb:                timestamp 1689702216\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: 🚀 View run FSR_Trainable_78aa2cbe at: https://wandb.ai/seokjin/FSR-prediction/runs/78aa2cbe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388182)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024340-78aa2cbe/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/36cddab1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/36cddab1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/36cddab1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/36cddab1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/36cddab1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/36cddab1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/36cddab1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/36cddab1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb:       training_iteration ▁█\n",
      "2023-07-19 02:43:52,860\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.846 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:43:52,864\tWARNING util.py:315 -- The `process_trial_result` operation took 1.850 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:43:52,866\tWARNING util.py:315 -- Processing trial results took 1.852 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:43:52,867\tWARNING util.py:315 -- The `process_trial_result` operation took 1.853 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388347)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_e51283d5_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-43-41/wandb/run-20230719_024355-e51283d5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: Syncing run FSR_Trainable_e51283d5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e51283d5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:43:59,446\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.687 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:43:59,448\tWARNING util.py:315 -- The `process_trial_result` operation took 1.690 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:43:59,450\tWARNING util.py:315 -- Processing trial results took 1.692 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:43:59,452\tWARNING util.py:315 -- The `process_trial_result` operation took 1.694 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb:                      mae 0.34288\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb:                     mape 0.10156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb:                     rmse 0.69529\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb:       time_since_restore 2.04267\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb:         time_this_iter_s 0.73436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb:             time_total_s 2.04267\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb:                timestamp 1689702233\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: 🚀 View run FSR_Trainable_e51283d5 at: https://wandb.ai/seokjin/FSR-prediction/runs/e51283d5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388591)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024355-e51283d5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_3448c238_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-43-49/wandb/run-20230719_024401-3448c238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: Syncing run FSR_Trainable_3448c238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3448c238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb:                      mae 0.37997\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb:                     mape 0.1055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb:                     rmse 0.70505\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb:       time_since_restore 1.64528\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb:         time_this_iter_s 0.57349\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb:             time_total_s 1.64528\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb:                timestamp 1689702240\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: 🚀 View run FSR_Trainable_3448c238 at: https://wandb.ai/seokjin/FSR-prediction/runs/3448c238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388772)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024401-3448c238/logs\n",
      "2023-07-19 02:44:07,804\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.990 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:44:07,808\tWARNING util.py:315 -- The `process_trial_result` operation took 1.995 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:44:07,810\tWARNING util.py:315 -- Processing trial results took 1.997 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:44:07,812\tWARNING util.py:315 -- The `process_trial_result` operation took 1.999 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_063e70f4_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-43-56/wandb/run-20230719_024410-063e70f4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: Syncing run FSR_Trainable_063e70f4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/063e70f4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:44:14,231\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.898 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:44:14,237\tWARNING util.py:315 -- The `process_trial_result` operation took 1.905 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:44:14,238\tWARNING util.py:315 -- Processing trial results took 1.907 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:44:14,240\tWARNING util.py:315 -- The `process_trial_result` operation took 1.908 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb:                      mae 0.37046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb:                     mape 0.10685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb:                     rmse 0.7029\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb:       time_since_restore 1.68498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb:         time_this_iter_s 0.65806\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb:             time_total_s 1.68498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb:                timestamp 1689702248\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: 🚀 View run FSR_Trainable_063e70f4 at: https://wandb.ai/seokjin/FSR-prediction/runs/063e70f4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=388995)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024410-063e70f4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024416-621022c5/logs\n",
      "2023-07-19 02:44:22,589\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.059 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:44:22,593\tWARNING util.py:315 -- The `process_trial_result` operation took 2.064 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:44:22,595\tWARNING util.py:315 -- Processing trial results took 2.066 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:44:22,597\tWARNING util.py:315 -- The `process_trial_result` operation took 2.068 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_8795d3e2_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-44-11/wandb/run-20230719_024424-8795d3e2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: Syncing run FSR_Trainable_8795d3e2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8795d3e2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:44:29,157\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.604 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:44:29,158\tWARNING util.py:315 -- The `process_trial_result` operation took 1.606 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:44:29,167\tWARNING util.py:315 -- Processing trial results took 1.615 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:44:29,172\tWARNING util.py:315 -- The `process_trial_result` operation took 1.621 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb:                      mae 0.35303\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb:                     mape 0.10597\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb:                     rmse 0.69981\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb:       time_since_restore 1.63872\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb:         time_this_iter_s 0.63734\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb:             time_total_s 1.63872\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb:                timestamp 1689702263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: 🚀 View run FSR_Trainable_8795d3e2 at: https://wandb.ai/seokjin/FSR-prediction/runs/8795d3e2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024424-8795d3e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389401)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_b8149bd6_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-44-19/wandb/run-20230719_024431-b8149bd6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: Syncing run FSR_Trainable_b8149bd6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b8149bd6\n",
      "2023-07-19 02:44:38,927\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.134 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:44:38,930\tWARNING util.py:315 -- The `process_trial_result` operation took 2.138 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:44:38,932\tWARNING util.py:315 -- Processing trial results took 2.140 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:44:38,935\tWARNING util.py:315 -- The `process_trial_result` operation took 2.143 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_56248280_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-44-26/wandb/run-20230719_024441-56248280\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: Syncing run FSR_Trainable_56248280\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/56248280\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:44:46,205\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.827 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:44:46,209\tWARNING util.py:315 -- The `process_trial_result` operation took 1.832 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:44:46,210\tWARNING util.py:315 -- Processing trial results took 1.833 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:44:46,211\tWARNING util.py:315 -- The `process_trial_result` operation took 1.834 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb:                      mae 0.42117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb:                     mape 0.1137\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb:                     rmse 0.7683\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb:       time_since_restore 1.05091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb:         time_this_iter_s 1.05091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb:             time_total_s 1.05091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb:                timestamp 1689702276\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: 🚀 View run FSR_Trainable_56248280 at: https://wandb.ai/seokjin/FSR-prediction/runs/56248280\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389801)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024441-56248280/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_4de2513e_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-44-35/wandb/run-20230719_024448-4de2513e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: Syncing run FSR_Trainable_4de2513e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4de2513e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb:                      mae 0.42527\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb:                     mape 0.12305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb:                     rmse 0.78575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb:       time_since_restore 1.10641\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb:         time_this_iter_s 1.10641\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb:             time_total_s 1.10641\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb:                timestamp 1689702284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: 🚀 View run FSR_Trainable_4de2513e at: https://wandb.ai/seokjin/FSR-prediction/runs/4de2513e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389978)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024448-4de2513e/logs\n",
      "2023-07-19 02:44:56,083\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.361 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:44:56,085\tWARNING util.py:315 -- The `process_trial_result` operation took 2.365 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:44:56,087\tWARNING util.py:315 -- Processing trial results took 2.366 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:44:56,089\tWARNING util.py:315 -- The `process_trial_result` operation took 2.369 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_b7ee8305_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-44-43/wandb/run-20230719_024459-b7ee8305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: Syncing run FSR_Trainable_b7ee8305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b7ee8305\n",
      "2023-07-19 02:45:07,665\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.143 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:45:07,670\tWARNING util.py:315 -- The `process_trial_result` operation took 2.149 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:45:07,672\tWARNING util.py:315 -- Processing trial results took 2.151 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:45:07,673\tWARNING util.py:315 -- The `process_trial_result` operation took 2.152 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_93bf26d0_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-44-52/wandb/run-20230719_024511-93bf26d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: Syncing run FSR_Trainable_93bf26d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/93bf26d0\n",
      "2023-07-19 02:45:20,280\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.136 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:45:20,283\tWARNING util.py:315 -- The `process_trial_result` operation took 2.140 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:45:20,285\tWARNING util.py:315 -- Processing trial results took 2.142 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:45:20,287\tWARNING util.py:315 -- The `process_trial_result` operation took 2.143 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_f8c708a6_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-45-04/wandb/run-20230719_024527-f8c708a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: Syncing run FSR_Trainable_f8c708a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f8c708a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb:                      mae █▅▇█▇▅▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb:                     mape █▅▄▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb:                     rmse █▆▃▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb:         time_this_iter_s █▃▂▁▂▂▂▅▃▂▂▄▃▂▂▂▁▂▄▄▄▃▃▄▄▆▆▅▄▄▄▆▅▄▄▇█▇▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb:                      mae 0.34394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb:                     mape 0.09736\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb:                     rmse 0.65835\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb:       time_since_restore 57.47035\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb:         time_this_iter_s 1.15178\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb:             time_total_s 57.47035\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb:                timestamp 1689702335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: 🚀 View run FSR_Trainable_b8149bd6 at: https://wandb.ai/seokjin/FSR-prediction/runs/b8149bd6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=389581)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024431-b8149bd6/logs\n",
      "2023-07-19 02:45:49,235\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.102 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:45:49,238\tWARNING util.py:315 -- The `process_trial_result` operation took 2.106 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:45:49,239\tWARNING util.py:315 -- Processing trial results took 2.107 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:45:49,240\tWARNING util.py:315 -- The `process_trial_result` operation took 2.107 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)06 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb:                      mae ▆▁▂▃▆▇████████▇▇▇▇▇█████████████▇▇▇▇▇▇▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb:                     mape ██▆▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb:                     rmse █▅▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb:         time_this_iter_s ▅▄▄▂▁▁▁▂▄▇▅▆▄▄▅▃▃▄▃▄▅▃▃█▆█▆▅▆▇▃▄▆▃▄▄▃▂▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb:                      mae 0.34437\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb:                     mape 0.09685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb:                     rmse 0.65649\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb:       time_since_restore 47.42329\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb:         time_this_iter_s 0.74304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb:             time_total_s 47.42329\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb:                timestamp 1689702350\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: 🚀 View run FSR_Trainable_b7ee8305 at: https://wandb.ai/seokjin/FSR-prediction/runs/b7ee8305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390201)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024459-b7ee8305/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "2023-07-19 02:46:02,465\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.076 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:46:02,468\tWARNING util.py:315 -- The `process_trial_result` operation took 2.080 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:46:02,469\tWARNING util.py:315 -- Processing trial results took 2.081 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:46:02,470\tWARNING util.py:315 -- The `process_trial_result` operation took 2.082 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb:                      mae █▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb:                     mape █▆▆▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb:                     rmse █▇▄▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb:         time_this_iter_s █▆▄▃▃▃▃▃▃▃▄▃▃▄▆▆▆▅▄▄▄▂▃▃▃▂▄▂▂▃▄▃▄▂▁▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb:                      mae 0.34458\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb:                     mape 0.09685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb:                     rmse 0.65736\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb:       time_since_restore 47.14715\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb:         time_this_iter_s 0.54227\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb:             time_total_s 47.14715\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb:                timestamp 1689702359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: 🚀 View run FSR_Trainable_93bf26d0 at: https://wandb.ai/seokjin/FSR-prediction/runs/93bf26d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390423)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024511-93bf26d0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_67300bae_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-45-17/wandb/run-20230719_024552-67300bae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Syncing run FSR_Trainable_67300bae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/67300bae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e264846f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024552-67300bae/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024552-67300bae/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024552-67300bae/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024552-67300bae/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024552-67300bae/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024552-67300bae/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024552-67300bae/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024552-67300bae/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024552-67300bae/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024552-67300bae/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024552-67300bae/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024552-67300bae/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024552-67300bae/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024552-67300bae/logs\n",
      "2023-07-19 02:46:13,833\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.173 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:46:13,836\tWARNING util.py:315 -- The `process_trial_result` operation took 2.177 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:46:13,839\tWARNING util.py:315 -- Processing trial results took 2.181 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:46:13,841\tWARNING util.py:315 -- The `process_trial_result` operation took 2.182 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_785536af_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-45-59/wandb/run-20230719_024616-785536af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: Syncing run FSR_Trainable_785536af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/785536af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb:                      mae 0.46259\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb:                     mape 0.12054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb:                     rmse 0.73495\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb:       time_since_restore 1.05145\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb:         time_this_iter_s 1.05145\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb:             time_total_s 1.05145\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb:                timestamp 1689702371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: 🚀 View run FSR_Trainable_785536af at: https://wandb.ai/seokjin/FSR-prediction/runs/785536af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391371)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024616-785536af/logs\n",
      "2023-07-19 02:46:24,938\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.075 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:46:24,942\tWARNING util.py:315 -- The `process_trial_result` operation took 2.081 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:46:24,944\tWARNING util.py:315 -- Processing trial results took 2.082 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:46:24,945\tWARNING util.py:315 -- The `process_trial_result` operation took 2.083 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_c7f330f8_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-46-10/wandb/run-20230719_024627-c7f330f8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: Syncing run FSR_Trainable_c7f330f8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c7f330f8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb:                      mae 0.4825\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb:                     mape 0.12451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb:                     rmse 0.76604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb:       time_since_restore 0.75647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb:         time_this_iter_s 0.75647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb:             time_total_s 0.75647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb:                timestamp 1689702382\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: 🚀 View run FSR_Trainable_c7f330f8 at: https://wandb.ai/seokjin/FSR-prediction/runs/c7f330f8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391596)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024627-c7f330f8/logs\n",
      "2023-07-19 02:46:35,975\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.992 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:46:35,976\tWARNING util.py:315 -- The `process_trial_result` operation took 1.994 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:46:35,978\tWARNING util.py:315 -- Processing trial results took 1.996 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:46:35,979\tWARNING util.py:315 -- The `process_trial_result` operation took 1.997 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_7a4ce919_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-46-22/wandb/run-20230719_024639-7a4ce919\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Syncing run FSR_Trainable_7a4ce919\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7a4ce919\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: / 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb:                      mae █▇▄▂▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb:                     mape █▆▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb:                     rmse █▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb:         time_this_iter_s ▇▅▃▃▇█▅▆▄▄▄▃▃▄▄▄▂▂▃▁▁▃▅▃▂▂█▄▄▁▂▁▃▂▃▁▂▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb:                      mae 0.34498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb:                     mape 0.09694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb:                     rmse 0.65601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb:       time_since_restore 67.69713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb:         time_this_iter_s 0.51674\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb:             time_total_s 67.69713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb:                timestamp 1689702399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: 🚀 View run FSR_Trainable_f8c708a6 at: https://wandb.ai/seokjin/FSR-prediction/runs/f8c708a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=390640)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024527-f8c708a6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb:       training_iteration ▁█\n",
      "2023-07-19 02:46:46,556\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.027 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:46:46,560\tWARNING util.py:315 -- The `process_trial_result` operation took 2.031 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:46:46,561\tWARNING util.py:315 -- Processing trial results took 2.033 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:46:46,563\tWARNING util.py:315 -- The `process_trial_result` operation took 2.034 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_1cc2942e_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-46-33/wandb/run-20230719_024649-1cc2942e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: Syncing run FSR_Trainable_1cc2942e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1cc2942e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391818)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb:                      mae █▄▇█▇▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb:                     mape █▆▅▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb:                     rmse █▆▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb:         time_this_iter_s █▅▅▃▄▂▄▃▃█▅▄▄▃▃▃▂▇▄▆▃▃▃▃▃▃▆█▃▄▃▁▁▃▄▅▄▄▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb:                      mae 0.34509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb:                     mape 0.09701\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb:                     rmse 0.65657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb:       time_since_restore 40.61918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb:         time_this_iter_s 0.643\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb:             time_total_s 40.61918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb:                timestamp 1689702411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: 🚀 View run FSR_Trainable_e264846f at: https://wandb.ai/seokjin/FSR-prediction/runs/e264846f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024605-e264846f/logs\n",
      "2023-07-19 02:46:57,687\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.763 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:46:57,689\tWARNING util.py:315 -- The `process_trial_result` operation took 1.766 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:46:57,692\tWARNING util.py:315 -- Processing trial results took 1.768 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:46:57,693\tWARNING util.py:315 -- The `process_trial_result` operation took 1.769 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=391114)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_5c455f49_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-46-43/wandb/run-20230719_024700-5c455f49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: Syncing run FSR_Trainable_5c455f49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5c455f49\n",
      "2023-07-19 02:47:09,081\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.238 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:47:09,084\tWARNING util.py:315 -- The `process_trial_result` operation took 2.243 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:47:09,085\tWARNING util.py:315 -- Processing trial results took 2.244 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:47:09,087\tWARNING util.py:315 -- The `process_trial_result` operation took 2.245 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_294a0cde_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-46-54/wandb/run-20230719_024712-294a0cde\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: Syncing run FSR_Trainable_294a0cde\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/294a0cde\n",
      "2023-07-19 02:47:21,496\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.012 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:47:21,499\tWARNING util.py:315 -- The `process_trial_result` operation took 2.016 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:47:21,503\tWARNING util.py:315 -- Processing trial results took 2.020 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:47:21,506\tWARNING util.py:315 -- The `process_trial_result` operation took 2.023 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_a64e431a_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-47-06/wandb/run-20230719_024724-a64e431a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Syncing run FSR_Trainable_a64e431a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a64e431a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb:                      mae ▄█▇▆▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb:                     mape █▆▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb:                     rmse █▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb:         time_this_iter_s ▄▃▄▂▂▄▁▂▂▄▄▃▂▄▂▁▂▁▂▁▂▁▂▃▂▂▃▄▃▃▂▂▃▅▅▂█▆▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb:                      mae 0.34544\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb:                     mape 0.09712\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb:                     rmse 0.65784\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb:       time_since_restore 47.20955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb:         time_this_iter_s 0.6846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb:             time_total_s 47.20955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb:                timestamp 1689702479\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: 🚀 View run FSR_Trainable_294a0cde at: https://wandb.ai/seokjin/FSR-prediction/runs/294a0cde\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392509)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024712-294a0cde/logs\n",
      "2023-07-19 02:48:16,765\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.770 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:48:16,769\tWARNING util.py:315 -- The `process_trial_result` operation took 2.775 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:48:16,771\tWARNING util.py:315 -- Processing trial results took 2.776 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:48:16,773\tWARNING util.py:315 -- The `process_trial_result` operation took 2.778 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_20b5734f_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-47-18/wandb/run-20230719_024821-20b5734f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Syncing run FSR_Trainable_20b5734f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/20b5734f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb:                      mae █▇▇▇▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb:                     mape ▃▁▄▅▇▇███████████▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb:                     rmse █▇▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb:         time_this_iter_s ▅▃▂▃▁▅▄▂▃▃▄▅▃▅▃▅▅▄▄▄▃▃▃▃▄▅▄▃▇█▄▆▃▄█▄▄▅▆▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb:                      mae 0.3447\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb:                     mape 0.09672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb:                     rmse 0.65603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb:       time_since_restore 83.77706\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb:         time_this_iter_s 0.85307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb:             time_total_s 83.77706\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb:                timestamp 1689702500\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: 🚀 View run FSR_Trainable_1cc2942e at: https://wandb.ai/seokjin/FSR-prediction/runs/1cc2942e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392053)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024649-1cc2942e/logs\n",
      "2023-07-19 02:48:36,139\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.687 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:48:36,143\tWARNING util.py:315 -- The `process_trial_result` operation took 1.691 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:48:36,145\tWARNING util.py:315 -- Processing trial results took 1.694 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:48:36,146\tWARNING util.py:315 -- The `process_trial_result` operation took 1.695 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_97704ed9_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-48-12/wandb/run-20230719_024839-97704ed9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: Syncing run FSR_Trainable_97704ed9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/97704ed9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb:                      mae 0.37422\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb:                     mape 0.10491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb:                     rmse 0.69882\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb:       time_since_restore 3.17806\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb:         time_this_iter_s 1.37206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb:             time_total_s 3.17806\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb:                timestamp 1689702517\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: 🚀 View run FSR_Trainable_97704ed9 at: https://wandb.ai/seokjin/FSR-prediction/runs/97704ed9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393238)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024839-97704ed9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb:                      mae █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb:                     mape █▅▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb:                     rmse █▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb:         time_this_iter_s ▅▅▃▅▂▂▃▂▂▁▂▃▄▄▂▃▄▂▅▂▂▅█▂▃▄▇▇▅▂▁▄▂▂▁▂▅▆▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "2023-07-19 02:48:51,547\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.886 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:48:51,550\tWARNING util.py:315 -- The `process_trial_result` operation took 1.889 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:48:51,551\tWARNING util.py:315 -- Processing trial results took 1.890 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:48:51,552\tWARNING util.py:315 -- The `process_trial_result` operation took 1.892 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_7ee5543c_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-48-32/wandb/run-20230719_024854-7ee5543c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Syncing run FSR_Trainable_7ee5543c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7ee5543c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb:                      mae ▁█▇▇▆▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb:                     mape █▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb:                     rmse █▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb:         time_this_iter_s ▇▃▂▁▃▄▃▃▃▄▃▄▂▃▂▃▃▄▃▄▆▇▃▄▇▄▄█▇▃▄▃▃▃▄▄▄▂▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb:                      mae 0.34515\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb:                     mape 0.097\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb:                     rmse 0.65606\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb:       time_since_restore 102.29079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb:         time_this_iter_s 1.08566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb:             time_total_s 102.29079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb:                timestamp 1689702529\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: 🚀 View run FSR_Trainable_5c455f49 at: https://wandb.ai/seokjin/FSR-prediction/runs/5c455f49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024700-5c455f49/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=392288)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024854-7ee5543c/logs\n",
      "2023-07-19 02:49:02,144\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.785 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:49:02,146\tWARNING util.py:315 -- The `process_trial_result` operation took 1.788 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:49:02,149\tWARNING util.py:315 -- Processing trial results took 1.792 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:49:02,150\tWARNING util.py:315 -- The `process_trial_result` operation took 1.793 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393485)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_10611099_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-48-48/wandb/run-20230719_024905-10611099\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: Syncing run FSR_Trainable_10611099\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/10611099\n",
      "2023-07-19 02:49:13,430\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.896 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:49:13,433\tWARNING util.py:315 -- The `process_trial_result` operation took 1.899 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:49:13,435\tWARNING util.py:315 -- Processing trial results took 1.902 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:49:13,436\tWARNING util.py:315 -- The `process_trial_result` operation took 1.903 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_5d250a68_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-48-59/wandb/run-20230719_024916-5d250a68\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: Syncing run FSR_Trainable_5d250a68\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5d250a68\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb:                      mae 0.41301\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb:                     mape 0.10464\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb:                     rmse 0.78485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb:       time_since_restore 1.09715\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb:         time_this_iter_s 1.09715\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb:             time_total_s 1.09715\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb:                timestamp 1689702551\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: 🚀 View run FSR_Trainable_5d250a68 at: https://wandb.ai/seokjin/FSR-prediction/runs/5d250a68\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393937)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024916-5d250a68/logs\n",
      "2023-07-19 02:49:25,109\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.115 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:49:25,113\tWARNING util.py:315 -- The `process_trial_result` operation took 2.119 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:49:25,114\tWARNING util.py:315 -- Processing trial results took 2.121 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:49:25,116\tWARNING util.py:315 -- The `process_trial_result` operation took 2.122 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_0c5f1635_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-49-10/wandb/run-20230719_024928-0c5f1635\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: Syncing run FSR_Trainable_0c5f1635\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0c5f1635\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb:                      mae 0.38659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb:                     mape 0.09638\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb:                     rmse 0.76534\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb:       time_since_restore 1.09936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb:         time_this_iter_s 1.09936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb:             time_total_s 1.09936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb:                timestamp 1689702562\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: 🚀 View run FSR_Trainable_0c5f1635 at: https://wandb.ai/seokjin/FSR-prediction/runs/0c5f1635\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394157)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024928-0c5f1635/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:49:36,127\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.120 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:49:36,134\tWARNING util.py:315 -- The `process_trial_result` operation took 2.127 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:49:36,136\tWARNING util.py:315 -- Processing trial results took 2.129 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:49:36,137\tWARNING util.py:315 -- The `process_trial_result` operation took 2.130 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb:                      mae █▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb:                     mape ██▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb:                     rmse █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb:         time_this_iter_s ██▃▃▃▃▃▃▃▅▆▄▃▂▂▂▂▃▂▂▂▁▁▆▃▃▃▃▂▅▄▂▂▃▂▆▄▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024821-20b5734f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393000)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_9cbfaa57_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-49-21/wandb/run-20230719_024938-9cbfaa57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: Syncing run FSR_Trainable_9cbfaa57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9cbfaa57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb:                      mae 0.38204\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb:                     mape 0.11738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb:                     rmse 0.70927\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb:       time_since_restore 0.86073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb:         time_this_iter_s 0.86073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb:             time_total_s 0.86073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb:                timestamp 1689702573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: 🚀 View run FSR_Trainable_9cbfaa57 at: https://wandb.ai/seokjin/FSR-prediction/runs/9cbfaa57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024938-9cbfaa57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394391)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 02:49:46,102\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.031 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:49:46,103\tWARNING util.py:315 -- The `process_trial_result` operation took 2.034 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:49:46,107\tWARNING util.py:315 -- Processing trial results took 2.037 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:49:46,108\tWARNING util.py:315 -- The `process_trial_result` operation took 2.038 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_8ccec9f9_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-49-33/wandb/run-20230719_024948-8ccec9f9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: Syncing run FSR_Trainable_8ccec9f9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8ccec9f9\n",
      "2023-07-19 02:49:57,565\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.143 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:49:57,566\tWARNING util.py:315 -- The `process_trial_result` operation took 2.145 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:49:57,568\tWARNING util.py:315 -- Processing trial results took 2.147 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:49:57,569\tWARNING util.py:315 -- The `process_trial_result` operation took 2.148 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_a0692d97_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-49-43/wandb/run-20230719_025000-a0692d97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Syncing run FSR_Trainable_a0692d97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a0692d97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:50:11,550\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.069 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:50:11,552\tWARNING util.py:315 -- The `process_trial_result` operation took 2.072 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:50:11,553\tWARNING util.py:315 -- Processing trial results took 2.073 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:50:11,554\tWARNING util.py:315 -- The `process_trial_result` operation took 2.074 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb:                      mae ▁▆██▇▆▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb:                     mape █▅▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb:                     rmse █▆▅▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb:         time_this_iter_s █▅▄▃▄▃▃▆▅▄▃▃▃▅▆▄▃▃▄▂▅▄▃▃▃▁▂▃▄▄▃▃▃▂█▆▄▄█▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb:                      mae 0.34478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb:                     mape 0.09684\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb:                     rmse 0.65647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb:       time_since_restore 56.51655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb:         time_this_iter_s 1.05669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb:             time_total_s 56.51655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb:                timestamp 1689702607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: 🚀 View run FSR_Trainable_10611099 at: https://wandb.ai/seokjin/FSR-prediction/runs/10611099\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=393717)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024905-10611099/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_715970c4_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-49-54/wandb/run-20230719_025015-715970c4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: Syncing run FSR_Trainable_715970c4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/715970c4\n",
      "2023-07-19 02:50:25,014\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.077 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:50:25,017\tWARNING util.py:315 -- The `process_trial_result` operation took 2.081 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:50:25,018\tWARNING util.py:315 -- Processing trial results took 2.082 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:50:25,018\tWARNING util.py:315 -- The `process_trial_result` operation took 2.083 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_a9b7fd49_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-50-08/wandb/run-20230719_025028-a9b7fd49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: Syncing run FSR_Trainable_a9b7fd49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9b7fd49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb:                      mae █▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb:                     mape █▆▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb:                     rmse █▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb:         time_this_iter_s ▃▂▂▂▂▂▁▂█▃▃▃▅▄▃▄▇▄▂▄▂▁▃▃▃▂▂▁▂▂▂▁▂▂▂▁▂▁▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb:                      mae 0.34541\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb:                     mape 0.09711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb:                     rmse 0.65717\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb:       time_since_restore 52.36666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb:         time_this_iter_s 0.89533\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb:             time_total_s 52.36666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb:                timestamp 1689702645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: 🚀 View run FSR_Trainable_8ccec9f9 at: https://wandb.ai/seokjin/FSR-prediction/runs/8ccec9f9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394624)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_024948-8ccec9f9/logs\n",
      "2023-07-19 02:51:00,105\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.988 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:51:00,109\tWARNING util.py:315 -- The `process_trial_result` operation took 1.992 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:51:00,111\tWARNING util.py:315 -- Processing trial results took 1.994 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:51:00,112\tWARNING util.py:315 -- The `process_trial_result` operation took 1.995 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_25190e32_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-50-21/wandb/run-20230719_025103-25190e32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb: Syncing run FSR_Trainable_25190e32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/25190e32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb:                      mae █▆▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb:                     mape █▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb:                     rmse █▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb:         time_this_iter_s █▇▅▂▃▂▂▄▃▃▃▄▃▂▂▂▁▂▂▂▂▂▁▃▂▂▂▄▂▂▂▂▂▃▃▃▃▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb:                      mae 0.34452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb:                     mape 0.09676\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb:                     rmse 0.65651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb:       time_since_restore 46.64518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb:         time_this_iter_s 0.77754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb:             time_total_s 46.64518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb:                timestamp 1689702662\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: 🚀 View run FSR_Trainable_715970c4 at: https://wandb.ai/seokjin/FSR-prediction/runs/715970c4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395063)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_025015-715970c4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:51:19,609\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.403 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:51:19,614\tWARNING util.py:315 -- The `process_trial_result` operation took 2.410 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:51:19,616\tWARNING util.py:315 -- Processing trial results took 2.412 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:51:19,618\tWARNING util.py:315 -- The `process_trial_result` operation took 2.414 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb:                      mae ▇█▆▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb:                     mape █▇▅▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb:                     rmse █▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb:         time_this_iter_s █▅▅▄▃▂▂▂▁▃▂▂▂▂▂▃▁▂▄▂▃▃▂▂▂▃▃█▃▄▃▂▅▅▅▄▃▆▆▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb:                      mae 0.34477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb:                     mape 0.09692\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb:                     rmse 0.65657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb:       time_since_restore 47.95613\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb:         time_this_iter_s 0.7956\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb:             time_total_s 47.95613\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb:                timestamp 1689702675\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: 🚀 View run FSR_Trainable_a9b7fd49 at: https://wandb.ai/seokjin/FSR-prediction/runs/a9b7fd49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395290)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_025028-a9b7fd49/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_28a0fe27_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-50-57/wandb/run-20230719_025123-28a0fe27\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: Syncing run FSR_Trainable_28a0fe27\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/28a0fe27\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb:                      mae 0.40599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb:                     mape 0.1198\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb:                     rmse 0.71935\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb:       time_since_restore 1.03575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb:         time_this_iter_s 1.03575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb:             time_total_s 1.03575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb:                timestamp 1689702677\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: 🚀 View run FSR_Trainable_28a0fe27 at: https://wandb.ai/seokjin/FSR-prediction/runs/28a0fe27\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395790)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_025123-28a0fe27/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_025123-28a0fe27/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb:                      mae ▆█▆▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb:                     mape █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb:                     rmse █▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb:         time_this_iter_s █▄▂▆▄▄▆▂▃▂▃▄▃▃▁▁▁▂▁▁▃▁▂▂▁▂▃▁▂▃▂▄▄▃▆▅▄▄▅▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "2023-07-19 02:51:33,317\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.509 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:51:33,323\tWARNING util.py:315 -- The `process_trial_result` operation took 2.516 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:51:33,324\tWARNING util.py:315 -- Processing trial results took 2.518 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:51:33,326\tWARNING util.py:315 -- The `process_trial_result` operation took 2.519 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=394842)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_e5963266_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_02-51-16/wandb/run-20230719_025136-e5963266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: Syncing run FSR_Trainable_e5963266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e5963266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb:                      mae 0.37811\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb:                     mape 0.10465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb:                     rmse 0.70906\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb:       time_since_restore 1.07522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb:         time_this_iter_s 1.07522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb:             time_total_s 1.07522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb:                timestamp 1689702690\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: 🚀 View run FSR_Trainable_e5963266 at: https://wandb.ai/seokjin/FSR-prediction/runs/e5963266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_025136-e5963266/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396024)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 02:51:44,585\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.162 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:51:44,588\tWARNING util.py:315 -- The `process_trial_result` operation took 2.165 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:51:44,589\tWARNING util.py:315 -- Processing trial results took 2.166 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:51:44,591\tWARNING util.py:315 -- The `process_trial_result` operation took 2.168 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_02-32-25/FSR_Trainable_b093e976_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_Simple_2023-07-19_02-51-29/wandb/run-20230719_025147-b093e976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: Syncing run FSR_Trainable_b093e976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b093e976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb:                      mae 0.42266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb:                     mape 0.12348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb:                     rmse 0.72589\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb:       time_since_restore 0.71716\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb:         time_this_iter_s 0.71716\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb:             time_total_s 0.71716\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb:                timestamp 1689702702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: 🚀 View run FSR_Trainable_b093e976 at: https://wandb.ai/seokjin/FSR-prediction/runs/b093e976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=396254)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_025147-b093e976/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb:                      mae █▇▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb:                     mape █▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb:                     rmse █▆▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇█████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb:         time_this_iter_s ▅▄▄▄▅▅▆▅▄█▅▅▆▆▇▅▄▂▃▃█▄▃▄▄▅▂▂▃▂▃▂▃▁▁▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇█████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇█████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=395549)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "2023-07-19 02:51:58,048\tINFO tune.py:1111 -- Total run time: 1168.81 seconds (1164.82 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
