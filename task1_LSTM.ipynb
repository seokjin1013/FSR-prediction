{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1_LSTM\n",
    "\n",
    "Index_X = FSR_for_force, FSR_for_coord\n",
    "\n",
    "Index_y = force, x_coord, y_coord\n",
    "\n",
    "Data = Splited by Time\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-08-10_18-18-32/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-08-10_18-18-32\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "192.334\n",
    "\n",
    "0.901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.LSTM'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    imputer = trial.suggest_categorical('imputer', ['sklearn.impute.SimpleImputer'])\n",
    "    if imputer == 'sklearn.impute.SimpleImputer':\n",
    "        trial.suggest_categorical('imputer_args/strategy', [\n",
    "            'mean',\n",
    "            'median',\n",
    "        ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': ['FSR_for_force', 'FSR_for_coord'],\n",
    "        'index_y': ['force', 'x_coord', 'y_coord'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_time'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-10 18:18:32,477] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='metric',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='metric',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='metric',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 18:18:34,753\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-08-10 18:18:36,230\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-08-10 18:59:13</td></tr>\n",
       "<tr><td>Running for: </td><td>00:40:36.73        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.2/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=97<br>Bracket: Iter 64.000: -0.6083692032100032 | Iter 32.000: -0.6325719777203163 | Iter 16.000: -0.6428042613236653 | Iter 8.000: -0.6594259677638074 | Iter 4.000: -0.6889697883779303 | Iter 2.000: -0.7370672870577141 | Iter 1.000: -0.8336430102204191<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 3<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_3d8c1dc7</td><td style=\"text-align: right;\">           1</td><td>/home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_3d8c1dc7_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-20-39/error.txt</td></tr>\n",
       "<tr><td>FSR_Trainable_b346be41</td><td style=\"text-align: right;\">           1</td><td>/home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_b346be41_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-26-19/error.txt</td></tr>\n",
       "<tr><td>FSR_Trainable_1a76a8a6</td><td style=\"text-align: right;\">           1</td><td>/home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_1a76a8a6_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-35-52/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>imputer             </th><th>imputer_args/strateg\n",
       "y       </th><th>index_X             </th><th>index_y             </th><th>model         </th><th style=\"text-align: right;\">    model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">  model_args/num_layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  tmae_force</th><th style=\"text-align: right;\">  trmse_force</th><th style=\"text-align: right;\">  tmape_force</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_e8fa8ea1</td><td>TERMINATED</td><td>172.26.215.93:101015</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_fcc0</td><td>[&#x27;force&#x27;, &#x27;x_co_fd00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        1.54376e-05</td><td>sklearn.preproc_cd50</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        25.4343 </td><td style=\"text-align: right;\">   10.9851  </td><td style=\"text-align: right;\">    13.5668  </td><td style=\"text-align: right;\">  8.49732e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_d30b0eaa</td><td>TERMINATED</td><td>172.26.215.93:101087</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_3f40</td><td>[&#x27;force&#x27;, &#x27;x_co_9840</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        7.08909e-05</td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       312.1    </td><td style=\"text-align: right;\">    0.740036</td><td style=\"text-align: right;\">     0.459044</td><td style=\"text-align: right;\">  8.54295e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_d9fc69c2</td><td>TERMINATED</td><td>172.26.215.93:101262</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_e5c0</td><td>[&#x27;force&#x27;, &#x27;x_co_cc00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0356847  </td><td>sklearn.preproc_cab0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       138.069  </td><td style=\"text-align: right;\">    2.89824 </td><td style=\"text-align: right;\">     1.8713  </td><td style=\"text-align: right;\"> 19.5005     </td></tr>\n",
       "<tr><td>FSR_Trainable_ca06df15</td><td>TERMINATED</td><td>172.26.215.93:101462</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_f440</td><td>[&#x27;force&#x27;, &#x27;x_co_dec0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000789201</td><td>sklearn.preproc_cab0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        12.6181 </td><td style=\"text-align: right;\">    3.00019 </td><td style=\"text-align: right;\">     2.09734 </td><td style=\"text-align: right;\"> 11.5747     </td></tr>\n",
       "<tr><td>FSR_Trainable_8043f276</td><td>TERMINATED</td><td>172.26.215.93:101764</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_0300</td><td>[&#x27;force&#x27;, &#x27;x_co_0440</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0176386  </td><td>sklearn.preproc_cd50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.8373 </td><td style=\"text-align: right;\">   10.806   </td><td style=\"text-align: right;\">    12.4853  </td><td style=\"text-align: right;\">  6.69929e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_d7c194e1</td><td>TERMINATED</td><td>172.26.215.93:102023</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_0240</td><td>[&#x27;force&#x27;, &#x27;x_co_c580</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00019911 </td><td>sklearn.preproc_cd50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.37264</td><td style=\"text-align: right;\">   10.8772  </td><td style=\"text-align: right;\">    13.5466  </td><td style=\"text-align: right;\">  3.9004e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_d9c86f35</td><td>TERMINATED</td><td>172.26.215.93:102230</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_b140</td><td>[&#x27;force&#x27;, &#x27;x_co_bb40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000103746</td><td>sklearn.preproc_cab0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.15327</td><td style=\"text-align: right;\">    4.06224 </td><td style=\"text-align: right;\">     2.53041 </td><td style=\"text-align: right;\">  9.11072    </td></tr>\n",
       "<tr><td>FSR_Trainable_3857d286</td><td>TERMINATED</td><td>172.26.215.93:102453</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_a100</td><td>[&#x27;force&#x27;, &#x27;x_co_4940</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0033923  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       208.215  </td><td style=\"text-align: right;\">    0.562124</td><td style=\"text-align: right;\">     0.34842 </td><td style=\"text-align: right;\">  9.30099e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_ab536d2b</td><td>TERMINATED</td><td>172.26.215.93:102674</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_d040</td><td>[&#x27;force&#x27;, &#x27;x_co_d980</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000165773</td><td>sklearn.preproc_cab0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.23294</td><td style=\"text-align: right;\">    3.99038 </td><td style=\"text-align: right;\">     2.50406 </td><td style=\"text-align: right;\">  6.47888    </td></tr>\n",
       "<tr><td>FSR_Trainable_6e1464a0</td><td>TERMINATED</td><td>172.26.215.93:102958</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_f880</td><td>[&#x27;force&#x27;, &#x27;x_co_eac0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00428174 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       313.71   </td><td style=\"text-align: right;\">    0.30468 </td><td style=\"text-align: right;\">     0.206142</td><td style=\"text-align: right;\">  3.12332e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_814e4b4c</td><td>TERMINATED</td><td>172.26.215.93:103456</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_7f40</td><td>[&#x27;force&#x27;, &#x27;x_co_8a40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0107126  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       125.556  </td><td style=\"text-align: right;\">    0.269784</td><td style=\"text-align: right;\">     0.177282</td><td style=\"text-align: right;\">  3.00583e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_bbc61711</td><td>TERMINATED</td><td>172.26.215.93:103751</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_6440</td><td>[&#x27;force&#x27;, &#x27;x_co_6500</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        2.11263e-05</td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        33.129  </td><td style=\"text-align: right;\">    0.594494</td><td style=\"text-align: right;\">     0.410417</td><td style=\"text-align: right;\">  5.55098e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_af2a1820</td><td>TERMINATED</td><td>172.26.215.93:103987</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_4d40</td><td>[&#x27;force&#x27;, &#x27;x_co_c940</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00134854 </td><td>sklearn.preproc_cab0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.99419</td><td style=\"text-align: right;\">    3.85889 </td><td style=\"text-align: right;\">     2.51445 </td><td style=\"text-align: right;\">  7.46546    </td></tr>\n",
       "<tr><td>FSR_Trainable_2b64f3bf</td><td>TERMINATED</td><td>172.26.215.93:104171</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_1080</td><td>[&#x27;force&#x27;, &#x27;x_co_4400</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0669519  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       117.291  </td><td style=\"text-align: right;\">    0.364166</td><td style=\"text-align: right;\">     0.238529</td><td style=\"text-align: right;\">  4.39139e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_30e64c59</td><td>TERMINATED</td><td>172.26.215.93:104399</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_f600</td><td>[&#x27;force&#x27;, &#x27;x_co_e480</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00651673 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       125.411  </td><td style=\"text-align: right;\">    0.546215</td><td style=\"text-align: right;\">     0.341106</td><td style=\"text-align: right;\">  8.87167e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_a0aaf09e</td><td>TERMINATED</td><td>172.26.215.93:104629</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_7140</td><td>[&#x27;force&#x27;, &#x27;x_co_08c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00540344 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        22.5133 </td><td style=\"text-align: right;\">    0.58831 </td><td style=\"text-align: right;\">     0.376727</td><td style=\"text-align: right;\">  8.02993e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_07a0a9ea</td><td>TERMINATED</td><td>172.26.215.93:104900</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_5000</td><td>[&#x27;force&#x27;, &#x27;x_co_0c00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00786564 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       121.682  </td><td style=\"text-align: right;\">    0.536786</td><td style=\"text-align: right;\">     0.333093</td><td style=\"text-align: right;\">  8.811e+14  </td></tr>\n",
       "<tr><td>FSR_Trainable_7c1a30a3</td><td>TERMINATED</td><td>172.26.215.93:105140</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_0e80</td><td>[&#x27;force&#x27;, &#x27;x_co_18c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.060988   </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       159.141  </td><td style=\"text-align: right;\">    0.55827 </td><td style=\"text-align: right;\">     0.349409</td><td style=\"text-align: right;\">  9.07425e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_14e1a525</td><td>TERMINATED</td><td>172.26.215.93:105610</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_1000</td><td>[&#x27;force&#x27;, &#x27;x_co_fc40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.01304    </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       230.718  </td><td style=\"text-align: right;\">    0.282034</td><td style=\"text-align: right;\">     0.188885</td><td style=\"text-align: right;\">  2.73956e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_d1d069cd</td><td>TERMINATED</td><td>172.26.215.93:105873</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_fbc0</td><td>[&#x27;force&#x27;, &#x27;x_co_8a80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0221121  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       247.636  </td><td style=\"text-align: right;\">    0.351431</td><td style=\"text-align: right;\">     0.214729</td><td style=\"text-align: right;\">  4.95541e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_810fa16b</td><td>TERMINATED</td><td>172.26.215.93:106137</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_70c0</td><td>[&#x27;force&#x27;, &#x27;x_co_4e40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0193696  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       238.751  </td><td style=\"text-align: right;\">    0.299768</td><td style=\"text-align: right;\">     0.209029</td><td style=\"text-align: right;\">  2.84568e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_a1b93071</td><td>TERMINATED</td><td>172.26.215.93:106352</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_e100</td><td>[&#x27;force&#x27;, &#x27;x_co_e200</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0217263  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        50.3806 </td><td style=\"text-align: right;\">    0.499757</td><td style=\"text-align: right;\">     0.360575</td><td style=\"text-align: right;\">  5.17248e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_9fa227d3</td><td>TERMINATED</td><td>172.26.215.93:106618</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_d180</td><td>[&#x27;force&#x27;, &#x27;x_co_5040</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0279192  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       264.266  </td><td style=\"text-align: right;\">    0.285918</td><td style=\"text-align: right;\">     0.20031 </td><td style=\"text-align: right;\">  2.34864e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_2428aa9a</td><td>TERMINATED</td><td>172.26.215.93:106861</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_6a40</td><td>[&#x27;force&#x27;, &#x27;x_co_f580</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00182763 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        34.9632 </td><td style=\"text-align: right;\">    0.513543</td><td style=\"text-align: right;\">     0.36254 </td><td style=\"text-align: right;\">  5.7605e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_5f9dc5f2</td><td>TERMINATED</td><td>172.26.215.93:107104</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_49c0</td><td>[&#x27;force&#x27;, &#x27;x_co_a880</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0204153  </td><td>sklearn.preproc_cd50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.31989</td><td style=\"text-align: right;\">    9.15548 </td><td style=\"text-align: right;\">    10.5782  </td><td style=\"text-align: right;\">  6.1955e+15 </td></tr>\n",
       "<tr><td>FSR_Trainable_064d057a</td><td>TERMINATED</td><td>172.26.215.93:107282</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_70c0</td><td>[&#x27;force&#x27;, &#x27;x_co_ff80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0100177  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       166.985  </td><td style=\"text-align: right;\">    0.271887</td><td style=\"text-align: right;\">     0.180354</td><td style=\"text-align: right;\">  2.97395e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_b6b8ae6f</td><td>TERMINATED</td><td>172.26.215.93:107505</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_2440</td><td>[&#x27;force&#x27;, &#x27;x_co_ffc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.010485   </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       166.828  </td><td style=\"text-align: right;\">    0.250702</td><td style=\"text-align: right;\">     0.172522</td><td style=\"text-align: right;\">  2.42436e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_c2c62f97</td><td>TERMINATED</td><td>172.26.215.93:107804</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_c2c0</td><td>[&#x27;force&#x27;, &#x27;x_co_1b80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00922864 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       170.787  </td><td style=\"text-align: right;\">    0.26254 </td><td style=\"text-align: right;\">     0.178363</td><td style=\"text-align: right;\">  2.82669e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_c62dfb24</td><td>TERMINATED</td><td>172.26.215.93:108092</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_fe80</td><td>[&#x27;force&#x27;, &#x27;x_co_de00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0857577  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       169.785  </td><td style=\"text-align: right;\">    0.494865</td><td style=\"text-align: right;\">     0.368456</td><td style=\"text-align: right;\">  4.12741e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_9d8e4ab3</td><td>TERMINATED</td><td>172.26.215.93:108296</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_96c0</td><td>[&#x27;force&#x27;, &#x27;x_co_5540</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0108729  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       160.297  </td><td style=\"text-align: right;\">    0.287413</td><td style=\"text-align: right;\">     0.19934 </td><td style=\"text-align: right;\">  2.5023e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_e1bafa1b</td><td>TERMINATED</td><td>172.26.215.93:108768</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_d7c0</td><td>[&#x27;force&#x27;, &#x27;x_co_d340</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0109069  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       160.1    </td><td style=\"text-align: right;\">    0.278665</td><td style=\"text-align: right;\">     0.194914</td><td style=\"text-align: right;\">  2.49853e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_d3976aff</td><td>TERMINATED</td><td>172.26.215.93:109015</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_31c0</td><td>[&#x27;force&#x27;, &#x27;x_co_ab00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0730738  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       153.618  </td><td style=\"text-align: right;\">    0.49742 </td><td style=\"text-align: right;\">     0.36777 </td><td style=\"text-align: right;\">  4.29441e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_7ae6b73f</td><td>TERMINATED</td><td>172.26.215.93:109293</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_9300</td><td>[&#x27;force&#x27;, &#x27;x_co_8440</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.094437   </td><td>sklearn.preproc_cd50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.86703</td><td style=\"text-align: right;\">   10.9355  </td><td style=\"text-align: right;\">    11.2052  </td><td style=\"text-align: right;\">  1.20223e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_71596cc8</td><td>TERMINATED</td><td>172.26.215.93:109473</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_7bc0</td><td>[&#x27;force&#x27;, &#x27;x_co_0e80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0398846  </td><td>sklearn.preproc_cd50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.02554</td><td style=\"text-align: right;\">   11.5001  </td><td style=\"text-align: right;\">    11.7419  </td><td style=\"text-align: right;\">  1.30707e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_3b45c3b3</td><td>TERMINATED</td><td>172.26.215.93:109692</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_92c0</td><td>[&#x27;force&#x27;, &#x27;x_co_0100</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00271095 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       111.442  </td><td style=\"text-align: right;\">    0.230328</td><td style=\"text-align: right;\">     0.156743</td><td style=\"text-align: right;\">  2.30573e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_eef4ead6</td><td>TERMINATED</td><td>172.26.215.93:109935</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_8840</td><td>[&#x27;force&#x27;, &#x27;x_co_89c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0124282  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       114.552  </td><td style=\"text-align: right;\">    0.261334</td><td style=\"text-align: right;\">     0.172411</td><td style=\"text-align: right;\">  2.75976e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_d3405b79</td><td>TERMINATED</td><td>172.26.215.93:110162</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_8900</td><td>[&#x27;force&#x27;, &#x27;x_co_8640</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0026172  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       113.129  </td><td style=\"text-align: right;\">    0.248269</td><td style=\"text-align: right;\">     0.166245</td><td style=\"text-align: right;\">  2.68836e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_1d3d4e4d</td><td>TERMINATED</td><td>172.26.215.93:110430</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_aa00</td><td>[&#x27;force&#x27;, &#x27;x_co_b980</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00279867 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       113.963  </td><td style=\"text-align: right;\">    0.242108</td><td style=\"text-align: right;\">     0.160935</td><td style=\"text-align: right;\">  2.53933e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_026edc0a</td><td>TERMINATED</td><td>172.26.215.93:110713</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_d100</td><td>[&#x27;force&#x27;, &#x27;x_co_e180</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00370273 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       116.923  </td><td style=\"text-align: right;\">    0.249758</td><td style=\"text-align: right;\">     0.168173</td><td style=\"text-align: right;\">  2.62503e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_bb43cf10</td><td>TERMINATED</td><td>172.26.215.93:110925</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_edc0</td><td>[&#x27;force&#x27;, &#x27;x_co_e340</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00270138 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       115.889  </td><td style=\"text-align: right;\">    0.231926</td><td style=\"text-align: right;\">     0.161771</td><td style=\"text-align: right;\">  2.13948e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_cab58447</td><td>TERMINATED</td><td>172.26.215.93:111148</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_f0c0</td><td>[&#x27;force&#x27;, &#x27;x_co_f000</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00285145 </td><td>sklearn.preproc_cab0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.92664</td><td style=\"text-align: right;\">    2.08689 </td><td style=\"text-align: right;\">     1.6143  </td><td style=\"text-align: right;\"> 12.3846     </td></tr>\n",
       "<tr><td>FSR_Trainable_4bfd99c3</td><td>TERMINATED</td><td>172.26.215.93:111381</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_3000</td><td>[&#x27;force&#x27;, &#x27;x_co_11c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00280791 </td><td>sklearn.preproc_cab0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.93239</td><td style=\"text-align: right;\">    2.10702 </td><td style=\"text-align: right;\">     1.64384 </td><td style=\"text-align: right;\"> 12.2088     </td></tr>\n",
       "<tr><td>FSR_Trainable_a4dc490d</td><td>TERMINATED</td><td>172.26.215.93:111602</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_05c0</td><td>[&#x27;force&#x27;, &#x27;x_co_2a00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000538407</td><td>sklearn.preproc_cd50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.50798</td><td style=\"text-align: right;\">   11.0789  </td><td style=\"text-align: right;\">    13.5205  </td><td style=\"text-align: right;\">  1.32396e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_559affac</td><td>TERMINATED</td><td>172.26.215.93:111833</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_2bc0</td><td>[&#x27;force&#x27;, &#x27;x_co_2640</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000899075</td><td>sklearn.preproc_cd50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.34306</td><td style=\"text-align: right;\">   11.2078  </td><td style=\"text-align: right;\">    12.9434  </td><td style=\"text-align: right;\">  6.00238e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_f64d9976</td><td>TERMINATED</td><td>172.26.215.93:112042</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_f680</td><td>[&#x27;force&#x27;, &#x27;x_co_c5c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00505675 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         9.42365</td><td style=\"text-align: right;\">    0.499265</td><td style=\"text-align: right;\">     0.366804</td><td style=\"text-align: right;\">  4.47334e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_4ee8eda9</td><td>TERMINATED</td><td>172.26.215.93:112260</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_07c0</td><td>[&#x27;force&#x27;, &#x27;x_co_1540</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00462859 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       113.235  </td><td style=\"text-align: right;\">    0.237799</td><td style=\"text-align: right;\">     0.15927 </td><td style=\"text-align: right;\">  2.44684e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_8568305e</td><td>TERMINATED</td><td>172.26.215.93:112505</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_2280</td><td>[&#x27;force&#x27;, &#x27;x_co_3f80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0043215  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       111.612  </td><td style=\"text-align: right;\">    0.249734</td><td style=\"text-align: right;\">     0.173308</td><td style=\"text-align: right;\">  2.49684e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_c7f219cd</td><td>TERMINATED</td><td>172.26.215.93:112789</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_8b80</td><td>[&#x27;force&#x27;, &#x27;x_co_aa80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00202077 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       115.921  </td><td style=\"text-align: right;\">    0.237356</td><td style=\"text-align: right;\">     0.161729</td><td style=\"text-align: right;\">  2.40038e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_98eea35e</td><td>TERMINATED</td><td>172.26.215.93:112996</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_9940</td><td>[&#x27;force&#x27;, &#x27;x_co_acc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00469169 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       113.481  </td><td style=\"text-align: right;\">    0.241349</td><td style=\"text-align: right;\">     0.173748</td><td style=\"text-align: right;\">  2.21358e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_b62849af</td><td>TERMINATED</td><td>172.26.215.93:113300</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_a300</td><td>[&#x27;force&#x27;, &#x27;x_co_9000</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00410232 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       117.2    </td><td style=\"text-align: right;\">    0.237334</td><td style=\"text-align: right;\">     0.16584 </td><td style=\"text-align: right;\">  2.19189e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_fb354333</td><td>TERMINATED</td><td>172.26.215.93:113494</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_5580</td><td>[&#x27;force&#x27;, &#x27;x_co_7e00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00197609 </td><td>sklearn.preproc_cab0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.46769</td><td style=\"text-align: right;\">    2.8773  </td><td style=\"text-align: right;\">     2.11377 </td><td style=\"text-align: right;\">  8.76942    </td></tr>\n",
       "<tr><td>FSR_Trainable_c374aa34</td><td>TERMINATED</td><td>172.26.215.93:113725</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_8c40</td><td>[&#x27;force&#x27;, &#x27;x_co_5240</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00162211 </td><td>sklearn.preproc_cab0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.62091</td><td style=\"text-align: right;\">    3.75428 </td><td style=\"text-align: right;\">     2.33339 </td><td style=\"text-align: right;\">  9.75962    </td></tr>\n",
       "<tr><td>FSR_Trainable_4f489564</td><td>TERMINATED</td><td>172.26.215.93:113954</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_0cc0</td><td>[&#x27;force&#x27;, &#x27;x_co_1f80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00128455 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.82655</td><td style=\"text-align: right;\">    0.607373</td><td style=\"text-align: right;\">     0.375576</td><td style=\"text-align: right;\">  9.0578e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_3dfabaf2</td><td>TERMINATED</td><td>172.26.215.93:114189</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_4080</td><td>[&#x27;force&#x27;, &#x27;x_co_7bc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00559489 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         4.81012</td><td style=\"text-align: right;\">    0.500315</td><td style=\"text-align: right;\">     0.346141</td><td style=\"text-align: right;\">  5.87943e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_eb10b985</td><td>TERMINATED</td><td>172.26.215.93:114397</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_ff80</td><td>[&#x27;force&#x27;, &#x27;x_co_e280</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00487215 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       160.512  </td><td style=\"text-align: right;\">    0.246007</td><td style=\"text-align: right;\">     0.17399 </td><td style=\"text-align: right;\">  2.22349e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_693db682</td><td>TERMINATED</td><td>172.26.215.93:114613</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_7300</td><td>[&#x27;force&#x27;, &#x27;x_co_4940</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00218431 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.87176</td><td style=\"text-align: right;\">    0.383556</td><td style=\"text-align: right;\">     0.275405</td><td style=\"text-align: right;\">  3.73143e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_fec44e75</td><td>TERMINATED</td><td>172.26.215.93:114846</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_24c0</td><td>[&#x27;force&#x27;, &#x27;x_co_d780</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00231521 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         7.03274</td><td style=\"text-align: right;\">    0.310961</td><td style=\"text-align: right;\">     0.220243</td><td style=\"text-align: right;\">  2.75029e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_66fede0d</td><td>TERMINATED</td><td>172.26.215.93:115065</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_e980</td><td>[&#x27;force&#x27;, &#x27;x_co_d7c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00673526 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       121.262  </td><td style=\"text-align: right;\">    0.270553</td><td style=\"text-align: right;\">     0.185202</td><td style=\"text-align: right;\">  2.77046e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_9d7f317c</td><td>TERMINATED</td><td>172.26.215.93:115308</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_48c0</td><td>[&#x27;force&#x27;, &#x27;x_co_5dc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.007403   </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        35.8934 </td><td style=\"text-align: right;\">    0.287865</td><td style=\"text-align: right;\">     0.188496</td><td style=\"text-align: right;\">  2.99815e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_657010b8</td><td>TERMINATED</td><td>172.26.215.93:115590</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_6700</td><td>[&#x27;force&#x27;, &#x27;x_co_4b00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00339337 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       121.03   </td><td style=\"text-align: right;\">    0.245783</td><td style=\"text-align: right;\">     0.16464 </td><td style=\"text-align: right;\">  2.54674e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_b377cc00</td><td>TERMINATED</td><td>172.26.215.93:115793</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_7180</td><td>[&#x27;force&#x27;, &#x27;x_co_5f00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00063973 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.51409</td><td style=\"text-align: right;\">    0.848095</td><td style=\"text-align: right;\">     0.472319</td><td style=\"text-align: right;\">  1.38031e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_870a8bf6</td><td>TERMINATED</td><td>172.26.215.93:116016</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_b100</td><td>[&#x27;force&#x27;, &#x27;x_co_8700</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00122532 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.4122 </td><td style=\"text-align: right;\">    0.742076</td><td style=\"text-align: right;\">     0.453132</td><td style=\"text-align: right;\">  9.3837e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_d7953de1</td><td>TERMINATED</td><td>172.26.215.93:116256</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_a0c0</td><td>[&#x27;force&#x27;, &#x27;x_co_a840</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00696982 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.82362</td><td style=\"text-align: right;\">    0.423168</td><td style=\"text-align: right;\">     0.296888</td><td style=\"text-align: right;\">  4.86736e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_6656c7f0</td><td>TERMINATED</td><td>172.26.215.93:116488</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_0900</td><td>[&#x27;force&#x27;, &#x27;x_co_25c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00755485 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         4.9039 </td><td style=\"text-align: right;\">    0.36802 </td><td style=\"text-align: right;\">     0.27774 </td><td style=\"text-align: right;\">  2.98186e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_a449dfdd</td><td>TERMINATED</td><td>172.26.215.93:116736</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_1fc0</td><td>[&#x27;force&#x27;, &#x27;x_co_3f40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00301627 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       116.489  </td><td style=\"text-align: right;\">    0.230498</td><td style=\"text-align: right;\">     0.161396</td><td style=\"text-align: right;\">  2.23144e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_f5f499b0</td><td>TERMINATED</td><td>172.26.215.93:116972</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_efc0</td><td>[&#x27;force&#x27;, &#x27;x_co_cd80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00378296 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       117.996  </td><td style=\"text-align: right;\">    0.24342 </td><td style=\"text-align: right;\">     0.166357</td><td style=\"text-align: right;\">  2.49721e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_7989f1e3</td><td>TERMINATED</td><td>172.26.215.93:117173</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_f8c0</td><td>[&#x27;force&#x27;, &#x27;x_co_c100</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00397201 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       115.236  </td><td style=\"text-align: right;\">    0.237237</td><td style=\"text-align: right;\">     0.159173</td><td style=\"text-align: right;\">  2.58331e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_0ec2df67</td><td>TERMINATED</td><td>172.26.215.93:117445</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_b300</td><td>[&#x27;force&#x27;, &#x27;x_co_a6c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00348529 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       118.473  </td><td style=\"text-align: right;\">    0.239379</td><td style=\"text-align: right;\">     0.163651</td><td style=\"text-align: right;\">  2.40775e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_fb7b995d</td><td>TERMINATED</td><td>172.26.215.93:117726</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_0bc0</td><td>[&#x27;force&#x27;, &#x27;x_co_b440</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00318818 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       118.74   </td><td style=\"text-align: right;\">    0.229371</td><td style=\"text-align: right;\">     0.155367</td><td style=\"text-align: right;\">  2.22979e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_5953e60a</td><td>TERMINATED</td><td>172.26.215.93:117960</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_9440</td><td>[&#x27;force&#x27;, &#x27;x_co_a380</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00342128 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.42616</td><td style=\"text-align: right;\">    0.295512</td><td style=\"text-align: right;\">     0.205305</td><td style=\"text-align: right;\">  2.76443e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_bf9fd07a</td><td>TERMINATED</td><td>172.26.215.93:118159</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_0780</td><td>[&#x27;force&#x27;, &#x27;x_co_1780</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0139159  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         9.75011</td><td style=\"text-align: right;\">    0.428741</td><td style=\"text-align: right;\">     0.321968</td><td style=\"text-align: right;\">  3.48827e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_e580fe88</td><td>TERMINATED</td><td>172.26.215.93:118373</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_8480</td><td>[&#x27;force&#x27;, &#x27;x_co_8880</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00152208 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        19.7051 </td><td style=\"text-align: right;\">    0.317902</td><td style=\"text-align: right;\">     0.215938</td><td style=\"text-align: right;\">  3.11534e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_e4b72c0b</td><td>TERMINATED</td><td>172.26.215.93:118591</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_88c0</td><td>[&#x27;force&#x27;, &#x27;x_co_6380</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00153462 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        10.3666 </td><td style=\"text-align: right;\">    0.379252</td><td style=\"text-align: right;\">     0.2727  </td><td style=\"text-align: right;\">  3.74524e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_81679c0e</td><td>TERMINATED</td><td>172.26.215.93:118843</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_8200</td><td>[&#x27;force&#x27;, &#x27;x_co_9900</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00167368 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.90726</td><td style=\"text-align: right;\">    0.467003</td><td style=\"text-align: right;\">     0.339016</td><td style=\"text-align: right;\">  4.66868e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_5506c20c</td><td>TERMINATED</td><td>172.26.215.93:119064</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_a640</td><td>[&#x27;force&#x27;, &#x27;x_co_b7c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00564475 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       118.397  </td><td style=\"text-align: right;\">    0.250835</td><td style=\"text-align: right;\">     0.161546</td><td style=\"text-align: right;\">  2.93793e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_3cf0dc98</td><td>TERMINATED</td><td>172.26.215.93:119298</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_8480</td><td>[&#x27;force&#x27;, &#x27;x_co_a240</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00263372 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       118.496  </td><td style=\"text-align: right;\">    0.2395  </td><td style=\"text-align: right;\">     0.167277</td><td style=\"text-align: right;\">  2.32419e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_ef468e9e</td><td>TERMINATED</td><td>172.26.215.93:119510</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_8780</td><td>[&#x27;force&#x27;, &#x27;x_co_9180</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00283808 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.69491</td><td style=\"text-align: right;\">    0.304255</td><td style=\"text-align: right;\">     0.209963</td><td style=\"text-align: right;\">  2.88249e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_8d7cf828</td><td>TERMINATED</td><td>172.26.215.93:119772</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_8b80</td><td>[&#x27;force&#x27;, &#x27;x_co_8280</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00297733 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.38939</td><td style=\"text-align: right;\">    0.306097</td><td style=\"text-align: right;\">     0.206756</td><td style=\"text-align: right;\">  3.30929e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_b7ffc6c7</td><td>TERMINATED</td><td>172.26.215.93:120004</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_3c00</td><td>[&#x27;force&#x27;, &#x27;x_co_2f00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00871216 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        20.2068 </td><td style=\"text-align: right;\">    0.286312</td><td style=\"text-align: right;\">     0.186239</td><td style=\"text-align: right;\">  3.17302e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_b1c1eb81</td><td>TERMINATED</td><td>172.26.215.93:120188</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_dec0</td><td>[&#x27;force&#x27;, &#x27;x_co_cf80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00860704 </td><td>sklearn.preproc_cd50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.60067</td><td style=\"text-align: right;\">    9.69353 </td><td style=\"text-align: right;\">    11.6172  </td><td style=\"text-align: right;\">  6.17498e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_82eb35d9</td><td>TERMINATED</td><td>172.26.215.93:120430</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_d1c0</td><td>[&#x27;force&#x27;, &#x27;x_co_eb80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00234984 </td><td>sklearn.preproc_cd50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.15019</td><td style=\"text-align: right;\">   13.0081  </td><td style=\"text-align: right;\">    12.8057  </td><td style=\"text-align: right;\">  1.39245e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_d959ef0c</td><td>TERMINATED</td><td>172.26.215.93:120642</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_2340</td><td>[&#x27;force&#x27;, &#x27;x_co_18c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00228949 </td><td>sklearn.preproc_cab0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.73978</td><td style=\"text-align: right;\">    3.62414 </td><td style=\"text-align: right;\">     2.28656 </td><td style=\"text-align: right;\">  8.47181    </td></tr>\n",
       "<tr><td>FSR_Trainable_6609d819</td><td>TERMINATED</td><td>172.26.215.93:120881</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_b2c0</td><td>[&#x27;force&#x27;, &#x27;x_co_90c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00355364 </td><td>sklearn.preproc_cab0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.43223</td><td style=\"text-align: right;\">    3.23051 </td><td style=\"text-align: right;\">     2.13896 </td><td style=\"text-align: right;\"> 11.8335     </td></tr>\n",
       "<tr><td>FSR_Trainable_67a7a972</td><td>TERMINATED</td><td>172.26.215.93:121091</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_6300</td><td>[&#x27;force&#x27;, &#x27;x_co_cb00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00565347 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.43456</td><td style=\"text-align: right;\">    1.01892 </td><td style=\"text-align: right;\">     0.541272</td><td style=\"text-align: right;\">  2.18073e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_71d3ee26</td><td>TERMINATED</td><td>172.26.215.93:121335</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_4380</td><td>[&#x27;force&#x27;, &#x27;x_co_b640</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0054255  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       119.246  </td><td style=\"text-align: right;\">    0.269884</td><td style=\"text-align: right;\">     0.176461</td><td style=\"text-align: right;\">  3.34206e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_412fc51c</td><td>TERMINATED</td><td>172.26.215.93:121558</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_5b40</td><td>[&#x27;force&#x27;, &#x27;x_co_6880</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00542687 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">        77.2173 </td><td style=\"text-align: right;\">    0.259021</td><td style=\"text-align: right;\">     0.176934</td><td style=\"text-align: right;\">  2.77295e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_59e2c3b1</td><td>TERMINATED</td><td>172.26.215.93:121787</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_bb00</td><td>[&#x27;force&#x27;, &#x27;x_co_a880</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00610218 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">        75.9915 </td><td style=\"text-align: right;\">    0.266041</td><td style=\"text-align: right;\">     0.173932</td><td style=\"text-align: right;\">  3.14132e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_97cbfcc7</td><td>TERMINATED</td><td>172.26.215.93:122011</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_4640</td><td>[&#x27;force&#x27;, &#x27;x_co_dfc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00546538 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        19.2682 </td><td style=\"text-align: right;\">    0.287899</td><td style=\"text-align: right;\">     0.183178</td><td style=\"text-align: right;\">  3.48774e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_63b28b94</td><td>TERMINATED</td><td>172.26.215.93:122285</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_f940</td><td>[&#x27;force&#x27;, &#x27;x_co_d400</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00578785 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       116.413  </td><td style=\"text-align: right;\">    0.254944</td><td style=\"text-align: right;\">     0.172279</td><td style=\"text-align: right;\">  2.56602e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_197e3b7f</td><td>TERMINATED</td><td>172.26.215.93:122535</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_7940</td><td>[&#x27;force&#x27;, &#x27;x_co_a340</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00618608 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        20.9277 </td><td style=\"text-align: right;\">    0.291619</td><td style=\"text-align: right;\">     0.181452</td><td style=\"text-align: right;\">  3.76533e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_23e8d890</td><td>TERMINATED</td><td>172.26.215.93:122736</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_59c0</td><td>[&#x27;force&#x27;, &#x27;x_co_4900</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00106575 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.70199</td><td style=\"text-align: right;\">    0.463876</td><td style=\"text-align: right;\">     0.342388</td><td style=\"text-align: right;\">  4.24611e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_0cd88631</td><td>TERMINATED</td><td>172.26.215.93:122961</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>median</td><td>[&#x27;FSR_for_force_5680</td><td>[&#x27;force&#x27;, &#x27;x_co_4140</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00390274 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         4.61309</td><td style=\"text-align: right;\">    0.361789</td><td style=\"text-align: right;\">     0.258807</td><td style=\"text-align: right;\">  3.41846e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_c3536b16</td><td>TERMINATED</td><td>172.26.215.93:123185</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_b3c0</td><td>[&#x27;force&#x27;, &#x27;x_co_b380</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00417875 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.76213</td><td style=\"text-align: right;\">    0.375194</td><td style=\"text-align: right;\">     0.266862</td><td style=\"text-align: right;\">  3.70836e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_ebcf36c8</td><td>TERMINATED</td><td>172.26.215.93:123405</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_2540</td><td>[&#x27;force&#x27;, &#x27;x_co_3440</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00416323 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        89.4717 </td><td style=\"text-align: right;\">    0.232416</td><td style=\"text-align: right;\">     0.159589</td><td style=\"text-align: right;\">  2.31318e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_a8768c06</td><td>TERMINATED</td><td>172.26.215.93:123652</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_f380</td><td>[&#x27;force&#x27;, &#x27;x_co_e800</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00300562 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        10.4624 </td><td style=\"text-align: right;\">    0.295186</td><td style=\"text-align: right;\">     0.189354</td><td style=\"text-align: right;\">  3.36898e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_661f2b44</td><td>TERMINATED</td><td>172.26.215.93:123864</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_9700</td><td>[&#x27;force&#x27;, &#x27;x_co_89c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00188305 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.05512</td><td style=\"text-align: right;\">    0.436892</td><td style=\"text-align: right;\">     0.319855</td><td style=\"text-align: right;\">  4.1563e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_96ca6737</td><td>TERMINATED</td><td>172.26.215.93:124094</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_ab80</td><td>[&#x27;force&#x27;, &#x27;x_co_8900</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0045116  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">        52.2488 </td><td style=\"text-align: right;\">    0.261055</td><td style=\"text-align: right;\">     0.174287</td><td style=\"text-align: right;\">  2.79384e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_3d8c1dc7</td><td>ERROR     </td><td>172.26.215.93:103230</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_a800</td><td>[&#x27;force&#x27;, &#x27;x_co_eb40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0880267  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">            </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td></tr>\n",
       "<tr><td>FSR_Trainable_b346be41</td><td>ERROR     </td><td>172.26.215.93:105391</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_e3c0</td><td>[&#x27;force&#x27;, &#x27;x_co_0f80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0546748  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">       137.127  </td><td style=\"text-align: right;\">    0.284064</td><td style=\"text-align: right;\">     0.198594</td><td style=\"text-align: right;\">  2.52303e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_1a76a8a6</td><td>ERROR     </td><td>172.26.215.93:108497</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ce10</td><td>sklearn.impute._bd20</td><td>mean  </td><td>[&#x27;FSR_for_force_c880</td><td>[&#x27;force&#x27;, &#x27;x_co_f4c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0776158  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">            </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 18:18:36,275\tINFO wandb.py:320 -- Already logged into W&B.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th>iterations_since_restore  </th><th>mae_coord         </th><th>mae_force         </th><th>mape_coord        </th><th>mape_force            </th><th>metric            </th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th>rmse_coord        </th><th>rmse_force        </th><th>time_since_restore  </th><th>time_this_iter_s  </th><th>time_total_s      </th><th style=\"text-align: right;\">  timestamp</th><th>tmae_coord        </th><th>tmae_force         </th><th>tmape_coord           </th><th>tmape_force           </th><th>training_iteration  </th><th>trial_id  </th><th>trmse_coord        </th><th>trmse_force        </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_026edc0a</td><td>2023-08-10_18-43-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.655659494644951 </td><td>666.3240915091992 </td><td>1.2023837899886696</td><td>6.387597883733373e+17 </td><td>0.5878820835033042</td><td>172.26.215.93</td><td style=\"text-align: right;\">110713</td><td>2.2688918708396697</td><td>498.52166522147326</td><td>116.9229085445404   </td><td>1.0757396221160889</td><td>116.9229085445404 </td><td style=\"text-align: right;\"> 1691660614</td><td>0.9343151398054406</td><td>0.24975792657763518</td><td>167895876132898.12    </td><td>262502976512680.7     </td><td>100                 </td><td>026edc0a  </td><td>0.4197087410874943 </td><td>0.16817334241580992</td></tr>\n",
       "<tr><td>FSR_Trainable_064d057a</td><td>2023-08-10_18-35-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>5.14628764262439  </td><td>726.4492219295495 </td><td>1.2445947423985533</td><td>7.61490025268033e+17  </td><td>0.6171556811337859</td><td>172.26.215.93</td><td style=\"text-align: right;\">107282</td><td>2.3732561800422873</td><td>542.0501046429708 </td><td>166.98536944389343  </td><td>1.716055154800415 </td><td>166.98536944389343</td><td style=\"text-align: right;\"> 1691660137</td><td>1.0256412401783985</td><td>0.271887414347788  </td><td>151634940469510.3     </td><td>297395390208246.6     </td><td>100                 </td><td>064d057a  </td><td>0.436801395458096  </td><td>0.18035428567569   </td></tr>\n",
       "<tr><td>FSR_Trainable_07a0a9ea</td><td>2023-08-10_18-27-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.287626299839251 </td><td>1509.1108134738363</td><td>1.2602068109076503</td><td>2.413981962423381e+18 </td><td>0.7923942639989865</td><td>172.26.215.93</td><td style=\"text-align: right;\">104900</td><td>2.43571074597425  </td><td>1049.320175883287 </td><td>121.68180227279663  </td><td>1.0714914798736572</td><td>121.68180227279663</td><td style=\"text-align: right;\"> 1691659673</td><td>0.872467396917376 </td><td>0.5367862606148913 </td><td>205045651566330.88    </td><td>881100429226434.6     </td><td>100                 </td><td>07a0a9ea  </td><td>0.45930097587701696</td><td>0.3330932881219696 </td></tr>\n",
       "<tr><td>FSR_Trainable_0cd88631</td><td>2023-08-10_18-57-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>5.5230202417875836</td><td>1023.3639878005702</td><td>1.4257678351053982</td><td>9.071875553801747e+17 </td><td>0.7413020094315681</td><td>172.26.215.93</td><td style=\"text-align: right;\">122961</td><td>2.5865695884232447</td><td>829.982646902357  </td><td>4.613093614578247   </td><td>2.091242790222168 </td><td>4.613093614578247 </td><td style=\"text-align: right;\"> 1691661440</td><td>1.086573535327065 </td><td>0.3617892966176046 </td><td>167419394424235.7     </td><td>341846474988531.3     </td><td>2                   </td><td>0cd88631  </td><td>0.48249494129928117</td><td>0.258807068132287  </td></tr>\n",
       "<tr><td>FSR_Trainable_0ec2df67</td><td>2023-08-10_18-52-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.676049398475571 </td><td>625.8287728715512 </td><td>1.2352078966654179</td><td>5.296333862476806e+17 </td><td>0.5819584235351413</td><td>172.26.215.93</td><td style=\"text-align: right;\">117445</td><td>2.2730002240039   </td><td>478.621651043089  </td><td>118.47313094139099  </td><td>1.3934788703918457</td><td>118.47313094139099</td><td style=\"text-align: right;\"> 1691661136</td><td>0.9383816810651143</td><td>0.2393792938187365 </td><td>175405416565473.7     </td><td>240774743938817.22    </td><td>100                 </td><td>0ec2df67  </td><td>0.4183071913951242 </td><td>0.16365123214001712</td></tr>\n",
       "<tr><td>FSR_Trainable_14e1a525</td><td>2023-08-10_18-31-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.9300416110932375</td><td>777.8352058634209 </td><td>1.1859010378352792</td><td>6.585835523307155e+17 </td><td>0.6136143679569254</td><td>172.26.215.93</td><td style=\"text-align: right;\">105610</td><td>2.32455569074209  </td><td>601.3032328320649 </td><td>230.71786165237427  </td><td>2.1542158126831055</td><td>230.71786165237427</td><td style=\"text-align: right;\"> 1691659884</td><td>0.9754604484575252</td><td>0.2820337069027736 </td><td>161128154114674.16    </td><td>273955750455129.6     </td><td>100                 </td><td>14e1a525  </td><td>0.4247290471380835 </td><td>0.18888532081884182</td></tr>\n",
       "<tr><td>FSR_Trainable_197e3b7f</td><td>2023-08-10_18-57-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td>16                        </td><td>5.582874065610014 </td><td>783.0397099512918 </td><td>1.2907286826732334</td><td>9.038929322502031e+17 </td><td>0.6441595777462986</td><td>172.26.215.93</td><td style=\"text-align: right;\">122535</td><td>2.5169595867997057</td><td>550.8042773261377 </td><td>20.927736043930054  </td><td>0.9446051120758057</td><td>20.927736043930054</td><td style=\"text-align: right;\"> 1691661429</td><td>1.1063789936861375</td><td>0.2916191594385882 </td><td>162077696400005.7     </td><td>376532926752057.56    </td><td>16                  </td><td>197e3b7f  </td><td>0.46270743272999665</td><td>0.18145214501630197</td></tr>\n",
       "<tr><td>FSR_Trainable_1a76a8a6</td><td>2023-08-10_18-36-03</td><td>      </td><td>DESKTOP-0P789CI</td><td>                          </td><td>                  </td><td>                  </td><td>                  </td><td>                      </td><td>                  </td><td>172.26.215.93</td><td style=\"text-align: right;\">108497</td><td>                  </td><td>                  </td><td>                    </td><td>                  </td><td>                  </td><td style=\"text-align: right;\"> 1691660163</td><td>                  </td><td>                   </td><td>                      </td><td>                      </td><td>                    </td><td>1a76a8a6  </td><td>                   </td><td>                   </td></tr>\n",
       "<tr><td>FSR_Trainable_1d3d4e4d</td><td>2023-08-10_18-42-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.614750393669644 </td><td>648.9766076975087 </td><td>1.175119455189068 </td><td>6.189155064105669e+17 </td><td>0.5809040430916799</td><td>172.26.215.93</td><td style=\"text-align: right;\">110430</td><td>2.27842740013944  </td><td>485.93548272248887</td><td>113.96274948120117  </td><td>1.3270039558410645</td><td>113.96274948120117</td><td style=\"text-align: right;\"> 1691660527</td><td>0.9251145384584389</td><td>0.24210785340571914</td><td>167916376666575.44    </td><td>253933138018514.6     </td><td>100                 </td><td>1d3d4e4d  </td><td>0.4199685764472182 </td><td>0.16093546664446165</td></tr>\n",
       "<tr><td>FSR_Trainable_23e8d890</td><td>2023-08-10_18-57-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>6.074051334491682 </td><td>1340.5623383517416</td><td>1.4616121735265304</td><td>1.28733529131761e+18  </td><td>0.8671572405917696</td><td>172.26.215.93</td><td style=\"text-align: right;\">122736</td><td>2.7517597752833374</td><td>1076.2870649940799</td><td>2.7019877433776855  </td><td>2.7019877433776855</td><td>2.7019877433776855</td><td style=\"text-align: right;\"> 1691661421</td><td>1.2152270312836448</td><td>0.4638755502904656 </td><td>172741459376080.47    </td><td>424610788677685.56    </td><td>1                   </td><td>23e8d890  </td><td>0.5247691941538655 </td><td>0.34238804643790416</td></tr>\n",
       "<tr><td>FSR_Trainable_2428aa9a</td><td>2023-08-10_18-32-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td>8                         </td><td>5.748176469229209 </td><td>1480.8743566095247</td><td>1.4851817932939297</td><td>1.759016551563352e+18 </td><td>0.8763787499934376</td><td>172.26.215.93</td><td style=\"text-align: right;\">106861</td><td>2.6892211248187765</td><td>1134.8097310117423</td><td>34.963165044784546  </td><td>4.335546970367432 </td><td>34.963165044784546</td><td style=\"text-align: right;\"> 1691659936</td><td>1.1509745943336516</td><td>0.513543075532884  </td><td>180552873283178.88    </td><td>576049838205985.1     </td><td>8                   </td><td>2428aa9a  </td><td>0.5138385015887836 </td><td>0.36254024840465393</td></tr>\n",
       "<tr><td>FSR_Trainable_2b64f3bf</td><td>2023-08-10_18-26-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td>64                        </td><td>7.975232458269955 </td><td>960.4527362705854 </td><td>1.4929376989132785</td><td>1.0716863768944003e+18</td><td>0.8205943084293921</td><td>172.26.215.93</td><td style=\"text-align: right;\">104171</td><td>3.1676712874171606</td><td>734.3696962736791 </td><td>117.29063868522644  </td><td>1.846130132675171 </td><td>117.29063868522644</td><td style=\"text-align: right;\"> 1691659608</td><td>1.5936245808641512</td><td>0.3641660723005804 </td><td>174361925137013.22    </td><td>439139373614088.94    </td><td>64                  </td><td>2b64f3bf  </td><td>0.5820656718556748 </td><td>0.2385286365737172 </td></tr>\n",
       "<tr><td>FSR_Trainable_30e64c59</td><td>2023-08-10_18-27-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.2830497817818465</td><td>1543.96002599702  </td><td>1.2289454149682089</td><td>2.4636742129765023e+18</td><td>0.797358217944877 </td><td>172.26.215.93</td><td style=\"text-align: right;\">104399</td><td>2.414907756034817 </td><td>1073.7948366896107</td><td>125.41109228134155  </td><td>1.6477420330047607</td><td>125.41109228134155</td><td style=\"text-align: right;\"> 1691659627</td><td>0.8707762713058814</td><td>0.5462153403856596 </td><td>201864051176213.34    </td><td>887167378760428.4     </td><td>100                 </td><td>30e64c59  </td><td>0.45625260736862683</td><td>0.34110561057625016</td></tr>\n",
       "<tr><td>FSR_Trainable_3857d286</td><td>2023-08-10_18-23-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.635590265036429 </td><td>1587.252667569337 </td><td>1.2210248887940014</td><td>2.5665450815687306e+18</td><td>0.8116344792816715</td><td>172.26.215.93</td><td style=\"text-align: right;\">102453</td><td>2.468119881477745 </td><td>1104.0486724836003</td><td>208.21474075317383  </td><td>2.208770513534546 </td><td>208.21474075317383</td><td style=\"text-align: right;\"> 1691659426</td><td>0.9350569977478481</td><td>0.562123892805735  </td><td>181589640738056.3     </td><td>930098867767599.6     </td><td>100                 </td><td>3857d286  </td><td>0.4632147089327223 </td><td>0.3484197703489492 </td></tr>\n",
       "<tr><td>FSR_Trainable_3b45c3b3</td><td>2023-08-10_18-41-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.530063861609819 </td><td>618.9561633938188 </td><td>1.1709851709534327</td><td>5.483927526012578e+17 </td><td>0.5750424747822209</td><td>172.26.215.93</td><td style=\"text-align: right;\">109692</td><td>2.265440945617919 </td><td>471.70589856609206</td><td>111.44202303886414  </td><td>1.0394442081451416</td><td>111.44202303886414</td><td style=\"text-align: right;\"> 1691660468</td><td>0.9134897474350597</td><td>0.23032832076486595</td><td>169514014996101.38    </td><td>230572822530512.12    </td><td>100                 </td><td>3b45c3b3  </td><td>0.41829907208280925</td><td>0.1567434026994117 </td></tr>\n",
       "<tr><td>FSR_Trainable_3cf0dc98</td><td>2023-08-10_18-55-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.624770844378378 </td><td>643.588988433534  </td><td>1.1291974475506503</td><td>5.836492301174633e+17 </td><td>0.5930452262455197</td><td>172.26.215.93</td><td style=\"text-align: right;\">119298</td><td>2.3198473610476538</td><td>494.18649920534597</td><td>118.49617886543274  </td><td>1.3638319969177246</td><td>118.49617886543274</td><td style=\"text-align: right;\"> 1691661310</td><td>0.9274154474421503</td><td>0.2395003394101901 </td><td>148544031699312.22    </td><td>232419150702846.78    </td><td>100                 </td><td>3cf0dc98  </td><td>0.4257681671095285 </td><td>0.1672770591359912 </td></tr>\n",
       "<tr><td>FSR_Trainable_3d8c1dc7</td><td>2023-08-10_18-21-46</td><td>      </td><td>DESKTOP-0P789CI</td><td>                          </td><td>                  </td><td>                  </td><td>                  </td><td>                      </td><td>                  </td><td>172.26.215.93</td><td style=\"text-align: right;\">103230</td><td>                  </td><td>                  </td><td>                    </td><td>                  </td><td>                  </td><td style=\"text-align: right;\"> 1691659306</td><td>                  </td><td>                   </td><td>                      </td><td>                      </td><td>                    </td><td>3d8c1dc7  </td><td>                   </td><td>                   </td></tr>\n",
       "<tr><td>FSR_Trainable_3dfabaf2</td><td>2023-08-10_18-46-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>5.661765654582705 </td><td>1432.2574790305162</td><td>1.346062196508646 </td><td>1.7998211726844795e+18</td><td>0.8466346885817018</td><td>172.26.215.93</td><td style=\"text-align: right;\">114189</td><td>2.6491698941991917</td><td>1081.9112329464097</td><td>4.810116767883301   </td><td>2.0909721851348877</td><td>4.810116767883301 </td><td style=\"text-align: right;\"> 1691660770</td><td>1.1306241660420915</td><td>0.5003149502110463 </td><td>180357158151700.06    </td><td>587942676886827.1     </td><td>2                   </td><td>3dfabaf2  </td><td>0.5004938680082959 </td><td>0.34614082057340584</td></tr>\n",
       "<tr><td>FSR_Trainable_412fc51c</td><td>2023-08-10_18-56-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td>64                        </td><td>5.06037711575597  </td><td>687.9910554511796 </td><td>1.2370375552916648</td><td>6.486040584203555e+17 </td><td>0.6147225734706878</td><td>172.26.215.93</td><td style=\"text-align: right;\">121558</td><td>2.3743562514404792</td><td>517.7572292808086 </td><td>77.21726870536804   </td><td>1.1425964832305908</td><td>77.21726870536804 </td><td style=\"text-align: right;\"> 1691661392</td><td>1.010211241455049 </td><td>0.2590206618220549 </td><td>173387047158931.16    </td><td>277294696640458.72    </td><td>64                  </td><td>412fc51c  </td><td>0.43778893806902863</td><td>0.17693363540165924</td></tr>\n",
       "<tr><td>FSR_Trainable_4bfd99c3</td><td>2023-08-10_18-42-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>4.658860026608729 </td><td>775.4144840884576 </td><td>1.2691124282080908</td><td>941338523.764735      </td><td>5.150096551214708 </td><td>172.26.215.93</td><td style=\"text-align: right;\">111381</td><td>2.372580191678775 </td><td>620.0537494964743 </td><td>2.9323906898498535  </td><td>2.9323906898498535</td><td>2.9323906898498535</td><td style=\"text-align: right;\"> 1691660528</td><td>7.561934597180286 </td><td>2.1070234060378685 </td><td>3567794147543055.5    </td><td>12.208810349667264    </td><td>1                   </td><td>4bfd99c3  </td><td>3.5062531863323887 </td><td>1.6438433648823196 </td></tr>\n",
       "<tr><td>FSR_Trainable_4ee8eda9</td><td>2023-08-10_18-44-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.792924235580367 </td><td>639.206291759454  </td><td>1.1978768387750391</td><td>5.984233371891813e+17 </td><td>0.583760279734666 </td><td>172.26.215.93</td><td style=\"text-align: right;\">112260</td><td>2.2873831284908857</td><td>478.1354924797756 </td><td>113.23481726646423  </td><td>1.071162462234497 </td><td>113.23481726646423</td><td style=\"text-align: right;\"> 1691660695</td><td>0.9643846210967336</td><td>0.23779855497586153</td><td>173096999910551.84    </td><td>244684357853990.03    </td><td>100                 </td><td>4ee8eda9  </td><td>0.4244903132141791 </td><td>0.1592699665204869 </td></tr>\n",
       "<tr><td>FSR_Trainable_4f489564</td><td>2023-08-10_18-45-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>21.26889047954592 </td><td>1644.1746735417767</td><td>3.4070420939454475</td><td>2.073742580602853e+18 </td><td>1.8120076187622467</td><td>172.26.215.93</td><td style=\"text-align: right;\">113954</td><td>7.196713246495204 </td><td>1203.556827569772 </td><td>1.8265538215637207  </td><td>1.8265538215637207</td><td>1.8265538215637207</td><td style=\"text-align: right;\"> 1691660751</td><td>4.458644255956987 </td><td>0.6073733436030656 </td><td>60381755123863.06     </td><td>905779518532899.2     </td><td>1                   </td><td>4f489564  </td><td>1.4364313785528018 </td><td>0.3755762402094448 </td></tr>\n",
       "<tr><td>FSR_Trainable_5506c20c</td><td>2023-08-10_18-54-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.753620380810058 </td><td>682.8241877259356 </td><td>1.153857510126809 </td><td>7.303498610284517e+17 </td><td>0.5798400644763736</td><td>172.26.215.93</td><td style=\"text-align: right;\">119064</td><td>2.2650901213169714</td><td>494.5410300773097 </td><td>118.39657688140869  </td><td>1.3942267894744873</td><td>118.39657688140869</td><td style=\"text-align: right;\"> 1691661298</td><td>0.9524499708947647</td><td>0.25083500526634533</td><td>148336833846883.9     </td><td>293792546550967.8     </td><td>100                 </td><td>5506c20c  </td><td>0.4182936057152833 </td><td>0.16154645876109033</td></tr>\n",
       "<tr><td>FSR_Trainable_559affac</td><td>2023-08-10_18-42-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>4.527771722390786 </td><td>1359.7817346552333</td><td>1.220630734738679 </td><td>1.2910064961555883e+17</td><td>20.75072476981171 </td><td>172.26.215.93</td><td style=\"text-align: right;\">111833</td><td>2.4781496305694173</td><td>1114.498485861354 </td><td>4.343064308166504   </td><td>4.343064308166504 </td><td>4.343064308166504 </td><td style=\"text-align: right;\"> 1691660554</td><td>12.482978399728397</td><td>11.207849431466583 </td><td>3434602860862649.5    </td><td>6002380662840844.0    </td><td>1                   </td><td>559affac  </td><td>7.807327804188591  </td><td>12.943396965623121 </td></tr>\n",
       "<tr><td>FSR_Trainable_5953e60a</td><td>2023-08-10_18-51-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td>4                         </td><td>5.882415039051651 </td><td>768.9783708886775 </td><td>1.4088082437850156</td><td>5.810727522898254e+17 </td><td>0.6927672389534982</td><td>172.26.215.93</td><td style=\"text-align: right;\">117960</td><td>2.6209266644334517</td><td>595.9013564326297 </td><td>5.426161766052246   </td><td>1.131000280380249 </td><td>5.426161766052246 </td><td style=\"text-align: right;\"> 1691661102</td><td>1.1618229642704232</td><td>0.29551180407017263</td><td>168850851182707.47    </td><td>276442806763622.78    </td><td>4                   </td><td>5953e60a  </td><td>0.4874619669249679 </td><td>0.20530527202853036</td></tr>\n",
       "<tr><td>FSR_Trainable_59e2c3b1</td><td>2023-08-10_18-56-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td>64                        </td><td>5.181563924399479 </td><td>707.1219265526545 </td><td>1.2417189973775755</td><td>7.435537835836913e+17 </td><td>0.623438750130909 </td><td>172.26.215.93</td><td style=\"text-align: right;\">121787</td><td>2.4401207231733784</td><td>521.2644228812593 </td><td>75.99145483970642   </td><td>1.0906703472137451</td><td>75.99145483970642 </td><td style=\"text-align: right;\"> 1691661402</td><td>1.029066847026485 </td><td>0.26604100533838754</td><td>151418722398646.5     </td><td>314131765607838.75    </td><td>64                  </td><td>59e2c3b1  </td><td>0.4495071829769091 </td><td>0.1739315671539999 </td></tr>\n",
       "<tr><td>FSR_Trainable_5f9dc5f2</td><td>2023-08-10_18-32-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>5.0225776940224005</td><td>1138.4161301807653</td><td>1.352784540400186 </td><td>1.296867867181274e+17 </td><td>18.638463677660813</td><td>172.26.215.93</td><td style=\"text-align: right;\">107104</td><td>2.46167518673357  </td><td>959.120814370453  </td><td>2.31988787651062    </td><td>2.31988787651062  </td><td>2.31988787651062  </td><td style=\"text-align: right;\"> 1691659951</td><td>14.590004193992586</td><td>9.155482487069257  </td><td>1.0972818231725304e+16</td><td>6195499752510341.0    </td><td>1                   </td><td>5f9dc5f2  </td><td>8.060228242372276  </td><td>10.578235435288537 </td></tr>\n",
       "<tr><td>FSR_Trainable_63b28b94</td><td>2023-08-10_18-58-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.885244543118859 </td><td>693.6980161573335 </td><td>1.1835359004697061</td><td>6.915858499208209e+17 </td><td>0.6048392379219825</td><td>172.26.215.93</td><td style=\"text-align: right;\">122285</td><td>2.357078249273675 </td><td>517.8971048664265 </td><td>116.41324472427368  </td><td>1.0639827251434326</td><td>116.41324472427368</td><td style=\"text-align: right;\"> 1691661499</td><td>0.9739922308862529</td><td>0.2549438828962991 </td><td>141638520400861.72    </td><td>256602383371107.78    </td><td>100                 </td><td>63b28b94  </td><td>0.43256031176871224</td><td>0.1722789261532702 </td></tr>\n",
       "<tr><td>FSR_Trainable_657010b8</td><td>2023-08-10_18-49-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.688684881408228 </td><td>643.3712901753548 </td><td>1.1755584232309662</td><td>5.982646888022163e+17 </td><td>0.5862956062235937</td><td>172.26.215.93</td><td style=\"text-align: right;\">115590</td><td>2.2875222660334975</td><td>482.2003086303472 </td><td>121.03002762794495  </td><td>1.1154561042785645</td><td>121.03002762794495</td><td style=\"text-align: right;\"> 1691660994</td><td>0.940250090332367 </td><td>0.24578281568723867</td><td>176436192474456.5     </td><td>254674016792480.1     </td><td>100                 </td><td>657010b8  </td><td>0.4216557309212346 </td><td>0.1646398753023591 </td></tr>\n",
       "<tr><td>FSR_Trainable_6609d819</td><td>2023-08-10_18-54-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>4.364009228575249 </td><td>1294.660519435062 </td><td>1.2241637418110223</td><td>4016998705.2389364    </td><td>5.575368223654753 </td><td>172.26.215.93</td><td style=\"text-align: right;\">120881</td><td>2.40206770385226  </td><td>966.5519013203099 </td><td>2.4322311878204346  </td><td>2.4322311878204346</td><td>2.4322311878204346</td><td style=\"text-align: right;\"> 1691661273</td><td>6.8065251408867375</td><td>3.2305083531957597 </td><td>21.73073981277069     </td><td>11.83346050694049     </td><td>1                   </td><td>6609d819  </td><td>3.4364122091281972 </td><td>2.138956014526556  </td></tr>\n",
       "<tr><td>FSR_Trainable_661f2b44</td><td>2023-08-10_18-57-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>5.924645868913044 </td><td>1267.5231750074688</td><td>1.4457367002329629</td><td>1.240486131336399e+18 </td><td>0.8392804185191343</td><td>172.26.215.93</td><td style=\"text-align: right;\">123864</td><td>2.726548352582695 </td><td>1017.0531714309302</td><td>2.0551164150238037  </td><td>2.0551164150238037</td><td>2.0551164150238037</td><td style=\"text-align: right;\"> 1691661479</td><td>1.202253810718211 </td><td>0.4368917589766339 </td><td>194554237931513.5     </td><td>415629541095644.3     </td><td>1                   </td><td>661f2b44  </td><td>0.5194249292399575 </td><td>0.3198554892791768 </td></tr>\n",
       "<tr><td>FSR_Trainable_6656c7f0</td><td>2023-08-10_18-48-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>5.407670964116444 </td><td>1037.4894761379232</td><td>1.4030375824973642</td><td>7.745252470729156e+17 </td><td>0.7556960904802794</td><td>172.26.215.93</td><td style=\"text-align: right;\">116488</td><td>2.548281825598205 </td><td>881.9985847545414 </td><td>4.903895378112793   </td><td>2.2607104778289795</td><td>4.903895378112793 </td><td style=\"text-align: right;\"> 1691660925</td><td>1.0809799073735722</td><td>0.368020036007123  </td><td>196400177744552.06    </td><td>298185637182216.9     </td><td>2                   </td><td>6656c7f0  </td><td>0.47795589240873093</td><td>0.27774019807154854</td></tr>\n",
       "<tr><td>FSR_Trainable_66fede0d</td><td>2023-08-10_18-49-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.891710140660245 </td><td>734.1949674589772 </td><td>1.2094140619063098</td><td>7.293528878855702e+17 </td><td>0.6179278788759434</td><td>172.26.215.93</td><td style=\"text-align: right;\">115065</td><td>2.3513754677976566</td><td>547.7457477522074 </td><td>121.26182508468628  </td><td>1.3809185028076172</td><td>121.26182508468628</td><td style=\"text-align: right;\"> 1691660942</td><td>0.9751769327510835</td><td>0.2705525740365446 </td><td>160360685073988.25    </td><td>277046067576227.78    </td><td>100                 </td><td>66fede0d  </td><td>0.43272595752591836</td><td>0.18520192135002497</td></tr>\n",
       "<tr><td>FSR_Trainable_67a7a972</td><td>2023-08-10_18-54-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>16.80080777300562 </td><td>2448.0792888745937</td><td>2.8514013166393024</td><td>4.612125819683817e+18 </td><td>1.693759607838346 </td><td>172.26.215.93</td><td style=\"text-align: right;\">121091</td><td>5.980268880199795 </td><td>1406.3979511599305</td><td>1.4345569610595703  </td><td>1.4345569610595703</td><td>1.4345569610595703</td><td style=\"text-align: right;\"> 1691661282</td><td>3.4755575435801873</td><td>1.018919059768741  </td><td>36542873954502.42     </td><td>2180728308051062.2    </td><td>1                   </td><td>67a7a972  </td><td>1.1524873302413998 </td><td>0.541272277596946  </td></tr>\n",
       "<tr><td>FSR_Trainable_693db682</td><td>2023-08-10_18-46-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>5.826610044279479 </td><td>1075.367786809644 </td><td>1.4327774265803008</td><td>9.975890702367896e+17 </td><td>0.7861174881302321</td><td>172.26.215.93</td><td style=\"text-align: right;\">114613</td><td>2.710860415841697 </td><td>857.8504247405422 </td><td>3.871762990951538   </td><td>1.868407964706421 </td><td>3.871762990951538 </td><td style=\"text-align: right;\"> 1691660788</td><td>1.1690847432193814</td><td>0.3835557340367112 </td><td>197181413440942.22    </td><td>373143445402241.2     </td><td>2                   </td><td>693db682  </td><td>0.5107128131370023 </td><td>0.27540467499322974</td></tr>\n",
       "<tr><td>FSR_Trainable_6e1464a0</td><td>2023-08-10_18-26-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.963795132606533 </td><td>829.7022932606149 </td><td>1.2501343406527399</td><td>7.815097620713126e+17 </td><td>0.6701538007890373</td><td>172.26.215.93</td><td style=\"text-align: right;\">102958</td><td>2.4627457309405645</td><td>635.0700222714396 </td><td>313.70996499061584  </td><td>3.257516860961914 </td><td>313.70996499061584</td><td style=\"text-align: right;\"> 1691659565</td><td>1.0063796303567762</td><td>0.30468049695930505</td><td>148793092207639.66    </td><td>312331921766861.0     </td><td>100                 </td><td>6e1464a0  </td><td>0.46401202966136507</td><td>0.20614177112767226</td></tr>\n",
       "<tr><td>FSR_Trainable_71596cc8</td><td>2023-08-10_18-38-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>4.920285457411131 </td><td>1366.968953526283 </td><td>1.4106406425622031</td><td>3.158492932843049e+17 </td><td>19.429415517517526</td><td>172.26.215.93</td><td style=\"text-align: right;\">109473</td><td>2.471603636478465 </td><td>1082.801022151191 </td><td>2.025541305541992   </td><td>2.025541305541992 </td><td>2.025541305541992 </td><td style=\"text-align: right;\"> 1691660339</td><td>13.180088509433398</td><td>11.500148006313706 </td><td>6054966506948064.0    </td><td>1.307072552641133e+16 </td><td>1                   </td><td>71596cc8  </td><td>7.6875322163644215 </td><td>11.741883301153104 </td></tr>\n",
       "<tr><td>FSR_Trainable_71d3ee26</td><td>2023-08-10_18-57-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.8321000407861225</td><td>731.1145099559028 </td><td>1.2320744761955225</td><td>8.473425187582676e+17 </td><td>0.6015016874741748</td><td>172.26.215.93</td><td style=\"text-align: right;\">121335</td><td>2.3145143366035494</td><td>538.6245915419116 </td><td>119.24562358856201  </td><td>1.1601524353027344</td><td>119.24562358856201</td><td style=\"text-align: right;\"> 1691661428</td><td>0.9662306481677335</td><td>0.26988380526027883</td><td>164222524507744.44    </td><td>334206013642840.7     </td><td>100                 </td><td>71d3ee26  </td><td>0.4250403636064477 </td><td>0.1764613238677271 </td></tr>\n",
       "<tr><td>FSR_Trainable_7989f1e3</td><td>2023-08-10_18-51-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.478191860248565 </td><td>634.6765048028476 </td><td>1.1536002336767905</td><td>6.254213126579826e+17 </td><td>0.5681236497293027</td><td>172.26.215.93</td><td style=\"text-align: right;\">117173</td><td>2.207591407986732 </td><td>476.0707125687979 </td><td>115.2357804775238   </td><td>1.1254627704620361</td><td>115.2357804775238 </td><td style=\"text-align: right;\"> 1691661093</td><td>0.9030508901339591</td><td>0.2372372537857034 </td><td>165641038382367.5     </td><td>258330526150008.53    </td><td>100                 </td><td>7989f1e3  </td><td>0.40895058090251707</td><td>0.15917306882678567</td></tr>\n",
       "<tr><td>FSR_Trainable_7ae6b73f</td><td>2023-08-10_18-38-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>5.435222688460805 </td><td>1242.253049595259 </td><td>1.3659813762186477</td><td>2.6370005330123968e+17</td><td>20.621863665536885</td><td>172.26.215.93</td><td style=\"text-align: right;\">109293</td><td>2.6109612865519876</td><td>1040.9524415431108</td><td>1.867030382156372   </td><td>1.867030382156372 </td><td>1.867030382156372 </td><td style=\"text-align: right;\"> 1691660329</td><td>17.417277881801812</td><td>10.935486904865197 </td><td>1.557343127697686e+16 </td><td>1.2022323147178144e+16</td><td>1                   </td><td>7ae6b73f  </td><td>9.416688139302604  </td><td>11.205175526234282 </td></tr>\n",
       "<tr><td>FSR_Trainable_7c1a30a3</td><td>2023-08-10_18-29-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.222665646276577 </td><td>1578.685937068754 </td><td>1.2226561525635014</td><td>2.4988762304348247e+18</td><td>0.8069047141700866</td><td>172.26.215.93</td><td style=\"text-align: right;\">105140</td><td>2.426584679847354 </td><td>1109.7823507755706</td><td>159.14091730117798  </td><td>1.8366878032684326</td><td>159.14091730117798</td><td style=\"text-align: right;\"> 1691659749</td><td>0.8558391940826301</td><td>0.5582698350907649 </td><td>203722546256873.5     </td><td>907425416724497.5     </td><td>100                 </td><td>7c1a30a3  </td><td>0.45749585204967474</td><td>0.34940886212041183</td></tr>\n",
       "<tr><td>FSR_Trainable_8043f276</td><td>2023-08-10_18-19-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>4.50501543802844  </td><td>1343.463084993248 </td><td>1.3423657453353852</td><td>1.6254305242208586e+17</td><td>20.21938967183394 </td><td>172.26.215.93</td><td style=\"text-align: right;\">101764</td><td>2.4226548075138794</td><td>1047.9734765702947</td><td>2.8373029232025146  </td><td>2.8373029232025146</td><td>2.8373029232025146</td><td style=\"text-align: right;\"> 1691659169</td><td>12.593063608233766</td><td>10.805994880582507 </td><td>3509439154933622.5    </td><td>6699290735216808.0    </td><td>1                   </td><td>8043f276  </td><td>7.734056780591839  </td><td>12.485332891242102 </td></tr>\n",
       "<tr><td>FSR_Trainable_810fa16b</td><td>2023-08-10_18-33-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.9141381874622   </td><td>800.2360922774488 </td><td>1.2300512001594137</td><td>6.323296512200028e+17 </td><td>0.6570444953755987</td><td>172.26.215.93</td><td style=\"text-align: right;\">106137</td><td>2.442863841033423 </td><td>636.3514072986666 </td><td>238.75078701972961  </td><td>2.094191551208496 </td><td>238.75078701972961</td><td style=\"text-align: right;\"> 1691660014</td><td>0.979202970296897 </td><td>0.2997683653888715 </td><td>139714685036390.45    </td><td>284567740997299.8     </td><td>100                 </td><td>810fa16b  </td><td>0.4480156172802696 </td><td>0.20902887809532905</td></tr>\n",
       "<tr><td>FSR_Trainable_814e4b4c</td><td>2023-08-10_18-24-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>5.166570638282859 </td><td>761.3633061100222 </td><td>1.2233213005865284</td><td>8.575137829229923e+17 </td><td>0.6157500464100194</td><td>172.26.215.93</td><td style=\"text-align: right;\">103456</td><td>2.3969011782804883</td><td>566.9985035711593 </td><td>125.55586075782776  </td><td>1.2434327602386475</td><td>125.55586075782776</td><td style=\"text-align: right;\"> 1691659457</td><td>1.0260047842276032</td><td>0.269784392786441  </td><td>165403333040653.28    </td><td>300583367368256.5     </td><td>100                 </td><td>814e4b4c  </td><td>0.4384683315365273 </td><td>0.17728171487349212</td></tr>\n",
       "<tr><td>FSR_Trainable_81679c0e</td><td>2023-08-10_18-52-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>6.038047454071105 </td><td>1335.0760340480767</td><td>1.4200104832819103</td><td>1.3583388948118106e+18</td><td>0.8628309963478744</td><td>172.26.215.93</td><td style=\"text-align: right;\">118843</td><td>2.7629350064758   </td><td>1055.515022768308 </td><td>1.9072623252868652  </td><td>1.9072623252868652</td><td>1.9072623252868652</td><td style=\"text-align: right;\"> 1691661150</td><td>1.1999040781949983</td><td>0.4670026574457704 </td><td>167825290212664.06    </td><td>466868231246065.3     </td><td>1                   </td><td>81679c0e  </td><td>0.5238151305347999 </td><td>0.3390158658130746 </td></tr>\n",
       "<tr><td>FSR_Trainable_82eb35d9</td><td>2023-08-10_18-54-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>4.635652269349211 </td><td>1517.8133982079635</td><td>1.273106413327504 </td><td>3.419725359718593e+17 </td><td>24.258407357392933</td><td>172.26.215.93</td><td style=\"text-align: right;\">120430</td><td>2.4706739675100446</td><td>1113.9026875778404</td><td>3.15019154548645    </td><td>3.15019154548645  </td><td>3.15019154548645  </td><td style=\"text-align: right;\"> 1691661251</td><td>15.979220368429825</td><td>13.008126634117657 </td><td>5790182091903932.0    </td><td>1.3924541293149068e+16</td><td>1                   </td><td>82eb35d9  </td><td>11.452668834564296 </td><td>12.805738522828635 </td></tr>\n",
       "<tr><td>FSR_Trainable_8568305e</td><td>2023-08-10_18-45-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.787944962758922 </td><td>668.6105899910158 </td><td>1.2088269065624055</td><td>6.113335475289336e+17 </td><td>0.6062310688259154</td><td>172.26.215.93</td><td style=\"text-align: right;\">112505</td><td>2.3223564307190414</td><td>512.5258056672989 </td><td>111.61169195175171  </td><td>1.104224681854248 </td><td>111.61169195175171</td><td style=\"text-align: right;\"> 1691660705</td><td>0.9631438906309526</td><td>0.24973397759952978</td><td>172207739868550.9     </td><td>249683607182591.56    </td><td>100                 </td><td>8568305e  </td><td>0.4329235686417277 </td><td>0.17330750018418764</td></tr>\n",
       "<tr><td>FSR_Trainable_870a8bf6</td><td>2023-08-10_18-48-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>25.685406506974413</td><td>1800.3526315840884</td><td>4.4443761160766995</td><td>1.3697129792521477e+18</td><td>2.1376354796978494</td><td>172.26.215.93</td><td style=\"text-align: right;\">116016</td><td>8.816709077638444 </td><td>1389.913168296831 </td><td>3.412203311920166   </td><td>3.412203311920166 </td><td>3.412203311920166 </td><td style=\"text-align: right;\"> 1691660888</td><td>5.250921674456958 </td><td>0.7420755637517482 </td><td>86775597866444.5      </td><td>938370343573848.4     </td><td>1                   </td><td>870a8bf6  </td><td>1.684503431996174  </td><td>0.4531320477016754 </td></tr>\n",
       "<tr><td>FSR_Trainable_8d7cf828</td><td>2023-08-10_18-53-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td>4                         </td><td>5.898558222095603 </td><td>815.928698657978  </td><td>1.384211156641389 </td><td>7.522391138059643e+17 </td><td>0.6932494134287359</td><td>172.26.215.93</td><td style=\"text-align: right;\">119772</td><td>2.6215819501094306</td><td>621.0103298535745 </td><td>5.389394998550415   </td><td>1.1012792587280273</td><td>5.389394998550415 </td><td style=\"text-align: right;\"> 1691661208</td><td>1.166828679664498 </td><td>0.30609672863461085</td><td>167293546219470.62    </td><td>330928690828463.0     </td><td>4                   </td><td>8d7cf828  </td><td>0.4864935695253571 </td><td>0.2067558439033788 </td></tr>\n",
       "<tr><td>FSR_Trainable_96ca6737</td><td>2023-08-10_18-59-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td>64                        </td><td>4.981579712155843 </td><td>697.948658107261  </td><td>1.216518991494607 </td><td>6.543449694456748e+17 </td><td>0.6130562395840407</td><td>172.26.215.93</td><td style=\"text-align: right;\">124094</td><td>2.38622716893351  </td><td>528.5211272050019 </td><td>52.248836278915405  </td><td>0.7175703048706055</td><td>52.248836278915405</td><td style=\"text-align: right;\"> 1691661546</td><td>0.9908616554924068</td><td>0.2610554195803682 </td><td>171022849197945.25    </td><td>279383966500282.66    </td><td>64                  </td><td>96ca6737  </td><td>0.43876940086756283</td><td>0.17428683871647777</td></tr>\n",
       "<tr><td>FSR_Trainable_97cbfcc7</td><td>2023-08-10_18-55-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td>16                        </td><td>5.525101414396585 </td><td>779.0658651867108 </td><td>1.2802423219475225</td><td>8.48497154379353e+17  </td><td>0.6432246751787107</td><td>172.26.215.93</td><td style=\"text-align: right;\">122011</td><td>2.508546032754182 </td><td>557.2177012680869 </td><td>19.268160820007324  </td><td>1.184648036956787 </td><td>19.268160820007324</td><td style=\"text-align: right;\"> 1691661352</td><td>1.091932670501871 </td><td>0.28789947448610437</td><td>161798601951588.75    </td><td>348774015543661.1     </td><td>16                  </td><td>97cbfcc7  </td><td>0.4600465717240268 </td><td>0.18317810345468397</td></tr>\n",
       "<tr><td>FSR_Trainable_98eea35e</td><td>2023-08-10_18-46-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.521455500713058 </td><td>642.0804530600386 </td><td>1.153121118396766 </td><td>5.684234074602202e+17 </td><td>0.5914503180459509</td><td>172.26.215.93</td><td style=\"text-align: right;\">112996</td><td>2.258709497552871 </td><td>493.80276629277427</td><td>113.481454372406    </td><td>1.445734977722168 </td><td>113.481454372406  </td><td style=\"text-align: right;\"> 1691660765</td><td>0.9062061027684281</td><td>0.24134862248849212</td><td>165688204068420.8     </td><td>221358200333302.12    </td><td>100                 </td><td>98eea35e  </td><td>0.41770245744966655</td><td>0.17374786059628441</td></tr>\n",
       "<tr><td>FSR_Trainable_9d7f317c</td><td>2023-08-10_18-47-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td>32                        </td><td>5.5218213229965425</td><td>782.4302285354238 </td><td>1.322599741644886 </td><td>7.497350963869071e+17 </td><td>0.6503813816080037</td><td>172.26.215.93</td><td style=\"text-align: right;\">115308</td><td>2.5082869032354895</td><td>568.0427722148465 </td><td>35.893354654312134  </td><td>1.1971006393432617</td><td>35.893354654312134</td><td style=\"text-align: right;\"> 1691660856</td><td>1.097660351132471 </td><td>0.2878651939229876 </td><td>179923908909764.3     </td><td>299815489229816.0     </td><td>32                  </td><td>9d7f317c  </td><td>0.4618855807535164 </td><td>0.1884958008544874 </td></tr>\n",
       "<tr><td>FSR_Trainable_9d8e4ab3</td><td>2023-08-10_18-38-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td>32                        </td><td>5.728532901698839 </td><td>764.1878999773655 </td><td>1.329876900685756 </td><td>5.560497876350844e+17 </td><td>0.6806954327866411</td><td>172.26.215.93</td><td style=\"text-align: right;\">108296</td><td>2.5828250951836424</td><td>596.7363556901938 </td><td>160.29749727249146  </td><td>4.410426378250122 </td><td>160.29749727249146</td><td style=\"text-align: right;\"> 1691660318</td><td>1.1487103274193262</td><td>0.2874132944320848 </td><td>179912003203445.28    </td><td>250230391044880.4     </td><td>32                  </td><td>9d8e4ab3  </td><td>0.48135580696692026</td><td>0.19933962581972084</td></tr>\n",
       "<tr><td>FSR_Trainable_9fa227d3</td><td>2023-08-10_18-35-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>5.454406350905143 </td><td>781.3215251703645 </td><td>1.2570941128931301</td><td>5.757298931625452e+17 </td><td>0.6679692739718701</td><td>172.26.215.93</td><td style=\"text-align: right;\">106618</td><td>2.5103668643040615</td><td>614.880220056642  </td><td>264.26598262786865  </td><td>2.4128122329711914</td><td>264.26598262786865</td><td style=\"text-align: right;\"> 1691660125</td><td>1.1050176521659172</td><td>0.28591828466590435</td><td>155740912757487.6     </td><td>234864420923030.03    </td><td>100                 </td><td>9fa227d3  </td><td>0.46765941853012655</td><td>0.2003098554417435 </td></tr>\n",
       "<tr><td>FSR_Trainable_a0aaf09e</td><td>2023-08-10_18-25-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td>16                        </td><td>10.794399357226606</td><td>1550.4802264604432</td><td>1.9558739997151364</td><td>1.6030866620310444e+18</td><td>1.1334954556391565</td><td>172.26.215.93</td><td style=\"text-align: right;\">104629</td><td>3.8713029958971776</td><td>1196.9459291274334</td><td>22.51334524154663   </td><td>1.1050598621368408</td><td>22.51334524154663 </td><td style=\"text-align: right;\"> 1691659530</td><td>2.24477657652485  </td><td>0.5883099943490637 </td><td>157130586292901.7     </td><td>802992840979491.9     </td><td>16                  </td><td>a0aaf09e  </td><td>0.7567681713110916 </td><td>0.376727284328065  </td></tr>\n",
       "<tr><td>FSR_Trainable_a1b93071</td><td>2023-08-10_18-30-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td>16                        </td><td>5.604983100722519 </td><td>1452.966878336718 </td><td>1.3205803386453738</td><td>1.631153981857847e+18 </td><td>0.8504632256802478</td><td>172.26.215.93</td><td style=\"text-align: right;\">106352</td><td>2.6217463837886017</td><td>1129.1771433983065</td><td>50.38062143325806   </td><td>3.455662488937378 </td><td>50.38062143325806 </td><td style=\"text-align: right;\"> 1691659835</td><td>1.124389992461646 </td><td>0.4997569585414268 </td><td>175585191023693.66    </td><td>517248476034941.3     </td><td>16                  </td><td>a1b93071  </td><td>0.48988862308560643</td><td>0.36057460259464136</td></tr>\n",
       "<tr><td>FSR_Trainable_a449dfdd</td><td>2023-08-10_18-51-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.68602146946972  </td><td>617.8276563633082 </td><td>1.11778916863457  </td><td>5.279436254018333e+17 </td><td>0.5810754069724162</td><td>172.26.215.93</td><td style=\"text-align: right;\">116736</td><td>2.2789143158744305</td><td>480.8441451681882 </td><td>116.48890781402588  </td><td>1.0992274284362793</td><td>116.48890781402588</td><td style=\"text-align: right;\"> 1691661065</td><td>0.9410291941068332</td><td>0.23049795342567958</td><td>146246833007481.4     </td><td>223143553549052.66    </td><td>100                 </td><td>a449dfdd  </td><td>0.41967902102093746</td><td>0.1613963859514788 </td></tr>\n",
       "<tr><td>FSR_Trainable_a4dc490d</td><td>2023-08-10_18-42-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>4.345738395606129 </td><td>1412.9849382147372</td><td>1.279476472431952 </td><td>4.684470610660095e+16 </td><td>21.246120671302947</td><td>172.26.215.93</td><td style=\"text-align: right;\">111602</td><td>2.4247057306424424</td><td>1179.0847690077958</td><td>1.5079772472381592  </td><td>1.5079772472381592</td><td>1.5079772472381592</td><td style=\"text-align: right;\"> 1691660540</td><td>12.173435095730134</td><td>11.078907230393797 </td><td>2455627335042844.0    </td><td>1323957686480296.2    </td><td>1                   </td><td>a4dc490d  </td><td>7.725637857778217  </td><td>13.52048281352473  </td></tr>\n",
       "<tr><td>FSR_Trainable_a8768c06</td><td>2023-08-10_18-57-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td>8                         </td><td>5.640443005774074 </td><td>786.449629072889  </td><td>1.3389614104843217</td><td>7.644253585806555e+17 </td><td>0.6598054198853535</td><td>172.26.215.93</td><td style=\"text-align: right;\">123652</td><td>2.5425905568552416</td><td>567.5858834871503 </td><td>10.462431907653809  </td><td>1.1427583694458008</td><td>10.462431907653809</td><td style=\"text-align: right;\"> 1691661476</td><td>1.1187774991980015</td><td>0.29518595537842   </td><td>186756843315572.7     </td><td>336897744178774.44    </td><td>8                   </td><td>a8768c06  </td><td>0.4704516161965948 </td><td>0.18935380368875862</td></tr>\n",
       "<tr><td>FSR_Trainable_ab536d2b</td><td>2023-08-10_18-20-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>4.406574793695324 </td><td>1577.6276595445272</td><td>1.2247379947748356</td><td>4820156201.380469     </td><td>6.033798454461417 </td><td>172.26.215.93</td><td style=\"text-align: right;\">102674</td><td>2.4605149082467594</td><td>1101.1956169387543</td><td>7.232941627502441   </td><td>7.232941627502441 </td><td>7.232941627502441 </td><td style=\"text-align: right;\"> 1691659225</td><td>6.863424666929355 </td><td>3.9903794308153837 </td><td>22.860219944499335    </td><td>6.4788792857160615    </td><td>1                   </td><td>ab536d2b  </td><td>3.5297411830097687 </td><td>2.504057271451648  </td></tr>\n",
       "<tr><td>FSR_Trainable_af2a1820</td><td>2023-08-10_18-24-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>4.24884167474061  </td><td>1541.2721823083532</td><td>1.2536798750392542</td><td>4249750723.2463145    </td><td>6.0421342398427   </td><td>172.26.215.93</td><td style=\"text-align: right;\">103987</td><td>2.407369677713559 </td><td>1090.2646062026024</td><td>6.994189262390137   </td><td>6.994189262390137 </td><td>6.994189262390137 </td><td style=\"text-align: right;\"> 1691659475</td><td>6.847511210036624 </td><td>3.858885337509266  </td><td>1068187901976018.1    </td><td>7.465464961154824     </td><td>1                   </td><td>af2a1820  </td><td>3.5276864403432984 </td><td>2.514447799499402  </td></tr>\n",
       "<tr><td>FSR_Trainable_b1c1eb81</td><td>2023-08-10_18-53-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>4.688753639278132 </td><td>1069.8400684157718</td><td>1.1718687912203327</td><td>1.176575262315594e+17 </td><td>22.75827884734785 </td><td>172.26.215.93</td><td style=\"text-align: right;\">120188</td><td>2.455527518443756 </td><td>840.1151935206589 </td><td>1.6006715297698975  </td><td>1.6006715297698975</td><td>1.6006715297698975</td><td style=\"text-align: right;\"> 1691661233</td><td>15.771318507522869</td><td>9.693525582720692  </td><td>8019104871982045.0    </td><td>6174984601169458.0    </td><td>1                   </td><td>b1c1eb81  </td><td>11.141061315571433 </td><td>11.617217531776417 </td></tr>\n",
       "<tr><td>FSR_Trainable_b346be41</td><td>2023-08-10_18-29-25</td><td>False </td><td>DESKTOP-0P789CI</td><td>79                        </td><td>5.365001579832684 </td><td>763.3561094848144 </td><td>1.3739085754593903</td><td>5.597359023361988e+17 </td><td>0.6516408985894551</td><td>172.26.215.93</td><td style=\"text-align: right;\">105391</td><td>2.426681526665452 </td><td>613.2073768755002 </td><td>137.1273810863495   </td><td>2.093050003051758 </td><td>137.1273810863495 </td><td style=\"text-align: right;\"> 1691659765</td><td>1.0778092400755879</td><td>0.2840635331195096 </td><td>183101820832915.44    </td><td>252303314836199.03    </td><td>79                  </td><td>b346be41  </td><td>0.45304674622571167</td><td>0.1985941523637434 </td></tr>\n",
       "<tr><td>FSR_Trainable_b377cc00</td><td>2023-08-10_18-47-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>27.899267505187495</td><td>2092.864986055035 </td><td>4.580266369801668 </td><td>2.8784622187525473e+18</td><td>2.3132358069099084</td><td>172.26.215.93</td><td style=\"text-align: right;\">115793</td><td>8.960395150676899 </td><td>1374.2640105678788</td><td>1.514087200164795   </td><td>1.514087200164795 </td><td>1.514087200164795 </td><td style=\"text-align: right;\"> 1691660871</td><td>5.889012866906747 </td><td>0.8480952118814858 </td><td>54348876880627.22     </td><td>1380309432148300.2    </td><td>1                   </td><td>b377cc00  </td><td>1.8409163731873646 </td><td>0.4723194337225438 </td></tr>\n",
       "<tr><td>FSR_Trainable_b62849af</td><td>2023-08-10_18-47-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.551430208575144 </td><td>629.8609618437032 </td><td>1.1800794421110619</td><td>5.328862479549449e+17 </td><td>0.5889131828187836</td><td>172.26.215.93</td><td style=\"text-align: right;\">113300</td><td>2.287832288803958 </td><td>482.1547771699053 </td><td>117.19987630844116  </td><td>1.1962246894836426</td><td>117.19987630844116</td><td style=\"text-align: right;\"> 1691660845</td><td>0.9127955705451892</td><td>0.23733433336475557</td><td>176845950519289.03    </td><td>219189017325514.72    </td><td>100                 </td><td>b62849af  </td><td>0.4230736189036645 </td><td>0.16583956391511906</td></tr>\n",
       "<tr><td>FSR_Trainable_b6b8ae6f</td><td>2023-08-10_18-35-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.7263947891541855</td><td>712.1885463214813 </td><td>1.185615423737418 </td><td>6.453766473141978e+17 </td><td>0.5944396339146213</td><td>172.26.215.93</td><td style=\"text-align: right;\">107505</td><td>2.28367757917763  </td><td>567.4914208062019 </td><td>166.8282287120819   </td><td>1.3394882678985596</td><td>166.8282287120819 </td><td style=\"text-align: right;\"> 1691660150</td><td>0.9406687819237187</td><td>0.25070185316947463</td><td>163617032974172.7     </td><td>242436103107391.75    </td><td>100                 </td><td>b6b8ae6f  </td><td>0.4219175940043796 </td><td>0.1725220399102417 </td></tr>\n",
       "<tr><td>FSR_Trainable_b7ffc6c7</td><td>2023-08-10_18-54-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td>16                        </td><td>5.470592006018864 </td><td>776.5072922792458 </td><td>1.2864587297626475</td><td>7.549351011276353e+17 </td><td>0.6447396991226895</td><td>172.26.215.93</td><td style=\"text-align: right;\">120004</td><td>2.4841235164144067</td><td>571.7269589232266 </td><td>20.206802129745483  </td><td>1.1778745651245117</td><td>20.206802129745483</td><td style=\"text-align: right;\"> 1691661244</td><td>1.09549795078155  </td><td>0.2863121352479854 </td><td>182880569173529.06    </td><td>317302036674930.75    </td><td>16                  </td><td>b7ffc6c7  </td><td>0.45850080620427724</td><td>0.18623889291841228</td></tr>\n",
       "<tr><td>FSR_Trainable_bb43cf10</td><td>2023-08-10_18-43-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.566975071702105 </td><td>623.6322909794866 </td><td>1.1573196733976416</td><td>5.226702581657034e+17 </td><td>0.5826570425507236</td><td>172.26.215.93</td><td style=\"text-align: right;\">110925</td><td>2.2747495542726113</td><td>484.2579642748044 </td><td>115.88914752006531  </td><td>1.1111564636230469</td><td>115.88914752006531</td><td style=\"text-align: right;\"> 1691660627</td><td>0.9227003626930139</td><td>0.23192626050721166</td><td>163816220759069.88    </td><td>213948149449955.06    </td><td>100                 </td><td>bb43cf10  </td><td>0.4208861665957338 </td><td>0.16177087595498973</td></tr>\n",
       "<tr><td>FSR_Trainable_bbc61711</td><td>2023-08-10_18-24-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td>8                         </td><td>27.965064923796003</td><td>1625.4134512835897</td><td>4.197656193633398 </td><td>1.1504933244004705e+18</td><td>2.3236657980262727</td><td>172.26.215.93</td><td style=\"text-align: right;\">103751</td><td>9.17493997857073  </td><td>1337.972389131344 </td><td>33.12897300720215   </td><td>4.755926609039307 </td><td>33.12897300720215 </td><td style=\"text-align: right;\"> 1691659473</td><td>5.955256719742463 </td><td>0.5944936732891961 </td><td>17617696139972.188    </td><td>555097552594812.7     </td><td>8                   </td><td>bbc61711  </td><td>1.9132485219774322 </td><td>0.41041727604884026</td></tr>\n",
       "<tr><td>FSR_Trainable_bf9fd07a</td><td>2023-08-10_18-51-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>5.351828613995637 </td><td>1204.446926070274 </td><td>1.4038405988081735</td><td>8.649063066004531e+17 </td><td>0.7966641152358899</td><td>172.26.215.93</td><td style=\"text-align: right;\">118159</td><td>2.543800547879312 </td><td>1027.148298278974 </td><td>9.750108242034912   </td><td>4.503917694091797 </td><td>9.750108242034912 </td><td style=\"text-align: right;\"> 1691661119</td><td>1.0545065181690405</td><td>0.42874078751636446</td><td>169740361626301.75    </td><td>348826537106133.5     </td><td>2                   </td><td>bf9fd07a  </td><td>0.47469570265134076</td><td>0.3219684125845491 </td></tr>\n",
       "<tr><td>FSR_Trainable_c2c62f97</td><td>2023-08-10_18-36-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.371355775799216 </td><td>736.4522520189533 </td><td>1.0772586890281668</td><td>7.753516910329288e+17 </td><td>0.5746016274661935</td><td>172.26.215.93</td><td style=\"text-align: right;\">107804</td><td>2.151623346295862 </td><td>556.1402646958812 </td><td>170.78664088249207  </td><td>1.6777653694152832</td><td>170.78664088249207</td><td style=\"text-align: right;\"> 1691660208</td><td>0.8785085855956832</td><td>0.26254018988480254</td><td>153878219371751.72    </td><td>282669252036818.94    </td><td>100                 </td><td>c2c62f97  </td><td>0.3962386506220458 </td><td>0.17836297684414767</td></tr>\n",
       "<tr><td>FSR_Trainable_c3536b16</td><td>2023-08-10_18-57-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>5.491326473054752 </td><td>1036.3175501928233</td><td>1.4205828913964365</td><td>9.613718078424302e+17 </td><td>0.748538797475585 </td><td>172.26.215.93</td><td style=\"text-align: right;\">123185</td><td>2.5594118309270715</td><td>813.2346681679624 </td><td>3.762133836746216   </td><td>1.700333595275879 </td><td>3.762133836746216 </td><td style=\"text-align: right;\"> 1691661449</td><td>1.096644478080924 </td><td>0.3751938828846472 </td><td>195076390057936.4     </td><td>370835801376063.8     </td><td>2                   </td><td>c3536b16  </td><td>0.48167701569165666</td><td>0.2668617817839284 </td></tr>\n",
       "<tr><td>FSR_Trainable_c374aa34</td><td>2023-08-10_18-45-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>4.6047350059049865</td><td>1497.9347799478476</td><td>1.2529573663690938</td><td>6241499422.221941     </td><td>5.896561586779709 </td><td>172.26.215.93</td><td style=\"text-align: right;\">113725</td><td>2.454145305506572 </td><td>1042.306503053922 </td><td>1.620912790298462   </td><td>1.620912790298462 </td><td>1.620912790298462 </td><td style=\"text-align: right;\"> 1691660736</td><td>7.389026882289107 </td><td>3.75428194246314   </td><td>2642176546081312.5    </td><td>9.759616608864128     </td><td>1                   </td><td>c374aa34  </td><td>3.5631698959423037 </td><td>2.333391690837406  </td></tr>\n",
       "<tr><td>FSR_Trainable_c62dfb24</td><td>2023-08-10_18-38-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>5.516805148659303 </td><td>1434.5425970693295</td><td>1.5275173104465052</td><td>1.3190940203501885e+18</td><td>0.8495353394264753</td><td>172.26.215.93</td><td style=\"text-align: right;\">108092</td><td>2.572259517948063 </td><td>1150.658986144934 </td><td>169.78535795211792  </td><td>1.4170587062835693</td><td>169.78535795211792</td><td style=\"text-align: right;\"> 1691660320</td><td>1.0968780057318082</td><td>0.4948645157378962 </td><td>203536248474781.47    </td><td>412741442721642.5     </td><td>100                 </td><td>c62dfb24  </td><td>0.48107885434167247</td><td>0.3684564850848028 </td></tr>\n",
       "<tr><td>FSR_Trainable_c7f219cd</td><td>2023-08-10_18-45-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.60556890245391  </td><td>625.572264091649  </td><td>1.1873404362269737</td><td>5.415546024817047e+17 </td><td>0.5850456488280938</td><td>172.26.215.93</td><td style=\"text-align: right;\">112789</td><td>2.293192654290427 </td><td>472.82647856370653</td><td>115.92100620269775  </td><td>1.220339059829712 </td><td>115.92100620269775</td><td style=\"text-align: right;\"> 1691660754</td><td>0.9257493898603144</td><td>0.23735573559246997</td><td>172449073718059.3     </td><td>240038011556871.06    </td><td>100                 </td><td>c7f219cd  </td><td>0.4233163136645941 </td><td>0.16172933516349974</td></tr>\n",
       "<tr><td>FSR_Trainable_ca06df15</td><td>2023-08-10_18-19-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>4.3000182033763545</td><td>1138.7441986232727</td><td>1.2509927147557638</td><td>2930741215.562544     </td><td>5.5977889875342814</td><td>172.26.215.93</td><td style=\"text-align: right;\">101462</td><td>2.379520675171943 </td><td>874.1260604698199 </td><td>12.6181161403656    </td><td>12.6181161403656  </td><td>12.6181161403656  </td><td style=\"text-align: right;\"> 1691659164</td><td>6.942337914651463 </td><td>3.0001850082569663 </td><td>1448479009738817.8    </td><td>11.574726880061872    </td><td>1                   </td><td>ca06df15  </td><td>3.500447230125307  </td><td>2.097341757408974  </td></tr>\n",
       "<tr><td>FSR_Trainable_cab58447</td><td>2023-08-10_18-41-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>4.7391883521171065</td><td>756.7868972065766 </td><td>1.2462492469174342</td><td>590983707.3583231     </td><td>5.1513189500665675</td><td>172.26.215.93</td><td style=\"text-align: right;\">111148</td><td>2.403725222024487 </td><td>619.0570224257273 </td><td>2.926643133163452   </td><td>2.926643133163452 </td><td>2.926643133163452 </td><td style=\"text-align: right;\"> 1691660511</td><td>7.655532281546531 </td><td>2.086892438792234  </td><td>3667677005635417.5    </td><td>12.384565884846262    </td><td>1                   </td><td>cab58447  </td><td>3.5370207739079396 </td><td>1.6142981761586284 </td></tr>\n",
       "<tr><td>FSR_Trainable_d1d069cd</td><td>2023-08-10_18-32-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>5.820716411695996 </td><td>861.2684508647504 </td><td>1.403263301119198 </td><td>1.0288623085720145e+18</td><td>0.6929873259613988</td><td>172.26.215.93</td><td style=\"text-align: right;\">105873</td><td>2.601738923416551 </td><td>600.9231567947718 </td><td>247.63590669631958  </td><td>2.2413086891174316</td><td>247.63590669631958</td><td style=\"text-align: right;\"> 1691659946</td><td>1.1469733760707144</td><td>0.35143142051498855</td><td>141769191863181.88    </td><td>495540682074055.9     </td><td>100                 </td><td>d1d069cd  </td><td>0.478258109527396  </td><td>0.21472921643400278</td></tr>\n",
       "<tr><td>FSR_Trainable_d30b0eaa</td><td>2023-08-10_18-24-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>28.2955081737479  </td><td>1984.9429379762034</td><td>4.327435067120079 </td><td>1.8336066490054182e+18</td><td>2.4932421910585147</td><td>172.26.215.93</td><td style=\"text-align: right;\">101087</td><td>9.425139482588802 </td><td>1407.8790837632687</td><td>312.09960293769836  </td><td>2.8846311569213867</td><td>312.09960293769836</td><td style=\"text-align: right;\"> 1691659462</td><td>6.130230516245462 </td><td>0.740036248946427  </td><td>40738592117082.18     </td><td>854295290763545.8     </td><td>100                 </td><td>d30b0eaa  </td><td>2.0341983659434675 </td><td>0.45904382511504727</td></tr>\n",
       "<tr><td>FSR_Trainable_d3405b79</td><td>2023-08-10_18-41-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.780236380304365 </td><td>659.3942103355037 </td><td>1.235905478795302 </td><td>6.420609745291602e+17 </td><td>0.5982799486402595</td><td>172.26.215.93</td><td style=\"text-align: right;\">110162</td><td>2.3337083183515097</td><td>489.8851807236648 </td><td>113.12901449203491  </td><td>1.0614879131317139</td><td>113.12901449203491</td><td style=\"text-align: right;\"> 1691660493</td><td>0.9559109115290477</td><td>0.24826939430575198</td><td>174214082086150.44    </td><td>268836398973391.8     </td><td>100                 </td><td>d3405b79  </td><td>0.43203509539552276</td><td>0.16624485324473673</td></tr>\n",
       "<tr><td>FSR_Trainable_d3976aff</td><td>2023-08-10_18-39-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>5.592944909039383 </td><td>1437.6669787783344</td><td>1.5073229125779082</td><td>1.351972452128107e+18 </td><td>0.8534263457504183</td><td>172.26.215.93</td><td style=\"text-align: right;\">109015</td><td>2.585243279743094 </td><td>1148.6400864322093</td><td>153.6178002357483   </td><td>1.4745237827301025</td><td>153.6178002357483 </td><td style=\"text-align: right;\"> 1691660393</td><td>1.1179599534352498</td><td>0.49742024857964146</td><td>202723555689293.22    </td><td>429441143164926.1     </td><td>100                 </td><td>d3976aff  </td><td>0.48565592432757615</td><td>0.36777042142284216</td></tr>\n",
       "<tr><td>FSR_Trainable_d7953de1</td><td>2023-08-10_18-48-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>5.569646608261532 </td><td>1164.706316862269 </td><td>1.4026483610950526</td><td>1.2281176804136876e+18</td><td>0.7953999416075501</td><td>172.26.215.93</td><td style=\"text-align: right;\">116256</td><td>2.633253742392663 </td><td>906.8663526787526 </td><td>3.823624610900879   </td><td>1.8246634006500244</td><td>3.823624610900879 </td><td style=\"text-align: right;\"> 1691660907</td><td>1.1150733001376256</td><td>0.4231676556027474 </td><td>188422217152557.16    </td><td>486735575912945.06    </td><td>2                   </td><td>d7953de1  </td><td>0.49851193446769   </td><td>0.2968880071398602 </td></tr>\n",
       "<tr><td>FSR_Trainable_d7c194e1</td><td>2023-08-10_18-19-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>4.127748023237577 </td><td>1393.7007709546729</td><td>1.0826871255649986</td><td>7198045532606317.0    </td><td>25.035930906499324</td><td>172.26.215.93</td><td style=\"text-align: right;\">102023</td><td>2.4948020934624986</td><td>1164.3172643489563</td><td>8.37264347076416    </td><td>8.37264347076416  </td><td>8.37264347076416  </td><td style=\"text-align: right;\"> 1691659192</td><td>14.999614853875364</td><td>10.877160800684475 </td><td>1490239668682561.5    </td><td>390040382951973.7     </td><td>1                   </td><td>d7c194e1  </td><td>11.489343192243028 </td><td>13.546587714256297 </td></tr>\n",
       "<tr><td>FSR_Trainable_d959ef0c</td><td>2023-08-10_18-54-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>4.347382710801191 </td><td>1416.9090372738422</td><td>1.267221558070403 </td><td>4592541198.302577     </td><td>5.724482843011129 </td><td>172.26.215.93</td><td style=\"text-align: right;\">120642</td><td>2.389338774823529 </td><td>1002.4662774573345</td><td>1.739783763885498   </td><td>1.739783763885498 </td><td>1.739783763885498 </td><td style=\"text-align: right;\"> 1691661260</td><td>6.791295971894137 </td><td>3.624141701834751  </td><td>22.77359642489958     </td><td>8.471810943731937     </td><td>1                   </td><td>d959ef0c  </td><td>3.4379271565164933 </td><td>2.286555686494636  </td></tr>\n",
       "<tr><td>FSR_Trainable_d9c86f35</td><td>2023-08-10_18-20-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>4.564299925887114 </td><td>1561.8740698610154</td><td>1.2474705024527666</td><td>4398294819.695187     </td><td>6.070511180662572 </td><td>172.26.215.93</td><td style=\"text-align: right;\">102230</td><td>2.465468271512889 </td><td>1104.233017918422 </td><td>7.153272867202759   </td><td>3.4674177169799805</td><td>7.153272867202759 </td><td style=\"text-align: right;\"> 1691659206</td><td>7.244788115573641 </td><td>4.062235747573023  </td><td>36.77287653709242     </td><td>9.110717905981447     </td><td>2                   </td><td>d9c86f35  </td><td>3.5400964600141376 </td><td>2.530414720648434  </td></tr>\n",
       "<tr><td>FSR_Trainable_d9fc69c2</td><td>2023-08-10_18-21-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>6.103457723191675 </td><td>1128.457987297725 </td><td>1.3339419519992224</td><td>2095317952.927461     </td><td>5.850861620635159 </td><td>172.26.215.93</td><td style=\"text-align: right;\">101262</td><td>2.6419508347747866</td><td>858.3748677946824 </td><td>138.06872010231018  </td><td>1.2448863983154297</td><td>138.06872010231018</td><td style=\"text-align: right;\"> 1691659294</td><td>10.120877324376474</td><td>2.898238328776473  </td><td>139.67642999924863    </td><td>19.50048952274133     </td><td>100                 </td><td>d9fc69c2  </td><td>3.979566239343553  </td><td>1.8712953812916062 </td></tr>\n",
       "<tr><td>FSR_Trainable_e1bafa1b</td><td>2023-08-10_18-39-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>5.245670500546537 </td><td>780.2006873475679 </td><td>1.2230439727500972</td><td>6.122112834724513e+17 </td><td>0.6403463923794246</td><td>172.26.215.93</td><td style=\"text-align: right;\">108768</td><td>2.414757768764376 </td><td>628.827339072709  </td><td>160.10047960281372  </td><td>1.4228847026824951</td><td>160.10047960281372</td><td style=\"text-align: right;\"> 1691660355</td><td>1.043617046941941 </td><td>0.2786646211116526 </td><td>169696741725785.56    </td><td>249853055382619.03    </td><td>100                 </td><td>e1bafa1b  </td><td>0.4454319114151152 </td><td>0.19491448096430938</td></tr>\n",
       "<tr><td>FSR_Trainable_e4b72c0b</td><td>2023-08-10_18-52-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>5.917772042761495 </td><td>1065.6050125040745</td><td>1.463953040949511 </td><td>1.0113380470830397e+18</td><td>0.7798761990081751</td><td>172.26.215.93</td><td style=\"text-align: right;\">118591</td><td>2.6982385106618936</td><td>846.2932041604424 </td><td>10.366637945175171  </td><td>4.491921901702881 </td><td>10.366637945175171</td><td style=\"text-align: right;\"> 1691661144</td><td>1.169307854596038 </td><td>0.3792523204676049 </td><td>170014437749876.53    </td><td>374523907252611.4     </td><td>2                   </td><td>e4b72c0b  </td><td>0.5071757967230737 </td><td>0.27270040228510134</td></tr>\n",
       "<tr><td>FSR_Trainable_e580fe88</td><td>2023-08-10_18-52-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td>4                         </td><td>5.8552160484919185</td><td>825.7574444446876 </td><td>1.4380555900162553</td><td>6.661905000795886e+17 </td><td>0.7039986660682491</td><td>172.26.215.93</td><td style=\"text-align: right;\">118373</td><td>2.636414863479565 </td><td>627.5412010894545 </td><td>19.70512318611145   </td><td>4.667452096939087 </td><td>19.70512318611145 </td><td style=\"text-align: right;\"> 1691661141</td><td>1.1530503687232891</td><td>0.3179016369958597 </td><td>171569339158822.38    </td><td>311533813863634.1     </td><td>4                   </td><td>e580fe88  </td><td>0.4880602233695328 </td><td>0.21593844269871634</td></tr>\n",
       "<tr><td>FSR_Trainable_e8fa8ea1</td><td>2023-08-10_18-19-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td>4                         </td><td>4.234855114770675 </td><td>1399.4637092233645</td><td>1.232083559399654 </td><td>1.7839479094779216e+16</td><td>21.363329770272962</td><td>172.26.215.93</td><td style=\"text-align: right;\">101015</td><td>2.420656635475911 </td><td>1170.5278718263032</td><td>25.434345483779907  </td><td>6.419172763824463 </td><td>25.434345483779907</td><td style=\"text-align: right;\"> 1691659149</td><td>12.126365638946702</td><td>10.98509597194587  </td><td>1860568103815411.5    </td><td>849731528676183.5     </td><td>4                   </td><td>e8fa8ea1  </td><td>7.79655128213128   </td><td>13.566778488141681 </td></tr>\n",
       "<tr><td>FSR_Trainable_eb10b985</td><td>2023-08-10_18-49-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.59438299913551  </td><td>670.0323949188639 </td><td>1.2175053333816754</td><td>5.7440232119207315e+17</td><td>0.6038382318321313</td><td>172.26.215.93</td><td style=\"text-align: right;\">114397</td><td>2.332000806937814 </td><td>538.980227544655  </td><td>160.51239609718323  </td><td>1.4423611164093018</td><td>160.51239609718323</td><td style=\"text-align: right;\"> 1691660953</td><td>0.9296707303836862</td><td>0.2460071395965779 </td><td>163294203452002.44    </td><td>222348656639378.25    </td><td>100                 </td><td>eb10b985  </td><td>0.4298482661341753 </td><td>0.1739899656979559 </td></tr>\n",
       "<tr><td>FSR_Trainable_ebcf36c8</td><td>2023-08-10_18-59-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.638234074703294 </td><td>624.5510988416523 </td><td>1.212613088304826 </td><td>5.768966834848913e+17 </td><td>0.5809414529222481</td><td>172.26.215.93</td><td style=\"text-align: right;\">123405</td><td>2.281587695033209 </td><td>473.9459794152265 </td><td>89.47168517112732   </td><td>0.5496459007263184</td><td>89.47168517112732 </td><td style=\"text-align: right;\"> 1691661552</td><td>0.9322912668123807</td><td>0.23241623099138012</td><td>168617154092631.3     </td><td>231318194939298.84    </td><td>100                 </td><td>ebcf36c8  </td><td>0.4213526635129027 </td><td>0.15958878940934537</td></tr>\n",
       "<tr><td>FSR_Trainable_eef4ead6</td><td>2023-08-10_18-41-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>5.296068667472058 </td><td>708.9015819064717 </td><td>1.2530633034432423</td><td>7.05117148571439e+17  </td><td>0.6192122071624402</td><td>172.26.215.93</td><td style=\"text-align: right;\">109935</td><td>2.4330022972338083</td><td>523.4105176589494 </td><td>114.55164694786072  </td><td>1.3351874351501465</td><td>114.55164694786072</td><td style=\"text-align: right;\"> 1691660484</td><td>1.0540729867335759</td><td>0.2613338556930183 </td><td>166481726043926.88    </td><td>275975724139709.44    </td><td>100                 </td><td>eef4ead6  </td><td>0.4468007460120738 </td><td>0.17241146115036635</td></tr>\n",
       "<tr><td>FSR_Trainable_ef468e9e</td><td>2023-08-10_18-53-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td>4                         </td><td>5.820000428346276 </td><td>811.9229488909759 </td><td>1.3672274372433348</td><td>6.501627312417868e+17 </td><td>0.6943497814469022</td><td>172.26.215.93</td><td style=\"text-align: right;\">119510</td><td>2.6077678113660254</td><td>630.2146743187928 </td><td>5.6949143409729     </td><td>1.1210849285125732</td><td>5.6949143409729   </td><td style=\"text-align: right;\"> 1691661189</td><td>1.1567836142671684</td><td>0.3042547014006788 </td><td>169232289304903.0     </td><td>288248804161841.56    </td><td>4                   </td><td>ef468e9e  </td><td>0.48438719786627926</td><td>0.20996258358062292</td></tr>\n",
       "<tr><td>FSR_Trainable_f5f499b0</td><td>2023-08-10_18-51-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.679300126724054 </td><td>662.795446733496  </td><td>1.1878060341970418</td><td>6.369308282744609e+17 </td><td>0.5942338485473513</td><td>172.26.215.93</td><td style=\"text-align: right;\">116972</td><td>2.323145013339337 </td><td>503.3702024396801 </td><td>117.99617218971252  </td><td>1.2336900234222412</td><td>117.99617218971252</td><td style=\"text-align: right;\"> 1691661084</td><td>0.936639293435428 </td><td>0.2434197412297848 </td><td>165243998910679.56    </td><td>249721281831201.56    </td><td>100                 </td><td>f5f499b0  </td><td>0.42787703901346064</td><td>0.1663568095338907 </td></tr>\n",
       "<tr><td>FSR_Trainable_f64d9976</td><td>2023-08-10_18-42-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>5.5249761192654345</td><td>1441.3547766015502</td><td>1.4591236695613787</td><td>1.3925097544635028e+18</td><td>0.8602552099063921</td><td>172.26.215.93</td><td style=\"text-align: right;\">112042</td><td>2.6148534973934123</td><td>1147.2651447065766</td><td>9.423652410507202   </td><td>4.59702730178833  </td><td>9.423652410507202 </td><td style=\"text-align: right;\"> 1691660571</td><td>1.1086513943542664</td><td>0.4992651316975033 </td><td>200828966548265.5     </td><td>447333545775134.7     </td><td>2                   </td><td>f64d9976  </td><td>0.4934507526724554 </td><td>0.36680445723393673</td></tr>\n",
       "<tr><td>FSR_Trainable_fb354333</td><td>2023-08-10_18-45-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>4.4081463335646305</td><td>1126.134833218373 </td><td>1.3079458873883338</td><td>2036762627.4900956    </td><td>5.6388054670150805</td><td>172.26.215.93</td><td style=\"text-align: right;\">113494</td><td>2.4024266117998114</td><td>888.1567933681321 </td><td>1.4676945209503174  </td><td>1.4676945209503174</td><td>1.4676945209503174</td><td style=\"text-align: right;\"> 1691660721</td><td>7.153445851164368 </td><td>2.877302019762099  </td><td>1707225851201709.2    </td><td>8.769419112220099     </td><td>1                   </td><td>fb354333  </td><td>3.525030758754521  </td><td>2.11377470826056   </td></tr>\n",
       "<tr><td>FSR_Trainable_fb7b995d</td><td>2023-08-10_18-53-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>4.371269362094094 </td><td>621.8369873426126 </td><td>1.0832803186990967</td><td>5.614363572880279e+17 </td><td>0.5647841054839656</td><td>172.26.215.93</td><td style=\"text-align: right;\">117726</td><td>2.2094223029918187</td><td>471.12236520957765</td><td>118.74033975601196  </td><td>1.091353178024292 </td><td>118.74033975601196</td><td style=\"text-align: right;\"> 1691661216</td><td>0.8857784723063508</td><td>0.22937074059701731</td><td>153558448638436.47    </td><td>222978992669499.34    </td><td>100                 </td><td>fb7b995d  </td><td>0.40941685172207726</td><td>0.15536725376188837</td></tr>\n",
       "<tr><td>FSR_Trainable_fec44e75</td><td>2023-08-10_18-46-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td>4                         </td><td>5.680082939596029 </td><td>826.1847926474693 </td><td>1.3800465816114063</td><td>6.553615183994228e+17 </td><td>0.7022173203645176</td><td>172.26.215.93</td><td style=\"text-align: right;\">114846</td><td>2.5942690668665795</td><td>638.1815447660713 </td><td>7.032741069793701   </td><td>1.519974708557129 </td><td>7.032741069793701 </td><td style=\"text-align: right;\"> 1691660802</td><td>1.135874971261714 </td><td>0.31096060228489153</td><td>191393084446307.66    </td><td>275029023861697.78    </td><td>4                   </td><td>fec44e75  </td><td>0.481974378811068  </td><td>0.2202429415534495 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_e8fa8ea1_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-08-10_18-18-36/wandb/run-20230810_181847-e8fa8ea1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb: Syncing run FSR_Trainable_e8fa8ea1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e8fa8ea1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_d30b0eaa_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-08-10_18-18-41/wandb/run-20230810_181858-d30b0eaa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Syncing run FSR_Trainable_d30b0eaa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d30b0eaa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_d9fc69c2_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-08-10_18-18-50/wandb/run-20230810_181909-d9fc69c2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: Syncing run FSR_Trainable_d9fc69c2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d9fc69c2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:                mae_coord 4.23486\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:                mae_force 1399.46371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:               mape_coord 1.23208\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:               mape_force 1.7839479094779216e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:                   metric 21.36333\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:               rmse_coord 2.42066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:               rmse_force 1170.52787\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:       time_since_restore 25.43435\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:         time_this_iter_s 6.41917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:             time_total_s 25.43435\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:                timestamp 1691659149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:               tmae_coord 12.12637\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:               tmae_force 10.9851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:              tmape_coord 1860568103815411.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:              tmape_force 849731528676183.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:              trmse_coord 7.79655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:              trmse_force 13.56678\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb:  View run FSR_Trainable_e8fa8ea1 at: https://wandb.ai/seokjin/FSR-prediction/runs/e8fa8ea1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101086)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_181847-e8fa8ea1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_ca06df15_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-08-10_18-19-01/wandb/run-20230810_181920-ca06df15\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb: Syncing run FSR_Trainable_ca06df15\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ca06df15\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:                mae_coord 4.30002\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:                mae_force 1138.7442\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:               mape_coord 1.25099\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:               mape_force 2930741215.56254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:                   metric 5.59779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:               rmse_coord 2.37952\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:               rmse_force 874.12606\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:       time_since_restore 12.61812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:         time_this_iter_s 12.61812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:             time_total_s 12.61812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:                timestamp 1691659164\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:               tmae_coord 6.94234\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:               tmae_force 3.00019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:              tmape_coord 1448479009738817.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:              tmape_force 11.57473\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:              trmse_coord 3.50045\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:              trmse_force 2.09734\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb:  View run FSR_Trainable_ca06df15 at: https://wandb.ai/seokjin/FSR-prediction/runs/ca06df15\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101645)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_181920-ca06df15/logs\n",
      "2023-08-10 18:19:31,934\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.342 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:19:31,941\tWARNING util.py:315 -- The `process_trial_result` operation took 2.349 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:19:31,943\tWARNING util.py:315 -- Processing trial results took 2.351 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:19:31,945\tWARNING util.py:315 -- The `process_trial_result` operation took 2.353 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_8043f276_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-08-10_18-19-12/wandb/run-20230810_181935-8043f276\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: Syncing run FSR_Trainable_8043f276\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8043f276\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:                mae_coord 4.50502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:                mae_force 1343.46308\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:               mape_coord 1.34237\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:               mape_force 1.6254305242208586e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:                   metric 20.21939\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:               rmse_coord 2.42265\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:               rmse_force 1047.97348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:       time_since_restore 2.8373\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:         time_this_iter_s 2.8373\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:             time_total_s 2.8373\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:                timestamp 1691659169\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:               tmae_coord 12.59306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:               tmae_force 10.80599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:              tmape_coord 3509439154933622.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:              tmape_force 6699290735216808.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:              trmse_coord 7.73406\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:              trmse_force 12.48533\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb:  View run FSR_Trainable_8043f276 at: https://wandb.ai/seokjin/FSR-prediction/runs/8043f276\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101865)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_181935-8043f276/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_d7c194e1_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-08-10_18-19-26/wandb/run-20230810_181952-d7c194e1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb: Syncing run FSR_Trainable_d7c194e1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d7c194e1\n",
      "2023-08-10 18:19:54,491\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.910 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:19:54,497\tWARNING util.py:315 -- The `process_trial_result` operation took 1.916 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:19:54,498\tWARNING util.py:315 -- Processing trial results took 1.917 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:19:54,501\tWARNING util.py:315 -- The `process_trial_result` operation took 1.920 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:                mae_coord 4.12775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:                mae_force 1393.70077\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:               mape_coord 1.08269\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:               mape_force 7198045532606317.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:                   metric 25.03593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:               rmse_coord 2.4948\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:               rmse_force 1164.31726\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:       time_since_restore 8.37264\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:         time_this_iter_s 8.37264\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:             time_total_s 8.37264\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:                timestamp 1691659192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:               tmae_coord 14.99961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:               tmae_force 10.87716\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:              tmape_coord 1490239668682561.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:              tmape_force 390040382951973.7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:              trmse_coord 11.48934\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:              trmse_force 13.54659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb:  View run FSR_Trainable_d7c194e1 at: https://wandb.ai/seokjin/FSR-prediction/runs/d7c194e1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102115)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_181952-d7c194e1/logs\n",
      "2023-08-10 18:20:02,552\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.812 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:20:02,557\tWARNING util.py:315 -- The `process_trial_result` operation took 1.818 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:20:02,560\tWARNING util.py:315 -- Processing trial results took 1.821 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:20:02,562\tWARNING util.py:315 -- The `process_trial_result` operation took 1.823 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_d9c86f35_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-08-10_18-19-44/wandb/run-20230810_182003-d9c86f35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb: Syncing run FSR_Trainable_d9c86f35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d9c86f35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:                mae_coord 4.5643\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:                mae_force 1561.87407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:               mape_coord 1.24747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:               mape_force 4398294819.69519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:                   metric 6.07051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:               rmse_coord 2.46547\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:               rmse_force 1104.23302\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:       time_since_restore 7.15327\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:         time_this_iter_s 3.46742\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:             time_total_s 7.15327\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:                timestamp 1691659206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:               tmae_coord 7.24479\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:               tmae_force 4.06224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:              tmape_coord 36.77288\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:              tmape_force 9.11072\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:              trmse_coord 3.5401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:              trmse_force 2.53041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb:  View run FSR_Trainable_d9c86f35 at: https://wandb.ai/seokjin/FSR-prediction/runs/d9c86f35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102333)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_182003-d9c86f35/logs\n",
      "2023-08-10 18:20:12,276\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.974 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:20:12,280\tWARNING util.py:315 -- The `process_trial_result` operation took 1.978 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:20:12,282\tWARNING util.py:315 -- Processing trial results took 1.981 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:20:12,284\tWARNING util.py:315 -- The `process_trial_result` operation took 1.982 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_3857d286_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-08-10_18-19-57/wandb/run-20230810_182014-3857d286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb: Syncing run FSR_Trainable_3857d286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/3857d286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-08-10 18:20:28,084\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.245 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:20:28,090\tWARNING util.py:315 -- The `process_trial_result` operation took 2.251 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:20:28,095\tWARNING util.py:315 -- Processing trial results took 2.255 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:20:28,097\tWARNING util.py:315 -- The `process_trial_result` operation took 2.258 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_ab536d2b_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-08-10_18-20-07/wandb/run-20230810_182027-ab536d2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: Syncing run FSR_Trainable_ab536d2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ab536d2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:                mae_coord 4.40657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:                mae_force 1577.62766\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:               mape_coord 1.22474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:               mape_force 4820156201.38047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:                   metric 6.0338\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:               rmse_coord 2.46051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:               rmse_force 1101.19562\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:       time_since_restore 7.23294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:         time_this_iter_s 7.23294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:             time_total_s 7.23294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:                timestamp 1691659225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:               tmae_coord 6.86342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:               tmae_force 3.99038\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:              tmape_coord 22.86022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:              tmape_force 6.47888\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:              trmse_coord 3.52974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:              trmse_force 2.50406\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb:  View run FSR_Trainable_ab536d2b at: https://wandb.ai/seokjin/FSR-prediction/runs/ab536d2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102775)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_182027-ab536d2b/logs\n",
      "2023-08-10 18:20:45,493\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.490 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:20:45,497\tWARNING util.py:315 -- The `process_trial_result` operation took 2.495 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:20:45,501\tWARNING util.py:315 -- Processing trial results took 2.499 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:20:45,505\tWARNING util.py:315 -- The `process_trial_result` operation took 2.503 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_6e1464a0_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-20-18/wandb/run-20230810_182047-6e1464a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: Syncing run FSR_Trainable_6e1464a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/6e1464a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:                mae_coord 6.10346\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:                mae_force 1128.45799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:               mape_coord 1.33394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:               mape_force 2095317952.92746\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:                   metric 5.85086\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:               rmse_coord 2.64195\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:               rmse_force 858.37487\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:       time_since_restore 138.06872\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:         time_this_iter_s 1.24489\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:             time_total_s 138.06872\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:                timestamp 1691659294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:               tmae_coord 10.12088\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:               tmae_force 2.89824\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:              tmape_coord 139.67643\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:              tmape_force 19.50049\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:              trmse_coord 3.97957\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:              trmse_force 1.8713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb:  View run FSR_Trainable_d9fc69c2 at: https://wandb.ai/seokjin/FSR-prediction/runs/d9fc69c2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101454)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_181909-d9fc69c2/logs\n",
      "2023-08-10 18:21:51,110\tERROR tune_controller.py:873 -- Trial task failed for trial FSR_Trainable_3d8c1dc7\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 2540, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::ResourceTrainable.train()\u001b[39m (pid=103230, ip=172.26.215.93, actor_id=2796fc7ab4ce66e20bea77ce01000000, repr=<ray.tune.trainable.util.FSR_Trainable object at 0x7fe71a29f760>)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 386, in train\n",
      "    result = self.step()\n",
      "  File \"/home/seokj/workspace/FSR-prediction/fsr_trainable.py\", line 72, in step\n",
      "    tmae.append(sklearn.metrics.mean_absolute_error(y, pred, multioutput='raw_values'))\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 196, in mean_absolute_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103286)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103286)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103286)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103286)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103286)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_3d8c1dc7_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-20-39/wandb/run-20230810_182154-3d8c1dc7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103286)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103286)\u001b[0m wandb: Syncing run FSR_Trainable_3d8c1dc7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103286)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103286)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/3d8c1dc7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103286)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103286)\u001b[0m wandb: - 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103286)\u001b[0m wandb: \\ 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103286)\u001b[0m wandb: | 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103286)\u001b[0m wandb:  View run FSR_Trainable_3d8c1dc7 at: https://wandb.ai/seokjin/FSR-prediction/runs/3d8c1dc7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103286)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103286)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_182154-3d8c1dc7/logs\n",
      "2023-08-10 18:22:09,698\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.236 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:22:09,700\tWARNING util.py:315 -- The `process_trial_result` operation took 2.239 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:22:09,703\tWARNING util.py:315 -- Processing trial results took 2.242 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:22:09,706\tWARNING util.py:315 -- The `process_trial_result` operation took 2.244 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_814e4b4c_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-21-46/wandb/run-20230810_182213-814e4b4c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: Syncing run FSR_Trainable_814e4b4c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/814e4b4c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:                mae_coord 4.63559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:                mae_force 1587.25267\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:               mape_coord 1.22102\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:               mape_force 2.5665450815687306e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:                   metric 0.81163\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:               rmse_coord 2.46812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:               rmse_force 1104.04867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:       time_since_restore 208.21474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:         time_this_iter_s 2.20877\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:             time_total_s 208.21474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:                timestamp 1691659426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:               tmae_coord 0.93506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:               tmae_force 0.56212\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:              tmape_coord 181589640738056.3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:              tmape_force 930098867767599.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:              trmse_coord 0.46321\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:              trmse_force 0.34842\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb:  View run FSR_Trainable_3857d286 at: https://wandb.ai/seokjin/FSR-prediction/runs/3857d286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=102557)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_182014-3857d286/logs\n",
      "2023-08-10 18:24:04,795\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.164 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:24:04,798\tWARNING util.py:315 -- The `process_trial_result` operation took 2.167 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:24:04,799\tWARNING util.py:315 -- Processing trial results took 2.169 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:24:04,802\tWARNING util.py:315 -- The `process_trial_result` operation took 2.172 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_bbc61711_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-22-05/wandb/run-20230810_182405-bbc61711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb: Syncing run FSR_Trainable_bbc61711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/bbc61711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: \\ 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:                mae_coord 5.16657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:                mae_force 761.36331\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:               mape_coord 1.22332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:               mape_force 8.575137829229923e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:                   metric 0.61575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:               rmse_coord 2.3969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:               rmse_force 566.9985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:       time_since_restore 125.55586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:         time_this_iter_s 1.24343\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:             time_total_s 125.55586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:                timestamp 1691659457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:               tmae_coord 1.026\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:               tmae_force 0.26978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:              tmape_coord 165403333040653.28\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:              tmape_force 300583367368256.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:              trmse_coord 0.43847\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:              trmse_force 0.17728\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb:  View run FSR_Trainable_814e4b4c at: https://wandb.ai/seokjin/FSR-prediction/runs/814e4b4c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103515)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_182213-814e4b4c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_181858-d30b0eaa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_181858-d30b0eaa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=101261)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_af2a1820_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-23-57/wandb/run-20230810_182436-af2a1820\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Syncing run FSR_Trainable_af2a1820\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/af2a1820\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:                mae_coord 27.96506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:                mae_force 1625.41345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:               mape_coord 4.19766\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:               mape_force 1.1504933244004705e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:                   metric 2.32367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:               rmse_coord 9.17494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:               rmse_force 1337.97239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:       time_since_restore 33.12897\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:         time_this_iter_s 4.75593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:             time_total_s 33.12897\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:                timestamp 1691659473\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:               tmae_coord 5.95526\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:               tmae_force 0.59449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:              tmape_coord 17617696139972.188\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:              tmape_force 555097552594812.7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:              trmse_coord 1.91325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:              trmse_force 0.41042\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb:  View run FSR_Trainable_bbc61711 at: https://wandb.ai/seokjin/FSR-prediction/runs/bbc61711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_182405-bbc61711/logs\n",
      "2023-08-10 18:24:37,759\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.078 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:24:37,763\tWARNING util.py:315 -- The `process_trial_result` operation took 2.083 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:24:37,766\tWARNING util.py:315 -- Processing trial results took 2.085 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:24:37,768\tWARNING util.py:315 -- The `process_trial_result` operation took 2.088 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103806)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: \\ 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_182436-af2a1820/logs\n",
      "2023-08-10 18:24:43,673\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.094 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:24:43,677\tWARNING util.py:315 -- The `process_trial_result` operation took 2.099 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:24:43,678\tWARNING util.py:315 -- Processing trial results took 2.100 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:24:43,680\tWARNING util.py:315 -- The `process_trial_result` operation took 2.101 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104055)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_2b64f3bf_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-24-28/wandb/run-20230810_182446-2b64f3bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb: Syncing run FSR_Trainable_2b64f3bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2b64f3bf\n",
      "2023-08-10 18:24:53,102\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.788 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:24:53,106\tWARNING util.py:315 -- The `process_trial_result` operation took 1.794 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:24:53,108\tWARNING util.py:315 -- Processing trial results took 1.796 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:24:53,109\tWARNING util.py:315 -- The `process_trial_result` operation took 1.797 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_30e64c59_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-24-39/wandb/run-20230810_182456-30e64c59\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: Syncing run FSR_Trainable_30e64c59\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/30e64c59\n",
      "2023-08-10 18:25:09,400\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.035 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:25:09,403\tWARNING util.py:315 -- The `process_trial_result` operation took 3.040 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:25:09,407\tWARNING util.py:315 -- Processing trial results took 3.044 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:25:09,409\tWARNING util.py:315 -- The `process_trial_result` operation took 3.046 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_a0aaf09e_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-24-49/wandb/run-20230810_182513-a0aaf09e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb: Syncing run FSR_Trainable_a0aaf09e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a0aaf09e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)05 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:                mae_coord 10.7944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:                mae_force 1550.48023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:               mape_coord 1.95587\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:               mape_force 1.6030866620310444e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:                   metric 1.1335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:               rmse_coord 3.8713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:               rmse_force 1196.94593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:       time_since_restore 22.51335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:         time_this_iter_s 1.10506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:             time_total_s 22.51335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:                timestamp 1691659530\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:               tmae_coord 2.24478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:               tmae_force 0.58831\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:              tmape_coord 157130586292901.7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:              tmape_force 802992840979491.9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:              trmse_coord 0.75677\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:              trmse_force 0.37673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb:  View run FSR_Trainable_a0aaf09e at: https://wandb.ai/seokjin/FSR-prediction/runs/a0aaf09e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104713)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_182513-a0aaf09e/logs\n",
      "2023-08-10 18:25:44,840\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.957 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:25:44,842\tWARNING util.py:315 -- The `process_trial_result` operation took 1.960 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:25:44,846\tWARNING util.py:315 -- Processing trial results took 1.964 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:25:44,850\tWARNING util.py:315 -- The `process_trial_result` operation took 1.968 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_07a0a9ea_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-25-04/wandb/run-20230810_182548-07a0a9ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb: Syncing run FSR_Trainable_07a0a9ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/07a0a9ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:                mae_coord 4.9638\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:                mae_force 829.70229\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:               mape_coord 1.25013\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:               mape_force 7.815097620713126e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:                   metric 0.67015\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:               rmse_coord 2.46275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:               rmse_force 635.07002\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:       time_since_restore 313.70996\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:         time_this_iter_s 3.25752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:             time_total_s 313.70996\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:                timestamp 1691659565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:               tmae_coord 1.00638\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:               tmae_force 0.30468\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:              tmape_coord 148793092207639.66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:              tmape_force 312331921766861.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:              trmse_coord 0.46401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:              trmse_force 0.20614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb:  View run FSR_Trainable_6e1464a0 at: https://wandb.ai/seokjin/FSR-prediction/runs/6e1464a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=103013)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_182047-6e1464a0/logs\n",
      "2023-08-10 18:26:23,699\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.222 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:26:23,703\tWARNING util.py:315 -- The `process_trial_result` operation took 2.228 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:26:23,706\tWARNING util.py:315 -- Processing trial results took 2.230 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:26:23,710\tWARNING util.py:315 -- The `process_trial_result` operation took 2.234 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_7c1a30a3_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-25-41/wandb/run-20230810_182627-7c1a30a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb: Syncing run FSR_Trainable_7c1a30a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7c1a30a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:                mae_coord 7.97523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:                mae_force 960.45274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:               mape_coord 1.49294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:               mape_force 1.0716863768944003e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:                   metric 0.82059\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:               rmse_coord 3.16767\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:               rmse_force 734.3697\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:       time_since_restore 117.29064\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:         time_this_iter_s 1.84613\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:             time_total_s 117.29064\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:                timestamp 1691659608\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:               tmae_coord 1.59362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:               tmae_force 0.36417\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:              tmape_coord 174361925137013.22\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:              tmape_force 439139373614088.94\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:              trmse_coord 0.58207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:              trmse_force 0.23853\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb:  View run FSR_Trainable_2b64f3bf at: https://wandb.ai/seokjin/FSR-prediction/runs/2b64f3bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104283)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_182446-2b64f3bf/logs\n",
      "2023-08-10 18:27:06,244\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.804 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:27:06,247\tWARNING util.py:315 -- The `process_trial_result` operation took 1.808 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:27:06,248\tWARNING util.py:315 -- Processing trial results took 1.809 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:27:06,250\tWARNING util.py:315 -- The `process_trial_result` operation took 1.811 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "wandb: \\ Waiting for wandb.init()...447)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_b346be41_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-26-19/wandb/run-20230810_182709-b346be41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: Syncing run FSR_Trainable_b346be41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b346be41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:                mae_coord 4.28305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:                mae_force 1543.96003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:               mape_coord 1.22895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:               mape_force 2.4636742129765023e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:                   metric 0.79736\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:               rmse_coord 2.41491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:               rmse_force 1073.79484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:       time_since_restore 125.41109\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:         time_this_iter_s 1.64774\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:             time_total_s 125.41109\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:                timestamp 1691659627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:               tmae_coord 0.87078\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:               tmae_force 0.54622\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:              tmape_coord 201864051176213.34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:              tmape_force 887167378760428.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:              trmse_coord 0.45625\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:              trmse_force 0.34111\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb:  View run FSR_Trainable_30e64c59 at: https://wandb.ai/seokjin/FSR-prediction/runs/30e64c59\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104500)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_182456-30e64c59/logs\n",
      "2023-08-10 18:27:25,365\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.795 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:27:25,367\tWARNING util.py:315 -- The `process_trial_result` operation took 1.798 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:27:25,370\tWARNING util.py:315 -- Processing trial results took 1.801 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:27:25,373\tWARNING util.py:315 -- The `process_trial_result` operation took 1.804 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_14e1a525_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-27-01/wandb/run-20230810_182727-14e1a525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb: Syncing run FSR_Trainable_14e1a525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/14e1a525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:                mae_coord 4.28763\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:                mae_force 1509.11081\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:               mape_coord 1.26021\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:               mape_force 2.413981962423381e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:                   metric 0.79239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:               rmse_coord 2.43571\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:               rmse_force 1049.32018\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:       time_since_restore 121.6818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:         time_this_iter_s 1.07149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:             time_total_s 121.6818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:                timestamp 1691659673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:               tmae_coord 0.87247\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:               tmae_force 0.53679\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:              tmape_coord 205045651566330.88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:              tmape_force 881100429226434.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:              trmse_coord 0.4593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:              trmse_force 0.33309\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb:  View run FSR_Trainable_07a0a9ea at: https://wandb.ai/seokjin/FSR-prediction/runs/07a0a9ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=104956)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_182548-07a0a9ea/logs\n",
      "2023-08-10 18:28:12,919\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.599 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:28:12,922\tWARNING util.py:315 -- The `process_trial_result` operation took 2.604 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:28:12,924\tWARNING util.py:315 -- Processing trial results took 2.606 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:28:12,925\tWARNING util.py:315 -- The `process_trial_result` operation took 2.607 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_d1d069cd_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-27-21/wandb/run-20230810_182815-d1d069cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: Syncing run FSR_Trainable_d1d069cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d1d069cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:                mae_coord 4.22267\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:                mae_force 1578.68594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:               mape_coord 1.22266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:               mape_force 2.4988762304348247e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:                   metric 0.8069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:               rmse_coord 2.42658\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:               rmse_force 1109.78235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:       time_since_restore 159.14092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:         time_this_iter_s 1.83669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:             time_total_s 159.14092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:                timestamp 1691659749\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:               tmae_coord 0.85584\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:               tmae_force 0.55827\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:              tmape_coord 203722546256873.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:              tmape_force 907425416724497.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:              trmse_coord 0.4575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:              trmse_force 0.34941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb:  View run FSR_Trainable_7c1a30a3 at: https://wandb.ai/seokjin/FSR-prediction/runs/7c1a30a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105198)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_182627-7c1a30a3/logs\n",
      "2023-08-10 18:29:27,032\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.176 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:29:27,035\tWARNING util.py:315 -- The `process_trial_result` operation took 2.181 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:29:27,036\tWARNING util.py:315 -- Processing trial results took 2.182 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:29:27,038\tWARNING util.py:315 -- The `process_trial_result` operation took 2.183 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:29:28,539\tERROR tune_controller.py:873 -- Trial task failed for trial FSR_Trainable_b346be41\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 2540, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::ResourceTrainable.train()\u001b[39m (pid=105391, ip=172.26.215.93, actor_id=751606acf5a53e9f2c4d0f4901000000, repr=<ray.tune.trainable.util.FSR_Trainable object at 0x7f7bda0d3880>)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 386, in train\n",
      "    result = self.step()\n",
      "  File \"/home/seokj/workspace/FSR-prediction/fsr_trainable.py\", line 72, in step\n",
      "    tmae.append(sklearn.metrics.mean_absolute_error(y, pred, multioutput='raw_values'))\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 196, in mean_absolute_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_810fa16b_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-28-07/wandb/run-20230810_182929-810fa16b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb: Syncing run FSR_Trainable_810fa16b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/810fa16b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: iterations_since_restore 79\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:                mae_coord 5.365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:                mae_force 763.35611\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:               mape_coord 1.37391\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:               mape_force 5.597359023361988e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:                   metric 0.65164\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:               rmse_coord 2.42668\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:               rmse_force 613.20738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:       time_since_restore 137.12738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:         time_this_iter_s 2.09305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:             time_total_s 137.12738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:                timestamp 1691659765\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:               tmae_coord 1.07781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:               tmae_force 0.28406\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:              tmape_coord 183101820832915.44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:              tmape_force 252303314836199.03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:       training_iteration 79\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:              trmse_coord 0.45305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:              trmse_force 0.19859\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb:  View run FSR_Trainable_b346be41 at: https://wandb.ai/seokjin/FSR-prediction/runs/b346be41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105447)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_182709-b346be41/logs\n",
      "2023-08-10 18:29:48,293\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.060 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:29:48,299\tWARNING util.py:315 -- The `process_trial_result` operation took 3.068 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:29:48,302\tWARNING util.py:315 -- Processing trial results took 3.070 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:29:48,303\tWARNING util.py:315 -- The `process_trial_result` operation took 3.072 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_a1b93071_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-29-21/wandb/run-20230810_182950-a1b93071\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: Syncing run FSR_Trainable_a1b93071\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a1b93071\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: | 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:                mae_coord 5.60498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:                mae_force 1452.96688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:               mape_coord 1.32058\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:               mape_force 1.631153981857847e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:                   metric 0.85046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:               rmse_coord 2.62175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:               rmse_force 1129.17714\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:       time_since_restore 50.38062\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:         time_this_iter_s 3.45566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:             time_total_s 50.38062\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:                timestamp 1691659835\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:               tmae_coord 1.12439\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:               tmae_force 0.49976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:              tmape_coord 175585191023693.66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:              tmape_force 517248476034941.3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:              trmse_coord 0.48989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:              trmse_force 0.36057\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb:  View run FSR_Trainable_a1b93071 at: https://wandb.ai/seokjin/FSR-prediction/runs/a1b93071\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106421)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_182950-a1b93071/logs\n",
      "2023-08-10 18:30:55,521\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.812 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:30:55,524\tWARNING util.py:315 -- The `process_trial_result` operation took 1.816 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:30:55,526\tWARNING util.py:315 -- Processing trial results took 1.818 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:30:55,528\tWARNING util.py:315 -- The `process_trial_result` operation took 1.820 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_9fa227d3_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-29-41/wandb/run-20230810_183057-9fa227d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb: Syncing run FSR_Trainable_9fa227d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9fa227d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:                mae_coord 4.93004\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:                mae_force 777.83521\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:               mape_coord 1.1859\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:               mape_force 6.585835523307155e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:                   metric 0.61361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:               rmse_coord 2.32456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:               rmse_force 601.30323\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:       time_since_restore 230.71786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:         time_this_iter_s 2.15422\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:             time_total_s 230.71786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:                timestamp 1691659884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:               tmae_coord 0.97546\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:               tmae_force 0.28203\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:              tmape_coord 161128154114674.16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:              tmape_force 273955750455129.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:              trmse_coord 0.42473\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:              trmse_force 0.18889\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb:  View run FSR_Trainable_14e1a525 at: https://wandb.ai/seokjin/FSR-prediction/runs/14e1a525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105679)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_182727-14e1a525/logs\n",
      "2023-08-10 18:31:46,106\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.558 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:31:46,108\tWARNING util.py:315 -- The `process_trial_result` operation took 2.561 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:31:46,112\tWARNING util.py:315 -- Processing trial results took 2.565 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:31:46,115\tWARNING util.py:315 -- The `process_trial_result` operation took 2.568 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_2428aa9a_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-30-50/wandb/run-20230810_183147-2428aa9a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb: Syncing run FSR_Trainable_2428aa9a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2428aa9a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb: - 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:                mae_coord 5.74818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:                mae_force 1480.87436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:               mape_coord 1.48518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:               mape_force 1.759016551563352e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:                   metric 0.87638\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:               rmse_coord 2.68922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:               rmse_force 1134.80973\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:       time_since_restore 34.96317\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:         time_this_iter_s 4.33555\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:             time_total_s 34.96317\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:                timestamp 1691659936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:               tmae_coord 1.15097\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:               tmae_force 0.51354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:              tmape_coord 180552873283178.88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:              tmape_force 576049838205985.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:              trmse_coord 0.51384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:              trmse_force 0.36254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb:  View run FSR_Trainable_2428aa9a at: https://wandb.ai/seokjin/FSR-prediction/runs/2428aa9a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106917)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_183147-2428aa9a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:                mae_coord 5.82072\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:                mae_force 861.26845\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:               mape_coord 1.40326\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:               mape_force 1.0288623085720145e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:                   metric 0.69299\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:               rmse_coord 2.60174\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:               rmse_force 600.92316\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:       time_since_restore 247.63591\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:         time_this_iter_s 2.24131\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:             time_total_s 247.63591\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:                timestamp 1691659946\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:               tmae_coord 1.14697\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:               tmae_force 0.35143\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:              tmape_coord 141769191863181.88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:              tmape_force 495540682074055.9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:              trmse_coord 0.47826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:              trmse_force 0.21473\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb:  View run FSR_Trainable_d1d069cd at: https://wandb.ai/seokjin/FSR-prediction/runs/d1d069cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=105928)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_182815-d1d069cd/logs\n",
      "2023-08-10 18:32:34,779\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.046 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:32:34,786\tWARNING util.py:315 -- The `process_trial_result` operation took 3.053 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:32:34,788\tWARNING util.py:315 -- Processing trial results took 3.056 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:32:34,790\tWARNING util.py:315 -- The `process_trial_result` operation took 3.058 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_5f9dc5f2_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-31-38/wandb/run-20230810_183237-5f9dc5f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: Syncing run FSR_Trainable_5f9dc5f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5f9dc5f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:                mae_coord 5.02258\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:                mae_force 1138.41613\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:               mape_coord 1.35278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:               mape_force 1.296867867181274e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:                   metric 18.63846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:               rmse_coord 2.46168\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:               rmse_force 959.12081\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:       time_since_restore 2.31989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:         time_this_iter_s 2.31989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:             time_total_s 2.31989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:                timestamp 1691659951\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:               tmae_coord 14.59\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:               tmae_force 9.15548\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:              tmape_coord 1.0972818231725304e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:              tmape_force 6195499752510341.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:              trmse_coord 8.06023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:              trmse_force 10.57824\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb:  View run FSR_Trainable_5f9dc5f2 at: https://wandb.ai/seokjin/FSR-prediction/runs/5f9dc5f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107166)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_183237-5f9dc5f2/logs\n",
      "2023-08-10 18:32:45,717\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.255 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:32:45,722\tWARNING util.py:315 -- The `process_trial_result` operation took 2.262 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:32:45,725\tWARNING util.py:315 -- Processing trial results took 2.264 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:32:45,727\tWARNING util.py:315 -- The `process_trial_result` operation took 2.266 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_064d057a_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-32-29/wandb/run-20230810_183248-064d057a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: Syncing run FSR_Trainable_064d057a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/064d057a\n",
      "2023-08-10 18:32:57,633\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.222 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:32:57,638\tWARNING util.py:315 -- The `process_trial_result` operation took 2.227 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:32:57,639\tWARNING util.py:315 -- Processing trial results took 2.228 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:32:57,640\tWARNING util.py:315 -- The `process_trial_result` operation took 2.229 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_b6b8ae6f_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-32-41/wandb/run-20230810_183300-b6b8ae6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb: Syncing run FSR_Trainable_b6b8ae6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b6b8ae6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb: - 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:                mae_coord 4.91414\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:                mae_force 800.23609\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:               mape_coord 1.23005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:               mape_force 6.323296512200028e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:                   metric 0.65704\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:               rmse_coord 2.44286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:               rmse_force 636.35141\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:       time_since_restore 238.75079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:         time_this_iter_s 2.09419\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:             time_total_s 238.75079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:                timestamp 1691660014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:               tmae_coord 0.9792\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:               tmae_force 0.29977\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:              tmape_coord 139714685036390.45\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:              tmape_force 284567740997299.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:              trmse_coord 0.44802\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:              trmse_force 0.20903\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb:  View run FSR_Trainable_810fa16b at: https://wandb.ai/seokjin/FSR-prediction/runs/810fa16b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106193)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_182929-810fa16b/logs\n",
      "2023-08-10 18:33:51,065\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.776 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:33:51,069\tWARNING util.py:315 -- The `process_trial_result` operation took 1.781 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:33:51,072\tWARNING util.py:315 -- Processing trial results took 1.783 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:33:51,073\tWARNING util.py:315 -- The `process_trial_result` operation took 1.785 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_c2c62f97_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-32-53/wandb/run-20230810_183353-c2c62f97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb: Syncing run FSR_Trainable_c2c62f97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c2c62f97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:                mae_coord 5.45441\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:                mae_force 781.32153\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:               mape_coord 1.25709\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:               mape_force 5.757298931625452e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:                   metric 0.66797\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:               rmse_coord 2.51037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:               rmse_force 614.88022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:       time_since_restore 264.26598\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:         time_this_iter_s 2.41281\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:             time_total_s 264.26598\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:                timestamp 1691660125\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:               tmae_coord 1.10502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:               tmae_force 0.28592\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:              tmape_coord 155740912757487.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:              tmape_force 234864420923030.03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:              trmse_coord 0.46766\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:              trmse_force 0.20031\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb:  View run FSR_Trainable_9fa227d3 at: https://wandb.ai/seokjin/FSR-prediction/runs/9fa227d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=106674)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_183057-9fa227d3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:                mae_coord 5.14629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:                mae_force 726.44922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:               mape_coord 1.24459\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:               mape_force 7.61490025268033e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:                   metric 0.61716\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:               rmse_coord 2.37326\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:               rmse_force 542.0501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:       time_since_restore 166.98537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:         time_this_iter_s 1.71606\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:             time_total_s 166.98537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:                timestamp 1691660137\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:               tmae_coord 1.02564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:               tmae_force 0.27189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:              tmape_coord 151634940469510.3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:              tmape_force 297395390208246.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:              trmse_coord 0.4368\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:              trmse_force 0.18035\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb:  View run FSR_Trainable_064d057a at: https://wandb.ai/seokjin/FSR-prediction/runs/064d057a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107389)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_183248-064d057a/logs\n",
      "2023-08-10 18:35:42,651\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.854 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:35:42,661\tWARNING util.py:315 -- The `process_trial_result` operation took 2.865 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:35:42,663\tWARNING util.py:315 -- Processing trial results took 2.867 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:35:42,665\tWARNING util.py:315 -- The `process_trial_result` operation took 2.870 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_c62dfb24_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-33-47/wandb/run-20230810_183545-c62dfb24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Syncing run FSR_Trainable_c62dfb24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c62dfb24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb: - 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:                mae_coord 4.72639\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:                mae_force 712.18855\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:               mape_coord 1.18562\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:               mape_force 6.453766473141978e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:                   metric 0.59444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:               rmse_coord 2.28368\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:               rmse_force 567.49142\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:       time_since_restore 166.82823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:         time_this_iter_s 1.33949\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:             time_total_s 166.82823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:                timestamp 1691660150\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:               tmae_coord 0.94067\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:               tmae_force 0.2507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:              tmape_coord 163617032974172.7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:              tmape_force 242436103107391.75\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:              trmse_coord 0.42192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:              trmse_force 0.17252\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb:  View run FSR_Trainable_b6b8ae6f at: https://wandb.ai/seokjin/FSR-prediction/runs/b6b8ae6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107612)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_183300-b6b8ae6f/logs\n",
      "2023-08-10 18:35:59,387\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.813 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:35:59,390\tWARNING util.py:315 -- The `process_trial_result` operation took 1.817 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:35:59,391\tWARNING util.py:315 -- Processing trial results took 1.818 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:35:59,393\tWARNING util.py:315 -- The `process_trial_result` operation took 1.820 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_9d8e4ab3_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-35-37/wandb/run-20230810_183559-9d8e4ab3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb: Syncing run FSR_Trainable_9d8e4ab3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9d8e4ab3\n",
      "2023-08-10 18:36:07,865\tERROR tune_controller.py:873 -- Trial task failed for trial FSR_Trainable_1a76a8a6\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 2540, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::ResourceTrainable.train()\u001b[39m (pid=108497, ip=172.26.215.93, actor_id=46805264618011b30afc044901000000, repr=<ray.tune.trainable.util.FSR_Trainable object at 0x7fc49f93f820>)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 386, in train\n",
      "    result = self.step()\n",
      "  File \"/home/seokj/workspace/FSR-prediction/fsr_trainable.py\", line 72, in step\n",
      "    tmae.append(sklearn.metrics.mean_absolute_error(y, pred, multioutput='raw_values'))\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 196, in mean_absolute_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108599)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108599)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108599)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108599)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108599)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_1a76a8a6_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-35-52/wandb/run-20230810_183610-1a76a8a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108599)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108599)\u001b[0m wandb: Syncing run FSR_Trainable_1a76a8a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108599)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108599)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/1a76a8a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108599)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108599)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108599)\u001b[0m wandb: \\ 0.004 MB of 0.007 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108599)\u001b[0m wandb: | 0.004 MB of 0.007 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108599)\u001b[0m wandb:  View run FSR_Trainable_1a76a8a6 at: https://wandb.ai/seokjin/FSR-prediction/runs/1a76a8a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108599)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108599)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_183610-1a76a8a6/logs\n",
      "2023-08-10 18:36:26,178\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.448 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:36:26,188\tWARNING util.py:315 -- The `process_trial_result` operation took 2.459 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:36:26,192\tWARNING util.py:315 -- Processing trial results took 2.463 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:36:26,196\tWARNING util.py:315 -- The `process_trial_result` operation took 2.467 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_e1bafa1b_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-36-03/wandb/run-20230810_183630-e1bafa1b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb: Syncing run FSR_Trainable_e1bafa1b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e1bafa1b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:                mae_coord 4.37136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:                mae_force 736.45225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:               mape_coord 1.07726\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:               mape_force 7.753516910329288e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:                   metric 0.5746\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:               rmse_coord 2.15162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:               rmse_force 556.14026\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:       time_since_restore 170.78664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:         time_this_iter_s 1.67777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:             time_total_s 170.78664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:                timestamp 1691660208\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:               tmae_coord 0.87851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:               tmae_force 0.26254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:              tmape_coord 153878219371751.72\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:              tmape_force 282669252036818.94\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:              trmse_coord 0.39624\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:              trmse_force 0.17836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb:  View run FSR_Trainable_c2c62f97 at: https://wandb.ai/seokjin/FSR-prediction/runs/c2c62f97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=107862)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_183353-c2c62f97/logs\n",
      "2023-08-10 18:37:13,950\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.805 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:37:13,952\tWARNING util.py:315 -- The `process_trial_result` operation took 2.809 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:37:13,956\tWARNING util.py:315 -- Processing trial results took 2.813 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:37:13,960\tWARNING util.py:315 -- The `process_trial_result` operation took 2.817 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_d3976aff_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-36-22/wandb/run-20230810_183717-d3976aff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb: Syncing run FSR_Trainable_d3976aff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d3976aff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:                mae_coord 5.72853\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:                mae_force 764.1879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:               mape_coord 1.32988\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:               mape_force 5.560497876350844e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:                   metric 0.6807\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:               rmse_coord 2.58283\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:               rmse_force 596.73636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:       time_since_restore 160.2975\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:         time_this_iter_s 4.41043\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:             time_total_s 160.2975\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:                timestamp 1691660318\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:               tmae_coord 1.14871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:               tmae_force 0.28741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:              tmape_coord 179912003203445.28\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:              tmape_force 250230391044880.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:              trmse_coord 0.48136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:              trmse_force 0.19934\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb:  View run FSR_Trainable_9d8e4ab3 at: https://wandb.ai/seokjin/FSR-prediction/runs/9d8e4ab3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108382)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_183559-9d8e4ab3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: \\ 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_183545-c62dfb24/logs\n",
      "2023-08-10 18:38:51,850\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.923 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:38:51,865\tWARNING util.py:315 -- The `process_trial_result` operation took 1.938 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:38:51,874\tWARNING util.py:315 -- Processing trial results took 1.948 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:38:51,879\tWARNING util.py:315 -- The `process_trial_result` operation took 1.953 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_7ae6b73f_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-37-07/wandb/run-20230810_183854-7ae6b73f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb: Syncing run FSR_Trainable_7ae6b73f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7ae6b73f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:                mae_coord 5.43522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:                mae_force 1242.25305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:               mape_coord 1.36598\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:               mape_force 2.6370005330123968e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:                   metric 20.62186\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:               rmse_coord 2.61096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:               rmse_force 1040.95244\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:       time_since_restore 1.86703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:         time_this_iter_s 1.86703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:             time_total_s 1.86703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:                timestamp 1691660329\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:               tmae_coord 17.41728\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:               tmae_force 10.93549\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:              tmape_coord 1.557343127697686e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:              tmape_force 1.2022323147178144e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:              trmse_coord 9.41669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:              trmse_force 11.20518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb:  View run FSR_Trainable_7ae6b73f at: https://wandb.ai/seokjin/FSR-prediction/runs/7ae6b73f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_183854-7ae6b73f/logs\n",
      "2023-08-10 18:39:01,644\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.867 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:39:01,650\tWARNING util.py:315 -- The `process_trial_result` operation took 1.874 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:39:01,652\tWARNING util.py:315 -- Processing trial results took 1.876 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:39:01,654\tWARNING util.py:315 -- The `process_trial_result` operation took 1.878 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109360)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_71596cc8_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-38-48/wandb/run-20230810_183904-71596cc8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb: Syncing run FSR_Trainable_71596cc8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/71596cc8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:                mae_coord 4.92029\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:                mae_force 1366.96895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:               mape_coord 1.41064\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:               mape_force 3.158492932843049e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:                   metric 19.42942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:               rmse_coord 2.4716\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:               rmse_force 1082.80102\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:       time_since_restore 2.02554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:         time_this_iter_s 2.02554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:             time_total_s 2.02554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:                timestamp 1691660339\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:               tmae_coord 13.18009\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:               tmae_force 11.50015\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:              tmape_coord 6054966506948064.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:              tmape_force 1.307072552641133e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:              trmse_coord 7.68753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:              trmse_force 11.74188\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb:  View run FSR_Trainable_71596cc8 at: https://wandb.ai/seokjin/FSR-prediction/runs/71596cc8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_183904-71596cc8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109577)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-08-10 18:39:10,939\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.832 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:39:10,944\tWARNING util.py:315 -- The `process_trial_result` operation took 1.838 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:39:10,945\tWARNING util.py:315 -- Processing trial results took 1.839 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:39:10,946\tWARNING util.py:315 -- The `process_trial_result` operation took 1.841 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_3b45c3b3_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-38-57/wandb/run-20230810_183914-3b45c3b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: Syncing run FSR_Trainable_3b45c3b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/3b45c3b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:                mae_coord 5.24567\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:                mae_force 780.20069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:               mape_coord 1.22304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:               mape_force 6.122112834724513e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:                   metric 0.64035\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:               rmse_coord 2.41476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:               rmse_force 628.82734\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:       time_since_restore 160.10048\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:         time_this_iter_s 1.42288\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:             time_total_s 160.10048\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:                timestamp 1691660355\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:               tmae_coord 1.04362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:               tmae_force 0.27866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:              tmape_coord 169696741725785.56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:              tmape_force 249853055382619.03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:              trmse_coord 0.44543\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:              trmse_force 0.19491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb:  View run FSR_Trainable_e1bafa1b at: https://wandb.ai/seokjin/FSR-prediction/runs/e1bafa1b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=108828)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_183630-e1bafa1b/logs\n",
      "2023-08-10 18:39:22,793\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.719 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:39:22,797\tWARNING util.py:315 -- The `process_trial_result` operation took 1.724 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:39:22,798\tWARNING util.py:315 -- Processing trial results took 1.725 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:39:22,800\tWARNING util.py:315 -- The `process_trial_result` operation took 1.727 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_eef4ead6_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-39-07/wandb/run-20230810_183925-eef4ead6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb: Syncing run FSR_Trainable_eef4ead6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/eef4ead6\n",
      "2023-08-10 18:39:35,401\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.671 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:39:35,405\tWARNING util.py:315 -- The `process_trial_result` operation took 1.676 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:39:35,406\tWARNING util.py:315 -- Processing trial results took 1.678 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:39:35,408\tWARNING util.py:315 -- The `process_trial_result` operation took 1.680 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_d3405b79_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-39-19/wandb/run-20230810_183938-d3405b79\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: Syncing run FSR_Trainable_d3405b79\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d3405b79\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:                mae_coord 5.59294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:                mae_force 1437.66698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:               mape_coord 1.50732\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:               mape_force 1.351972452128107e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:                   metric 0.85343\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:               rmse_coord 2.58524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:               rmse_force 1148.64009\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:       time_since_restore 153.6178\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:         time_this_iter_s 1.47452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:             time_total_s 153.6178\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:                timestamp 1691660393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:               tmae_coord 1.11796\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:               tmae_force 0.49742\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:              tmape_coord 202723555689293.22\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:              tmape_force 429441143164926.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:              trmse_coord 0.48566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:              trmse_force 0.36777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb:  View run FSR_Trainable_d3976aff at: https://wandb.ai/seokjin/FSR-prediction/runs/d3976aff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109070)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_183717-d3976aff/logs\n",
      "2023-08-10 18:40:07,281\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.660 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:40:07,283\tWARNING util.py:315 -- The `process_trial_result` operation took 1.663 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:40:07,286\tWARNING util.py:315 -- Processing trial results took 1.666 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:40:07,288\tWARNING util.py:315 -- The `process_trial_result` operation took 1.668 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_1d3d4e4d_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-39-32/wandb/run-20230810_184010-1d3d4e4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: Syncing run FSR_Trainable_1d3d4e4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/1d3d4e4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:                mae_coord 4.53006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:                mae_force 618.95616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:               mape_coord 1.17099\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:               mape_force 5.483927526012578e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:                   metric 0.57504\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:               rmse_coord 2.26544\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:               rmse_force 471.7059\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:       time_since_restore 111.44202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:         time_this_iter_s 1.03944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:             time_total_s 111.44202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:                timestamp 1691660468\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:               tmae_coord 0.91349\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:               tmae_force 0.23033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:              tmape_coord 169514014996101.38\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:              tmape_force 230572822530512.12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:              trmse_coord 0.4183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:              trmse_force 0.15674\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb:  View run FSR_Trainable_3b45c3b3 at: https://wandb.ai/seokjin/FSR-prediction/runs/3b45c3b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=109800)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_183914-3b45c3b3/logs\n",
      "2023-08-10 18:41:22,685\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.608 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:41:22,688\tWARNING util.py:315 -- The `process_trial_result` operation took 1.611 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:41:22,691\tWARNING util.py:315 -- Processing trial results took 1.614 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:41:22,694\tWARNING util.py:315 -- The `process_trial_result` operation took 1.617 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_026edc0a_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-40-04/wandb/run-20230810_184126-026edc0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: Syncing run FSR_Trainable_026edc0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/026edc0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:                mae_coord 5.29607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:                mae_force 708.90158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:               mape_coord 1.25306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:               mape_force 7.05117148571439e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:                   metric 0.61921\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:               rmse_coord 2.433\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:               rmse_force 523.41052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:       time_since_restore 114.55165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:         time_this_iter_s 1.33519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:             time_total_s 114.55165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:                timestamp 1691660484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:               tmae_coord 1.05407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:               tmae_force 0.26133\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:              tmape_coord 166481726043926.88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:              tmape_force 275975724139709.44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:              trmse_coord 0.4468\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:              trmse_force 0.17241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb:  View run FSR_Trainable_eef4ead6 at: https://wandb.ai/seokjin/FSR-prediction/runs/eef4ead6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110029)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_183925-eef4ead6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 18:41:38,746\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.710 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:41:38,749\tWARNING util.py:315 -- The `process_trial_result` operation took 1.713 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:41:38,751\tWARNING util.py:315 -- Processing trial results took 1.715 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:41:38,753\tWARNING util.py:315 -- The `process_trial_result` operation took 1.717 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:                mae_coord 4.78024\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:                mae_force 659.39421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:               mape_coord 1.23591\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:               mape_force 6.420609745291602e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:                   metric 0.59828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:               rmse_coord 2.33371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:               rmse_force 489.88518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:       time_since_restore 113.12901\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:         time_this_iter_s 1.06149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:             time_total_s 113.12901\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:                timestamp 1691660493\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:               tmae_coord 0.95591\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:               tmae_force 0.24827\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:              tmape_coord 174214082086150.44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:              tmape_force 268836398973391.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:              trmse_coord 0.43204\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:              trmse_force 0.16624\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb:  View run FSR_Trainable_d3405b79 at: https://wandb.ai/seokjin/FSR-prediction/runs/d3405b79\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110243)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_183938-d3405b79/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_bb43cf10_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-41-19/wandb/run-20230810_184141-bb43cf10\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: Syncing run FSR_Trainable_bb43cf10\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/bb43cf10\n",
      "2023-08-10 18:41:53,047\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.854 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:41:53,053\tWARNING util.py:315 -- The `process_trial_result` operation took 1.861 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:41:53,057\tWARNING util.py:315 -- Processing trial results took 1.865 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:41:53,060\tWARNING util.py:315 -- The `process_trial_result` operation took 1.868 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_cab58447_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-41-35/wandb/run-20230810_184154-cab58447\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: Syncing run FSR_Trainable_cab58447\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/cab58447\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:                mae_coord 4.73919\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:                mae_force 756.7869\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:               mape_coord 1.24625\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:               mape_force 590983707.35832\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:                   metric 5.15132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:               rmse_coord 2.40373\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:               rmse_force 619.05702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:       time_since_restore 2.92664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:         time_this_iter_s 2.92664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:             time_total_s 2.92664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:                timestamp 1691660511\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:               tmae_coord 7.65553\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:               tmae_force 2.08689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:              tmape_coord 3667677005635417.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:              tmape_force 12.38457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:              trmse_coord 3.53702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:              trmse_force 1.6143\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb:  View run FSR_Trainable_cab58447 at: https://wandb.ai/seokjin/FSR-prediction/runs/cab58447\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111231)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184154-cab58447/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 18:42:10,239\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.777 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:42:10,246\tWARNING util.py:315 -- The `process_trial_result` operation took 1.785 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:42:10,248\tWARNING util.py:315 -- Processing trial results took 1.787 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:42:10,250\tWARNING util.py:315 -- The `process_trial_result` operation took 1.789 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:                mae_coord 4.61475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:                mae_force 648.97661\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:               mape_coord 1.17512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:               mape_force 6.189155064105669e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:                   metric 0.5809\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:               rmse_coord 2.27843\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:               rmse_force 485.93548\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:       time_since_restore 113.96275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:         time_this_iter_s 1.327\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:             time_total_s 113.96275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:                timestamp 1691660527\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:               tmae_coord 0.92511\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:               tmae_force 0.24211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:              tmape_coord 167916376666575.44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:              tmape_force 253933138018514.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:              trmse_coord 0.41997\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:              trmse_force 0.16094\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb:  View run FSR_Trainable_1d3d4e4d at: https://wandb.ai/seokjin/FSR-prediction/runs/1d3d4e4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110487)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184010-1d3d4e4d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_4bfd99c3_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-41-48/wandb/run-20230810_184212-4bfd99c3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Syncing run FSR_Trainable_4bfd99c3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4bfd99c3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111460)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184212-4bfd99c3/logs\n",
      "2023-08-10 18:42:21,633\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.618 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:42:21,635\tWARNING util.py:315 -- The `process_trial_result` operation took 1.620 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:42:21,638\tWARNING util.py:315 -- Processing trial results took 1.623 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:42:21,639\tWARNING util.py:315 -- The `process_trial_result` operation took 1.625 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_a4dc490d_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-42-05/wandb/run-20230810_184224-a4dc490d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: Syncing run FSR_Trainable_a4dc490d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a4dc490d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:                mae_coord 4.34574\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:                mae_force 1412.98494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:               mape_coord 1.27948\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:               mape_force 4.684470610660095e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:                   metric 21.24612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:               rmse_coord 2.42471\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:               rmse_force 1179.08477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:       time_since_restore 1.50798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:         time_this_iter_s 1.50798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:             time_total_s 1.50798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:                timestamp 1691660540\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:               tmae_coord 12.17344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:               tmae_force 11.07891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:              tmape_coord 2455627335042844.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:              tmape_force 1323957686480296.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:              trmse_coord 7.72564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:              trmse_force 13.52048\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb:  View run FSR_Trainable_a4dc490d at: https://wandb.ai/seokjin/FSR-prediction/runs/a4dc490d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111704)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184224-a4dc490d/logs\n",
      "2023-08-10 18:42:35,997\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.775 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:42:36,001\tWARNING util.py:315 -- The `process_trial_result` operation took 1.780 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:42:36,004\tWARNING util.py:315 -- Processing trial results took 1.783 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:42:36,006\tWARNING util.py:315 -- The `process_trial_result` operation took 1.785 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_559affac_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-42-18/wandb/run-20230810_184236-559affac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: Syncing run FSR_Trainable_559affac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/559affac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:                mae_coord 4.52777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:                mae_force 1359.78173\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:               mape_coord 1.22063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:               mape_force 1.2910064961555883e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:                   metric 20.75072\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:               rmse_coord 2.47815\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:               rmse_force 1114.49849\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:       time_since_restore 4.34306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:         time_this_iter_s 4.34306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:             time_total_s 4.34306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:                timestamp 1691660554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:               tmae_coord 12.48298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:               tmae_force 11.20785\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:              tmape_coord 3434602860862649.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:              tmape_force 6002380662840844.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:              trmse_coord 7.80733\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:              trmse_force 12.9434\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb:  View run FSR_Trainable_559affac at: https://wandb.ai/seokjin/FSR-prediction/runs/559affac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111924)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184236-559affac/logs\n",
      "2023-08-10 18:42:46,907\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.008 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:42:46,912\tWARNING util.py:315 -- The `process_trial_result` operation took 2.013 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:42:46,913\tWARNING util.py:315 -- Processing trial results took 2.014 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:42:46,914\tWARNING util.py:315 -- The `process_trial_result` operation took 2.016 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_f64d9976_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-42-29/wandb/run-20230810_184247-f64d9976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb: Syncing run FSR_Trainable_f64d9976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f64d9976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 18:42:55,434\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.176 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:42:55,437\tWARNING util.py:315 -- The `process_trial_result` operation took 2.180 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:42:55,439\tWARNING util.py:315 -- Processing trial results took 2.182 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:42:55,443\tWARNING util.py:315 -- The `process_trial_result` operation took 2.186 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:                mae_coord 5.52498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:                mae_force 1441.35478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:               mape_coord 1.45912\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:               mape_force 1.3925097544635028e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:                   metric 0.86026\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:               rmse_coord 2.61485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:               rmse_force 1147.26514\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:       time_since_restore 9.42365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:         time_this_iter_s 4.59703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:             time_total_s 9.42365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:                timestamp 1691660571\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:               tmae_coord 1.10865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:               tmae_force 0.49927\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:              tmape_coord 200828966548265.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:              tmape_force 447333545775134.7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:              trmse_coord 0.49345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:              trmse_force 0.3668\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb:  View run FSR_Trainable_f64d9976 at: https://wandb.ai/seokjin/FSR-prediction/runs/f64d9976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112145)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184247-f64d9976/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_4ee8eda9_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-42-40/wandb/run-20230810_184258-4ee8eda9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb: Syncing run FSR_Trainable_4ee8eda9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4ee8eda9\n",
      "2023-08-10 18:43:07,948\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.609 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:43:07,950\tWARNING util.py:315 -- The `process_trial_result` operation took 1.611 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:43:07,954\tWARNING util.py:315 -- Processing trial results took 1.615 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:43:07,956\tWARNING util.py:315 -- The `process_trial_result` operation took 1.618 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_8568305e_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-42-51/wandb/run-20230810_184311-8568305e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: Syncing run FSR_Trainable_8568305e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8568305e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: | 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: / 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: - 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:                mae_coord 4.65566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:                mae_force 666.32409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:               mape_coord 1.20238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:               mape_force 6.387597883733373e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:                   metric 0.58788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:               rmse_coord 2.26889\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:               rmse_force 498.52167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:       time_since_restore 116.92291\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:         time_this_iter_s 1.07574\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:             time_total_s 116.92291\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:                timestamp 1691660614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:               tmae_coord 0.93432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:               tmae_force 0.24976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:              tmape_coord 167895876132898.12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:              tmape_force 262502976512680.7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:              trmse_coord 0.41971\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:              trmse_force 0.16817\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb:  View run FSR_Trainable_026edc0a at: https://wandb.ai/seokjin/FSR-prediction/runs/026edc0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=110771)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184126-026edc0a/logs\n",
      "2023-08-10 18:43:48,662\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.731 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:43:48,665\tWARNING util.py:315 -- The `process_trial_result` operation took 1.736 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:43:48,667\tWARNING util.py:315 -- Processing trial results took 1.738 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:43:48,668\tWARNING util.py:315 -- The `process_trial_result` operation took 1.739 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: | 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_c7f219cd_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-43-04/wandb/run-20230810_184352-c7f219cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb: Syncing run FSR_Trainable_c7f219cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c7f219cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:                mae_coord 4.56698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:                mae_force 623.63229\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:               mape_coord 1.15732\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:               mape_force 5.226702581657034e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:                   metric 0.58266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:               rmse_coord 2.27475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:               rmse_force 484.25796\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:       time_since_restore 115.88915\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:         time_this_iter_s 1.11116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:             time_total_s 115.88915\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:                timestamp 1691660627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:               tmae_coord 0.9227\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:               tmae_force 0.23193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:              tmape_coord 163816220759069.88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:              tmape_force 213948149449955.06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:              trmse_coord 0.42089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:              trmse_force 0.16177\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb:  View run FSR_Trainable_bb43cf10 at: https://wandb.ai/seokjin/FSR-prediction/runs/bb43cf10\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=111008)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184141-bb43cf10/logs\n",
      "2023-08-10 18:44:03,706\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.560 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:44:03,711\tWARNING util.py:315 -- The `process_trial_result` operation took 1.566 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:44:03,713\tWARNING util.py:315 -- Processing trial results took 1.568 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:44:03,715\tWARNING util.py:315 -- The `process_trial_result` operation took 1.570 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_98eea35e_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-43-45/wandb/run-20230810_184407-98eea35e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb: Syncing run FSR_Trainable_98eea35e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/98eea35e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb: - 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:                mae_coord 4.79292\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:                mae_force 639.20629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:               mape_coord 1.19788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:               mape_force 5.984233371891813e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:                   metric 0.58376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:               rmse_coord 2.28738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:               rmse_force 478.13549\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:       time_since_restore 113.23482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:         time_this_iter_s 1.07116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:             time_total_s 113.23482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:                timestamp 1691660695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:               tmae_coord 0.96438\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:               tmae_force 0.2378\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:              tmape_coord 173096999910551.84\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:              tmape_force 244684357853990.03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:              trmse_coord 0.42449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:              trmse_force 0.15927\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb:  View run FSR_Trainable_4ee8eda9 at: https://wandb.ai/seokjin/FSR-prediction/runs/4ee8eda9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112364)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184258-4ee8eda9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: | 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 18:45:10,428\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.670 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:45:10,429\tWARNING util.py:315 -- The `process_trial_result` operation took 1.673 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:45:10,433\tWARNING util.py:315 -- Processing trial results took 1.676 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:45:10,434\tWARNING util.py:315 -- The `process_trial_result` operation took 1.678 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: / 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: - 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:                mae_coord 4.78794\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:                mae_force 668.61059\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:               mape_coord 1.20883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:               mape_force 6.113335475289336e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:                   metric 0.60623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:               rmse_coord 2.32236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:               rmse_force 512.52581\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:       time_since_restore 111.61169\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:         time_this_iter_s 1.10422\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:             time_total_s 111.61169\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:                timestamp 1691660705\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:               tmae_coord 0.96314\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:               tmae_force 0.24973\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:              tmape_coord 172207739868550.9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:              tmape_force 249683607182591.56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:              trmse_coord 0.43292\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:              trmse_force 0.17331\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb:  View run FSR_Trainable_8568305e at: https://wandb.ai/seokjin/FSR-prediction/runs/8568305e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112592)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184311-8568305e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_b62849af_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-44-00/wandb/run-20230810_184513-b62849af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb: Syncing run FSR_Trainable_b62849af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b62849af\n",
      "2023-08-10 18:45:23,148\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.734 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:45:23,152\tWARNING util.py:315 -- The `process_trial_result` operation took 1.738 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:45:23,153\tWARNING util.py:315 -- Processing trial results took 1.740 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:45:23,155\tWARNING util.py:315 -- The `process_trial_result` operation took 1.741 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_fb354333_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-45-07/wandb/run-20230810_184526-fb354333\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: Syncing run FSR_Trainable_fb354333\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fb354333\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:                mae_coord 4.40815\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:                mae_force 1126.13483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:               mape_coord 1.30795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:               mape_force 2036762627.4901\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:                   metric 5.63881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:               rmse_coord 2.40243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:               rmse_force 888.15679\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:       time_since_restore 1.46769\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:         time_this_iter_s 1.46769\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:             time_total_s 1.46769\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:                timestamp 1691660721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:               tmae_coord 7.15345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:               tmae_force 2.8773\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:              tmape_coord 1707225851201709.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:              tmape_force 8.76942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:              trmse_coord 3.52503\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:              trmse_force 2.11377\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb:  View run FSR_Trainable_fb354333 at: https://wandb.ai/seokjin/FSR-prediction/runs/fb354333\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113587)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184526-fb354333/logs\n",
      "2023-08-10 18:45:38,108\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.905 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:45:38,112\tWARNING util.py:315 -- The `process_trial_result` operation took 1.911 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:45:38,114\tWARNING util.py:315 -- Processing trial results took 1.912 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:45:38,116\tWARNING util.py:315 -- The `process_trial_result` operation took 1.914 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_c374aa34_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-45-20/wandb/run-20230810_184541-c374aa34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: Syncing run FSR_Trainable_c374aa34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c374aa34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:                mae_coord 4.60474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:                mae_force 1497.93478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:               mape_coord 1.25296\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:               mape_force 6241499422.22194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:                   metric 5.89656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:               rmse_coord 2.45415\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:               rmse_force 1042.3065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:       time_since_restore 1.62091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:         time_this_iter_s 1.62091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:             time_total_s 1.62091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:                timestamp 1691660736\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:               tmae_coord 7.38903\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:               tmae_force 3.75428\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:              tmape_coord 2642176546081312.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:              tmape_force 9.75962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:              trmse_coord 3.56317\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:              trmse_force 2.33339\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb:  View run FSR_Trainable_c374aa34 at: https://wandb.ai/seokjin/FSR-prediction/runs/c374aa34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113815)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184541-c374aa34/logs\n",
      "2023-08-10 18:45:53,178\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.576 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:45:53,181\tWARNING util.py:315 -- The `process_trial_result` operation took 1.581 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:45:53,182\tWARNING util.py:315 -- Processing trial results took 1.582 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:45:53,184\tWARNING util.py:315 -- The `process_trial_result` operation took 1.584 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_4f489564_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-45-34/wandb/run-20230810_184556-4f489564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Syncing run FSR_Trainable_4f489564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4f489564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:                mae_coord 4.60557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:                mae_force 625.57226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:               mape_coord 1.18734\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:               mape_force 5.415546024817047e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:                   metric 0.58505\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:               rmse_coord 2.29319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:               rmse_force 472.82648\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:       time_since_restore 115.92101\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:         time_this_iter_s 1.22034\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:             time_total_s 115.92101\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:                timestamp 1691660754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:               tmae_coord 0.92575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:               tmae_force 0.23736\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:              tmape_coord 172449073718059.3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:              tmape_force 240038011556871.06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:              trmse_coord 0.42332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:              trmse_force 0.16173\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb:  View run FSR_Trainable_c7f219cd at: https://wandb.ai/seokjin/FSR-prediction/runs/c7f219cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=112849)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184352-c7f219cd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 18:46:08,176\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.946 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:46:08,178\tWARNING util.py:315 -- The `process_trial_result` operation took 1.949 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:46:08,181\tWARNING util.py:315 -- Processing trial results took 1.952 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:46:08,183\tWARNING util.py:315 -- The `process_trial_result` operation took 1.954 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb: \\ 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:                mae_coord 4.52146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:                mae_force 642.08045\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:               mape_coord 1.15312\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:               mape_force 5.684234074602202e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:                   metric 0.59145\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:               rmse_coord 2.25871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:               rmse_force 493.80277\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:       time_since_restore 113.48145\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:         time_this_iter_s 1.44573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:             time_total_s 113.48145\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:                timestamp 1691660765\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:               tmae_coord 0.90621\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:               tmae_force 0.24135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:              tmape_coord 165688204068420.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:              tmape_force 221358200333302.12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:              trmse_coord 0.4177\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:              trmse_force 0.17375\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb:  View run FSR_Trainable_98eea35e at: https://wandb.ai/seokjin/FSR-prediction/runs/98eea35e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184407-98eea35e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113079)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_3dfabaf2_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-45-49/wandb/run-20230810_184613-3dfabaf2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb: Syncing run FSR_Trainable_3dfabaf2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/3dfabaf2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 18:46:16,735\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.042 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:46:16,737\tWARNING util.py:315 -- The `process_trial_result` operation took 2.044 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:46:16,743\tWARNING util.py:315 -- Processing trial results took 2.050 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:46:16,745\tWARNING util.py:315 -- The `process_trial_result` operation took 2.052 s, which may be a performance bottleneck.\n",
      "wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)04 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:                mae_coord 5.66177\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:                mae_force 1432.25748\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:               mape_coord 1.34606\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:               mape_force 1.7998211726844795e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:                   metric 0.84663\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:               rmse_coord 2.64917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:               rmse_force 1081.91123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:       time_since_restore 4.81012\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:         time_this_iter_s 2.09097\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:             time_total_s 4.81012\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:                timestamp 1691660770\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:               tmae_coord 1.13062\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:               tmae_force 0.50031\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:              tmape_coord 180357158151700.06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:              tmape_force 587942676886827.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:              trmse_coord 0.50049\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:              trmse_force 0.34614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb:  View run FSR_Trainable_3dfabaf2 at: https://wandb.ai/seokjin/FSR-prediction/runs/3dfabaf2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184613-3dfabaf2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114284)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/eb10b985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/eb10b985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/eb10b985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/eb10b985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/eb10b985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/eb10b985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/eb10b985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/eb10b985\n",
      "2023-08-10 18:46:26,817\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.066 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:46:26,822\tWARNING util.py:315 -- The `process_trial_result` operation took 2.072 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:46:26,824\tWARNING util.py:315 -- Processing trial results took 2.074 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:46:26,826\tWARNING util.py:315 -- The `process_trial_result` operation took 2.076 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_693db682_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-46-12/wandb/run-20230810_184630-693db682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: Syncing run FSR_Trainable_693db682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/693db682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:                mae_coord 5.82661\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:                mae_force 1075.36779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:               mape_coord 1.43278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:               mape_force 9.975890702367896e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:                   metric 0.78612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:               rmse_coord 2.71086\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:               rmse_force 857.85042\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:       time_since_restore 3.87176\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:         time_this_iter_s 1.86841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:             time_total_s 3.87176\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:                timestamp 1691660788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:               tmae_coord 1.16908\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:               tmae_force 0.38356\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:              tmape_coord 197181413440942.22\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:              tmape_force 373143445402241.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:              trmse_coord 0.51071\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:              trmse_force 0.2754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb:  View run FSR_Trainable_693db682 at: https://wandb.ai/seokjin/FSR-prediction/runs/693db682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114730)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184630-693db682/logs\n",
      "2023-08-10 18:46:37,622\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.322 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:46:37,626\tWARNING util.py:315 -- The `process_trial_result` operation took 2.328 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:46:37,628\tWARNING util.py:315 -- Processing trial results took 2.329 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:46:37,629\tWARNING util.py:315 -- The `process_trial_result` operation took 2.331 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_fec44e75_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-46-22/wandb/run-20230810_184640-fec44e75\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: Syncing run FSR_Trainable_fec44e75\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fec44e75\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 18:46:47,615\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.797 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:46:47,618\tWARNING util.py:315 -- The `process_trial_result` operation took 1.801 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:46:47,620\tWARNING util.py:315 -- Processing trial results took 1.803 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:46:47,621\tWARNING util.py:315 -- The `process_trial_result` operation took 1.804 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:                mae_coord 5.68008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:                mae_force 826.18479\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:               mape_coord 1.38005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:               mape_force 6.553615183994228e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:                   metric 0.70222\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:               rmse_coord 2.59427\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:               rmse_force 638.18154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:       time_since_restore 7.03274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:         time_this_iter_s 1.51997\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:             time_total_s 7.03274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:                timestamp 1691660802\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:               tmae_coord 1.13587\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:               tmae_force 0.31096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:              tmape_coord 191393084446307.66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:              tmape_force 275029023861697.78\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:              trmse_coord 0.48197\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:              trmse_force 0.22024\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb:  View run FSR_Trainable_fec44e75 at: https://wandb.ai/seokjin/FSR-prediction/runs/fec44e75\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114947)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184640-fec44e75/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_66fede0d_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-46-33/wandb/run-20230810_184651-66fede0d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb: Syncing run FSR_Trainable_66fede0d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/66fede0d\n",
      "2023-08-10 18:47:00,398\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.815 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:47:00,400\tWARNING util.py:315 -- The `process_trial_result` operation took 1.818 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:47:00,403\tWARNING util.py:315 -- Processing trial results took 1.821 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:47:00,405\tWARNING util.py:315 -- The `process_trial_result` operation took 1.823 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_9d7f317c_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-46-44/wandb/run-20230810_184706-9d7f317c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb: Syncing run FSR_Trainable_9d7f317c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9d7f317c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:                mae_coord 4.55143\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:                mae_force 629.86096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:               mape_coord 1.18008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:               mape_force 5.328862479549449e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:                   metric 0.58891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:               rmse_coord 2.28783\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:               rmse_force 482.15478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:       time_since_restore 117.19988\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:         time_this_iter_s 1.19622\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:             time_total_s 117.19988\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:                timestamp 1691660845\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:               tmae_coord 0.9128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:               tmae_force 0.23733\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:              tmape_coord 176845950519289.03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:              tmape_force 219189017325514.72\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:              trmse_coord 0.42307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:              trmse_force 0.16584\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb:  View run FSR_Trainable_b62849af at: https://wandb.ai/seokjin/FSR-prediction/runs/b62849af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=113362)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184513-b62849af/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 18:47:39,952\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.723 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:47:39,955\tWARNING util.py:315 -- The `process_trial_result` operation took 1.727 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:47:39,956\tWARNING util.py:315 -- Processing trial results took 1.728 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:47:39,957\tWARNING util.py:315 -- The `process_trial_result` operation took 1.730 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:                mae_coord 5.52182\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:                mae_force 782.43023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:               mape_coord 1.3226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:               mape_force 7.497350963869071e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:                   metric 0.65038\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:               rmse_coord 2.50829\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:               rmse_force 568.04277\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:       time_since_restore 35.89335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:         time_this_iter_s 1.1971\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:             time_total_s 35.89335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:                timestamp 1691660856\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:               tmae_coord 1.09766\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:               tmae_force 0.28787\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:              tmape_coord 179923908909764.3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:              tmape_force 299815489229816.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:              trmse_coord 0.46189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:              trmse_force 0.1885\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb:  View run FSR_Trainable_9d7f317c at: https://wandb.ai/seokjin/FSR-prediction/runs/9d7f317c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115398)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184706-9d7f317c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_657010b8_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-46-57/wandb/run-20230810_184743-657010b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: Syncing run FSR_Trainable_657010b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/657010b8\n",
      "2023-08-10 18:47:53,498\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.779 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:47:53,502\tWARNING util.py:315 -- The `process_trial_result` operation took 1.784 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:47:53,503\tWARNING util.py:315 -- Processing trial results took 1.785 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:47:53,504\tWARNING util.py:315 -- The `process_trial_result` operation took 1.786 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_b377cc00_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-47-36/wandb/run-20230810_184757-b377cc00\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb: Syncing run FSR_Trainable_b377cc00\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b377cc00\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:                mae_coord 27.89927\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:                mae_force 2092.86499\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:               mape_coord 4.58027\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:               mape_force 2.8784622187525473e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:                   metric 2.31324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:               rmse_coord 8.9604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:               rmse_force 1374.26401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:       time_since_restore 1.51409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:         time_this_iter_s 1.51409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:             time_total_s 1.51409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:                timestamp 1691660871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:               tmae_coord 5.88901\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:               tmae_force 0.8481\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:              tmape_coord 54348876880627.22\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:              tmape_force 1380309432148300.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:              trmse_coord 1.84092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:              trmse_force 0.47232\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb:  View run FSR_Trainable_b377cc00 at: https://wandb.ai/seokjin/FSR-prediction/runs/b377cc00\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115877)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184757-b377cc00/logs\n",
      "2023-08-10 18:48:10,355\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.797 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:48:10,359\tWARNING util.py:315 -- The `process_trial_result` operation took 1.802 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:48:10,361\tWARNING util.py:315 -- Processing trial results took 1.803 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:48:10,363\tWARNING util.py:315 -- The `process_trial_result` operation took 1.806 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_870a8bf6_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-47-50/wandb/run-20230810_184811-870a8bf6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: Syncing run FSR_Trainable_870a8bf6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/870a8bf6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:                mae_coord 25.68541\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:                mae_force 1800.35263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:               mape_coord 4.44438\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:               mape_force 1.3697129792521477e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:                   metric 2.13764\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:               rmse_coord 8.81671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:               rmse_force 1389.91317\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:       time_since_restore 3.4122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:         time_this_iter_s 3.4122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:             time_total_s 3.4122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:                timestamp 1691660888\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:               tmae_coord 5.25092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:               tmae_force 0.74208\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:              tmape_coord 86775597866444.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:              tmape_force 938370343573848.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:              trmse_coord 1.6845\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:              trmse_force 0.45313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb:  View run FSR_Trainable_870a8bf6 at: https://wandb.ai/seokjin/FSR-prediction/runs/870a8bf6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116106)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184811-870a8bf6/logs\n",
      "2023-08-10 18:48:25,549\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.619 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:48:25,552\tWARNING util.py:315 -- The `process_trial_result` operation took 1.623 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:48:25,554\tWARNING util.py:315 -- Processing trial results took 1.625 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:48:25,558\tWARNING util.py:315 -- The `process_trial_result` operation took 1.628 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_d7953de1_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-48-05/wandb/run-20230810_184828-d7953de1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb: Syncing run FSR_Trainable_d7953de1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d7953de1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:                mae_coord 5.56965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:                mae_force 1164.70632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:               mape_coord 1.40265\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:               mape_force 1.2281176804136876e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:                   metric 0.7954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:               rmse_coord 2.63325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:               rmse_force 906.86635\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:       time_since_restore 3.82362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:         time_this_iter_s 1.82466\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:             time_total_s 3.82362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:                timestamp 1691660907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:               tmae_coord 1.11507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:               tmae_force 0.42317\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:              tmape_coord 188422217152557.16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:              tmape_force 486735575912945.06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:              trmse_coord 0.49851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:              trmse_force 0.29689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb:  View run FSR_Trainable_d7953de1 at: https://wandb.ai/seokjin/FSR-prediction/runs/d7953de1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116335)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184828-d7953de1/logs\n",
      "2023-08-10 18:48:43,581\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.969 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:48:43,584\tWARNING util.py:315 -- The `process_trial_result` operation took 1.972 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:48:43,586\tWARNING util.py:315 -- Processing trial results took 1.974 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:48:43,589\tWARNING util.py:315 -- The `process_trial_result` operation took 1.977 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_6656c7f0_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-48-22/wandb/run-20230810_184845-6656c7f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: Syncing run FSR_Trainable_6656c7f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/6656c7f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:                mae_coord 5.40767\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:                mae_force 1037.48948\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:               mape_coord 1.40304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:               mape_force 7.745252470729156e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:                   metric 0.7557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:               rmse_coord 2.54828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:               rmse_force 881.99858\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:       time_since_restore 4.9039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:         time_this_iter_s 2.26071\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:             time_total_s 4.9039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:                timestamp 1691660925\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:               tmae_coord 1.08098\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:               tmae_force 0.36802\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:              tmape_coord 196400177744552.06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:              tmape_force 298185637182216.9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:              trmse_coord 0.47796\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:              trmse_force 0.27774\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb:  View run FSR_Trainable_6656c7f0 at: https://wandb.ai/seokjin/FSR-prediction/runs/6656c7f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116565)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184845-6656c7f0/logs\n",
      "2023-08-10 18:49:01,277\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.627 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:49:01,280\tWARNING util.py:315 -- The `process_trial_result` operation took 1.631 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:49:01,281\tWARNING util.py:315 -- Processing trial results took 1.632 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:49:01,282\tWARNING util.py:315 -- The `process_trial_result` operation took 1.633 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_a449dfdd_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-48-39/wandb/run-20230810_184904-a449dfdd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb: Syncing run FSR_Trainable_a449dfdd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a449dfdd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:                mae_coord 4.89171\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:                mae_force 734.19497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:               mape_coord 1.20941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:               mape_force 7.293528878855702e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:                   metric 0.61793\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:               rmse_coord 2.35138\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:               rmse_force 547.74575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:       time_since_restore 121.26183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:         time_this_iter_s 1.38092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:             time_total_s 121.26183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:                timestamp 1691660942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:               tmae_coord 0.97518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:               tmae_force 0.27055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:              tmape_coord 160360685073988.25\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:              tmape_force 277046067576227.78\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:              trmse_coord 0.43273\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:              trmse_force 0.1852\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb:  View run FSR_Trainable_66fede0d at: https://wandb.ai/seokjin/FSR-prediction/runs/66fede0d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115156)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184651-66fede0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:                mae_coord 4.59438\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:                mae_force 670.03239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:               mape_coord 1.21751\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:               mape_force 5.7440232119207315e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:                   metric 0.60384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:               rmse_coord 2.332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:               rmse_force 538.98023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:       time_since_restore 160.5124\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:         time_this_iter_s 1.44236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:             time_total_s 160.5124\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:                timestamp 1691660953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:               tmae_coord 0.92967\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:               tmae_force 0.24601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:              tmape_coord 163294203452002.44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:              tmape_force 222348656639378.25\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:              trmse_coord 0.42985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:              trmse_force 0.17399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb:  View run FSR_Trainable_eb10b985 at: https://wandb.ai/seokjin/FSR-prediction/runs/eb10b985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=114483)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184619-eb10b985/logs\n",
      "2023-08-10 18:49:18,889\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.889 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:49:18,891\tWARNING util.py:315 -- The `process_trial_result` operation took 1.892 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:49:18,893\tWARNING util.py:315 -- Processing trial results took 1.894 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:49:18,896\tWARNING util.py:315 -- The `process_trial_result` operation took 1.897 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_f5f499b0_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-48-58/wandb/run-20230810_184922-f5f499b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: Syncing run FSR_Trainable_f5f499b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f5f499b0\n",
      "2023-08-10 18:49:31,699\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.521 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:49:31,702\tWARNING util.py:315 -- The `process_trial_result` operation took 1.524 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:49:31,705\tWARNING util.py:315 -- Processing trial results took 1.527 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:49:31,709\tWARNING util.py:315 -- The `process_trial_result` operation took 1.531 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_7989f1e3_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-49-15/wandb/run-20230810_184934-7989f1e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: Syncing run FSR_Trainable_7989f1e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7989f1e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:                mae_coord 4.68868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:                mae_force 643.37129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:               mape_coord 1.17556\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:               mape_force 5.982646888022163e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:                   metric 0.5863\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:               rmse_coord 2.28752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:               rmse_force 482.20031\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:       time_since_restore 121.03003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:         time_this_iter_s 1.11546\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:             time_total_s 121.03003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:                timestamp 1691660994\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:               tmae_coord 0.94025\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:               tmae_force 0.24578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:              tmape_coord 176436192474456.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:              tmape_force 254674016792480.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:              trmse_coord 0.42166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:              trmse_force 0.16464\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb:  View run FSR_Trainable_657010b8 at: https://wandb.ai/seokjin/FSR-prediction/runs/657010b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=115650)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184743-657010b8/logs\n",
      "2023-08-10 18:50:09,157\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.735 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:50:09,160\tWARNING util.py:315 -- The `process_trial_result` operation took 1.738 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:50:09,163\tWARNING util.py:315 -- Processing trial results took 1.741 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:50:09,167\tWARNING util.py:315 -- The `process_trial_result` operation took 1.745 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_0ec2df67_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-49-28/wandb/run-20230810_185012-0ec2df67\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: Syncing run FSR_Trainable_0ec2df67\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0ec2df67\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb: \\ 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:                mae_coord 4.68602\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:                mae_force 617.82766\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:               mape_coord 1.11779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:               mape_force 5.279436254018333e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:                   metric 0.58108\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:               rmse_coord 2.27891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:               rmse_force 480.84415\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:       time_since_restore 116.48891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:         time_this_iter_s 1.09923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:             time_total_s 116.48891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:                timestamp 1691661065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:               tmae_coord 0.94103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:               tmae_force 0.2305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:              tmape_coord 146246833007481.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:              tmape_force 223143553549052.66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:              trmse_coord 0.41968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:              trmse_force 0.1614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb:  View run FSR_Trainable_a449dfdd at: https://wandb.ai/seokjin/FSR-prediction/runs/a449dfdd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=116796)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184904-a449dfdd/logs\n",
      "2023-08-10 18:51:20,309\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.854 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:51:20,313\tWARNING util.py:315 -- The `process_trial_result` operation took 1.858 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:51:20,314\tWARNING util.py:315 -- Processing trial results took 1.859 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:51:20,315\tWARNING util.py:315 -- The `process_trial_result` operation took 1.861 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_fb7b995d_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-50-05/wandb/run-20230810_185123-fb7b995d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: Syncing run FSR_Trainable_fb7b995d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fb7b995d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: | 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:                mae_coord 4.6793\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:                mae_force 662.79545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:               mape_coord 1.18781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:               mape_force 6.369308282744609e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:                   metric 0.59423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:               rmse_coord 2.32315\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:               rmse_force 503.3702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:       time_since_restore 117.99617\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:         time_this_iter_s 1.23369\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:             time_total_s 117.99617\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:                timestamp 1691661084\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:               tmae_coord 0.93664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:               tmae_force 0.24342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:              tmape_coord 165243998910679.56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:              tmape_force 249721281831201.56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:              trmse_coord 0.42788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:              trmse_force 0.16636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb:  View run FSR_Trainable_f5f499b0 at: https://wandb.ai/seokjin/FSR-prediction/runs/f5f499b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117033)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184922-f5f499b0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: | 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:                mae_coord 4.47819\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:                mae_force 634.6765\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:               mape_coord 1.1536\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:               mape_force 6.254213126579826e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:                   metric 0.56812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:               rmse_coord 2.20759\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:               rmse_force 476.07071\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:       time_since_restore 115.23578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:         time_this_iter_s 1.12546\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:             time_total_s 115.23578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:                timestamp 1691661093\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:               tmae_coord 0.90305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:               tmae_force 0.23724\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:              tmape_coord 165641038382367.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:              tmape_force 258330526150008.53\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:              trmse_coord 0.40895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:              trmse_force 0.15917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb:  View run FSR_Trainable_7989f1e3 at: https://wandb.ai/seokjin/FSR-prediction/runs/7989f1e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117254)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_184934-7989f1e3/logs\n",
      "2023-08-10 18:51:39,007\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.775 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:51:39,011\tWARNING util.py:315 -- The `process_trial_result` operation took 1.780 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:51:39,013\tWARNING util.py:315 -- Processing trial results took 1.782 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:51:39,016\tWARNING util.py:315 -- The `process_trial_result` operation took 1.785 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_5953e60a_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-51-16/wandb/run-20230810_185142-5953e60a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: Syncing run FSR_Trainable_5953e60a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5953e60a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:                mae_coord 5.88242\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:                mae_force 768.97837\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:               mape_coord 1.40881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:               mape_force 5.810727522898254e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:                   metric 0.69277\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:               rmse_coord 2.62093\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:               rmse_force 595.90136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:       time_since_restore 5.42616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:         time_this_iter_s 1.131\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:             time_total_s 5.42616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:                timestamp 1691661102\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:               tmae_coord 1.16182\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:               tmae_force 0.29551\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:              tmape_coord 168850851182707.47\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:              tmape_force 276442806763622.78\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:              trmse_coord 0.48746\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:              trmse_force 0.20531\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb:  View run FSR_Trainable_5953e60a at: https://wandb.ai/seokjin/FSR-prediction/runs/5953e60a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118024)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185142-5953e60a/logs\n",
      "2023-08-10 18:51:55,435\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.931 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:51:55,441\tWARNING util.py:315 -- The `process_trial_result` operation took 1.939 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:51:55,444\tWARNING util.py:315 -- Processing trial results took 1.941 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:51:55,445\tWARNING util.py:315 -- The `process_trial_result` operation took 1.943 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_bf9fd07a_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-51-35/wandb/run-20230810_185155-bf9fd07a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb: Syncing run FSR_Trainable_bf9fd07a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/bf9fd07a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:                mae_coord 5.35183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:                mae_force 1204.44693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:               mape_coord 1.40384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:               mape_force 8.649063066004531e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:                   metric 0.79666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:               rmse_coord 2.5438\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:               rmse_force 1027.1483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:       time_since_restore 9.75011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:         time_this_iter_s 4.50392\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:             time_total_s 9.75011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:                timestamp 1691661119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:               tmae_coord 1.05451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:               tmae_force 0.42874\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:              tmape_coord 169740361626301.75\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:              tmape_force 348826537106133.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:              trmse_coord 0.4747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:              trmse_force 0.32197\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb:  View run FSR_Trainable_bf9fd07a at: https://wandb.ai/seokjin/FSR-prediction/runs/bf9fd07a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118257)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185155-bf9fd07a/logs\n",
      "2023-08-10 18:52:06,873\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.970 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:52:06,876\tWARNING util.py:315 -- The `process_trial_result` operation took 1.974 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:52:06,878\tWARNING util.py:315 -- Processing trial results took 1.975 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:52:06,879\tWARNING util.py:315 -- The `process_trial_result` operation took 1.976 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_e580fe88_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-51-48/wandb/run-20230810_185206-e580fe88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: Syncing run FSR_Trainable_e580fe88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e580fe88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 18:52:20,195\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.137 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:52:20,199\tWARNING util.py:315 -- The `process_trial_result` operation took 2.142 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:52:20,200\tWARNING util.py:315 -- Processing trial results took 2.143 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:52:20,202\tWARNING util.py:315 -- The `process_trial_result` operation took 2.145 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_e4b72c0b_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-51-59/wandb/run-20230810_185220-e4b72c0b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb: Syncing run FSR_Trainable_e4b72c0b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e4b72c0b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:                mae_coord 4.67605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:                mae_force 625.82877\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:               mape_coord 1.23521\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:               mape_force 5.296333862476806e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:                   metric 0.58196\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:               rmse_coord 2.273\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:               rmse_force 478.62165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:       time_since_restore 118.47313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:         time_this_iter_s 1.39348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:             time_total_s 118.47313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:                timestamp 1691661136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:               tmae_coord 0.93838\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:               tmae_force 0.23938\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:              tmape_coord 175405416565473.7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:              tmape_force 240774743938817.22\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:              trmse_coord 0.41831\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:              trmse_force 0.16365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb:  View run FSR_Trainable_0ec2df67 at: https://wandb.ai/seokjin/FSR-prediction/runs/0ec2df67\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117503)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185012-0ec2df67/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185206-e580fe88/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118468)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:                mae_coord 5.91777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:                mae_force 1065.60501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:               mape_coord 1.46395\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:               mape_force 1.0113380470830397e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:                   metric 0.77988\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:               rmse_coord 2.69824\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:               rmse_force 846.2932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:       time_since_restore 10.36664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:         time_this_iter_s 4.49192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:             time_total_s 10.36664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:                timestamp 1691661144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:               tmae_coord 1.16931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:               tmae_force 0.37925\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:              tmape_coord 170014437749876.53\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:              tmape_force 374523907252611.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:              trmse_coord 0.50718\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:              trmse_force 0.2727\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb:  View run FSR_Trainable_e4b72c0b at: https://wandb.ai/seokjin/FSR-prediction/runs/e4b72c0b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185220-e4b72c0b/logs\n",
      "2023-08-10 18:52:32,449\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.930 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:52:32,453\tWARNING util.py:315 -- The `process_trial_result` operation took 1.934 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:52:32,455\tWARNING util.py:315 -- Processing trial results took 1.936 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:52:32,456\tWARNING util.py:315 -- The `process_trial_result` operation took 1.937 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118696)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_81679c0e_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-52-12/wandb/run-20230810_185234-81679c0e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb: Syncing run FSR_Trainable_81679c0e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/81679c0e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:                mae_coord 6.03805\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:                mae_force 1335.07603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:               mape_coord 1.42001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:               mape_force 1.3583388948118106e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:                   metric 0.86283\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:               rmse_coord 2.76294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:               rmse_force 1055.51502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:       time_since_restore 1.90726\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:         time_this_iter_s 1.90726\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:             time_total_s 1.90726\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:                timestamp 1691661150\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:               tmae_coord 1.1999\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:               tmae_force 0.467\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:              tmape_coord 167825290212664.06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:              tmape_force 466868231246065.3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:              trmse_coord 0.52382\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:              trmse_force 0.33902\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb:  View run FSR_Trainable_81679c0e at: https://wandb.ai/seokjin/FSR-prediction/runs/81679c0e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185234-81679c0e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=118942)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-08-10 18:52:40,629\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.707 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:52:40,633\tWARNING util.py:315 -- The `process_trial_result` operation took 1.711 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:52:40,634\tWARNING util.py:315 -- Processing trial results took 1.713 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:52:40,635\tWARNING util.py:315 -- The `process_trial_result` operation took 1.714 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_5506c20c_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-52-28/wandb/run-20230810_185243-5506c20c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb: Syncing run FSR_Trainable_5506c20c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5506c20c\n",
      "2023-08-10 18:52:52,351\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.623 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:52:52,353\tWARNING util.py:315 -- The `process_trial_result` operation took 1.625 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:52:52,356\tWARNING util.py:315 -- Processing trial results took 1.628 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:52:52,358\tWARNING util.py:315 -- The `process_trial_result` operation took 1.630 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_3cf0dc98_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-52-37/wandb/run-20230810_185255-3cf0dc98\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb: Syncing run FSR_Trainable_3cf0dc98\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/3cf0dc98\n",
      "2023-08-10 18:53:05,777\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.085 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:53:05,782\tWARNING util.py:315 -- The `process_trial_result` operation took 2.091 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:53:05,784\tWARNING util.py:315 -- Processing trial results took 2.093 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:53:05,786\tWARNING util.py:315 -- The `process_trial_result` operation took 2.096 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_ef468e9e_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-52-48/wandb/run-20230810_185309-ef468e9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb: Syncing run FSR_Trainable_ef468e9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ef468e9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:                mae_coord 5.82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:                mae_force 811.92295\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:               mape_coord 1.36723\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:               mape_force 6.501627312417868e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:                   metric 0.69435\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:               rmse_coord 2.60777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:               rmse_force 630.21467\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:       time_since_restore 5.69491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:         time_this_iter_s 1.12108\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:             time_total_s 5.69491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:                timestamp 1691661189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:               tmae_coord 1.15678\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:               tmae_force 0.30425\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:              tmape_coord 169232289304903.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:              tmape_force 288248804161841.56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:              trmse_coord 0.48439\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:              trmse_force 0.20996\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb:  View run FSR_Trainable_ef468e9e at: https://wandb.ai/seokjin/FSR-prediction/runs/ef468e9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119596)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185309-ef468e9e/logs\n",
      "2023-08-10 18:53:25,009\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.780 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:53:25,011\tWARNING util.py:315 -- The `process_trial_result` operation took 1.784 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:53:25,014\tWARNING util.py:315 -- Processing trial results took 1.787 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:53:25,017\tWARNING util.py:315 -- The `process_trial_result` operation took 1.790 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_8d7cf828_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-53-02/wandb/run-20230810_185328-8d7cf828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb: Syncing run FSR_Trainable_8d7cf828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8d7cf828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb: \\ 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:                mae_coord 5.89856\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:                mae_force 815.9287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:               mape_coord 1.38421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:               mape_force 7.522391138059643e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:                   metric 0.69325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:               rmse_coord 2.62158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:               rmse_force 621.01033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:       time_since_restore 5.38939\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:         time_this_iter_s 1.10128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:             time_total_s 5.38939\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:                timestamp 1691661208\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:               tmae_coord 1.16683\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:               tmae_force 0.3061\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:              tmape_coord 167293546219470.62\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:              tmape_force 330928690828463.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:              trmse_coord 0.48649\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:              trmse_force 0.20676\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb:  View run FSR_Trainable_8d7cf828 at: https://wandb.ai/seokjin/FSR-prediction/runs/8d7cf828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119829)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185328-8d7cf828/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:                mae_coord 4.37127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:                mae_force 621.83699\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:               mape_coord 1.08328\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:               mape_force 5.614363572880279e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:                   metric 0.56478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:               rmse_coord 2.20942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:               rmse_force 471.12237\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:       time_since_restore 118.74034\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:         time_this_iter_s 1.09135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:             time_total_s 118.74034\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:                timestamp 1691661216\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:               tmae_coord 0.88578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:               tmae_force 0.22937\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:              tmape_coord 153558448638436.47\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:              tmape_force 222978992669499.34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:              trmse_coord 0.40942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:              trmse_force 0.15537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb:  View run FSR_Trainable_fb7b995d at: https://wandb.ai/seokjin/FSR-prediction/runs/fb7b995d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=117783)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185123-fb7b995d/logs\n",
      "2023-08-10 18:53:44,504\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.323 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:53:44,508\tWARNING util.py:315 -- The `process_trial_result` operation took 2.328 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:53:44,510\tWARNING util.py:315 -- Processing trial results took 2.330 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:53:44,512\tWARNING util.py:315 -- The `process_trial_result` operation took 2.332 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_b7ffc6c7_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-53-21/wandb/run-20230810_185347-b7ffc6c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Syncing run FSR_Trainable_b7ffc6c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b7ffc6c7\n",
      "2023-08-10 18:53:55,517\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.672 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:53:55,522\tWARNING util.py:315 -- The `process_trial_result` operation took 1.678 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:53:55,523\tWARNING util.py:315 -- Processing trial results took 1.679 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:53:55,525\tWARNING util.py:315 -- The `process_trial_result` operation took 1.681 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_b1c1eb81_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-53-40/wandb/run-20230810_185358-b1c1eb81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: Syncing run FSR_Trainable_b1c1eb81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b1c1eb81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:                mae_coord 4.68875\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:                mae_force 1069.84007\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:               mape_coord 1.17187\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:               mape_force 1.176575262315594e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:                   metric 22.75828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:               rmse_coord 2.45553\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:               rmse_force 840.11519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:       time_since_restore 1.60067\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:         time_this_iter_s 1.60067\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:             time_total_s 1.60067\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:                timestamp 1691661233\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:               tmae_coord 15.77132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:               tmae_force 9.69353\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:              tmape_coord 8019104871982045.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:              tmape_force 6174984601169458.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:              trmse_coord 11.14106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:              trmse_force 11.61722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb:  View run FSR_Trainable_b1c1eb81 at: https://wandb.ai/seokjin/FSR-prediction/runs/b1c1eb81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120287)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185358-b1c1eb81/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb:              trmse_force \n",
      "2023-08-10 18:54:13,362\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.140 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:54:13,369\tWARNING util.py:315 -- The `process_trial_result` operation took 2.148 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:54:13,372\tWARNING util.py:315 -- Processing trial results took 2.151 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:54:13,375\tWARNING util.py:315 -- The `process_trial_result` operation took 2.154 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_82eb35d9_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-53-52/wandb/run-20230810_185414-82eb35d9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb: Syncing run FSR_Trainable_82eb35d9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/82eb35d9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:                mae_coord 4.63565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:                mae_force 1517.8134\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:               mape_coord 1.27311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:               mape_force 3.419725359718593e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:                   metric 24.25841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:               rmse_coord 2.47067\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:               rmse_force 1113.90269\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:       time_since_restore 3.15019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:         time_this_iter_s 3.15019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:             time_total_s 3.15019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:                timestamp 1691661251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:               tmae_coord 15.97922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:               tmae_force 13.00813\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:              tmape_coord 5790182091903932.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:              tmape_force 1.3924541293149068e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:              trmse_coord 11.45267\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:              trmse_force 12.80574\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb:  View run FSR_Trainable_82eb35d9 at: https://wandb.ai/seokjin/FSR-prediction/runs/82eb35d9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185414-82eb35d9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120522)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-08-10 18:54:22,018\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.799 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:54:22,021\tWARNING util.py:315 -- The `process_trial_result` operation took 1.803 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:54:22,023\tWARNING util.py:315 -- Processing trial results took 1.806 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:54:22,025\tWARNING util.py:315 -- The `process_trial_result` operation took 1.807 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_d959ef0c_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-54-08/wandb/run-20230810_185425-d959ef0c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: Syncing run FSR_Trainable_d959ef0c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d959ef0c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:                mae_coord 4.34738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:                mae_force 1416.90904\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:               mape_coord 1.26722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:               mape_force 4592541198.30258\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:                   metric 5.72448\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:               rmse_coord 2.38934\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:               rmse_force 1002.46628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:       time_since_restore 1.73978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:         time_this_iter_s 1.73978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:             time_total_s 1.73978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:                timestamp 1691661260\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:               tmae_coord 6.7913\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:               tmae_force 3.62414\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:              tmape_coord 22.7736\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:              tmape_force 8.47181\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:              trmse_coord 3.43793\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:              trmse_force 2.28656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb:  View run FSR_Trainable_d959ef0c at: https://wandb.ai/seokjin/FSR-prediction/runs/d959ef0c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120745)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185425-d959ef0c/logs\n",
      "2023-08-10 18:54:35,193\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.117 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:54:35,195\tWARNING util.py:315 -- The `process_trial_result` operation took 2.121 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:54:35,198\tWARNING util.py:315 -- Processing trial results took 2.123 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:54:35,201\tWARNING util.py:315 -- The `process_trial_result` operation took 2.127 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_6609d819_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-54-18/wandb/run-20230810_185437-6609d819\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: Syncing run FSR_Trainable_6609d819\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/6609d819\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:                mae_coord 4.36401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:                mae_force 1294.66052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:               mape_coord 1.22416\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:               mape_force 4016998705.23894\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:                   metric 5.57537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:               rmse_coord 2.40207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:               rmse_force 966.5519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:       time_since_restore 2.43223\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:         time_this_iter_s 2.43223\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:             time_total_s 2.43223\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:                timestamp 1691661273\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:               tmae_coord 6.80653\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:               tmae_force 3.23051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:              tmape_coord 21.73074\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:              tmape_force 11.83346\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:              trmse_coord 3.43641\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:              trmse_force 2.13896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb:  View run FSR_Trainable_6609d819 at: https://wandb.ai/seokjin/FSR-prediction/runs/6609d819\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=120973)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185437-6609d819/logs\n",
      "2023-08-10 18:54:44,769\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.000 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:54:44,772\tWARNING util.py:315 -- The `process_trial_result` operation took 2.004 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:54:44,776\tWARNING util.py:315 -- Processing trial results took 2.008 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:54:44,780\tWARNING util.py:315 -- The `process_trial_result` operation took 2.012 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_67a7a972_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-54-30/wandb/run-20230810_185447-67a7a972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: Syncing run FSR_Trainable_67a7a972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/67a7a972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:                mae_coord 16.80081\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:                mae_force 2448.07929\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:               mape_coord 2.8514\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:               mape_force 4.612125819683817e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:                   metric 1.69376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:               rmse_coord 5.98027\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:               rmse_force 1406.39795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:       time_since_restore 1.43456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:         time_this_iter_s 1.43456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:             time_total_s 1.43456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:                timestamp 1691661282\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:               tmae_coord 3.47556\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:               tmae_force 1.01892\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:              tmape_coord 36542873954502.42\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:              tmape_force 2180728308051062.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:              trmse_coord 1.15249\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:              trmse_force 0.54127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb:  View run FSR_Trainable_67a7a972 at: https://wandb.ai/seokjin/FSR-prediction/runs/67a7a972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121197)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185447-67a7a972/logs\n",
      "2023-08-10 18:54:56,609\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.588 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:54:56,612\tWARNING util.py:315 -- The `process_trial_result` operation took 1.592 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:54:56,617\tWARNING util.py:315 -- Processing trial results took 1.596 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:54:56,619\tWARNING util.py:315 -- The `process_trial_result` operation took 1.598 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_71d3ee26_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-54-41/wandb/run-20230810_185459-71d3ee26\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb: Syncing run FSR_Trainable_71d3ee26\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/71d3ee26\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:                mae_coord 4.75362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:                mae_force 682.82419\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:               mape_coord 1.15386\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:               mape_force 7.303498610284517e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:                   metric 0.57984\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:               rmse_coord 2.26509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:               rmse_force 494.54103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:       time_since_restore 118.39658\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:         time_this_iter_s 1.39423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:             time_total_s 118.39658\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:                timestamp 1691661298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:               tmae_coord 0.95245\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:               tmae_force 0.25084\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:              tmape_coord 148336833846883.9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:              tmape_force 293792546550967.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:              trmse_coord 0.41829\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:              trmse_force 0.16155\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb:  View run FSR_Trainable_5506c20c at: https://wandb.ai/seokjin/FSR-prediction/runs/5506c20c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119162)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185243-5506c20c/logs\n",
      "2023-08-10 18:55:08,645\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.647 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:55:08,648\tWARNING util.py:315 -- The `process_trial_result` operation took 1.651 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:55:08,649\tWARNING util.py:315 -- Processing trial results took 1.652 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:55:08,651\tWARNING util.py:315 -- The `process_trial_result` operation took 1.654 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_412fc51c_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-54-53/wandb/run-20230810_185512-412fc51c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb: Syncing run FSR_Trainable_412fc51c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/412fc51c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb: \\ 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:                mae_coord 4.62477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:                mae_force 643.58899\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:               mape_coord 1.1292\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:               mape_force 5.836492301174633e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:                   metric 0.59305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:               rmse_coord 2.31985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:               rmse_force 494.1865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:       time_since_restore 118.49618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:         time_this_iter_s 1.36383\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:             time_total_s 118.49618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:                timestamp 1691661310\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:               tmae_coord 0.92742\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:               tmae_force 0.2395\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:              tmape_coord 148544031699312.22\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:              tmape_force 232419150702846.78\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:              trmse_coord 0.42577\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:              trmse_force 0.16728\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb:  View run FSR_Trainable_3cf0dc98 at: https://wandb.ai/seokjin/FSR-prediction/runs/3cf0dc98\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=119381)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185255-3cf0dc98/logs\n",
      "2023-08-10 18:55:21,477\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.838 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:55:21,479\tWARNING util.py:315 -- The `process_trial_result` operation took 1.840 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:55:21,483\tWARNING util.py:315 -- Processing trial results took 1.845 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:55:21,485\tWARNING util.py:315 -- The `process_trial_result` operation took 1.847 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_59e2c3b1_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-55-05/wandb/run-20230810_185524-59e2c3b1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb: Syncing run FSR_Trainable_59e2c3b1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/59e2c3b1\n",
      "2023-08-10 18:55:34,610\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.640 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:55:34,613\tWARNING util.py:315 -- The `process_trial_result` operation took 1.644 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:55:34,615\tWARNING util.py:315 -- Processing trial results took 1.645 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:55:34,616\tWARNING util.py:315 -- The `process_trial_result` operation took 1.647 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_97cbfcc7_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-55-18/wandb/run-20230810_185537-97cbfcc7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb: Syncing run FSR_Trainable_97cbfcc7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/97cbfcc7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb: - 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:                mae_coord 5.5251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:                mae_force 779.06587\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:               mape_coord 1.28024\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:               mape_force 8.48497154379353e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:                   metric 0.64322\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:               rmse_coord 2.50855\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:               rmse_force 557.2177\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:       time_since_restore 19.26816\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:         time_this_iter_s 1.18465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:             time_total_s 19.26816\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:                timestamp 1691661352\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:               tmae_coord 1.09193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:               tmae_force 0.2879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:              tmape_coord 161798601951588.75\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:              tmape_force 348774015543661.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:              trmse_coord 0.46005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:              trmse_force 0.18318\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb:  View run FSR_Trainable_97cbfcc7 at: https://wandb.ai/seokjin/FSR-prediction/runs/97cbfcc7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122095)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185537-97cbfcc7/logs\n",
      "2023-08-10 18:56:08,427\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.663 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:56:08,431\tWARNING util.py:315 -- The `process_trial_result` operation took 1.668 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:56:08,433\tWARNING util.py:315 -- Processing trial results took 1.669 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:56:08,434\tWARNING util.py:315 -- The `process_trial_result` operation took 1.671 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_63b28b94_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-55-31/wandb/run-20230810_185611-63b28b94\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb: Syncing run FSR_Trainable_63b28b94\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/63b28b94\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:                mae_coord 5.06038\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:                mae_force 687.99106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:               mape_coord 1.23704\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:               mape_force 6.486040584203555e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:                   metric 0.61472\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:               rmse_coord 2.37436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:               rmse_force 517.75723\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:       time_since_restore 77.21727\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:         time_this_iter_s 1.1426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:             time_total_s 77.21727\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:                timestamp 1691661392\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:               tmae_coord 1.01021\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:               tmae_force 0.25902\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:              tmape_coord 173387047158931.16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:              tmape_force 277294696640458.72\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:              trmse_coord 0.43779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:              trmse_force 0.17693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb:  View run FSR_Trainable_412fc51c at: https://wandb.ai/seokjin/FSR-prediction/runs/412fc51c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121655)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185512-412fc51c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:                mae_coord 5.18156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:                mae_force 707.12193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:               mape_coord 1.24172\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:               mape_force 7.435537835836913e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:                   metric 0.62344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:               rmse_coord 2.44012\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:               rmse_force 521.26442\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:       time_since_restore 75.99145\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:         time_this_iter_s 1.09067\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:             time_total_s 75.99145\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:                timestamp 1691661402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:               tmae_coord 1.02907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:               tmae_force 0.26604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:              tmape_coord 151418722398646.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:              tmape_force 314131765607838.75\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:              trmse_coord 0.44951\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:              trmse_force 0.17393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb:  View run FSR_Trainable_59e2c3b1 at: https://wandb.ai/seokjin/FSR-prediction/runs/59e2c3b1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121880)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185524-59e2c3b1/logs\n",
      "2023-08-10 18:56:48,337\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.742 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:56:48,340\tWARNING util.py:315 -- The `process_trial_result` operation took 1.746 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:56:48,341\tWARNING util.py:315 -- Processing trial results took 1.747 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:56:48,342\tWARNING util.py:315 -- The `process_trial_result` operation took 1.749 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_197e3b7f_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-56-05/wandb/run-20230810_185651-197e3b7f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb: Syncing run FSR_Trainable_197e3b7f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/197e3b7f\n",
      "2023-08-10 18:57:02,904\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.889 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:57:02,908\tWARNING util.py:315 -- The `process_trial_result` operation took 1.895 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:57:02,910\tWARNING util.py:315 -- Processing trial results took 1.896 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:57:02,911\tWARNING util.py:315 -- The `process_trial_result` operation took 1.898 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_23e8d890_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-56-45/wandb/run-20230810_185705-23e8d890\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb: Syncing run FSR_Trainable_23e8d890\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/23e8d890\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)04 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:                mae_coord 6.07405\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:                mae_force 1340.56234\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:               mape_coord 1.46161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:               mape_force 1.28733529131761e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:                   metric 0.86716\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:               rmse_coord 2.75176\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:               rmse_force 1076.28706\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:       time_since_restore 2.70199\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:         time_this_iter_s 2.70199\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:             time_total_s 2.70199\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:                timestamp 1691661421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:               tmae_coord 1.21523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:               tmae_force 0.46388\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:              tmape_coord 172741459376080.47\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:              tmape_force 424610788677685.56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:              trmse_coord 0.52477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:              trmse_force 0.34239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb:  View run FSR_Trainable_23e8d890 at: https://wandb.ai/seokjin/FSR-prediction/runs/23e8d890\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122818)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185705-23e8d890/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb: Waiting for W&B process to finish... (success).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=121423)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2023-08-10 18:57:18,224\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.051 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:57:18,228\tWARNING util.py:315 -- The `process_trial_result` operation took 2.056 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:57:18,230\tWARNING util.py:315 -- Processing trial results took 2.058 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:57:18,232\tWARNING util.py:315 -- The `process_trial_result` operation took 2.059 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb: iterations_since_restore 16\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:                mae_coord 5.58287\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:                mae_force 783.03971\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:               mape_coord 1.29073\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:               mape_force 9.038929322502031e+17\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:                   metric 0.64416\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:               rmse_coord 2.51696\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:               rmse_force 550.80428\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:       time_since_restore 20.92774\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:         time_this_iter_s 0.94461\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:             time_total_s 20.92774\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:                timestamp 1691661429\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:               tmae_coord 1.10638\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:               tmae_force 0.29162\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:              tmape_coord 162077696400005.7\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:              tmape_force 376532926752057.56\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:       training_iteration 16\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:              trmse_coord 0.46271\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:              trmse_force 0.18145\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb:  View run FSR_Trainable_197e3b7f at: https://wandb.ai/seokjin/FSR-prediction/runs/197e3b7f\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185651-197e3b7f/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122598)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_0cd88631_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-56-58/wandb/run-20230810_185720-0cd88631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Syncing run FSR_Trainable_0cd88631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0cd88631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123066)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185720-0cd88631/logs\n",
      "2023-08-10 18:57:27,327\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.039 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:57:27,333\tWARNING util.py:315 -- The `process_trial_result` operation took 2.045 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:57:27,336\tWARNING util.py:315 -- Processing trial results took 2.049 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:57:27,338\tWARNING util.py:315 -- The `process_trial_result` operation took 2.051 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_c3536b16_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-57-13/wandb/run-20230810_185729-c3536b16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb: Syncing run FSR_Trainable_c3536b16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c3536b16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:                mae_coord 5.49133\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:                mae_force 1036.31755\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:               mape_coord 1.42058\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:               mape_force 9.613718078424302e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:                   metric 0.74854\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:               rmse_coord 2.55941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:               rmse_force 813.23467\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:       time_since_restore 3.76213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:         time_this_iter_s 1.70033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:             time_total_s 3.76213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:                timestamp 1691661449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:               tmae_coord 1.09664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:               tmae_force 0.37519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:              tmape_coord 195076390057936.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:              tmape_force 370835801376063.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:              trmse_coord 0.48168\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:              trmse_force 0.26686\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb:  View run FSR_Trainable_c3536b16 at: https://wandb.ai/seokjin/FSR-prediction/runs/c3536b16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123288)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185729-c3536b16/logs\n",
      "2023-08-10 18:57:35,997\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.782 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:57:36,000\tWARNING util.py:315 -- The `process_trial_result` operation took 1.786 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:57:36,002\tWARNING util.py:315 -- Processing trial results took 1.788 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:57:36,004\tWARNING util.py:315 -- The `process_trial_result` operation took 1.790 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_ebcf36c8_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-57-23/wandb/run-20230810_185738-ebcf36c8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Syncing run FSR_Trainable_ebcf36c8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ebcf36c8\n",
      "2023-08-10 18:57:47,476\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.634 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:57:47,479\tWARNING util.py:315 -- The `process_trial_result` operation took 1.637 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:57:47,480\tWARNING util.py:315 -- Processing trial results took 1.638 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:57:47,481\tWARNING util.py:315 -- The `process_trial_result` operation took 1.639 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_a8768c06_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-57-32/wandb/run-20230810_185751-a8768c06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb: Syncing run FSR_Trainable_a8768c06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a8768c06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:                mae_coord 5.64044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:                mae_force 786.44963\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:               mape_coord 1.33896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:               mape_force 7.644253585806555e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:                   metric 0.65981\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:               rmse_coord 2.54259\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:               rmse_force 567.58588\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:       time_since_restore 10.46243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:         time_this_iter_s 1.14276\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:             time_total_s 10.46243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:                timestamp 1691661476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:               tmae_coord 1.11878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:               tmae_force 0.29519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:              tmape_coord 186756843315572.7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:              tmape_force 336897744178774.44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:              trmse_coord 0.47045\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:              trmse_force 0.18935\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb:  View run FSR_Trainable_a8768c06 at: https://wandb.ai/seokjin/FSR-prediction/runs/a8768c06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123732)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185751-a8768c06/logs\n",
      "2023-08-10 18:58:01,858\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.008 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:58:01,865\tWARNING util.py:315 -- The `process_trial_result` operation took 2.016 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:58:01,867\tWARNING util.py:315 -- Processing trial results took 2.017 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:58:01,868\tWARNING util.py:315 -- The `process_trial_result` operation took 2.019 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_661f2b44_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-57-44/wandb/run-20230810_185804-661f2b44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: Syncing run FSR_Trainable_661f2b44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/661f2b44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:                mae_coord 5.92465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:                mae_force 1267.52318\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:               mape_coord 1.44574\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:               mape_force 1.240486131336399e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:                   metric 0.83928\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:               rmse_coord 2.72655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:               rmse_force 1017.05317\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:       time_since_restore 2.05512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:         time_this_iter_s 2.05512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:             time_total_s 2.05512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:                timestamp 1691661479\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:               tmae_coord 1.20225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:               tmae_force 0.43689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:              tmape_coord 194554237931513.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:              tmape_force 415629541095644.3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:              trmse_coord 0.51942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:              trmse_force 0.31986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb:  View run FSR_Trainable_661f2b44 at: https://wandb.ai/seokjin/FSR-prediction/runs/661f2b44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123950)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185804-661f2b44/logs\n",
      "2023-08-10 18:58:13,607\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.512 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:58:13,609\tWARNING util.py:315 -- The `process_trial_result` operation took 1.514 s, which may be a performance bottleneck.\n",
      "2023-08-10 18:58:13,612\tWARNING util.py:315 -- Processing trial results took 1.517 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 18:58:13,613\tWARNING util.py:315 -- The `process_trial_result` operation took 1.518 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_96ca6737_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_Simple_2023-08-10_18-57-57/wandb/run-20230810_185816-96ca6737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb: Syncing run FSR_Trainable_96ca6737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/96ca6737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:                mae_coord 4.88524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:                mae_force 693.69802\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:               mape_coord 1.18354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:               mape_force 6.915858499208209e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:                   metric 0.60484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:               rmse_coord 2.35708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:               rmse_force 517.8971\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:       time_since_restore 116.41324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:         time_this_iter_s 1.06398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:             time_total_s 116.41324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:                timestamp 1691661499\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:               tmae_coord 0.97399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:               tmae_force 0.25494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:              tmape_coord 141638520400861.72\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:              tmape_force 256602383371107.78\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:              trmse_coord 0.43256\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:              trmse_force 0.17228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb:  View run FSR_Trainable_63b28b94 at: https://wandb.ai/seokjin/FSR-prediction/runs/63b28b94\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=122341)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185611-63b28b94/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:                mae_coord 4.98158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:                mae_force 697.94866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:               mape_coord 1.21652\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:               mape_force 6.543449694456748e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:                   metric 0.61306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:               rmse_coord 2.38623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:               rmse_force 528.52113\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:       time_since_restore 52.24884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:         time_this_iter_s 0.71757\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:             time_total_s 52.24884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:                timestamp 1691661546\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:               tmae_coord 0.99086\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:               tmae_force 0.26106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:              tmape_coord 171022849197945.25\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:              tmape_force 279383966500282.66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:              trmse_coord 0.43877\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:              trmse_force 0.17429\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb:  View run FSR_Trainable_96ca6737 at: https://wandb.ai/seokjin/FSR-prediction/runs/96ca6737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=124184)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185816-96ca6737/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=123513)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_185738-ebcf36c8/logs\n",
      "2023-08-10 18:59:16,986\tERROR tune.py:1107 -- Trials did not complete: [FSR_Trainable_3d8c1dc7, FSR_Trainable_b346be41, FSR_Trainable_1a76a8a6]\n",
      "2023-08-10 18:59:16,988\tINFO tune.py:1111 -- Total run time: 2440.75 seconds (2436.70 seconds for the tuning loop).\n",
      "2023-08-10 18:59:17,322\tWARNING experiment_analysis.py:910 -- Failed to read the results for 2 trials:\n",
      "- /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_3d8c1dc7_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-20-39\n",
      "- /home/seokj/ray_results/FSR_Trainable_2023-08-10_18-18-32/FSR_Trainable_1a76a8a6_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-08-10_18-35-52\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
