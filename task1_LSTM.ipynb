{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1_LSTM\n",
    "\n",
    "Index_X = FSR_for_force, FSR_for_coord\n",
    "\n",
    "Index_y = force, x_coord, y_coord\n",
    "\n",
    "Data = Splited by Time\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-07-18_22-35-35/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-07-18_22-35-35\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "97.267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.LSTM'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    imputer = trial.suggest_categorical('imputer', ['sklearn.impute.SimpleImputer'])\n",
    "    if imputer == 'sklearn.impute.SimpleImputer':\n",
    "        trial.suggest_categorical('imputer_args/strategy', [\n",
    "            'mean',\n",
    "            'median',\n",
    "        ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': ['FSR_for_force', 'FSR_for_coord'],\n",
    "        'index_y': ['force', 'x_coord', 'y_coord'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_time'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-18 22:35:35,279] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:35:37,467\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-07-18 22:35:39,194\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-18 22:54:50</td></tr>\n",
       "<tr><td>Running for: </td><td>00:19:11.40        </td></tr>\n",
       "<tr><td>Memory:      </td><td>2.6/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=100<br>Bracket: Iter 64.000: -108.56002663722724 | Iter 32.000: -109.91957313122005 | Iter 16.000: -112.96248993645 | Iter 8.000: -116.24848177335967 | Iter 4.000: -126.37108689671601 | Iter 2.000: -155.79285796948835 | Iter 1.000: -218.1151596987745<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>imputer             </th><th>imputer_args/strateg\n",
       "y       </th><th>index_X             </th><th>index_y             </th><th>model         </th><th style=\"text-align: right;\">    model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">  model_args/num_layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">       mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_d2860675</td><td>TERMINATED</td><td>172.26.215.93:229803</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_71c0</td><td>[&#x27;force&#x27;, &#x27;x_co_6b40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00153612 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        83.4531 </td><td style=\"text-align: right;\">164.287 </td><td style=\"text-align: right;\"> 48.2684</td><td style=\"text-align: right;\">9.70951e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e30ebaa5</td><td>TERMINATED</td><td>172.26.215.93:229875</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_6c80</td><td>[&#x27;force&#x27;, &#x27;x_co_3a00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00313738 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       115.295  </td><td style=\"text-align: right;\">109.37  </td><td style=\"text-align: right;\"> 33.8178</td><td style=\"text-align: right;\">6.31453e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_d4e691e6</td><td>TERMINATED</td><td>172.26.215.93:230048</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_d100</td><td>[&#x27;force&#x27;, &#x27;x_co_f400</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0267733  </td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.65732</td><td style=\"text-align: right;\">271.502 </td><td style=\"text-align: right;\"> 86.9212</td><td style=\"text-align: right;\">9.48567e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_c3031d24</td><td>TERMINATED</td><td>172.26.215.93:230223</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_4040</td><td>[&#x27;force&#x27;, &#x27;x_co_4800</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00652632 </td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.6607 </td><td style=\"text-align: right;\">278     </td><td style=\"text-align: right;\"> 84.281 </td><td style=\"text-align: right;\">8.15607e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_f97eec4f</td><td>TERMINATED</td><td>172.26.215.93:230516</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_9080</td><td>[&#x27;force&#x27;, &#x27;x_co_0500</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00246107 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.68286</td><td style=\"text-align: right;\">262.885 </td><td style=\"text-align: right;\"> 83.3455</td><td style=\"text-align: right;\">2.27548e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_a04a1408</td><td>TERMINATED</td><td>172.26.215.93:230740</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_5800</td><td>[&#x27;force&#x27;, &#x27;x_co_2700</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0206076  </td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.6755 </td><td style=\"text-align: right;\">275.946 </td><td style=\"text-align: right;\"> 87.4112</td><td style=\"text-align: right;\">1.0309e+17 </td></tr>\n",
       "<tr><td>FSR_Trainable_edbdf45b</td><td>TERMINATED</td><td>172.26.215.93:230964</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_8d80</td><td>[&#x27;force&#x27;, &#x27;x_co_1a40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000162778</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         5.94086</td><td style=\"text-align: right;\">221.959 </td><td style=\"text-align: right;\"> 70.0214</td><td style=\"text-align: right;\">2.12476e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_f6699148</td><td>TERMINATED</td><td>172.26.215.93:231170</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_d0c0</td><td>[&#x27;force&#x27;, &#x27;x_co_f580</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00475367 </td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.66931</td><td style=\"text-align: right;\">367.83  </td><td style=\"text-align: right;\">113.678 </td><td style=\"text-align: right;\">8.89668e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_3745123c</td><td>TERMINATED</td><td>172.26.215.93:231422</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_d440</td><td>[&#x27;force&#x27;, &#x27;x_co_5480</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        2.14418e-05</td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.85372</td><td style=\"text-align: right;\">323.318 </td><td style=\"text-align: right;\">128.969 </td><td style=\"text-align: right;\">1.92618e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_fcb10e6b</td><td>TERMINATED</td><td>172.26.215.93:231653</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_f440</td><td>[&#x27;force&#x27;, &#x27;x_co_78c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00108492 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.30881</td><td style=\"text-align: right;\">258.579 </td><td style=\"text-align: right;\"> 86.1998</td><td style=\"text-align: right;\">2.76003e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_05fdab89</td><td>TERMINATED</td><td>172.26.215.93:231879</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_6080</td><td>[&#x27;force&#x27;, &#x27;x_co_6000</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        2.55029e-05</td><td>sklearn.preproc_43f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.66501</td><td style=\"text-align: right;\">272.188 </td><td style=\"text-align: right;\"> 81.8684</td><td style=\"text-align: right;\">2.21655e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_a8326506</td><td>TERMINATED</td><td>172.26.215.93:232106</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_4780</td><td>[&#x27;force&#x27;, &#x27;x_co_8780</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000683699</td><td>sklearn.preproc_43f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.10157</td><td style=\"text-align: right;\">274.306 </td><td style=\"text-align: right;\"> 77.8435</td><td style=\"text-align: right;\">5.16691e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_348c4cf7</td><td>TERMINATED</td><td>172.26.215.93:232339</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_9180</td><td>[&#x27;force&#x27;, &#x27;x_co_a7c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0756181  </td><td>sklearn.preproc_43f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        32.2016 </td><td style=\"text-align: right;\">277.937 </td><td style=\"text-align: right;\"> 79.7732</td><td style=\"text-align: right;\">1.61366e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_eb3cd214</td><td>TERMINATED</td><td>172.26.215.93:232570</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_b7c0</td><td>[&#x27;force&#x27;, &#x27;x_co_3b80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000156785</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        16.1799 </td><td style=\"text-align: right;\">260.876 </td><td style=\"text-align: right;\"> 87.9286</td><td style=\"text-align: right;\">2.9921e+08 </td></tr>\n",
       "<tr><td>FSR_Trainable_42d8a5ff</td><td>TERMINATED</td><td>172.26.215.93:232777</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_3f00</td><td>[&#x27;force&#x27;, &#x27;x_co_7040</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000236909</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        15.8528 </td><td style=\"text-align: right;\">255.661 </td><td style=\"text-align: right;\"> 86.3191</td><td style=\"text-align: right;\">2.77884e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_45e58b43</td><td>TERMINATED</td><td>172.26.215.93:233030</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_74c0</td><td>[&#x27;force&#x27;, &#x27;x_co_3700</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000170984</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        13.6822 </td><td style=\"text-align: right;\">259.089 </td><td style=\"text-align: right;\"> 87.8172</td><td style=\"text-align: right;\">2.83059e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_a61d1336</td><td>TERMINATED</td><td>172.26.215.93:233244</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_7b40</td><td>[&#x27;force&#x27;, &#x27;x_co_2a40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00102843 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        83.0556 </td><td style=\"text-align: right;\">105.685 </td><td style=\"text-align: right;\"> 32.1159</td><td style=\"text-align: right;\">5.76235e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_29738bc1</td><td>TERMINATED</td><td>172.26.215.93:233490</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_e140</td><td>[&#x27;force&#x27;, &#x27;x_co_b380</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00192319 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        83.4331 </td><td style=\"text-align: right;\">109.746 </td><td style=\"text-align: right;\"> 34.0658</td><td style=\"text-align: right;\">4.90616e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_78c5d427</td><td>TERMINATED</td><td>172.26.215.93:233714</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_cb80</td><td>[&#x27;force&#x27;, &#x27;x_co_7e80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00194726 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        14.2287 </td><td style=\"text-align: right;\">114.457 </td><td style=\"text-align: right;\"> 35.377 </td><td style=\"text-align: right;\">6.43879e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_d6e0b655</td><td>TERMINATED</td><td>172.26.215.93:233928</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_d7c0</td><td>[&#x27;force&#x27;, &#x27;x_co_e5c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00225251 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        26.9754 </td><td style=\"text-align: right;\">115.136 </td><td style=\"text-align: right;\"> 35.5259</td><td style=\"text-align: right;\">6.76459e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_b751d296</td><td>TERMINATED</td><td>172.26.215.93:234190</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_4140</td><td>[&#x27;force&#x27;, &#x27;x_co_1300</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00195816 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">        53.8768 </td><td style=\"text-align: right;\">111.822 </td><td style=\"text-align: right;\"> 34.13  </td><td style=\"text-align: right;\">6.16013e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_27d2a402</td><td>TERMINATED</td><td>172.26.215.93:234436</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_af00</td><td>[&#x27;force&#x27;, &#x27;x_co_bf40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00832319 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         7.09602</td><td style=\"text-align: right;\">123.694 </td><td style=\"text-align: right;\"> 37.3762</td><td style=\"text-align: right;\">7.49946e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_121adfd9</td><td>TERMINATED</td><td>172.26.215.93:234674</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_d8c0</td><td>[&#x27;force&#x27;, &#x27;x_co_b240</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00849067 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         5.12199</td><td style=\"text-align: right;\">221.161 </td><td style=\"text-align: right;\"> 67.1574</td><td style=\"text-align: right;\">2.26143e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_e06dc26c</td><td>TERMINATED</td><td>172.26.215.93:234878</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_81c0</td><td>[&#x27;force&#x27;, &#x27;x_co_ca80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000553642</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.35665</td><td style=\"text-align: right;\">276.84  </td><td style=\"text-align: right;\"> 92.6903</td><td style=\"text-align: right;\">3.81269e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_55359899</td><td>TERMINATED</td><td>172.26.215.93:235096</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_4240</td><td>[&#x27;force&#x27;, &#x27;x_co_2540</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000642659</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.91082</td><td style=\"text-align: right;\">233.336 </td><td style=\"text-align: right;\"> 77.4202</td><td style=\"text-align: right;\">2.50669e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_d5d22a70</td><td>TERMINATED</td><td>172.26.215.93:235319</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_40c0</td><td>[&#x27;force&#x27;, &#x27;x_co_8680</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000530109</td><td>sklearn.preproc_43f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.66245</td><td style=\"text-align: right;\">294.255 </td><td style=\"text-align: right;\"> 77.6594</td><td style=\"text-align: right;\">3.707e+15  </td></tr>\n",
       "<tr><td>FSR_Trainable_1860b485</td><td>TERMINATED</td><td>172.26.215.93:235554</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_0840</td><td>[&#x27;force&#x27;, &#x27;x_co_b080</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00401679 </td><td>sklearn.preproc_43f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.26903</td><td style=\"text-align: right;\">225.839 </td><td style=\"text-align: right;\"> 65.4336</td><td style=\"text-align: right;\">6.86384e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_0bbbc27e</td><td>TERMINATED</td><td>172.26.215.93:235776</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_24c0</td><td>[&#x27;force&#x27;, &#x27;x_co_0e40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.003814   </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3.9401 </td><td style=\"text-align: right;\">139.202 </td><td style=\"text-align: right;\"> 42.3549</td><td style=\"text-align: right;\">4.26911e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_5a5db314</td><td>TERMINATED</td><td>172.26.215.93:235999</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_9ac0</td><td>[&#x27;force&#x27;, &#x27;x_co_93c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00355653 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3.93366</td><td style=\"text-align: right;\">138.202 </td><td style=\"text-align: right;\"> 41.7061</td><td style=\"text-align: right;\">2.72292e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ea26b751</td><td>TERMINATED</td><td>172.26.215.93:236225</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_af40</td><td>[&#x27;force&#x27;, &#x27;x_co_aa00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00132956 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3.9636 </td><td style=\"text-align: right;\">143.223 </td><td style=\"text-align: right;\"> 43.6111</td><td style=\"text-align: right;\">7.53767e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f6d35aea</td><td>TERMINATED</td><td>172.26.215.93:236447</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_f4c0</td><td>[&#x27;force&#x27;, &#x27;x_co_b3c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00119694 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         6.26438</td><td style=\"text-align: right;\">119.418 </td><td style=\"text-align: right;\"> 37.4318</td><td style=\"text-align: right;\">7.81944e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_7dce2c2a</td><td>TERMINATED</td><td>172.26.215.93:236556</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_5000</td><td>[&#x27;force&#x27;, &#x27;x_co_4940</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00120514 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        73.2241 </td><td style=\"text-align: right;\">107.913 </td><td style=\"text-align: right;\"> 32.9961</td><td style=\"text-align: right;\">5.00801e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ba4130e6</td><td>TERMINATED</td><td>172.26.215.93:236734</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_ed40</td><td>[&#x27;force&#x27;, &#x27;x_co_ffc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0010934  </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         1.95308</td><td style=\"text-align: right;\">179.827 </td><td style=\"text-align: right;\"> 56.0533</td><td style=\"text-align: right;\">1.59361e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_25957da4</td><td>TERMINATED</td><td>172.26.215.93:237047</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_9c00</td><td>[&#x27;force&#x27;, &#x27;x_co_8f40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00204084 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        12.7843 </td><td style=\"text-align: right;\">114.123 </td><td style=\"text-align: right;\"> 35.5937</td><td style=\"text-align: right;\">5.67404e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f3dbe86b</td><td>TERMINATED</td><td>172.26.215.93:237271</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_1980</td><td>[&#x27;force&#x27;, &#x27;x_co_1f80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00173582 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        13.4365 </td><td style=\"text-align: right;\">115.194 </td><td style=\"text-align: right;\"> 35.8804</td><td style=\"text-align: right;\">6.70866e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_85aca320</td><td>TERMINATED</td><td>172.26.215.93:237486</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_0740</td><td>[&#x27;force&#x27;, &#x27;x_co_4bc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00266526 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        13.651  </td><td style=\"text-align: right;\">115.55  </td><td style=\"text-align: right;\"> 35.2707</td><td style=\"text-align: right;\">5.07765e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_0d75f4f5</td><td>TERMINATED</td><td>172.26.215.93:237713</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_5740</td><td>[&#x27;force&#x27;, &#x27;x_co_9c40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0104504  </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4.34016</td><td style=\"text-align: right;\">139.483 </td><td style=\"text-align: right;\"> 40.3405</td><td style=\"text-align: right;\">5.26401e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_7d853856</td><td>TERMINATED</td><td>172.26.215.93:237941</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_02c0</td><td>[&#x27;force&#x27;, &#x27;x_co_0e00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00587634 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.49129</td><td style=\"text-align: right;\">261.751 </td><td style=\"text-align: right;\"> 88.1527</td><td style=\"text-align: right;\">3.01061e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_5be81a4e</td><td>TERMINATED</td><td>172.26.215.93:238160</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_3740</td><td>[&#x27;force&#x27;, &#x27;x_co_3400</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00549909 </td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.66782</td><td style=\"text-align: right;\">321.802 </td><td style=\"text-align: right;\">111.846 </td><td style=\"text-align: right;\">1.24041e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_f860c45d</td><td>TERMINATED</td><td>172.26.215.93:238381</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_9300</td><td>[&#x27;force&#x27;, &#x27;x_co_64c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0151796  </td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.74396</td><td style=\"text-align: right;\">271.277 </td><td style=\"text-align: right;\"> 84.6556</td><td style=\"text-align: right;\">9.16631e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_33f93632</td><td>TERMINATED</td><td>172.26.215.93:238582</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_bb40</td><td>[&#x27;force&#x27;, &#x27;x_co_9200</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0141691  </td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.13261</td><td style=\"text-align: right;\">269.258 </td><td style=\"text-align: right;\"> 81.564 </td><td style=\"text-align: right;\">7.50016e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_4a95a76d</td><td>TERMINATED</td><td>172.26.215.93:238826</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_70c0</td><td>[&#x27;force&#x27;, &#x27;x_co_4500</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0031664  </td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.42628</td><td style=\"text-align: right;\">265.294 </td><td style=\"text-align: right;\"> 79.2057</td><td style=\"text-align: right;\">7.91997e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_720cd2c0</td><td>TERMINATED</td><td>172.26.215.93:239053</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_a840</td><td>[&#x27;force&#x27;, &#x27;x_co_12c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00302134 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        14.8959 </td><td style=\"text-align: right;\">114.513 </td><td style=\"text-align: right;\"> 35.0776</td><td style=\"text-align: right;\">5.65367e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_503b8d2a</td><td>TERMINATED</td><td>172.26.215.93:239355</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_1180</td><td>[&#x27;force&#x27;, &#x27;x_co_2300</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00168795 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        26.9097 </td><td style=\"text-align: right;\">112.006 </td><td style=\"text-align: right;\"> 35.2269</td><td style=\"text-align: right;\">7.08462e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_a2cb264c</td><td>TERMINATED</td><td>172.26.215.93:239804</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_bfc0</td><td>[&#x27;force&#x27;, &#x27;x_co_7a40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00152874 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         6.12423</td><td style=\"text-align: right;\">117.064 </td><td style=\"text-align: right;\"> 35.2989</td><td style=\"text-align: right;\">6.77795e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_a65bd78b</td><td>TERMINATED</td><td>172.26.215.93:240030</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_d8c0</td><td>[&#x27;force&#x27;, &#x27;x_co_e6c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00158903 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         6.09028</td><td style=\"text-align: right;\">148.623 </td><td style=\"text-align: right;\"> 45.1023</td><td style=\"text-align: right;\">8.53415e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_678d2469</td><td>TERMINATED</td><td>172.26.215.93:240236</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_8840</td><td>[&#x27;force&#x27;, &#x27;x_co_9d40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000875412</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.52133</td><td style=\"text-align: right;\">258.977 </td><td style=\"text-align: right;\"> 83.4264</td><td style=\"text-align: right;\">1.86244e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_eadefdfb</td><td>TERMINATED</td><td>172.26.215.93:240488</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_a9c0</td><td>[&#x27;force&#x27;, &#x27;x_co_aa80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00077493 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         1.72075</td><td style=\"text-align: right;\">217.892 </td><td style=\"text-align: right;\"> 75.9764</td><td style=\"text-align: right;\">2.04748e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_3a9b8ffd</td><td>TERMINATED</td><td>172.26.215.93:240582</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_f2c0</td><td>[&#x27;force&#x27;, &#x27;x_co_5040</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00037718 </td><td>sklearn.preproc_43f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.19447</td><td style=\"text-align: right;\">272.184 </td><td style=\"text-align: right;\"> 78.3188</td><td style=\"text-align: right;\">1.96497e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_948308b0</td><td>TERMINATED</td><td>172.26.215.93:240896</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_fb40</td><td>[&#x27;force&#x27;, &#x27;x_co_d180</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00539975 </td><td>sklearn.preproc_43f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.1311 </td><td style=\"text-align: right;\">275.198 </td><td style=\"text-align: right;\"> 78.6448</td><td style=\"text-align: right;\">2.66857e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_a8e4be48</td><td>TERMINATED</td><td>172.26.215.93:240986</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_8600</td><td>[&#x27;force&#x27;, &#x27;x_co_9a40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00467355 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        16.7804 </td><td style=\"text-align: right;\">191.346 </td><td style=\"text-align: right;\"> 58.3597</td><td style=\"text-align: right;\">1.3779e+08 </td></tr>\n",
       "<tr><td>FSR_Trainable_1c185ec6</td><td>TERMINATED</td><td>172.26.215.93:241283</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_7a80</td><td>[&#x27;force&#x27;, &#x27;x_co_7480</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000924171</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.61413</td><td style=\"text-align: right;\">209.585 </td><td style=\"text-align: right;\"> 70.3687</td><td style=\"text-align: right;\">1.77044e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_436306bd</td><td>TERMINATED</td><td>172.26.215.93:241375</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_6ac0</td><td>[&#x27;force&#x27;, &#x27;x_co_5c40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000896261</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.12022</td><td style=\"text-align: right;\">260.578 </td><td style=\"text-align: right;\"> 92.2034</td><td style=\"text-align: right;\">2.91736e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_2b50aae9</td><td>TERMINATED</td><td>172.26.215.93:241677</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_07c0</td><td>[&#x27;force&#x27;, &#x27;x_co_0ac0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00234171 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        24.9507 </td><td style=\"text-align: right;\">112.482 </td><td style=\"text-align: right;\"> 34.4022</td><td style=\"text-align: right;\">5.63788e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_6341284f</td><td>TERMINATED</td><td>172.26.215.93:241927</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_ff40</td><td>[&#x27;force&#x27;, &#x27;x_co_4cc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00232734 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        26.5529 </td><td style=\"text-align: right;\">114.949 </td><td style=\"text-align: right;\"> 35.2943</td><td style=\"text-align: right;\">5.39653e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f147eae0</td><td>TERMINATED</td><td>172.26.215.93:242143</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_5f00</td><td>[&#x27;force&#x27;, &#x27;x_co_53c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00247976 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        14.4463 </td><td style=\"text-align: right;\">117.759 </td><td style=\"text-align: right;\"> 37.4469</td><td style=\"text-align: right;\">6.73227e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_fd2ae85c</td><td>TERMINATED</td><td>172.26.215.93:242358</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_4180</td><td>[&#x27;force&#x27;, &#x27;x_co_c4c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00259932 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         7.14775</td><td style=\"text-align: right;\">117.59  </td><td style=\"text-align: right;\"> 35.6761</td><td style=\"text-align: right;\">5.21257e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_c737bd0b</td><td>TERMINATED</td><td>172.26.215.93:242582</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_7540</td><td>[&#x27;force&#x27;, &#x27;x_co_6ec0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00149069 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3.52376</td><td style=\"text-align: right;\">138.913 </td><td style=\"text-align: right;\"> 43.1566</td><td style=\"text-align: right;\">9.08132e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_7c53b654</td><td>TERMINATED</td><td>172.26.215.93:242837</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_7d80</td><td>[&#x27;force&#x27;, &#x27;x_co_4600</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00139257 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63012</td><td style=\"text-align: right;\">266.429 </td><td style=\"text-align: right;\"> 87.4065</td><td style=\"text-align: right;\">3.25949e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_d6d16950</td><td>TERMINATED</td><td>172.26.215.93:243038</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_7080</td><td>[&#x27;force&#x27;, &#x27;x_co_6080</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000403848</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.48986</td><td style=\"text-align: right;\">252.901 </td><td style=\"text-align: right;\"> 87.49  </td><td style=\"text-align: right;\">2.68609e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_d70340e7</td><td>TERMINATED</td><td>172.26.215.93:243145</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_7c40</td><td>[&#x27;force&#x27;, &#x27;x_co_44c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000542894</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.49128</td><td style=\"text-align: right;\">261.071 </td><td style=\"text-align: right;\"> 83.787 </td><td style=\"text-align: right;\">2.79123e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_c6531895</td><td>TERMINATED</td><td>172.26.215.93:243327</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_7fc0</td><td>[&#x27;force&#x27;, &#x27;x_co_f780</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00397568 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.08906</td><td style=\"text-align: right;\">260.68  </td><td style=\"text-align: right;\"> 85.5652</td><td style=\"text-align: right;\">2.80134e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_ddc57f21</td><td>TERMINATED</td><td>172.26.215.93:243627</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_5fc0</td><td>[&#x27;force&#x27;, &#x27;x_co_71c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00190678 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">        51.2936 </td><td style=\"text-align: right;\">111.732 </td><td style=\"text-align: right;\"> 35.5439</td><td style=\"text-align: right;\">6.10474e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e43793b5</td><td>TERMINATED</td><td>172.26.215.93:243868</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_7500</td><td>[&#x27;force&#x27;, &#x27;x_co_5800</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0018541  </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        28.9777 </td><td style=\"text-align: right;\">113.109 </td><td style=\"text-align: right;\"> 35.1222</td><td style=\"text-align: right;\">5.72149e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_29fe79a7</td><td>TERMINATED</td><td>172.26.215.93:244084</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_c8c0</td><td>[&#x27;force&#x27;, &#x27;x_co_c580</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00192396 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4.58378</td><td style=\"text-align: right;\">130.275 </td><td style=\"text-align: right;\"> 39.9016</td><td style=\"text-align: right;\">7.0657e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_904e5216</td><td>TERMINATED</td><td>172.26.215.93:244301</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_8e80</td><td>[&#x27;force&#x27;, &#x27;x_co_9dc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00193909 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         7.9603 </td><td style=\"text-align: right;\">119.074 </td><td style=\"text-align: right;\"> 36.5558</td><td style=\"text-align: right;\">6.88048e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_77db7443</td><td>TERMINATED</td><td>172.26.215.93:244532</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_cc80</td><td>[&#x27;force&#x27;, &#x27;x_co_f1c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00107881 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.0785 </td><td style=\"text-align: right;\">196.112 </td><td style=\"text-align: right;\"> 60.733 </td><td style=\"text-align: right;\">1.71976e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_aa9978a5</td><td>TERMINATED</td><td>172.26.215.93:244769</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_0380</td><td>[&#x27;force&#x27;, &#x27;x_co_3340</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00108069 </td><td>sklearn.preproc_43f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.34193</td><td style=\"text-align: right;\">275.284 </td><td style=\"text-align: right;\"> 82.4736</td><td style=\"text-align: right;\">4.89426e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_908e38b8</td><td>TERMINATED</td><td>172.26.215.93:244974</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_2b80</td><td>[&#x27;force&#x27;, &#x27;x_co_3280</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00118022 </td><td>sklearn.preproc_43f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.31753</td><td style=\"text-align: right;\">210.295 </td><td style=\"text-align: right;\"> 58.8681</td><td style=\"text-align: right;\">5.73531e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_c3cab307</td><td>TERMINATED</td><td>172.26.215.93:245202</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_dc40</td><td>[&#x27;force&#x27;, &#x27;x_co_e780</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00328967 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4.58418</td><td style=\"text-align: right;\">138.804 </td><td style=\"text-align: right;\"> 39.5362</td><td style=\"text-align: right;\">3.73966e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_98a3ec20</td><td>TERMINATED</td><td>172.26.215.93:245430</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_d800</td><td>[&#x27;force&#x27;, &#x27;x_co_c1c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0032446  </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         1.91308</td><td style=\"text-align: right;\">166.177 </td><td style=\"text-align: right;\"> 49.4397</td><td style=\"text-align: right;\">1.0783e+08 </td></tr>\n",
       "<tr><td>FSR_Trainable_fa43b5fc</td><td>TERMINATED</td><td>172.26.215.93:245539</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_cc40</td><td>[&#x27;force&#x27;, &#x27;x_co_e440</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000698872</td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.24645</td><td style=\"text-align: right;\">242.544 </td><td style=\"text-align: right;\"> 72.2535</td><td style=\"text-align: right;\">7.54373e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_78122c6c</td><td>TERMINATED</td><td>172.26.215.93:245702</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_1000</td><td>[&#x27;force&#x27;, &#x27;x_co_0dc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000727896</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.03912</td><td style=\"text-align: right;\">195.935 </td><td style=\"text-align: right;\"> 63.6371</td><td style=\"text-align: right;\">1.55501e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_37ed06cd</td><td>TERMINATED</td><td>172.26.215.93:245906</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_3100</td><td>[&#x27;force&#x27;, &#x27;x_co_0b80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00230831 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         5.94674</td><td style=\"text-align: right;\">117.08  </td><td style=\"text-align: right;\"> 35.6791</td><td style=\"text-align: right;\">5.40928e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f3d4c294</td><td>TERMINATED</td><td>172.26.215.93:246224</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_7d80</td><td>[&#x27;force&#x27;, &#x27;x_co_4900</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00228033 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         6.01144</td><td style=\"text-align: right;\">118.664 </td><td style=\"text-align: right;\"> 36.963 </td><td style=\"text-align: right;\">6.83252e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_56d1e4fa</td><td>TERMINATED</td><td>172.26.215.93:246431</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_ce80</td><td>[&#x27;force&#x27;, &#x27;x_co_c680</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00717455 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3.19647</td><td style=\"text-align: right;\">129.5   </td><td style=\"text-align: right;\"> 38.3141</td><td style=\"text-align: right;\">6.30081e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_347b06e3</td><td>TERMINATED</td><td>172.26.215.93:246531</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_c240</td><td>[&#x27;force&#x27;, &#x27;x_co_c180</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00760276 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         6.1719 </td><td style=\"text-align: right;\">119.617 </td><td style=\"text-align: right;\"> 35.4897</td><td style=\"text-align: right;\">4.37655e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_92c39305</td><td>TERMINATED</td><td>172.26.215.93:246711</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_f100</td><td>[&#x27;force&#x27;, &#x27;x_co_dd40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00440669 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       132.309  </td><td style=\"text-align: right;\">100.71  </td><td style=\"text-align: right;\"> 30.6124</td><td style=\"text-align: right;\">6.49042e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_a2319f85</td><td>TERMINATED</td><td>172.26.215.93:246900</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_9700</td><td>[&#x27;force&#x27;, &#x27;x_co_8980</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00415749 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       130.449  </td><td style=\"text-align: right;\">105.523 </td><td style=\"text-align: right;\"> 31.8838</td><td style=\"text-align: right;\">5.82575e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f1f0a561</td><td>TERMINATED</td><td>172.26.215.93:247216</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_f5c0</td><td>[&#x27;force&#x27;, &#x27;x_co_eb80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00137498 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.39752</td><td style=\"text-align: right;\">251.194 </td><td style=\"text-align: right;\"> 84.9704</td><td style=\"text-align: right;\">2.63931e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_d754af28</td><td>TERMINATED</td><td>172.26.215.93:247409</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_adc0</td><td>[&#x27;force&#x27;, &#x27;x_co_9b00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00445996 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         8.76566</td><td style=\"text-align: right;\">143.232 </td><td style=\"text-align: right;\"> 43.7644</td><td style=\"text-align: right;\">1.16035e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_cc55eb29</td><td>TERMINATED</td><td>172.26.215.93:247628</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_3980</td><td>[&#x27;force&#x27;, &#x27;x_co_0140</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00488705 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        10.2832 </td><td style=\"text-align: right;\">117.695 </td><td style=\"text-align: right;\"> 36.1517</td><td style=\"text-align: right;\">7.29945e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_c9a07f03</td><td>TERMINATED</td><td>172.26.215.93:247869</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_e440</td><td>[&#x27;force&#x27;, &#x27;x_co_efc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0047477  </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        34.615  </td><td style=\"text-align: right;\">113.364 </td><td style=\"text-align: right;\"> 35.8492</td><td style=\"text-align: right;\">6.91533e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_8fd06241</td><td>TERMINATED</td><td>172.26.215.93:248107</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_f780</td><td>[&#x27;force&#x27;, &#x27;x_co_aa00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00166764 </td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.2673 </td><td style=\"text-align: right;\">259.514 </td><td style=\"text-align: right;\"> 80.3497</td><td style=\"text-align: right;\">1.01425e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_9220cbaa</td><td>TERMINATED</td><td>172.26.215.93:248334</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_fb40</td><td>[&#x27;force&#x27;, &#x27;x_co_f940</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00179197 </td><td>sklearn.preproc_4810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.6706 </td><td style=\"text-align: right;\">247.274 </td><td style=\"text-align: right;\"> 73.9099</td><td style=\"text-align: right;\">7.86815e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_0994684c</td><td>TERMINATED</td><td>172.26.215.93:248560</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_3000</td><td>[&#x27;force&#x27;, &#x27;x_co_21c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00180336 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4.59197</td><td style=\"text-align: right;\">129.211 </td><td style=\"text-align: right;\"> 38.6847</td><td style=\"text-align: right;\">6.82264e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_09d43379</td><td>TERMINATED</td><td>172.26.215.93:248793</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_3400</td><td>[&#x27;force&#x27;, &#x27;x_co_8a00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0027094  </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         7.99837</td><td style=\"text-align: right;\">118.407 </td><td style=\"text-align: right;\"> 36.3393</td><td style=\"text-align: right;\">5.41145e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f4cceab3</td><td>TERMINATED</td><td>172.26.215.93:249020</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_9340</td><td>[&#x27;force&#x27;, &#x27;x_co_2e00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0028493  </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         7.85445</td><td style=\"text-align: right;\">119.432 </td><td style=\"text-align: right;\"> 36.7234</td><td style=\"text-align: right;\">7.12995e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_18431645</td><td>TERMINATED</td><td>172.26.215.93:249251</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_35c0</td><td>[&#x27;force&#x27;, &#x27;x_co_a3c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0012243  </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.23627</td><td style=\"text-align: right;\">165.468 </td><td style=\"text-align: right;\"> 53.6337</td><td style=\"text-align: right;\">1.41533e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_534340b0</td><td>TERMINATED</td><td>172.26.215.93:249476</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_0d00</td><td>[&#x27;force&#x27;, &#x27;x_co_c700</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00123546 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.55631</td><td style=\"text-align: right;\">263.358 </td><td style=\"text-align: right;\"> 90.8299</td><td style=\"text-align: right;\">3.11751e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_359145d9</td><td>TERMINATED</td><td>172.26.215.93:249703</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>mean  </td><td>[&#x27;FSR_for_force_2140</td><td>[&#x27;force&#x27;, &#x27;x_co_3600</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00095588 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.31136</td><td style=\"text-align: right;\">265.003 </td><td style=\"text-align: right;\"> 89.9914</td><td style=\"text-align: right;\">2.76299e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_0ece50a3</td><td>TERMINATED</td><td>172.26.215.93:249941</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_31c0</td><td>[&#x27;force&#x27;, &#x27;x_co_0a40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00371714 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.81802</td><td style=\"text-align: right;\">158.607 </td><td style=\"text-align: right;\"> 49.6288</td><td style=\"text-align: right;\">9.16697e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_3d56171b</td><td>TERMINATED</td><td>172.26.215.93:250157</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_e380</td><td>[&#x27;force&#x27;, &#x27;x_co_f500</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00624726 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        18.9558 </td><td style=\"text-align: right;\">121.611 </td><td style=\"text-align: right;\"> 36.501 </td><td style=\"text-align: right;\">7.00982e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_56a68b84</td><td>TERMINATED</td><td>172.26.215.93:250377</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_0680</td><td>[&#x27;force&#x27;, &#x27;x_co_0d00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00651965 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">        83.8933 </td><td style=\"text-align: right;\">110.644 </td><td style=\"text-align: right;\"> 33.1503</td><td style=\"text-align: right;\">8.09055e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_0c6a6284</td><td>TERMINATED</td><td>172.26.215.93:250481</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_29c0</td><td>[&#x27;force&#x27;, &#x27;x_co_1680</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00378617 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       129.699  </td><td style=\"text-align: right;\">101.463 </td><td style=\"text-align: right;\"> 30.4056</td><td style=\"text-align: right;\">5.66483e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_48d3be65</td><td>TERMINATED</td><td>172.26.215.93:250772</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_cf40</td><td>[&#x27;force&#x27;, &#x27;x_co_d700</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0029497  </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.31753</td><td style=\"text-align: right;\">163.379 </td><td style=\"text-align: right;\"> 52.1788</td><td style=\"text-align: right;\">1.32189e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_fc48ca5d</td><td>TERMINATED</td><td>172.26.215.93:250996</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_5a80</td><td>[&#x27;force&#x27;, &#x27;x_co_5700</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00345875 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       131.794  </td><td style=\"text-align: right;\"> 97.2674</td><td style=\"text-align: right;\"> 29.8088</td><td style=\"text-align: right;\">5.38408e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_6238d631</td><td>TERMINATED</td><td>172.26.215.93:251219</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_33c0</td><td>[&#x27;force&#x27;, &#x27;x_co_3500</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00368058 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       128.655  </td><td style=\"text-align: right;\">103.128 </td><td style=\"text-align: right;\"> 32.11  </td><td style=\"text-align: right;\">5.86227e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_89f4577e</td><td>TERMINATED</td><td>172.26.215.93:251543</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_0d00</td><td>[&#x27;force&#x27;, &#x27;x_co_3640</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00384444 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       117.304  </td><td style=\"text-align: right;\">102.869 </td><td style=\"text-align: right;\"> 32.448 </td><td style=\"text-align: right;\">5.61358e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e9a68070</td><td>TERMINATED</td><td>172.26.215.93:251882</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._7280</td><td>median</td><td>[&#x27;FSR_for_force_2780</td><td>[&#x27;force&#x27;, &#x27;x_co_1740</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00388223 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        90.4186 </td><td style=\"text-align: right;\"> 98.9794</td><td style=\"text-align: right;\"> 29.8147</td><td style=\"text-align: right;\">5.01016e+07</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:35:39,261\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">       mape</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_05fdab89</td><td>2023-07-18_22-37-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 81.8684</td><td style=\"text-align: right;\">2.21655e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">231879</td><td style=\"text-align: right;\">272.188 </td><td style=\"text-align: right;\">             1.66501</td><td style=\"text-align: right;\">          1.66501 </td><td style=\"text-align: right;\">       1.66501</td><td style=\"text-align: right;\"> 1689687439</td><td style=\"text-align: right;\">                   1</td><td>05fdab89  </td></tr>\n",
       "<tr><td>FSR_Trainable_0994684c</td><td>2023-07-18_22-49-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 38.6847</td><td style=\"text-align: right;\">6.82264e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">248560</td><td style=\"text-align: right;\">129.211 </td><td style=\"text-align: right;\">             4.59197</td><td style=\"text-align: right;\">          0.894536</td><td style=\"text-align: right;\">       4.59197</td><td style=\"text-align: right;\"> 1689688159</td><td style=\"text-align: right;\">                   4</td><td>0994684c  </td></tr>\n",
       "<tr><td>FSR_Trainable_09d43379</td><td>2023-07-18_22-49-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 36.3393</td><td style=\"text-align: right;\">5.41145e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">248793</td><td style=\"text-align: right;\">118.407 </td><td style=\"text-align: right;\">             7.99837</td><td style=\"text-align: right;\">          0.858184</td><td style=\"text-align: right;\">       7.99837</td><td style=\"text-align: right;\"> 1689688174</td><td style=\"text-align: right;\">                   8</td><td>09d43379  </td></tr>\n",
       "<tr><td>FSR_Trainable_0bbbc27e</td><td>2023-07-18_22-40-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 42.3549</td><td style=\"text-align: right;\">4.26911e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">235776</td><td style=\"text-align: right;\">139.202 </td><td style=\"text-align: right;\">             3.9401 </td><td style=\"text-align: right;\">          0.783182</td><td style=\"text-align: right;\">       3.9401 </td><td style=\"text-align: right;\"> 1689687637</td><td style=\"text-align: right;\">                   4</td><td>0bbbc27e  </td></tr>\n",
       "<tr><td>FSR_Trainable_0c6a6284</td><td>2023-07-18_22-53-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 30.4056</td><td style=\"text-align: right;\">5.66483e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">250481</td><td style=\"text-align: right;\">101.463 </td><td style=\"text-align: right;\">           129.699  </td><td style=\"text-align: right;\">          1.33805 </td><td style=\"text-align: right;\">     129.699  </td><td style=\"text-align: right;\"> 1689688384</td><td style=\"text-align: right;\">                 100</td><td>0c6a6284  </td></tr>\n",
       "<tr><td>FSR_Trainable_0d75f4f5</td><td>2023-07-18_22-41-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 40.3405</td><td style=\"text-align: right;\">5.26401e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">237713</td><td style=\"text-align: right;\">139.483 </td><td style=\"text-align: right;\">             4.34016</td><td style=\"text-align: right;\">          0.893074</td><td style=\"text-align: right;\">       4.34016</td><td style=\"text-align: right;\"> 1689687712</td><td style=\"text-align: right;\">                   4</td><td>0d75f4f5  </td></tr>\n",
       "<tr><td>FSR_Trainable_0ece50a3</td><td>2023-07-18_22-50-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 49.6288</td><td style=\"text-align: right;\">9.16697e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">249941</td><td style=\"text-align: right;\">158.607 </td><td style=\"text-align: right;\">             3.81802</td><td style=\"text-align: right;\">          1.55942 </td><td style=\"text-align: right;\">       3.81802</td><td style=\"text-align: right;\"> 1689688224</td><td style=\"text-align: right;\">                   2</td><td>0ece50a3  </td></tr>\n",
       "<tr><td>FSR_Trainable_121adfd9</td><td>2023-07-18_22-39-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 67.1574</td><td style=\"text-align: right;\">2.26143e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">234674</td><td style=\"text-align: right;\">221.161 </td><td style=\"text-align: right;\">             5.12199</td><td style=\"text-align: right;\">          2.40316 </td><td style=\"text-align: right;\">       5.12199</td><td style=\"text-align: right;\"> 1689687597</td><td style=\"text-align: right;\">                   2</td><td>121adfd9  </td></tr>\n",
       "<tr><td>FSR_Trainable_18431645</td><td>2023-07-18_22-49-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 53.6337</td><td style=\"text-align: right;\">1.41533e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">249251</td><td style=\"text-align: right;\">165.468 </td><td style=\"text-align: right;\">             2.23627</td><td style=\"text-align: right;\">          1.05103 </td><td style=\"text-align: right;\">       2.23627</td><td style=\"text-align: right;\"> 1689688192</td><td style=\"text-align: right;\">                   2</td><td>18431645  </td></tr>\n",
       "<tr><td>FSR_Trainable_1860b485</td><td>2023-07-18_22-40-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 65.4336</td><td style=\"text-align: right;\">6.86384e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">235554</td><td style=\"text-align: right;\">225.839 </td><td style=\"text-align: right;\">             2.26903</td><td style=\"text-align: right;\">          0.936373</td><td style=\"text-align: right;\">       2.26903</td><td style=\"text-align: right;\"> 1689687628</td><td style=\"text-align: right;\">                   2</td><td>1860b485  </td></tr>\n",
       "<tr><td>FSR_Trainable_1c185ec6</td><td>2023-07-18_22-44-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 70.3687</td><td style=\"text-align: right;\">1.77044e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">241283</td><td style=\"text-align: right;\">209.585 </td><td style=\"text-align: right;\">             3.61413</td><td style=\"text-align: right;\">          1.69059 </td><td style=\"text-align: right;\">       3.61413</td><td style=\"text-align: right;\"> 1689687841</td><td style=\"text-align: right;\">                   2</td><td>1c185ec6  </td></tr>\n",
       "<tr><td>FSR_Trainable_25957da4</td><td>2023-07-18_22-41-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 35.5937</td><td style=\"text-align: right;\">5.67404e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">237047</td><td style=\"text-align: right;\">114.123 </td><td style=\"text-align: right;\">            12.7843 </td><td style=\"text-align: right;\">          0.798826</td><td style=\"text-align: right;\">      12.7843 </td><td style=\"text-align: right;\"> 1689687691</td><td style=\"text-align: right;\">                  16</td><td>25957da4  </td></tr>\n",
       "<tr><td>FSR_Trainable_27d2a402</td><td>2023-07-18_22-39-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 37.3762</td><td style=\"text-align: right;\">7.49946e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">234436</td><td style=\"text-align: right;\">123.694 </td><td style=\"text-align: right;\">             7.09602</td><td style=\"text-align: right;\">          0.797987</td><td style=\"text-align: right;\">       7.09602</td><td style=\"text-align: right;\"> 1689687579</td><td style=\"text-align: right;\">                   8</td><td>27d2a402  </td></tr>\n",
       "<tr><td>FSR_Trainable_29738bc1</td><td>2023-07-18_22-40-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 34.0658</td><td style=\"text-align: right;\">4.90616e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">233490</td><td style=\"text-align: right;\">109.746 </td><td style=\"text-align: right;\">            83.4331 </td><td style=\"text-align: right;\">          0.683568</td><td style=\"text-align: right;\">      83.4331 </td><td style=\"text-align: right;\"> 1689687603</td><td style=\"text-align: right;\">                 100</td><td>29738bc1  </td></tr>\n",
       "<tr><td>FSR_Trainable_29fe79a7</td><td>2023-07-18_22-45-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 39.9016</td><td style=\"text-align: right;\">7.0657e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">244084</td><td style=\"text-align: right;\">130.275 </td><td style=\"text-align: right;\">             4.58378</td><td style=\"text-align: right;\">          1.05356 </td><td style=\"text-align: right;\">       4.58378</td><td style=\"text-align: right;\"> 1689687953</td><td style=\"text-align: right;\">                   4</td><td>29fe79a7  </td></tr>\n",
       "<tr><td>FSR_Trainable_2b50aae9</td><td>2023-07-18_22-44-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 34.4022</td><td style=\"text-align: right;\">5.63788e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">241677</td><td style=\"text-align: right;\">112.482 </td><td style=\"text-align: right;\">            24.9507 </td><td style=\"text-align: right;\">          0.803806</td><td style=\"text-align: right;\">      24.9507 </td><td style=\"text-align: right;\"> 1689687880</td><td style=\"text-align: right;\">                  32</td><td>2b50aae9  </td></tr>\n",
       "<tr><td>FSR_Trainable_33f93632</td><td>2023-07-18_22-42-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 81.564 </td><td style=\"text-align: right;\">7.50016e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">238582</td><td style=\"text-align: right;\">269.258 </td><td style=\"text-align: right;\">             2.13261</td><td style=\"text-align: right;\">          2.13261 </td><td style=\"text-align: right;\">       2.13261</td><td style=\"text-align: right;\"> 1689687742</td><td style=\"text-align: right;\">                   1</td><td>33f93632  </td></tr>\n",
       "<tr><td>FSR_Trainable_347b06e3</td><td>2023-07-18_22-47-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 35.4897</td><td style=\"text-align: right;\">4.37655e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">246531</td><td style=\"text-align: right;\">119.617 </td><td style=\"text-align: right;\">             6.1719 </td><td style=\"text-align: right;\">          0.929513</td><td style=\"text-align: right;\">       6.1719 </td><td style=\"text-align: right;\"> 1689688054</td><td style=\"text-align: right;\">                   8</td><td>347b06e3  </td></tr>\n",
       "<tr><td>FSR_Trainable_348c4cf7</td><td>2023-07-18_22-38-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 79.7732</td><td style=\"text-align: right;\">1.61366e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">232339</td><td style=\"text-align: right;\">277.937 </td><td style=\"text-align: right;\">            32.2016 </td><td style=\"text-align: right;\">         14.0746  </td><td style=\"text-align: right;\">      32.2016 </td><td style=\"text-align: right;\"> 1689687490</td><td style=\"text-align: right;\">                   2</td><td>348c4cf7  </td></tr>\n",
       "<tr><td>FSR_Trainable_359145d9</td><td>2023-07-18_22-50-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 89.9914</td><td style=\"text-align: right;\">2.76299e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">249703</td><td style=\"text-align: right;\">265.003 </td><td style=\"text-align: right;\">             1.31136</td><td style=\"text-align: right;\">          1.31136 </td><td style=\"text-align: right;\">       1.31136</td><td style=\"text-align: right;\"> 1689688211</td><td style=\"text-align: right;\">                   1</td><td>359145d9  </td></tr>\n",
       "<tr><td>FSR_Trainable_3745123c</td><td>2023-07-18_22-37-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">128.969 </td><td style=\"text-align: right;\">1.92618e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">231422</td><td style=\"text-align: right;\">323.318 </td><td style=\"text-align: right;\">             1.85372</td><td style=\"text-align: right;\">          1.85372 </td><td style=\"text-align: right;\">       1.85372</td><td style=\"text-align: right;\"> 1689687421</td><td style=\"text-align: right;\">                   1</td><td>3745123c  </td></tr>\n",
       "<tr><td>FSR_Trainable_37ed06cd</td><td>2023-07-18_22-47-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 35.6791</td><td style=\"text-align: right;\">5.40928e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">245906</td><td style=\"text-align: right;\">117.08  </td><td style=\"text-align: right;\">             5.94674</td><td style=\"text-align: right;\">          0.560209</td><td style=\"text-align: right;\">       5.94674</td><td style=\"text-align: right;\"> 1689688031</td><td style=\"text-align: right;\">                   8</td><td>37ed06cd  </td></tr>\n",
       "<tr><td>FSR_Trainable_3a9b8ffd</td><td>2023-07-18_22-43-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 78.3188</td><td style=\"text-align: right;\">1.96497e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">240582</td><td style=\"text-align: right;\">272.184 </td><td style=\"text-align: right;\">             1.19447</td><td style=\"text-align: right;\">          1.19447 </td><td style=\"text-align: right;\">       1.19447</td><td style=\"text-align: right;\"> 1689687813</td><td style=\"text-align: right;\">                   1</td><td>3a9b8ffd  </td></tr>\n",
       "<tr><td>FSR_Trainable_3d56171b</td><td>2023-07-18_22-50-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 36.501 </td><td style=\"text-align: right;\">7.00982e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">250157</td><td style=\"text-align: right;\">121.611 </td><td style=\"text-align: right;\">            18.9558 </td><td style=\"text-align: right;\">          1.33293 </td><td style=\"text-align: right;\">      18.9558 </td><td style=\"text-align: right;\"> 1689688249</td><td style=\"text-align: right;\">                  16</td><td>3d56171b  </td></tr>\n",
       "<tr><td>FSR_Trainable_42d8a5ff</td><td>2023-07-18_22-38-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 86.3191</td><td style=\"text-align: right;\">2.77884e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">232777</td><td style=\"text-align: right;\">255.661 </td><td style=\"text-align: right;\">            15.8528 </td><td style=\"text-align: right;\">          6.90792 </td><td style=\"text-align: right;\">      15.8528 </td><td style=\"text-align: right;\"> 1689687493</td><td style=\"text-align: right;\">                   2</td><td>42d8a5ff  </td></tr>\n",
       "<tr><td>FSR_Trainable_436306bd</td><td>2023-07-18_22-44-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 92.2034</td><td style=\"text-align: right;\">2.91736e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">241375</td><td style=\"text-align: right;\">260.578 </td><td style=\"text-align: right;\">             2.12022</td><td style=\"text-align: right;\">          2.12022 </td><td style=\"text-align: right;\">       2.12022</td><td style=\"text-align: right;\"> 1689687845</td><td style=\"text-align: right;\">                   1</td><td>436306bd  </td></tr>\n",
       "<tr><td>FSR_Trainable_45e58b43</td><td>2023-07-18_22-38-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 87.8172</td><td style=\"text-align: right;\">2.83059e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">233030</td><td style=\"text-align: right;\">259.089 </td><td style=\"text-align: right;\">            13.6822 </td><td style=\"text-align: right;\">          6.7643  </td><td style=\"text-align: right;\">      13.6822 </td><td style=\"text-align: right;\"> 1689687506</td><td style=\"text-align: right;\">                   2</td><td>45e58b43  </td></tr>\n",
       "<tr><td>FSR_Trainable_48d3be65</td><td>2023-07-18_22-50-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 52.1788</td><td style=\"text-align: right;\">1.32189e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">250772</td><td style=\"text-align: right;\">163.379 </td><td style=\"text-align: right;\">             3.31753</td><td style=\"text-align: right;\">          1.50393 </td><td style=\"text-align: right;\">       3.31753</td><td style=\"text-align: right;\"> 1689688259</td><td style=\"text-align: right;\">                   2</td><td>48d3be65  </td></tr>\n",
       "<tr><td>FSR_Trainable_4a95a76d</td><td>2023-07-18_22-42-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 79.2057</td><td style=\"text-align: right;\">7.91997e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">238826</td><td style=\"text-align: right;\">265.294 </td><td style=\"text-align: right;\">             1.42628</td><td style=\"text-align: right;\">          1.42628 </td><td style=\"text-align: right;\">       1.42628</td><td style=\"text-align: right;\"> 1689687750</td><td style=\"text-align: right;\">                   1</td><td>4a95a76d  </td></tr>\n",
       "<tr><td>FSR_Trainable_503b8d2a</td><td>2023-07-18_22-43-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 35.2269</td><td style=\"text-align: right;\">7.08462e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">239355</td><td style=\"text-align: right;\">112.006 </td><td style=\"text-align: right;\">            26.9097 </td><td style=\"text-align: right;\">          1.07844 </td><td style=\"text-align: right;\">      26.9097 </td><td style=\"text-align: right;\"> 1689687799</td><td style=\"text-align: right;\">                  32</td><td>503b8d2a  </td></tr>\n",
       "<tr><td>FSR_Trainable_534340b0</td><td>2023-07-18_22-50-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 90.8299</td><td style=\"text-align: right;\">3.11751e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">249476</td><td style=\"text-align: right;\">263.358 </td><td style=\"text-align: right;\">             1.55631</td><td style=\"text-align: right;\">          1.55631 </td><td style=\"text-align: right;\">       1.55631</td><td style=\"text-align: right;\"> 1689688201</td><td style=\"text-align: right;\">                   1</td><td>534340b0  </td></tr>\n",
       "<tr><td>FSR_Trainable_55359899</td><td>2023-07-18_22-40-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 77.4202</td><td style=\"text-align: right;\">2.50669e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">235096</td><td style=\"text-align: right;\">233.336 </td><td style=\"text-align: right;\">             2.91082</td><td style=\"text-align: right;\">          1.21691 </td><td style=\"text-align: right;\">       2.91082</td><td style=\"text-align: right;\"> 1689687615</td><td style=\"text-align: right;\">                   2</td><td>55359899  </td></tr>\n",
       "<tr><td>FSR_Trainable_56a68b84</td><td>2023-07-18_22-52-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 33.1503</td><td style=\"text-align: right;\">8.09055e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">250377</td><td style=\"text-align: right;\">110.644 </td><td style=\"text-align: right;\">            83.8933 </td><td style=\"text-align: right;\">          1.29116 </td><td style=\"text-align: right;\">      83.8933 </td><td style=\"text-align: right;\"> 1689688329</td><td style=\"text-align: right;\">                  64</td><td>56a68b84  </td></tr>\n",
       "<tr><td>FSR_Trainable_56d1e4fa</td><td>2023-07-18_22-47-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 38.3141</td><td style=\"text-align: right;\">6.30081e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">246431</td><td style=\"text-align: right;\">129.5   </td><td style=\"text-align: right;\">             3.19647</td><td style=\"text-align: right;\">          0.646285</td><td style=\"text-align: right;\">       3.19647</td><td style=\"text-align: right;\"> 1689688045</td><td style=\"text-align: right;\">                   4</td><td>56d1e4fa  </td></tr>\n",
       "<tr><td>FSR_Trainable_5a5db314</td><td>2023-07-18_22-40-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 41.7061</td><td style=\"text-align: right;\">2.72292e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">235999</td><td style=\"text-align: right;\">138.202 </td><td style=\"text-align: right;\">             3.93366</td><td style=\"text-align: right;\">          0.776826</td><td style=\"text-align: right;\">       3.93366</td><td style=\"text-align: right;\"> 1689687644</td><td style=\"text-align: right;\">                   4</td><td>5a5db314  </td></tr>\n",
       "<tr><td>FSR_Trainable_5be81a4e</td><td>2023-07-18_22-42-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">111.846 </td><td style=\"text-align: right;\">1.24041e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">238160</td><td style=\"text-align: right;\">321.802 </td><td style=\"text-align: right;\">             1.66782</td><td style=\"text-align: right;\">          1.66782 </td><td style=\"text-align: right;\">       1.66782</td><td style=\"text-align: right;\"> 1689687726</td><td style=\"text-align: right;\">                   1</td><td>5be81a4e  </td></tr>\n",
       "<tr><td>FSR_Trainable_6238d631</td><td>2023-07-18_22-53-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 32.11  </td><td style=\"text-align: right;\">5.86227e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">251219</td><td style=\"text-align: right;\">103.128 </td><td style=\"text-align: right;\">           128.655  </td><td style=\"text-align: right;\">          1.06753 </td><td style=\"text-align: right;\">     128.655  </td><td style=\"text-align: right;\"> 1689688413</td><td style=\"text-align: right;\">                 100</td><td>6238d631  </td></tr>\n",
       "<tr><td>FSR_Trainable_6341284f</td><td>2023-07-18_22-44-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 35.2943</td><td style=\"text-align: right;\">5.39653e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">241927</td><td style=\"text-align: right;\">114.949 </td><td style=\"text-align: right;\">            26.5529 </td><td style=\"text-align: right;\">          0.5367  </td><td style=\"text-align: right;\">      26.5529 </td><td style=\"text-align: right;\"> 1689687891</td><td style=\"text-align: right;\">                  32</td><td>6341284f  </td></tr>\n",
       "<tr><td>FSR_Trainable_678d2469</td><td>2023-07-18_22-43-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 83.4264</td><td style=\"text-align: right;\">1.86244e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">240236</td><td style=\"text-align: right;\">258.977 </td><td style=\"text-align: right;\">             1.52133</td><td style=\"text-align: right;\">          1.52133 </td><td style=\"text-align: right;\">       1.52133</td><td style=\"text-align: right;\"> 1689687799</td><td style=\"text-align: right;\">                   1</td><td>678d2469  </td></tr>\n",
       "<tr><td>FSR_Trainable_720cd2c0</td><td>2023-07-18_22-42-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 35.0776</td><td style=\"text-align: right;\">5.65367e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">239053</td><td style=\"text-align: right;\">114.513 </td><td style=\"text-align: right;\">            14.8959 </td><td style=\"text-align: right;\">          0.723396</td><td style=\"text-align: right;\">      14.8959 </td><td style=\"text-align: right;\"> 1689687774</td><td style=\"text-align: right;\">                  16</td><td>720cd2c0  </td></tr>\n",
       "<tr><td>FSR_Trainable_77db7443</td><td>2023-07-18_22-46-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 60.733 </td><td style=\"text-align: right;\">1.71976e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">244532</td><td style=\"text-align: right;\">196.112 </td><td style=\"text-align: right;\">             2.0785 </td><td style=\"text-align: right;\">          0.942123</td><td style=\"text-align: right;\">       2.0785 </td><td style=\"text-align: right;\"> 1689687975</td><td style=\"text-align: right;\">                   2</td><td>77db7443  </td></tr>\n",
       "<tr><td>FSR_Trainable_78122c6c</td><td>2023-07-18_22-47-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 63.6371</td><td style=\"text-align: right;\">1.55501e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">245702</td><td style=\"text-align: right;\">195.935 </td><td style=\"text-align: right;\">             2.03912</td><td style=\"text-align: right;\">          0.825257</td><td style=\"text-align: right;\">       2.03912</td><td style=\"text-align: right;\"> 1689688020</td><td style=\"text-align: right;\">                   2</td><td>78122c6c  </td></tr>\n",
       "<tr><td>FSR_Trainable_78c5d427</td><td>2023-07-18_22-38-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 35.377 </td><td style=\"text-align: right;\">6.43879e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">233714</td><td style=\"text-align: right;\">114.457 </td><td style=\"text-align: right;\">            14.2287 </td><td style=\"text-align: right;\">          0.779332</td><td style=\"text-align: right;\">      14.2287 </td><td style=\"text-align: right;\"> 1689687537</td><td style=\"text-align: right;\">                  16</td><td>78c5d427  </td></tr>\n",
       "<tr><td>FSR_Trainable_7c53b654</td><td>2023-07-18_22-45-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 87.4065</td><td style=\"text-align: right;\">3.25949e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">242837</td><td style=\"text-align: right;\">266.429 </td><td style=\"text-align: right;\">             2.63012</td><td style=\"text-align: right;\">          2.63012 </td><td style=\"text-align: right;\">       2.63012</td><td style=\"text-align: right;\"> 1689687903</td><td style=\"text-align: right;\">                   1</td><td>7c53b654  </td></tr>\n",
       "<tr><td>FSR_Trainable_7d853856</td><td>2023-07-18_22-41-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 88.1527</td><td style=\"text-align: right;\">3.01061e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">237941</td><td style=\"text-align: right;\">261.751 </td><td style=\"text-align: right;\">             1.49129</td><td style=\"text-align: right;\">          1.49129 </td><td style=\"text-align: right;\">       1.49129</td><td style=\"text-align: right;\"> 1689687717</td><td style=\"text-align: right;\">                   1</td><td>7d853856  </td></tr>\n",
       "<tr><td>FSR_Trainable_7dce2c2a</td><td>2023-07-18_22-42-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 32.9961</td><td style=\"text-align: right;\">5.00801e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">236556</td><td style=\"text-align: right;\">107.913 </td><td style=\"text-align: right;\">            73.2241 </td><td style=\"text-align: right;\">          0.650664</td><td style=\"text-align: right;\">      73.2241 </td><td style=\"text-align: right;\"> 1689687748</td><td style=\"text-align: right;\">                 100</td><td>7dce2c2a  </td></tr>\n",
       "<tr><td>FSR_Trainable_85aca320</td><td>2023-07-18_22-41-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 35.2707</td><td style=\"text-align: right;\">5.07765e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">237486</td><td style=\"text-align: right;\">115.55  </td><td style=\"text-align: right;\">            13.651  </td><td style=\"text-align: right;\">          0.753134</td><td style=\"text-align: right;\">      13.651  </td><td style=\"text-align: right;\"> 1689687712</td><td style=\"text-align: right;\">                  16</td><td>85aca320  </td></tr>\n",
       "<tr><td>FSR_Trainable_89f4577e</td><td>2023-07-18_22-54-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 32.448 </td><td style=\"text-align: right;\">5.61358e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">251543</td><td style=\"text-align: right;\">102.869 </td><td style=\"text-align: right;\">           117.304  </td><td style=\"text-align: right;\">          0.890531</td><td style=\"text-align: right;\">     117.304  </td><td style=\"text-align: right;\"> 1689688462</td><td style=\"text-align: right;\">                 100</td><td>89f4577e  </td></tr>\n",
       "<tr><td>FSR_Trainable_8fd06241</td><td>2023-07-18_22-48-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 80.3497</td><td style=\"text-align: right;\">1.01425e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">248107</td><td style=\"text-align: right;\">259.514 </td><td style=\"text-align: right;\">             1.2673 </td><td style=\"text-align: right;\">          1.2673  </td><td style=\"text-align: right;\">       1.2673 </td><td style=\"text-align: right;\"> 1689688120</td><td style=\"text-align: right;\">                   1</td><td>8fd06241  </td></tr>\n",
       "<tr><td>FSR_Trainable_904e5216</td><td>2023-07-18_22-46-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 36.5558</td><td style=\"text-align: right;\">6.88048e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">244301</td><td style=\"text-align: right;\">119.074 </td><td style=\"text-align: right;\">             7.9603 </td><td style=\"text-align: right;\">          0.915845</td><td style=\"text-align: right;\">       7.9603 </td><td style=\"text-align: right;\"> 1689687968</td><td style=\"text-align: right;\">                   8</td><td>904e5216  </td></tr>\n",
       "<tr><td>FSR_Trainable_908e38b8</td><td>2023-07-18_22-46-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 58.8681</td><td style=\"text-align: right;\">5.73531e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">244974</td><td style=\"text-align: right;\">210.295 </td><td style=\"text-align: right;\">             3.31753</td><td style=\"text-align: right;\">          1.208   </td><td style=\"text-align: right;\">       3.31753</td><td style=\"text-align: right;\"> 1689687994</td><td style=\"text-align: right;\">                   2</td><td>908e38b8  </td></tr>\n",
       "<tr><td>FSR_Trainable_9220cbaa</td><td>2023-07-18_22-48-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 73.9099</td><td style=\"text-align: right;\">7.86815e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">248334</td><td style=\"text-align: right;\">247.274 </td><td style=\"text-align: right;\">             1.6706 </td><td style=\"text-align: right;\">          1.6706  </td><td style=\"text-align: right;\">       1.6706 </td><td style=\"text-align: right;\"> 1689688136</td><td style=\"text-align: right;\">                   1</td><td>9220cbaa  </td></tr>\n",
       "<tr><td>FSR_Trainable_92c39305</td><td>2023-07-18_22-50-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 30.6124</td><td style=\"text-align: right;\">6.49042e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">246711</td><td style=\"text-align: right;\">100.71  </td><td style=\"text-align: right;\">           132.309  </td><td style=\"text-align: right;\">          1.00517 </td><td style=\"text-align: right;\">     132.309  </td><td style=\"text-align: right;\"> 1689688207</td><td style=\"text-align: right;\">                 100</td><td>92c39305  </td></tr>\n",
       "<tr><td>FSR_Trainable_948308b0</td><td>2023-07-18_22-43-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 78.6448</td><td style=\"text-align: right;\">2.66857e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">240896</td><td style=\"text-align: right;\">275.198 </td><td style=\"text-align: right;\">             5.1311 </td><td style=\"text-align: right;\">          5.1311  </td><td style=\"text-align: right;\">       5.1311 </td><td style=\"text-align: right;\"> 1689687825</td><td style=\"text-align: right;\">                   1</td><td>948308b0  </td></tr>\n",
       "<tr><td>FSR_Trainable_98a3ec20</td><td>2023-07-18_22-46-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 49.4397</td><td style=\"text-align: right;\">1.0783e+08 </td><td>172.26.215.93</td><td style=\"text-align: right;\">245430</td><td style=\"text-align: right;\">166.177 </td><td style=\"text-align: right;\">             1.91308</td><td style=\"text-align: right;\">          0.727091</td><td style=\"text-align: right;\">       1.91308</td><td style=\"text-align: right;\"> 1689688007</td><td style=\"text-align: right;\">                   2</td><td>98a3ec20  </td></tr>\n",
       "<tr><td>FSR_Trainable_a04a1408</td><td>2023-07-18_22-36-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 87.4112</td><td style=\"text-align: right;\">1.0309e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">230740</td><td style=\"text-align: right;\">275.946 </td><td style=\"text-align: right;\">             2.6755 </td><td style=\"text-align: right;\">          2.6755  </td><td style=\"text-align: right;\">       2.6755 </td><td style=\"text-align: right;\"> 1689687391</td><td style=\"text-align: right;\">                   1</td><td>a04a1408  </td></tr>\n",
       "<tr><td>FSR_Trainable_a2319f85</td><td>2023-07-18_22-50-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 31.8838</td><td style=\"text-align: right;\">5.82575e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">246900</td><td style=\"text-align: right;\">105.523 </td><td style=\"text-align: right;\">           130.449  </td><td style=\"text-align: right;\">          1.10685 </td><td style=\"text-align: right;\">     130.449  </td><td style=\"text-align: right;\"> 1689688214</td><td style=\"text-align: right;\">                 100</td><td>a2319f85  </td></tr>\n",
       "<tr><td>FSR_Trainable_a2cb264c</td><td>2023-07-18_22-43-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 35.2989</td><td style=\"text-align: right;\">6.77795e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">239804</td><td style=\"text-align: right;\">117.064 </td><td style=\"text-align: right;\">             6.12423</td><td style=\"text-align: right;\">          0.756662</td><td style=\"text-align: right;\">       6.12423</td><td style=\"text-align: right;\"> 1689687785</td><td style=\"text-align: right;\">                   8</td><td>a2cb264c  </td></tr>\n",
       "<tr><td>FSR_Trainable_a61d1336</td><td>2023-07-18_22-39-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 32.1159</td><td style=\"text-align: right;\">5.76235e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">233244</td><td style=\"text-align: right;\">105.685 </td><td style=\"text-align: right;\">            83.0556 </td><td style=\"text-align: right;\">          0.985091</td><td style=\"text-align: right;\">      83.0556 </td><td style=\"text-align: right;\"> 1689687593</td><td style=\"text-align: right;\">                 100</td><td>a61d1336  </td></tr>\n",
       "<tr><td>FSR_Trainable_a65bd78b</td><td>2023-07-18_22-43-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 45.1023</td><td style=\"text-align: right;\">8.53415e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">240030</td><td style=\"text-align: right;\">148.623 </td><td style=\"text-align: right;\">             6.09028</td><td style=\"text-align: right;\">          0.895849</td><td style=\"text-align: right;\">       6.09028</td><td style=\"text-align: right;\"> 1689687796</td><td style=\"text-align: right;\">                   4</td><td>a65bd78b  </td></tr>\n",
       "<tr><td>FSR_Trainable_a8326506</td><td>2023-07-18_22-37-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 77.8435</td><td style=\"text-align: right;\">5.16691e+14</td><td>172.26.215.93</td><td style=\"text-align: right;\">232106</td><td style=\"text-align: right;\">274.306 </td><td style=\"text-align: right;\">             6.10157</td><td style=\"text-align: right;\">          6.10157 </td><td style=\"text-align: right;\">       6.10157</td><td style=\"text-align: right;\"> 1689687452</td><td style=\"text-align: right;\">                   1</td><td>a8326506  </td></tr>\n",
       "<tr><td>FSR_Trainable_a8e4be48</td><td>2023-07-18_22-44-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 58.3597</td><td style=\"text-align: right;\">1.3779e+08 </td><td>172.26.215.93</td><td style=\"text-align: right;\">240986</td><td style=\"text-align: right;\">191.346 </td><td style=\"text-align: right;\">            16.7804 </td><td style=\"text-align: right;\">          8.58291 </td><td style=\"text-align: right;\">      16.7804 </td><td style=\"text-align: right;\"> 1689687846</td><td style=\"text-align: right;\">                   2</td><td>a8e4be48  </td></tr>\n",
       "<tr><td>FSR_Trainable_aa9978a5</td><td>2023-07-18_22-46-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 82.4736</td><td style=\"text-align: right;\">4.89426e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">244769</td><td style=\"text-align: right;\">275.284 </td><td style=\"text-align: right;\">             1.34193</td><td style=\"text-align: right;\">          1.34193 </td><td style=\"text-align: right;\">       1.34193</td><td style=\"text-align: right;\"> 1689687982</td><td style=\"text-align: right;\">                   1</td><td>aa9978a5  </td></tr>\n",
       "<tr><td>FSR_Trainable_b751d296</td><td>2023-07-18_22-40-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 34.13  </td><td style=\"text-align: right;\">6.16013e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">234190</td><td style=\"text-align: right;\">111.822 </td><td style=\"text-align: right;\">            53.8768 </td><td style=\"text-align: right;\">          0.821569</td><td style=\"text-align: right;\">      53.8768 </td><td style=\"text-align: right;\"> 1689687606</td><td style=\"text-align: right;\">                  64</td><td>b751d296  </td></tr>\n",
       "<tr><td>FSR_Trainable_ba4130e6</td><td>2023-07-18_22-41-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 56.0533</td><td style=\"text-align: right;\">1.59361e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">236734</td><td style=\"text-align: right;\">179.827 </td><td style=\"text-align: right;\">             1.95308</td><td style=\"text-align: right;\">          0.857743</td><td style=\"text-align: right;\">       1.95308</td><td style=\"text-align: right;\"> 1689687670</td><td style=\"text-align: right;\">                   2</td><td>ba4130e6  </td></tr>\n",
       "<tr><td>FSR_Trainable_c3031d24</td><td>2023-07-18_22-36-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 84.281 </td><td style=\"text-align: right;\">8.15607e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">230223</td><td style=\"text-align: right;\">278     </td><td style=\"text-align: right;\">             3.6607 </td><td style=\"text-align: right;\">          3.6607  </td><td style=\"text-align: right;\">       3.6607 </td><td style=\"text-align: right;\"> 1689687372</td><td style=\"text-align: right;\">                   1</td><td>c3031d24  </td></tr>\n",
       "<tr><td>FSR_Trainable_c3cab307</td><td>2023-07-18_22-46-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 39.5362</td><td style=\"text-align: right;\">3.73966e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">245202</td><td style=\"text-align: right;\">138.804 </td><td style=\"text-align: right;\">             4.58418</td><td style=\"text-align: right;\">          0.879238</td><td style=\"text-align: right;\">       4.58418</td><td style=\"text-align: right;\"> 1689688002</td><td style=\"text-align: right;\">                   4</td><td>c3cab307  </td></tr>\n",
       "<tr><td>FSR_Trainable_c6531895</td><td>2023-07-18_22-45-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 85.5652</td><td style=\"text-align: right;\">2.80134e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">243327</td><td style=\"text-align: right;\">260.68  </td><td style=\"text-align: right;\">             2.08906</td><td style=\"text-align: right;\">          2.08906 </td><td style=\"text-align: right;\">       2.08906</td><td style=\"text-align: right;\"> 1689687921</td><td style=\"text-align: right;\">                   1</td><td>c6531895  </td></tr>\n",
       "<tr><td>FSR_Trainable_c737bd0b</td><td>2023-07-18_22-44-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 43.1566</td><td style=\"text-align: right;\">9.08132e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">242582</td><td style=\"text-align: right;\">138.913 </td><td style=\"text-align: right;\">             3.52376</td><td style=\"text-align: right;\">          0.662209</td><td style=\"text-align: right;\">       3.52376</td><td style=\"text-align: right;\"> 1689687897</td><td style=\"text-align: right;\">                   4</td><td>c737bd0b  </td></tr>\n",
       "<tr><td>FSR_Trainable_c9a07f03</td><td>2023-07-18_22-49-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 35.8492</td><td style=\"text-align: right;\">6.91533e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">247869</td><td style=\"text-align: right;\">113.364 </td><td style=\"text-align: right;\">            34.615  </td><td style=\"text-align: right;\">          1.41569 </td><td style=\"text-align: right;\">      34.615  </td><td style=\"text-align: right;\"> 1689688147</td><td style=\"text-align: right;\">                  32</td><td>c9a07f03  </td></tr>\n",
       "<tr><td>FSR_Trainable_cc55eb29</td><td>2023-07-18_22-48-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 36.1517</td><td style=\"text-align: right;\">7.29945e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">247628</td><td style=\"text-align: right;\">117.695 </td><td style=\"text-align: right;\">            10.2832 </td><td style=\"text-align: right;\">          1.09767 </td><td style=\"text-align: right;\">      10.2832 </td><td style=\"text-align: right;\"> 1689688104</td><td style=\"text-align: right;\">                   8</td><td>cc55eb29  </td></tr>\n",
       "<tr><td>FSR_Trainable_d2860675</td><td>2023-07-18_22-37-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 48.2684</td><td style=\"text-align: right;\">9.70951e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">229803</td><td style=\"text-align: right;\">164.287 </td><td style=\"text-align: right;\">            83.4531 </td><td style=\"text-align: right;\">          0.666799</td><td style=\"text-align: right;\">      83.4531 </td><td style=\"text-align: right;\"> 1689687446</td><td style=\"text-align: right;\">                 100</td><td>d2860675  </td></tr>\n",
       "<tr><td>FSR_Trainable_d4e691e6</td><td>2023-07-18_22-36-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 86.9212</td><td style=\"text-align: right;\">9.48567e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">230048</td><td style=\"text-align: right;\">271.502 </td><td style=\"text-align: right;\">             1.65732</td><td style=\"text-align: right;\">          1.65732 </td><td style=\"text-align: right;\">       1.65732</td><td style=\"text-align: right;\"> 1689687362</td><td style=\"text-align: right;\">                   1</td><td>d4e691e6  </td></tr>\n",
       "<tr><td>FSR_Trainable_d5d22a70</td><td>2023-07-18_22-40-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 77.6594</td><td style=\"text-align: right;\">3.707e+15  </td><td>172.26.215.93</td><td style=\"text-align: right;\">235319</td><td style=\"text-align: right;\">294.255 </td><td style=\"text-align: right;\">             1.66245</td><td style=\"text-align: right;\">          1.66245 </td><td style=\"text-align: right;\">       1.66245</td><td style=\"text-align: right;\"> 1689687619</td><td style=\"text-align: right;\">                   1</td><td>d5d22a70  </td></tr>\n",
       "<tr><td>FSR_Trainable_d6d16950</td><td>2023-07-18_22-45-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 87.49  </td><td style=\"text-align: right;\">2.68609e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">243038</td><td style=\"text-align: right;\">252.901 </td><td style=\"text-align: right;\">             1.48986</td><td style=\"text-align: right;\">          1.48986 </td><td style=\"text-align: right;\">       1.48986</td><td style=\"text-align: right;\"> 1689687909</td><td style=\"text-align: right;\">                   1</td><td>d6d16950  </td></tr>\n",
       "<tr><td>FSR_Trainable_d6e0b655</td><td>2023-07-18_22-39-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 35.5259</td><td style=\"text-align: right;\">6.76459e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">233928</td><td style=\"text-align: right;\">115.136 </td><td style=\"text-align: right;\">            26.9754 </td><td style=\"text-align: right;\">          0.822867</td><td style=\"text-align: right;\">      26.9754 </td><td style=\"text-align: right;\"> 1689687561</td><td style=\"text-align: right;\">                  32</td><td>d6e0b655  </td></tr>\n",
       "<tr><td>FSR_Trainable_d70340e7</td><td>2023-07-18_22-45-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 83.787 </td><td style=\"text-align: right;\">2.79123e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">243145</td><td style=\"text-align: right;\">261.071 </td><td style=\"text-align: right;\">             1.49128</td><td style=\"text-align: right;\">          1.49128 </td><td style=\"text-align: right;\">       1.49128</td><td style=\"text-align: right;\"> 1689687915</td><td style=\"text-align: right;\">                   1</td><td>d70340e7  </td></tr>\n",
       "<tr><td>FSR_Trainable_d754af28</td><td>2023-07-18_22-48-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 43.7644</td><td style=\"text-align: right;\">1.16035e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">247409</td><td style=\"text-align: right;\">143.232 </td><td style=\"text-align: right;\">             8.76566</td><td style=\"text-align: right;\">          1.93224 </td><td style=\"text-align: right;\">       8.76566</td><td style=\"text-align: right;\"> 1689688091</td><td style=\"text-align: right;\">                   4</td><td>d754af28  </td></tr>\n",
       "<tr><td>FSR_Trainable_ddc57f21</td><td>2023-07-18_22-46-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 35.5439</td><td style=\"text-align: right;\">6.10474e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">243627</td><td style=\"text-align: right;\">111.732 </td><td style=\"text-align: right;\">            51.2936 </td><td style=\"text-align: right;\">          0.704136</td><td style=\"text-align: right;\">      51.2936 </td><td style=\"text-align: right;\"> 1689687988</td><td style=\"text-align: right;\">                  64</td><td>ddc57f21  </td></tr>\n",
       "<tr><td>FSR_Trainable_e06dc26c</td><td>2023-07-18_22-40-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 92.6903</td><td style=\"text-align: right;\">3.81269e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">234878</td><td style=\"text-align: right;\">276.84  </td><td style=\"text-align: right;\">             2.35665</td><td style=\"text-align: right;\">          2.35665 </td><td style=\"text-align: right;\">       2.35665</td><td style=\"text-align: right;\"> 1689687606</td><td style=\"text-align: right;\">                   1</td><td>e06dc26c  </td></tr>\n",
       "<tr><td>FSR_Trainable_e30ebaa5</td><td>2023-07-18_22-38-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 33.8178</td><td style=\"text-align: right;\">6.31453e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">229875</td><td style=\"text-align: right;\">109.37  </td><td style=\"text-align: right;\">           115.295  </td><td style=\"text-align: right;\">          1.16833 </td><td style=\"text-align: right;\">     115.295  </td><td style=\"text-align: right;\"> 1689687488</td><td style=\"text-align: right;\">                 100</td><td>e30ebaa5  </td></tr>\n",
       "<tr><td>FSR_Trainable_e43793b5</td><td>2023-07-18_22-46-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 35.1222</td><td style=\"text-align: right;\">5.72149e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">243868</td><td style=\"text-align: right;\">113.109 </td><td style=\"text-align: right;\">            28.9777 </td><td style=\"text-align: right;\">          0.773139</td><td style=\"text-align: right;\">      28.9777 </td><td style=\"text-align: right;\"> 1689687970</td><td style=\"text-align: right;\">                  32</td><td>e43793b5  </td></tr>\n",
       "<tr><td>FSR_Trainable_e9a68070</td><td>2023-07-18_22-54-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 29.8147</td><td style=\"text-align: right;\">5.01016e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">251882</td><td style=\"text-align: right;\"> 98.9794</td><td style=\"text-align: right;\">            90.4186 </td><td style=\"text-align: right;\">          0.601767</td><td style=\"text-align: right;\">      90.4186 </td><td style=\"text-align: right;\"> 1689688490</td><td style=\"text-align: right;\">                 100</td><td>e9a68070  </td></tr>\n",
       "<tr><td>FSR_Trainable_ea26b751</td><td>2023-07-18_22-40-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 43.6111</td><td style=\"text-align: right;\">7.53767e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">236225</td><td style=\"text-align: right;\">143.223 </td><td style=\"text-align: right;\">             3.9636 </td><td style=\"text-align: right;\">          0.785338</td><td style=\"text-align: right;\">       3.9636 </td><td style=\"text-align: right;\"> 1689687651</td><td style=\"text-align: right;\">                   4</td><td>ea26b751  </td></tr>\n",
       "<tr><td>FSR_Trainable_eadefdfb</td><td>2023-07-18_22-43-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 75.9764</td><td style=\"text-align: right;\">2.04748e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">240488</td><td style=\"text-align: right;\">217.892 </td><td style=\"text-align: right;\">             1.72075</td><td style=\"text-align: right;\">          0.675843</td><td style=\"text-align: right;\">       1.72075</td><td style=\"text-align: right;\"> 1689687810</td><td style=\"text-align: right;\">                   2</td><td>eadefdfb  </td></tr>\n",
       "<tr><td>FSR_Trainable_eb3cd214</td><td>2023-07-18_22-38-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 87.9286</td><td style=\"text-align: right;\">2.9921e+08 </td><td>172.26.215.93</td><td style=\"text-align: right;\">232570</td><td style=\"text-align: right;\">260.876 </td><td style=\"text-align: right;\">            16.1799 </td><td style=\"text-align: right;\">          7.85556 </td><td style=\"text-align: right;\">      16.1799 </td><td style=\"text-align: right;\"> 1689687482</td><td style=\"text-align: right;\">                   2</td><td>eb3cd214  </td></tr>\n",
       "<tr><td>FSR_Trainable_edbdf45b</td><td>2023-07-18_22-36-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 70.0214</td><td style=\"text-align: right;\">2.12476e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">230964</td><td style=\"text-align: right;\">221.959 </td><td style=\"text-align: right;\">             5.94086</td><td style=\"text-align: right;\">          2.84323 </td><td style=\"text-align: right;\">       5.94086</td><td style=\"text-align: right;\"> 1689687406</td><td style=\"text-align: right;\">                   2</td><td>edbdf45b  </td></tr>\n",
       "<tr><td>FSR_Trainable_f147eae0</td><td>2023-07-18_22-44-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 37.4469</td><td style=\"text-align: right;\">6.73227e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">242143</td><td style=\"text-align: right;\">117.759 </td><td style=\"text-align: right;\">            14.4463 </td><td style=\"text-align: right;\">          0.882411</td><td style=\"text-align: right;\">      14.4463 </td><td style=\"text-align: right;\"> 1689687887</td><td style=\"text-align: right;\">                  16</td><td>f147eae0  </td></tr>\n",
       "<tr><td>FSR_Trainable_f1f0a561</td><td>2023-07-18_22-47-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 84.9704</td><td style=\"text-align: right;\">2.63931e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">247216</td><td style=\"text-align: right;\">251.194 </td><td style=\"text-align: right;\">             2.39752</td><td style=\"text-align: right;\">          2.39752 </td><td style=\"text-align: right;\">       2.39752</td><td style=\"text-align: right;\"> 1689688073</td><td style=\"text-align: right;\">                   1</td><td>f1f0a561  </td></tr>\n",
       "<tr><td>FSR_Trainable_f3d4c294</td><td>2023-07-18_22-47-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 36.963 </td><td style=\"text-align: right;\">6.83252e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">246224</td><td style=\"text-align: right;\">118.664 </td><td style=\"text-align: right;\">             6.01144</td><td style=\"text-align: right;\">          0.643152</td><td style=\"text-align: right;\">       6.01144</td><td style=\"text-align: right;\"> 1689688040</td><td style=\"text-align: right;\">                   8</td><td>f3d4c294  </td></tr>\n",
       "<tr><td>FSR_Trainable_f3dbe86b</td><td>2023-07-18_22-41-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 35.8804</td><td style=\"text-align: right;\">6.70866e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">237271</td><td style=\"text-align: right;\">115.194 </td><td style=\"text-align: right;\">            13.4365 </td><td style=\"text-align: right;\">          0.791157</td><td style=\"text-align: right;\">      13.4365 </td><td style=\"text-align: right;\"> 1689687702</td><td style=\"text-align: right;\">                  16</td><td>f3dbe86b  </td></tr>\n",
       "<tr><td>FSR_Trainable_f4cceab3</td><td>2023-07-18_22-49-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 36.7234</td><td style=\"text-align: right;\">7.12995e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">249020</td><td style=\"text-align: right;\">119.432 </td><td style=\"text-align: right;\">             7.85445</td><td style=\"text-align: right;\">          0.856607</td><td style=\"text-align: right;\">       7.85445</td><td style=\"text-align: right;\"> 1689688186</td><td style=\"text-align: right;\">                   8</td><td>f4cceab3  </td></tr>\n",
       "<tr><td>FSR_Trainable_f6699148</td><td>2023-07-18_22-36-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">113.678 </td><td style=\"text-align: right;\">8.89668e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">231170</td><td style=\"text-align: right;\">367.83  </td><td style=\"text-align: right;\">             1.66931</td><td style=\"text-align: right;\">          1.66931 </td><td style=\"text-align: right;\">       1.66931</td><td style=\"text-align: right;\"> 1689687411</td><td style=\"text-align: right;\">                   1</td><td>f6699148  </td></tr>\n",
       "<tr><td>FSR_Trainable_f6d35aea</td><td>2023-07-18_22-41-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 37.4318</td><td style=\"text-align: right;\">7.81944e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">236447</td><td style=\"text-align: right;\">119.418 </td><td style=\"text-align: right;\">             6.26438</td><td style=\"text-align: right;\">          0.867465</td><td style=\"text-align: right;\">       6.26438</td><td style=\"text-align: right;\"> 1689687660</td><td style=\"text-align: right;\">                   8</td><td>f6d35aea  </td></tr>\n",
       "<tr><td>FSR_Trainable_f860c45d</td><td>2023-07-18_22-42-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 84.6556</td><td style=\"text-align: right;\">9.16631e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">238381</td><td style=\"text-align: right;\">271.277 </td><td style=\"text-align: right;\">             2.74396</td><td style=\"text-align: right;\">          2.74396 </td><td style=\"text-align: right;\">       2.74396</td><td style=\"text-align: right;\"> 1689687735</td><td style=\"text-align: right;\">                   1</td><td>f860c45d  </td></tr>\n",
       "<tr><td>FSR_Trainable_f97eec4f</td><td>2023-07-18_22-36-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 83.3455</td><td style=\"text-align: right;\">2.27548e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">230516</td><td style=\"text-align: right;\">262.885 </td><td style=\"text-align: right;\">             3.68286</td><td style=\"text-align: right;\">          3.68286 </td><td style=\"text-align: right;\">       3.68286</td><td style=\"text-align: right;\"> 1689687383</td><td style=\"text-align: right;\">                   1</td><td>f97eec4f  </td></tr>\n",
       "<tr><td>FSR_Trainable_fa43b5fc</td><td>2023-07-18_22-46-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 72.2535</td><td style=\"text-align: right;\">7.54373e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">245539</td><td style=\"text-align: right;\">242.544 </td><td style=\"text-align: right;\">             2.24645</td><td style=\"text-align: right;\">          2.24645 </td><td style=\"text-align: right;\">       2.24645</td><td style=\"text-align: right;\"> 1689688012</td><td style=\"text-align: right;\">                   1</td><td>fa43b5fc  </td></tr>\n",
       "<tr><td>FSR_Trainable_fc48ca5d</td><td>2023-07-18_22-53-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 29.8088</td><td style=\"text-align: right;\">5.38408e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">250996</td><td style=\"text-align: right;\"> 97.2674</td><td style=\"text-align: right;\">           131.794  </td><td style=\"text-align: right;\">          1.41811 </td><td style=\"text-align: right;\">     131.794  </td><td style=\"text-align: right;\"> 1689688405</td><td style=\"text-align: right;\">                 100</td><td>fc48ca5d  </td></tr>\n",
       "<tr><td>FSR_Trainable_fcb10e6b</td><td>2023-07-18_22-37-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 86.1998</td><td style=\"text-align: right;\">2.76003e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">231653</td><td style=\"text-align: right;\">258.579 </td><td style=\"text-align: right;\">             3.30881</td><td style=\"text-align: right;\">          1.52508 </td><td style=\"text-align: right;\">       3.30881</td><td style=\"text-align: right;\"> 1689687433</td><td style=\"text-align: right;\">                   2</td><td>fcb10e6b  </td></tr>\n",
       "<tr><td>FSR_Trainable_fd2ae85c</td><td>2023-07-18_22-44-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 35.6761</td><td style=\"text-align: right;\">5.21257e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">242358</td><td style=\"text-align: right;\">117.59  </td><td style=\"text-align: right;\">             7.14775</td><td style=\"text-align: right;\">          0.743541</td><td style=\"text-align: right;\">       7.14775</td><td style=\"text-align: right;\"> 1689687890</td><td style=\"text-align: right;\">                   8</td><td>fd2ae85c  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_d2860675_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-18_22-35-39/wandb/run-20230718_223550-d2860675\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: Syncing run FSR_Trainable_d2860675\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d2860675\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...047)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_e30ebaa5_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-18_22-35-44/wandb/run-20230718_223557-e30ebaa5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: Syncing run FSR_Trainable_e30ebaa5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e30ebaa5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_d4e691e6_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-18_22-35-51/wandb/run-20230718_223606-d4e691e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: Syncing run FSR_Trainable_d4e691e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d4e691e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb:                      mae 86.92123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb:                     mape 9.485669348298114e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb:                     rmse 271.50222\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb:       time_since_restore 1.65732\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb:         time_this_iter_s 1.65732\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb:             time_total_s 1.65732\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb:                timestamp 1689687362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: 🚀 View run FSR_Trainable_d4e691e6 at: https://wandb.ai/seokjin/FSR-prediction/runs/d4e691e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230221)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223606-d4e691e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_c3031d24_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-18_22-36-00/wandb/run-20230718_223615-c3031d24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: Syncing run FSR_Trainable_c3031d24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c3031d24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb:                      mae 84.28105\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb:                     mape 8.156066436046978e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb:                     rmse 277.99985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb:       time_since_restore 3.6607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb:         time_this_iter_s 3.6607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb:             time_total_s 3.6607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb:                timestamp 1689687372\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: 🚀 View run FSR_Trainable_c3031d24 at: https://wandb.ai/seokjin/FSR-prediction/runs/c3031d24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230398)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223615-c3031d24/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_f97eec4f_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-18_22-36-08/wandb/run-20230718_223626-f97eec4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: Syncing run FSR_Trainable_f97eec4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f97eec4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb:                      mae 83.34546\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb:                     mape 227548103.4546\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb:                     rmse 262.88478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb:       time_since_restore 3.68286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb:         time_this_iter_s 3.68286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb:             time_total_s 3.68286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb:                timestamp 1689687383\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: 🚀 View run FSR_Trainable_f97eec4f at: https://wandb.ai/seokjin/FSR-prediction/runs/f97eec4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230625)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223626-f97eec4f/logs\n",
      "2023-07-18 22:36:33,561\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.752 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:36:33,565\tWARNING util.py:315 -- The `process_trial_result` operation took 1.756 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:36:33,567\tWARNING util.py:315 -- Processing trial results took 1.758 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:36:33,568\tWARNING util.py:315 -- The `process_trial_result` operation took 1.760 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "2023-07-18 22:36:43,705\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.093 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:36:43,709\tWARNING util.py:315 -- The `process_trial_result` operation took 2.097 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:36:43,711\tWARNING util.py:315 -- Processing trial results took 2.100 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:36:43,715\tWARNING util.py:315 -- The `process_trial_result` operation took 2.104 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_a04a1408_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-18_22-36-19/wandb/run-20230718_223635-a04a1408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: Syncing run FSR_Trainable_a04a1408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a04a1408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb:                      mae 87.41116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb:                     mape 1.030899208431891e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb:                     rmse 275.94561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb:       time_since_restore 2.6755\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb:         time_this_iter_s 2.6755\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb:             time_total_s 2.6755\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb:                timestamp 1689687391\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: 🚀 View run FSR_Trainable_a04a1408 at: https://wandb.ai/seokjin/FSR-prediction/runs/a04a1408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230849)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223635-a04a1408/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223635-a04a1408/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223635-a04a1408/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223635-a04a1408/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223635-a04a1408/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223635-a04a1408/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223635-a04a1408/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223635-a04a1408/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223635-a04a1408/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223635-a04a1408/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223645-edbdf45b/logs\n",
      "2023-07-18 22:36:53,277\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.996 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:36:53,283\tWARNING util.py:315 -- The `process_trial_result` operation took 2.003 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:36:53,285\tWARNING util.py:315 -- Processing trial results took 2.005 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:36:53,287\tWARNING util.py:315 -- The `process_trial_result` operation took 2.007 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_f6699148_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-18_22-36-38/wandb/run-20230718_223656-f6699148\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: Syncing run FSR_Trainable_f6699148\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f6699148\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:37:03,367\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.718 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:37:03,372\tWARNING util.py:315 -- The `process_trial_result` operation took 1.724 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:37:03,373\tWARNING util.py:315 -- Processing trial results took 1.726 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:37:03,377\tWARNING util.py:315 -- The `process_trial_result` operation took 1.729 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb:                      mae 113.67845\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb:                     mape 8.896677704457277e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb:                     rmse 367.83037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb:       time_since_restore 1.66931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb:         time_this_iter_s 1.66931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb:             time_total_s 1.66931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb:                timestamp 1689687411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: 🚀 View run FSR_Trainable_f6699148 at: https://wandb.ai/seokjin/FSR-prediction/runs/f6699148\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231308)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223656-f6699148/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_3745123c_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-18_22-36-49/wandb/run-20230718_223706-3745123c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: Syncing run FSR_Trainable_3745123c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3745123c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:37:12,346\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.741 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:37:12,349\tWARNING util.py:315 -- The `process_trial_result` operation took 1.745 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:37:12,351\tWARNING util.py:315 -- Processing trial results took 1.747 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:37:12,353\tWARNING util.py:315 -- The `process_trial_result` operation took 1.749 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb:                      mae 128.96854\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb:                     mape 1.9261840228113165e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb:                     rmse 323.31812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb:       time_since_restore 1.85372\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb:         time_this_iter_s 1.85372\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb:             time_total_s 1.85372\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb:                timestamp 1689687421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: 🚀 View run FSR_Trainable_3745123c at: https://wandb.ai/seokjin/FSR-prediction/runs/3745123c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231536)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223706-3745123c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_fcb10e6b_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-36-59/wandb/run-20230718_223715-fcb10e6b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: Syncing run FSR_Trainable_fcb10e6b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fcb10e6b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:37:21,425\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.811 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:37:21,429\tWARNING util.py:315 -- The `process_trial_result` operation took 1.815 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:37:21,431\tWARNING util.py:315 -- Processing trial results took 1.817 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:37:21,433\tWARNING util.py:315 -- The `process_trial_result` operation took 1.819 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb:                      mae 86.19978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb:                     mape 276002533.79497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb:                     rmse 258.57887\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb:       time_since_restore 3.30881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb:         time_this_iter_s 1.52508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb:             time_total_s 3.30881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb:                timestamp 1689687433\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: 🚀 View run FSR_Trainable_fcb10e6b at: https://wandb.ai/seokjin/FSR-prediction/runs/fcb10e6b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231762)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223715-fcb10e6b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_05fdab89_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-37-08/wandb/run-20230718_223724-05fdab89\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: Syncing run FSR_Trainable_05fdab89\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/05fdab89\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb:                      mae 81.86836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb:                     mape 2216548554956652.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb:                     rmse 272.18821\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb:       time_since_restore 1.66501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb:         time_this_iter_s 1.66501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb:             time_total_s 1.66501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb:                timestamp 1689687439\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: 🚀 View run FSR_Trainable_05fdab89 at: https://wandb.ai/seokjin/FSR-prediction/runs/05fdab89\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=231989)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223724-05fdab89/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb:                      mae █▇▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb:                     mape ██▇▇▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb:                     rmse █▇▇▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb:         time_this_iter_s ▇▅▁▄▄▆▆▃▂▃▆▄▅▅▅▃▃▆▄▂▂▅▅▄█▅▄▂▄▄▂▂▃▄▂▁▃▄▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_a8326506_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-37-17/wandb/run-20230718_223733-a8326506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: Syncing run FSR_Trainable_a8326506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a8326506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a8326506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a8326506\n",
      "2023-07-18 22:37:34,416\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.451 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:37:34,419\tWARNING util.py:315 -- The `process_trial_result` operation took 1.456 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:37:34,422\tWARNING util.py:315 -- Processing trial results took 1.458 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:37:34,424\tWARNING util.py:315 -- The `process_trial_result` operation took 1.460 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=229874)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb:                      mae 77.84347\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb:                     mape 516690590228151.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb:                     rmse 274.30592\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb:       time_since_restore 6.10157\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb:         time_this_iter_s 6.10157\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb:             time_total_s 6.10157\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb:                timestamp 1689687452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: 🚀 View run FSR_Trainable_a8326506 at: https://wandb.ai/seokjin/FSR-prediction/runs/a8326506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223733-a8326506/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232223)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_348c4cf7_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-37-26/wandb/run-20230718_223741-348c4cf7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Syncing run FSR_Trainable_348c4cf7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/348c4cf7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_eb3cd214_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-37-35/wandb/run-20230718_223751-eb3cd214\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: Syncing run FSR_Trainable_eb3cd214\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/eb3cd214\n",
      "2023-07-18 22:37:54,291\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.668 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:37:54,293\tWARNING util.py:315 -- The `process_trial_result` operation took 1.671 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:37:54,295\tWARNING util.py:315 -- Processing trial results took 1.673 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:37:54,296\tWARNING util.py:315 -- The `process_trial_result` operation took 1.674 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:37:56,024\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.651 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:37:56,025\tWARNING util.py:315 -- The `process_trial_result` operation took 1.653 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:37:56,026\tWARNING util.py:315 -- Processing trial results took 1.654 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:37:56,028\tWARNING util.py:315 -- The `process_trial_result` operation took 1.655 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_42d8a5ff_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-37-44/wandb/run-20230718_223803-42d8a5ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: Syncing run FSR_Trainable_42d8a5ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/42d8a5ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb:                      mae 87.92857\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb:                     mape 299210391.85709\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb:                     rmse 260.87622\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb:       time_since_restore 16.17986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb:         time_this_iter_s 7.85556\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb:             time_total_s 16.17986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb:                timestamp 1689687482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: 🚀 View run FSR_Trainable_eb3cd214 at: https://wandb.ai/seokjin/FSR-prediction/runs/eb3cd214\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232664)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223751-eb3cd214/logs\n",
      "2023-07-18 22:38:06,895\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.853 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:38:06,900\tWARNING util.py:315 -- The `process_trial_result` operation took 1.858 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:38:06,902\tWARNING util.py:315 -- Processing trial results took 1.861 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:38:06,904\tWARNING util.py:315 -- The `process_trial_result` operation took 1.863 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb:                      mae █▅▃▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb:                     mape █▄▂▁▁▁▂▂▂▂▂▁▂▂▁▁▁▁▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb:                     rmse █▄▃▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb:         time_this_iter_s ▄▃▄▃▂█▂▂▄▂▃▃▁▅▄▁▄▆▂▄▂▂▅▁▁▄▂▂▃▁▃▃▁▁▄▃▁▄▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb:                      mae 33.81778\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb:                     mape 63145297.94216\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb:                     rmse 109.36995\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb:       time_since_restore 115.29494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb:         time_this_iter_s 1.16833\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb:             time_total_s 115.29494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb:                timestamp 1689687488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: 🚀 View run FSR_Trainable_e30ebaa5 at: https://wandb.ai/seokjin/FSR-prediction/runs/e30ebaa5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=230047)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223557-e30ebaa5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232454)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb:                      mae 86.31911\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb:                     mape 277883567.45536\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb:                     rmse 255.66087\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb:       time_since_restore 15.85284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb:         time_this_iter_s 6.90792\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb:             time_total_s 15.85284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb:                timestamp 1689687493\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: 🚀 View run FSR_Trainable_42d8a5ff at: https://wandb.ai/seokjin/FSR-prediction/runs/42d8a5ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223803-42d8a5ff/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223803-42d8a5ff/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_45e58b43_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-37-56/wandb/run-20230718_223817-45e58b43\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: Syncing run FSR_Trainable_45e58b43\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/45e58b43\n",
      "2023-07-18 22:38:19,966\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.453 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:38:19,968\tWARNING util.py:315 -- The `process_trial_result` operation took 1.455 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:38:19,969\tWARNING util.py:315 -- Processing trial results took 1.456 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:38:19,969\tWARNING util.py:315 -- The `process_trial_result` operation took 1.457 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:38:22,940\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.845 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:38:22,944\tWARNING util.py:315 -- The `process_trial_result` operation took 1.850 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:38:22,947\tWARNING util.py:315 -- Processing trial results took 1.852 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:38:22,950\tWARNING util.py:315 -- The `process_trial_result` operation took 1.855 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=232885)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_a61d1336_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-38-11/wandb/run-20230718_223825-a61d1336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: Syncing run FSR_Trainable_a61d1336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a61d1336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb:                      mae 87.81716\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb:                     mape 283058668.1704\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb:                     rmse 259.08869\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb:       time_since_restore 13.68224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb:         time_this_iter_s 6.7643\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb:             time_total_s 13.68224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb:                timestamp 1689687506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: 🚀 View run FSR_Trainable_45e58b43 at: https://wandb.ai/seokjin/FSR-prediction/runs/45e58b43\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223817-45e58b43/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233124)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-18 22:38:32,663\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.503 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:38:32,667\tWARNING util.py:315 -- The `process_trial_result` operation took 1.508 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:38:32,668\tWARNING util.py:315 -- Processing trial results took 1.509 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:38:32,669\tWARNING util.py:315 -- The `process_trial_result` operation took 1.511 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_29738bc1_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-38-20/wandb/run-20230718_223835-29738bc1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: Syncing run FSR_Trainable_29738bc1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/29738bc1\n",
      "2023-07-18 22:38:42,338\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.604 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:38:42,341\tWARNING util.py:315 -- The `process_trial_result` operation took 1.608 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:38:42,343\tWARNING util.py:315 -- Processing trial results took 1.611 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:38:42,345\tWARNING util.py:315 -- The `process_trial_result` operation took 1.613 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_78c5d427_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-38-29/wandb/run-20230718_223845-78c5d427\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: Syncing run FSR_Trainable_78c5d427\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/78c5d427\n",
      "2023-07-18 22:38:53,254\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.553 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:38:53,257\tWARNING util.py:315 -- The `process_trial_result` operation took 1.557 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:38:53,259\tWARNING util.py:315 -- Processing trial results took 1.559 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:38:53,262\tWARNING util.py:315 -- The `process_trial_result` operation took 1.562 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_d6e0b655_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-38-39/wandb/run-20230718_223856-d6e0b655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: Syncing run FSR_Trainable_d6e0b655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d6e0b655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb:                      mae █▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb:                     mape ██▇▄▃▃▂▂▂▂▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb:                     rmse █▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb:         time_this_iter_s █▃▄▃▂▁▃▁▁▁▄▅█▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb:                timestamp ▁▂▃▃▃▃▄▄▅▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb:                      mae 35.37701\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb:                     mape 64387871.01024\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb:                     rmse 114.45734\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb:       time_since_restore 14.2287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb:         time_this_iter_s 0.77933\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb:             time_total_s 14.2287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb:                timestamp 1689687537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: 🚀 View run FSR_Trainable_78c5d427 at: https://wandb.ai/seokjin/FSR-prediction/runs/78c5d427\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233799)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223845-78c5d427/logs\n",
      "2023-07-18 22:39:09,506\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.626 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:39:09,510\tWARNING util.py:315 -- The `process_trial_result` operation took 1.631 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:39:09,512\tWARNING util.py:315 -- Processing trial results took 1.632 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:39:09,513\tWARNING util.py:315 -- The `process_trial_result` operation took 1.634 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_b751d296_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-38-50/wandb/run-20230718_223912-b751d296\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: Syncing run FSR_Trainable_b751d296\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b751d296\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb:                      mae █▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb:                     mape █▃▂▃▂▂▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▃▂▂▃▄▄▄▄▄▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb:                     rmse █▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb:         time_this_iter_s █▇▆▅▃▃▄▃▃▃▂▂▂▂▂▂▃▆▅▅▂▄▃▂▂▃▂▃▃▁▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb:                      mae 35.52588\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb:                     mape 67645923.57943\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb:                     rmse 115.13578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb:       time_since_restore 26.97543\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb:         time_this_iter_s 0.82287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb:             time_total_s 26.97543\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb:                timestamp 1689687561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: 🚀 View run FSR_Trainable_d6e0b655 at: https://wandb.ai/seokjin/FSR-prediction/runs/d6e0b655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234014)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223856-d6e0b655/logs\n",
      "2023-07-18 22:39:33,376\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.873 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:39:33,378\tWARNING util.py:315 -- The `process_trial_result` operation took 1.876 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:39:33,382\tWARNING util.py:315 -- Processing trial results took 1.881 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:39:33,383\tWARNING util.py:315 -- The `process_trial_result` operation took 1.882 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_27d2a402_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-39-06/wandb/run-20230718_223936-27d2a402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: Syncing run FSR_Trainable_27d2a402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/27d2a402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb:                      mae █▆▂▁▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb:                     mape ▁▆▁▄▇▅██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb:                     rmse █▇▃▃▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb:         time_this_iter_s █▄▂▂▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb:                timestamp ▁▄▅▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb:                      mae 37.37623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb:                     mape 74994642.92394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb:                     rmse 123.69446\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb:       time_since_restore 7.09602\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb:         time_this_iter_s 0.79799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb:             time_total_s 7.09602\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb:                timestamp 1689687579\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: 🚀 View run FSR_Trainable_27d2a402 at: https://wandb.ai/seokjin/FSR-prediction/runs/27d2a402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234494)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223936-27d2a402/logs\n",
      "2023-07-18 22:39:54,912\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.767 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:39:54,916\tWARNING util.py:315 -- The `process_trial_result` operation took 1.772 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:39:54,918\tWARNING util.py:315 -- Processing trial results took 1.774 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:39:54,920\tWARNING util.py:315 -- The `process_trial_result` operation took 1.776 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_121adfd9_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-39-30/wandb/run-20230718_223956-121adfd9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: Syncing run FSR_Trainable_121adfd9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/121adfd9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb:                      mae █▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb:                     mape █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb:                     rmse █▄▂▂▂▂▂▂▁▁▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb:         time_this_iter_s █▄▃▁▅▄▃▂▃▆▅▄▄▆▇▅▄▄▃▄▇▅▅▄▄▅▅▄▄▄█▆▅▅▅▇▄▅▆▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb:                      mae 32.1159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb:                     mape 57623487.00876\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb:                     rmse 105.68473\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb:       time_since_restore 83.05561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb:         time_this_iter_s 0.98509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb:             time_total_s 83.05561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb:                timestamp 1689687593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: 🚀 View run FSR_Trainable_a61d1336 at: https://wandb.ai/seokjin/FSR-prediction/runs/a61d1336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233359)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223825-a61d1336/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223956-121adfd9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234733)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:40:08,069\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.715 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:40:08,072\tWARNING util.py:315 -- The `process_trial_result` operation took 1.719 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:40:08,077\tWARNING util.py:315 -- Processing trial results took 1.724 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:40:08,080\tWARNING util.py:315 -- The `process_trial_result` operation took 1.727 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb:                      mae █▃▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb:                     mape █▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb:                     rmse █▃▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb:         time_this_iter_s █▃▂▁▂▄▃▃▃▄▄▄▃▂▃▄▅▄▃▂▃▃▃▂▃▂▃▄▃▃▄▄▄▃▆▅▃▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb:                      mae 34.06578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb:                     mape 49061554.9441\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb:                     rmse 109.7459\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb:       time_since_restore 83.43311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb:         time_this_iter_s 0.68357\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb:             time_total_s 83.43311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb:                timestamp 1689687603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: 🚀 View run FSR_Trainable_29738bc1 at: https://wandb.ai/seokjin/FSR-prediction/runs/29738bc1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=233580)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_223835-29738bc1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb:                      mae █▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb:                     mape █▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb:                     rmse █▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb:         time_this_iter_s █▅▄▄▃▃▃▃▃▃▃▂▃▃▃▃▂▄▄▄▄▃▁▄▇▄▄▄▄▄▇▆▄▃▂▂▂▁▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_e06dc26c_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-39-50/wandb/run-20230718_224009-e06dc26c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: Syncing run FSR_Trainable_e06dc26c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e06dc26c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-07-18 22:40:14,337\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.625 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:40:14,341\tWARNING util.py:315 -- The `process_trial_result` operation took 1.629 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:40:14,343\tWARNING util.py:315 -- Processing trial results took 1.631 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:40:14,345\tWARNING util.py:315 -- The `process_trial_result` operation took 1.633 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234248)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb:                      mae 92.69032\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb:                     mape 381268823.35155\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb:                     rmse 276.83992\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb:       time_since_restore 2.35665\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb:         time_this_iter_s 2.35665\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb:             time_total_s 2.35665\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb:                timestamp 1689687606\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: 🚀 View run FSR_Trainable_e06dc26c at: https://wandb.ai/seokjin/FSR-prediction/runs/e06dc26c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=234980)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224009-e06dc26c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_55359899_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-40-04/wandb/run-20230718_224016-55359899\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Syncing run FSR_Trainable_55359899\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/55359899\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:40:21,440\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.648 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:40:21,446\tWARNING util.py:315 -- The `process_trial_result` operation took 1.654 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:40:21,447\tWARNING util.py:315 -- Processing trial results took 1.655 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:40:21,448\tWARNING util.py:315 -- The `process_trial_result` operation took 1.656 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224016-55359899/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224016-55359899/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224016-55359899/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224016-55359899/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224016-55359899/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224016-55359899/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224016-55359899/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224016-55359899/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224016-55359899/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224016-55359899/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224016-55359899/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224016-55359899/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224016-55359899/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235198)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224016-55359899/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...441)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_d5d22a70_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-40-11/wandb/run-20230718_224023-d5d22a70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: Syncing run FSR_Trainable_d5d22a70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d5d22a70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:40:27,680\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.645 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:40:27,684\tWARNING util.py:315 -- The `process_trial_result` operation took 1.650 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:40:27,686\tWARNING util.py:315 -- Processing trial results took 1.652 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:40:27,688\tWARNING util.py:315 -- The `process_trial_result` operation took 1.654 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb:                      mae 77.65938\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb:                     mape 3706996347755249.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb:                     rmse 294.25463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb:       time_since_restore 1.66245\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb:         time_this_iter_s 1.66245\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb:             time_total_s 1.66245\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb:                timestamp 1689687619\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: 🚀 View run FSR_Trainable_d5d22a70 at: https://wandb.ai/seokjin/FSR-prediction/runs/d5d22a70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235441)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224023-d5d22a70/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...663)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_1860b485_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-40-18/wandb/run-20230718_224030-1860b485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: Syncing run FSR_Trainable_1860b485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1860b485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:40:34,591\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.729 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:40:34,594\tWARNING util.py:315 -- The `process_trial_result` operation took 1.734 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:40:34,597\tWARNING util.py:315 -- Processing trial results took 1.736 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:40:34,599\tWARNING util.py:315 -- The `process_trial_result` operation took 1.738 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb:                      mae 65.43359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb:                     mape 6863841324529800.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb:                     rmse 225.8391\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb:       time_since_restore 2.26903\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb:         time_this_iter_s 0.93637\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb:             time_total_s 2.26903\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb:                timestamp 1689687628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: 🚀 View run FSR_Trainable_1860b485 at: https://wandb.ai/seokjin/FSR-prediction/runs/1860b485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235663)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224030-1860b485/logs\n",
      "wandb: \\ Waiting for wandb.init()...885)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_0bbbc27e_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-40-24/wandb/run-20230718_224036-0bbbc27e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: Syncing run FSR_Trainable_0bbbc27e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0bbbc27e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:40:41,854\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.701 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:40:41,858\tWARNING util.py:315 -- The `process_trial_result` operation took 1.705 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:40:41,860\tWARNING util.py:315 -- Processing trial results took 1.707 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:40:41,862\tWARNING util.py:315 -- The `process_trial_result` operation took 1.709 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: / 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb:                      mae █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb:                     mape █▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb:                     rmse █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb:         time_this_iter_s █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb:                      mae 42.35494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb:                     mape 42691058.94947\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb:                     rmse 139.2023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb:       time_since_restore 3.9401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb:         time_this_iter_s 0.78318\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb:             time_total_s 3.9401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb:                timestamp 1689687637\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: 🚀 View run FSR_Trainable_0bbbc27e at: https://wandb.ai/seokjin/FSR-prediction/runs/0bbbc27e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=235885)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224036-0bbbc27e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_5a5db314_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-40-31/wandb/run-20230718_224044-5a5db314\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Syncing run FSR_Trainable_5a5db314\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5a5db314\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:40:48,946\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.678 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:40:48,949\tWARNING util.py:315 -- The `process_trial_result` operation took 1.682 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:40:48,950\tWARNING util.py:315 -- Processing trial results took 1.683 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:40:48,952\tWARNING util.py:315 -- The `process_trial_result` operation took 1.685 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb:                      mae █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb:                     mape █▇▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb:         time_this_iter_s █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224044-5a5db314/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_ea26b751_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-40-38/wandb/run-20230718_224050-ea26b751\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: Syncing run FSR_Trainable_ea26b751\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ea26b751\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)04 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:40:55,651\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.526 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:40:55,655\tWARNING util.py:315 -- The `process_trial_result` operation took 1.531 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:40:55,656\tWARNING util.py:315 -- Processing trial results took 1.532 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:40:55,658\tWARNING util.py:315 -- The `process_trial_result` operation took 1.534 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb:                      mae █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb:                     mape █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb:                     rmse █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb:         time_this_iter_s █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb:                      mae 43.61113\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb:                     mape 75376682.24549\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb:                     rmse 143.22262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb:       time_since_restore 3.9636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb:         time_this_iter_s 0.78534\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb:             time_total_s 3.9636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb:                timestamp 1689687651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: 🚀 View run FSR_Trainable_ea26b751 at: https://wandb.ai/seokjin/FSR-prediction/runs/ea26b751\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236332)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224050-ea26b751/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_f6d35aea_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-40-45/wandb/run-20230718_224058-f6d35aea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: Syncing run FSR_Trainable_f6d35aea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f6d35aea\n",
      "2023-07-18 22:41:02,674\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.877 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:41:02,677\tWARNING util.py:315 -- The `process_trial_result` operation took 1.881 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:41:02,679\tWARNING util.py:315 -- Processing trial results took 1.883 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:41:02,681\tWARNING util.py:315 -- The `process_trial_result` operation took 1.885 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_7dce2c2a_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-40-52/wandb/run-20230718_224105-7dce2c2a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: Syncing run FSR_Trainable_7dce2c2a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7dce2c2a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb:                      mae █▄▃▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb:                     mape █▄▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb:                     rmse █▅▃▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb:         time_this_iter_s █▃▅▂▂▁▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb:                timestamp ▁▃▅▅▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb:                      mae 37.43182\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb:                     mape 78194436.10908\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb:                     rmse 119.41822\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb:       time_since_restore 6.26438\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb:         time_this_iter_s 0.86746\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb:             time_total_s 6.26438\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb:                timestamp 1689687660\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: 🚀 View run FSR_Trainable_f6d35aea at: https://wandb.ai/seokjin/FSR-prediction/runs/f6d35aea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236555)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224058-f6d35aea/logs\n",
      "2023-07-18 22:41:09,270\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.591 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:41:09,271\tWARNING util.py:315 -- The `process_trial_result` operation took 1.594 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:41:09,273\tWARNING util.py:315 -- Processing trial results took 1.596 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:41:09,274\tWARNING util.py:315 -- The `process_trial_result` operation took 1.597 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_ba4130e6_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-40-59/wandb/run-20230718_224111-ba4130e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: Syncing run FSR_Trainable_ba4130e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ba4130e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb:                      mae 56.05332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb:                     mape 159361335.86494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb:                     rmse 179.82669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb:       time_since_restore 1.95308\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb:         time_this_iter_s 0.85774\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb:             time_total_s 1.95308\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb:                timestamp 1689687670\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: 🚀 View run FSR_Trainable_ba4130e6 at: https://wandb.ai/seokjin/FSR-prediction/runs/ba4130e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236913)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224111-ba4130e6/logs\n",
      "2023-07-18 22:41:18,075\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.677 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:41:18,078\tWARNING util.py:315 -- The `process_trial_result` operation took 1.681 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:41:18,080\tWARNING util.py:315 -- Processing trial results took 1.683 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:41:18,082\tWARNING util.py:315 -- The `process_trial_result` operation took 1.685 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_25957da4_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-41-06/wandb/run-20230718_224120-25957da4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: Syncing run FSR_Trainable_25957da4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/25957da4\n",
      "2023-07-18 22:41:27,989\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.705 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:41:27,991\tWARNING util.py:315 -- The `process_trial_result` operation took 1.708 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:41:27,993\tWARNING util.py:315 -- Processing trial results took 1.710 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:41:27,995\tWARNING util.py:315 -- The `process_trial_result` operation took 1.712 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_f3dbe86b_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-41-15/wandb/run-20230718_224130-f3dbe86b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: Syncing run FSR_Trainable_f3dbe86b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f3dbe86b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb:                      mae █▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb:                     mape █▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb:                     rmse █▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▄▅▅▆▆▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb:         time_this_iter_s █▄▅▃▁▂▃▁▁▂▄█▇█▆▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▄▅▅▆▆▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb:                timestamp ▁▂▂▃▃▃▄▄▅▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb:                      mae 35.59374\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb:                     mape 56740399.56388\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb:                     rmse 114.12309\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb:       time_since_restore 12.78429\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb:         time_this_iter_s 0.79883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb:             time_total_s 12.78429\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb:                timestamp 1689687691\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: 🚀 View run FSR_Trainable_25957da4 at: https://wandb.ai/seokjin/FSR-prediction/runs/25957da4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237135)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224120-25957da4/logs\n",
      "2023-07-18 22:41:38,619\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.665 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:41:38,622\tWARNING util.py:315 -- The `process_trial_result` operation took 1.669 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:41:38,623\tWARNING util.py:315 -- Processing trial results took 1.671 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:41:38,625\tWARNING util.py:315 -- The `process_trial_result` operation took 1.672 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_85aca320_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-41-25/wandb/run-20230718_224141-85aca320\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: Syncing run FSR_Trainable_85aca320\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/85aca320\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb:                      mae █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb:                     mape █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb:                     rmse █▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb:       time_since_restore ▁▁▂▃▃▃▄▄▅▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb:         time_this_iter_s █▅▇▄▃▃▃▁▁▁▃█▅▄▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb:             time_total_s ▁▁▂▃▃▃▄▄▅▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb:                timestamp ▁▂▂▃▃▄▄▄▅▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb:                      mae 35.88041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb:                     mape 67086578.96994\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb:                     rmse 115.19446\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb:       time_since_restore 13.43654\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb:         time_this_iter_s 0.79116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb:             time_total_s 13.43654\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb:                timestamp 1689687702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: 🚀 View run FSR_Trainable_f3dbe86b at: https://wandb.ai/seokjin/FSR-prediction/runs/f3dbe86b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237356)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224130-f3dbe86b/logs\n",
      "2023-07-18 22:41:49,153\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.451 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:41:49,155\tWARNING util.py:315 -- The `process_trial_result` operation took 1.454 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:41:49,156\tWARNING util.py:315 -- Processing trial results took 1.455 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:41:49,159\tWARNING util.py:315 -- The `process_trial_result` operation took 1.458 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_0d75f4f5_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-41-35/wandb/run-20230718_224152-0d75f4f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Syncing run FSR_Trainable_0d75f4f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0d75f4f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb:                      mae █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb:                     mape █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb:                     rmse █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb:       time_since_restore ▁▂▂▃▃▃▄▄▅▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb:         time_this_iter_s █▅▅▄▃▃▁▂▁▁▄▆▆▅▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb:             time_total_s ▁▂▂▃▃▃▄▄▅▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb:                timestamp ▁▂▃▃▄▄▄▅▅▅▅▆▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb:                      mae 35.27069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb:                     mape 50776528.41718\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb:                     rmse 115.55006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb:       time_since_restore 13.651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb:         time_this_iter_s 0.75313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb:             time_total_s 13.651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb:                timestamp 1689687712\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: 🚀 View run FSR_Trainable_85aca320 at: https://wandb.ai/seokjin/FSR-prediction/runs/85aca320\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237583)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224141-85aca320/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb:                      mae ██▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb:                     mape ▄█▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb:                     rmse ▆█▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb:         time_this_iter_s █▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224152-0d75f4f5/logs\n",
      "2023-07-18 22:41:59,577\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.718 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:41:59,583\tWARNING util.py:315 -- The `process_trial_result` operation took 1.724 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:41:59,584\tWARNING util.py:315 -- Processing trial results took 1.726 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:41:59,586\tWARNING util.py:315 -- The `process_trial_result` operation took 1.728 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_7d853856_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-41-46/wandb/run-20230718_224201-7d853856\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: Syncing run FSR_Trainable_7d853856\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7d853856\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=237807)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb:                      mae 88.15268\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb:                     mape 301060661.65521\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb:                     rmse 261.75091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb:       time_since_restore 1.49129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb:         time_this_iter_s 1.49129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb:             time_total_s 1.49129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb:                timestamp 1689687717\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: 🚀 View run FSR_Trainable_7d853856 at: https://wandb.ai/seokjin/FSR-prediction/runs/7d853856\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224201-7d853856/logs\n",
      "2023-07-18 22:42:08,169\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.798 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:42:08,173\tWARNING util.py:315 -- The `process_trial_result` operation took 1.802 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:42:08,175\tWARNING util.py:315 -- Processing trial results took 1.804 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:42:08,177\tWARNING util.py:315 -- The `process_trial_result` operation took 1.806 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238039)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_5be81a4e_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-41-56/wandb/run-20230718_224212-5be81a4e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: Syncing run FSR_Trainable_5be81a4e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5be81a4e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:42:16,938\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.749 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:42:16,941\tWARNING util.py:315 -- The `process_trial_result` operation took 1.753 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:42:16,942\tWARNING util.py:315 -- Processing trial results took 1.754 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:42:16,944\tWARNING util.py:315 -- The `process_trial_result` operation took 1.756 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb:                      mae 111.84575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb:                     mape 1.240411020156237e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb:                     rmse 321.8025\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb:       time_since_restore 1.66782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb:         time_this_iter_s 1.66782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb:             time_total_s 1.66782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb:                timestamp 1689687726\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: 🚀 View run FSR_Trainable_5be81a4e at: https://wandb.ai/seokjin/FSR-prediction/runs/5be81a4e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238266)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224212-5be81a4e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224218-f860c45d/logs\n",
      "2023-07-18 22:42:24,592\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.802 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:42:24,595\tWARNING util.py:315 -- The `process_trial_result` operation took 1.807 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:42:24,597\tWARNING util.py:315 -- Processing trial results took 1.808 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:42:24,598\tWARNING util.py:315 -- The `process_trial_result` operation took 1.809 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_33f93632_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-42-12/wandb/run-20230718_224226-33f93632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: Syncing run FSR_Trainable_33f93632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/33f93632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:42:31,862\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.754 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:42:31,867\tWARNING util.py:315 -- The `process_trial_result` operation took 1.761 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:42:31,869\tWARNING util.py:315 -- Processing trial results took 1.762 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:42:31,871\tWARNING util.py:315 -- The `process_trial_result` operation took 1.764 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb:                      mae 81.56397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb:                     mape 7.500161001626518e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb:                     rmse 269.25773\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb:       time_since_restore 2.13261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb:         time_this_iter_s 2.13261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb:             time_total_s 2.13261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb:                timestamp 1689687742\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: 🚀 View run FSR_Trainable_33f93632 at: https://wandb.ai/seokjin/FSR-prediction/runs/33f93632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238712)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224226-33f93632/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb:                      mae █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb:                     mape █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb:                     rmse █▃▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb:         time_this_iter_s █▃▂▆▄▁▁▃▅▄▄▃▆▆▄▃▇▆▅▅▃█▆▂▂█▄▃▂▆▃▁▃▆▂▃▇▅▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224105-7dce2c2a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_4a95a76d_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-42-20/wandb/run-20230718_224233-4a95a76d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: Syncing run FSR_Trainable_4a95a76d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4a95a76d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=236733)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:42:38,567\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.463 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:42:38,570\tWARNING util.py:315 -- The `process_trial_result` operation took 1.466 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:42:38,572\tWARNING util.py:315 -- Processing trial results took 1.469 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:42:38,574\tWARNING util.py:315 -- The `process_trial_result` operation took 1.470 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb:                      mae 79.2057\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb:                     mape 7.919972878379034e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb:                     rmse 265.29447\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb:       time_since_restore 1.42628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb:         time_this_iter_s 1.42628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb:             time_total_s 1.42628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb:                timestamp 1689687750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: 🚀 View run FSR_Trainable_4a95a76d at: https://wandb.ai/seokjin/FSR-prediction/runs/4a95a76d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224233-4a95a76d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_720cd2c0_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-42-28/wandb/run-20230718_224241-720cd2c0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: Syncing run FSR_Trainable_720cd2c0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/720cd2c0\n",
      "2023-07-18 22:42:50,674\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.021 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:42:50,677\tWARNING util.py:315 -- The `process_trial_result` operation took 2.025 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:42:50,678\tWARNING util.py:315 -- Processing trial results took 2.027 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:42:50,680\tWARNING util.py:315 -- The `process_trial_result` operation took 2.029 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=238935)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_503b8d2a_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-42-35/wandb/run-20230718_224253-503b8d2a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb: Syncing run FSR_Trainable_503b8d2a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/503b8d2a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb:                      mae █▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb:                     mape █▃▃▃▃▂▂▂▁▁▁▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb:                     rmse █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb:         time_this_iter_s █▄▃▃▂▅▃▃▃▂▄▆▄▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb:                timestamp ▁▂▂▃▃▃▄▄▄▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb:                      mae 35.07761\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb:                     mape 56536716.6434\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb:                     rmse 114.51339\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb:       time_since_restore 14.89593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb:         time_this_iter_s 0.7234\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb:             time_total_s 14.89593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb:                timestamp 1689687774\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: 🚀 View run FSR_Trainable_720cd2c0 at: https://wandb.ai/seokjin/FSR-prediction/runs/720cd2c0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224241-720cd2c0/logs\n",
      "2023-07-18 22:43:00,438\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.749 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:43:00,443\tWARNING util.py:315 -- The `process_trial_result` operation took 1.754 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:43:00,444\tWARNING util.py:315 -- Processing trial results took 1.756 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:43:00,446\tWARNING util.py:315 -- The `process_trial_result` operation took 1.757 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239169)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_a2cb264c_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-42-47/wandb/run-20230718_224303-a2cb264c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: Syncing run FSR_Trainable_a2cb264c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a2cb264c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb:                      mae █▄▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb:                     mape █▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb:                     rmse █▄▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb:         time_this_iter_s █▄▄▄▂▁▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb:                timestamp ▁▄▅▅▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb:                      mae 35.29888\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb:                     mape 67779469.72695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb:                     rmse 117.0642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb:       time_since_restore 6.12423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb:         time_this_iter_s 0.75666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb:             time_total_s 6.12423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb:                timestamp 1689687785\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: 🚀 View run FSR_Trainable_a2cb264c at: https://wandb.ai/seokjin/FSR-prediction/runs/a2cb264c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239896)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224303-a2cb264c/logs\n",
      "2023-07-18 22:43:13,052\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.686 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:43:13,058\tWARNING util.py:315 -- The `process_trial_result` operation took 2.694 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:43:13,060\tWARNING util.py:315 -- Processing trial results took 2.696 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:43:13,061\tWARNING util.py:315 -- The `process_trial_result` operation took 2.697 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_a65bd78b_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-42-57/wandb/run-20230718_224315-a65bd78b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: Syncing run FSR_Trainable_a65bd78b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a65bd78b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:43:21,622\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.585 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:43:21,625\tWARNING util.py:315 -- The `process_trial_result` operation took 1.588 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:43:21,628\tWARNING util.py:315 -- Processing trial results took 1.592 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:43:21,629\tWARNING util.py:315 -- The `process_trial_result` operation took 1.593 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb:                      mae █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb:                     mape █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb:                     rmse █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb:         time_this_iter_s █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb:                timestamp ▁▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb:                      mae 45.10227\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb:                     mape 85341528.02441\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb:                     rmse 148.62338\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb:       time_since_restore 6.09028\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb:         time_this_iter_s 0.89585\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb:             time_total_s 6.09028\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb:                timestamp 1689687796\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: 🚀 View run FSR_Trainable_a65bd78b at: https://wandb.ai/seokjin/FSR-prediction/runs/a65bd78b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240120)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224315-a65bd78b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224315-a65bd78b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb:                      mae █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb:                     mape █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb:                     rmse █▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb:         time_this_iter_s ▇▅▄▃▃▂▂▁▁▁▂▄▃▃▃▂▂▂▃▂▃▇█▇▄▅▂▃▁▂▂▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb:                timestamp ▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_678d2469_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-43-07/wandb/run-20230718_224323-678d2469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: Syncing run FSR_Trainable_678d2469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/678d2469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: iterations_since_restore 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb:                      mae 83.42645\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb:                     mape 186243927.57494\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb:                     rmse 258.9768\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb:       time_since_restore 1.52133\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb:         time_this_iter_s 1.52133\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb:             time_total_s 1.52133\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb:                timestamp 1689687799\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb:       training_iteration 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=239670)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: 🚀 View run FSR_Trainable_678d2469 at: https://wandb.ai/seokjin/FSR-prediction/runs/678d2469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240347)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224323-678d2469/logs\n",
      "2023-07-18 22:43:29,450\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.776 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:43:29,453\tWARNING util.py:315 -- The `process_trial_result` operation took 1.780 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:43:29,454\tWARNING util.py:315 -- Processing trial results took 1.781 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:43:29,455\tWARNING util.py:315 -- The `process_trial_result` operation took 1.783 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_eadefdfb_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-43-18/wandb/run-20230718_224331-eadefdfb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: Syncing run FSR_Trainable_eadefdfb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/eadefdfb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:43:35,422\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.424 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:43:35,426\tWARNING util.py:315 -- The `process_trial_result` operation took 1.429 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:43:35,427\tWARNING util.py:315 -- Processing trial results took 1.430 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:43:35,428\tWARNING util.py:315 -- The `process_trial_result` operation took 1.431 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb:                      mae 75.97644\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb:                     mape 204747715.15966\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb:                     rmse 217.89154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb:       time_since_restore 1.72075\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb:         time_this_iter_s 0.67584\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb:             time_total_s 1.72075\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb:                timestamp 1689687810\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: 🚀 View run FSR_Trainable_eadefdfb at: https://wandb.ai/seokjin/FSR-prediction/runs/eadefdfb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240581)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224331-eadefdfb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_3a9b8ffd_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-43-26/wandb/run-20230718_224337-3a9b8ffd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Syncing run FSR_Trainable_3a9b8ffd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3a9b8ffd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: 🚀 View run FSR_Trainable_3a9b8ffd at: https://wandb.ai/seokjin/FSR-prediction/runs/3a9b8ffd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224337-3a9b8ffd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224337-3a9b8ffd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224337-3a9b8ffd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224337-3a9b8ffd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224337-3a9b8ffd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224337-3a9b8ffd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224337-3a9b8ffd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224337-3a9b8ffd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224337-3a9b8ffd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224337-3a9b8ffd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224337-3a9b8ffd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224337-3a9b8ffd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240764)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224337-3a9b8ffd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_948308b0_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-43-32/wandb/run-20230718_224345-948308b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: Syncing run FSR_Trainable_948308b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/948308b0\n",
      "2023-07-18 22:43:46,977\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.403 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:43:46,980\tWARNING util.py:315 -- The `process_trial_result` operation took 1.407 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:43:46,982\tWARNING util.py:315 -- Processing trial results took 1.409 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:43:46,984\tWARNING util.py:315 -- The `process_trial_result` operation took 1.411 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)03 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb:                      mae 78.64482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb:                     mape 2668568936011820.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb:                     rmse 275.1978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb:       time_since_restore 5.1311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb:         time_this_iter_s 5.1311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb:             time_total_s 5.1311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb:                timestamp 1689687825\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: 🚀 View run FSR_Trainable_948308b0 at: https://wandb.ai/seokjin/FSR-prediction/runs/948308b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=240985)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224345-948308b0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_a8e4be48_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-43-40/wandb/run-20230718_224352-a8e4be48\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: Syncing run FSR_Trainable_a8e4be48\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a8e4be48\n",
      "2023-07-18 22:43:57,446\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.041 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:43:57,449\tWARNING util.py:315 -- The `process_trial_result` operation took 2.044 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:43:57,452\tWARNING util.py:315 -- Processing trial results took 2.047 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:43:57,454\tWARNING util.py:315 -- The `process_trial_result` operation took 2.049 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:43:59,629\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.079 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:43:59,632\tWARNING util.py:315 -- The `process_trial_result` operation took 2.085 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:43:59,634\tWARNING util.py:315 -- Processing trial results took 2.087 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:43:59,635\tWARNING util.py:315 -- The `process_trial_result` operation took 2.088 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_1c185ec6_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-43-47/wandb/run-20230718_224401-1c185ec6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: Syncing run FSR_Trainable_1c185ec6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1c185ec6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb:                      mae 70.36868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb:                     mape 177043511.4195\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb:                     rmse 209.58529\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb:       time_since_restore 3.61413\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb:         time_this_iter_s 1.69059\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb:             time_total_s 3.61413\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb:                timestamp 1689687841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: 🚀 View run FSR_Trainable_1c185ec6 at: https://wandb.ai/seokjin/FSR-prediction/runs/1c185ec6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241374)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224401-1c185ec6/logs\n",
      "2023-07-18 22:44:07,094\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.831 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:44:07,096\tWARNING util.py:315 -- The `process_trial_result` operation took 1.835 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:44:07,102\tWARNING util.py:315 -- Processing trial results took 1.840 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:44:07,106\tWARNING util.py:315 -- The `process_trial_result` operation took 1.844 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_436306bd_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-43-55/wandb/run-20230718_224408-436306bd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb: Syncing run FSR_Trainable_436306bd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/436306bd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224352-a8e4be48/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241167)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:44:13,546\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.425 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:44:13,548\tWARNING util.py:315 -- The `process_trial_result` operation took 1.428 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:44:13,550\tWARNING util.py:315 -- Processing trial results took 1.430 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:44:13,551\tWARNING util.py:315 -- The `process_trial_result` operation took 1.431 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb:                      mae 92.20342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb:                     mape 291736264.92158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb:                     rmse 260.57823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb:       time_since_restore 2.12022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb:         time_this_iter_s 2.12022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb:             time_total_s 2.12022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb:                timestamp 1689687845\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb: 🚀 View run FSR_Trainable_436306bd at: https://wandb.ai/seokjin/FSR-prediction/runs/436306bd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224408-436306bd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_2b50aae9_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-44-03/wandb/run-20230718_224416-2b50aae9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: Syncing run FSR_Trainable_2b50aae9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2b50aae9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241563)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-18 22:44:22,283\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.595 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:44:22,286\tWARNING util.py:315 -- The `process_trial_result` operation took 1.599 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:44:22,289\tWARNING util.py:315 -- Processing trial results took 1.601 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:44:22,291\tWARNING util.py:315 -- The `process_trial_result` operation took 1.604 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_6341284f_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-44-10/wandb/run-20230718_224424-6341284f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: Syncing run FSR_Trainable_6341284f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6341284f\n",
      "2023-07-18 22:44:32,348\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.753 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:44:32,350\tWARNING util.py:315 -- The `process_trial_result` operation took 1.756 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:44:32,351\tWARNING util.py:315 -- Processing trial results took 1.757 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:44:32,352\tWARNING util.py:315 -- The `process_trial_result` operation took 1.758 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...227)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_f147eae0_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-44-19/wandb/run-20230718_224435-f147eae0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: Syncing run FSR_Trainable_f147eae0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f147eae0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:44:44,190\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.840 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:44:44,193\tWARNING util.py:315 -- The `process_trial_result` operation took 1.844 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:44:44,197\tWARNING util.py:315 -- Processing trial results took 1.847 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:44:44,199\tWARNING util.py:315 -- The `process_trial_result` operation took 1.849 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb:                      mae █▅▄▃▂▂▁▁▁▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb:                     mape █▄▂▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▃▃▂▃▃▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb:                     rmse █▅▄▃▂▂▂▂▁▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb:         time_this_iter_s █▃▃▂▂▁▁▁▁▁▃▃▃▃▂▂▃▂▂▂▂▄▄▄▄▄▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb:                      mae 34.40215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb:                     mape 56378838.20775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb:                     rmse 112.48224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb:       time_since_restore 24.95073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb:         time_this_iter_s 0.80381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb:             time_total_s 24.95073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb:                timestamp 1689687880\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: 🚀 View run FSR_Trainable_2b50aae9 at: https://wandb.ai/seokjin/FSR-prediction/runs/2b50aae9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=241789)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224416-2b50aae9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_fd2ae85c_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-44-29/wandb/run-20230718_224447-fd2ae85c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb: Syncing run FSR_Trainable_fd2ae85c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd2ae85c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb:                      mae █▄▂▁▁▁▁▁▁▁▁▁▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb:                     mape █▄▃▂▁▁▁▁▁▁▁▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb:                     rmse █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb:       time_since_restore ▁▂▂▃▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb:         time_this_iter_s █▅▄▃▁▂▂▂▂▁▂▆▂▄▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb:             time_total_s ▁▂▂▃▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb:                timestamp ▁▂▃▃▃▃▄▄▅▅▆▆▆▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb:                      mae 37.44695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb:                     mape 67322696.36952\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb:                     rmse 117.75928\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb:       time_since_restore 14.44635\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb:         time_this_iter_s 0.88241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb:             time_total_s 14.44635\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb:                timestamp 1689687887\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: 🚀 View run FSR_Trainable_f147eae0 at: https://wandb.ai/seokjin/FSR-prediction/runs/f147eae0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242227)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224435-f147eae0/logs\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb:                      mae █▄▃▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb:                     mape █▂▃▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb:                     rmse █▄▃▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb:         time_this_iter_s █▄▅▃▃▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb:                timestamp ▁▄▅▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242448)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "2023-07-18 22:44:54,980\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.636 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:44:54,982\tWARNING util.py:315 -- The `process_trial_result` operation took 1.639 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:44:54,985\tWARNING util.py:315 -- Processing trial results took 1.642 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:44:54,987\tWARNING util.py:315 -- The `process_trial_result` operation took 1.644 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb:                      mae █▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb:                     mape █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb:                     rmse █▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb:         time_this_iter_s ▇▅▅▃▃▄▄▃▃▃▆▆▆▆▅▅▅▅▅▅▄▅█▆▇▅▅▄▄▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224424-6341284f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224424-6341284f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: iterations_since_restore 32\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb:                      mae 35.29428\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb:                     mape 53965289.8281\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb:                     rmse 114.94886\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb:       time_since_restore 26.5529\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb:         time_this_iter_s 0.5367\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb:             time_total_s 26.5529\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb:                timestamp 1689687891\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb:       training_iteration 32\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: 🚀 View run FSR_Trainable_6341284f at: https://wandb.ai/seokjin/FSR-prediction/runs/6341284f\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242014)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224424-6341284f/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_c737bd0b_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-44-41/wandb/run-20230718_224457-c737bd0b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: Syncing run FSR_Trainable_c737bd0b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c737bd0b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb:                      mae █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb:                     mape █▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb:                     rmse █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb:         time_this_iter_s █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb:                      mae 43.15664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb:                     mape 90813244.05268\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb:                     rmse 138.91347\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb:       time_since_restore 3.52376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb:         time_this_iter_s 0.66221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb:             time_total_s 3.52376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb:                timestamp 1689687897\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: 🚀 View run FSR_Trainable_c737bd0b at: https://wandb.ai/seokjin/FSR-prediction/runs/c737bd0b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242693)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224457-c737bd0b/logs\n",
      "2023-07-18 22:45:05,169\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.689 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:45:05,174\tWARNING util.py:315 -- The `process_trial_result` operation took 1.694 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:45:05,176\tWARNING util.py:315 -- Processing trial results took 1.696 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:45:05,177\tWARNING util.py:315 -- The `process_trial_result` operation took 1.698 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_7c53b654_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-44-52/wandb/run-20230718_224506-7c53b654\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: Syncing run FSR_Trainable_7c53b654\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7c53b654\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:45:10,886\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.645 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:45:10,891\tWARNING util.py:315 -- The `process_trial_result` operation took 1.651 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:45:10,892\tWARNING util.py:315 -- Processing trial results took 1.652 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:45:10,894\tWARNING util.py:315 -- The `process_trial_result` operation took 1.654 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb:                      mae 87.40654\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb:                     mape 325949409.80811\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb:                     rmse 266.42939\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb:       time_since_restore 2.63012\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb:         time_this_iter_s 2.63012\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb:             time_total_s 2.63012\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb:                timestamp 1689687903\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: 🚀 View run FSR_Trainable_7c53b654 at: https://wandb.ai/seokjin/FSR-prediction/runs/7c53b654\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=242924)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224506-7c53b654/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_d6d16950_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-45-00/wandb/run-20230718_224512-d6d16950\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: Syncing run FSR_Trainable_d6d16950\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d6d16950\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:45:17,016\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.773 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:45:17,020\tWARNING util.py:315 -- The `process_trial_result` operation took 1.778 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:45:17,021\tWARNING util.py:315 -- Processing trial results took 1.779 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:45:17,023\tWARNING util.py:315 -- The `process_trial_result` operation took 1.781 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb:                      mae 87.49003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb:                     mape 268608554.15379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb:                     rmse 252.90062\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb:       time_since_restore 1.48986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb:         time_this_iter_s 1.48986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb:             time_total_s 1.48986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb:                timestamp 1689687909\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: 🚀 View run FSR_Trainable_d6d16950 at: https://wandb.ai/seokjin/FSR-prediction/runs/d6d16950\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243139)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224512-d6d16950/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_d70340e7_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-45-07/wandb/run-20230718_224518-d70340e7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Syncing run FSR_Trainable_d70340e7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d70340e7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:45:23,645\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.664 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:45:23,648\tWARNING util.py:315 -- The `process_trial_result` operation took 1.667 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:45:23,650\tWARNING util.py:315 -- Processing trial results took 1.669 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:45:23,652\tWARNING util.py:315 -- The `process_trial_result` operation took 1.671 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243323)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224518-d70340e7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_c6531895_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-45-13/wandb/run-20230718_224525-c6531895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: Syncing run FSR_Trainable_c6531895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c6531895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:45:30,038\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.430 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:45:30,041\tWARNING util.py:315 -- The `process_trial_result` operation took 1.433 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:45:30,042\tWARNING util.py:315 -- Processing trial results took 1.434 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:45:30,043\tWARNING util.py:315 -- The `process_trial_result` operation took 1.435 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb:                      mae 85.56519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb:                     mape 280133806.66096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb:                     rmse 260.67979\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb:       time_since_restore 2.08906\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb:         time_this_iter_s 2.08906\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb:             time_total_s 2.08906\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb:                timestamp 1689687921\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: 🚀 View run FSR_Trainable_c6531895 at: https://wandb.ai/seokjin/FSR-prediction/runs/c6531895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224525-c6531895/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243510)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_ddc57f21_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-45-19/wandb/run-20230718_224532-ddc57f21\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: Syncing run FSR_Trainable_ddc57f21\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ddc57f21\n",
      "2023-07-18 22:45:39,122\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.783 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:45:39,124\tWARNING util.py:315 -- The `process_trial_result` operation took 1.785 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:45:39,124\tWARNING util.py:315 -- Processing trial results took 1.786 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:45:39,125\tWARNING util.py:315 -- The `process_trial_result` operation took 1.787 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_e43793b5_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-45-27/wandb/run-20230718_224544-e43793b5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Syncing run FSR_Trainable_e43793b5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e43793b5\n",
      "2023-07-18 22:45:49,831\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.623 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:45:49,834\tWARNING util.py:315 -- The `process_trial_result` operation took 1.626 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:45:49,835\tWARNING util.py:315 -- Processing trial results took 1.627 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:45:49,836\tWARNING util.py:315 -- The `process_trial_result` operation took 1.628 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_29fe79a7_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-45-36/wandb/run-20230718_224553-29fe79a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: Syncing run FSR_Trainable_29fe79a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/29fe79a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb:                      mae █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb:                     mape █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb:                     rmse █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb:         time_this_iter_s █▁▆▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb:                timestamp ▁▄▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb:                      mae 39.90161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb:                     mape 70657049.58938\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb:                     rmse 130.27486\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb:       time_since_restore 4.58378\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb:         time_this_iter_s 1.05356\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb:             time_total_s 4.58378\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb:                timestamp 1689687953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: 🚀 View run FSR_Trainable_29fe79a7 at: https://wandb.ai/seokjin/FSR-prediction/runs/29fe79a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244172)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224553-29fe79a7/logs\n",
      "2023-07-18 22:46:02,342\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.721 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:46:02,344\tWARNING util.py:315 -- The `process_trial_result` operation took 1.723 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:46:02,348\tWARNING util.py:315 -- Processing trial results took 1.728 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:46:02,350\tWARNING util.py:315 -- The `process_trial_result` operation took 1.730 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_904e5216_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-45-47/wandb/run-20230718_224605-904e5216\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: Syncing run FSR_Trainable_904e5216\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/904e5216\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb:                      mae █▅▃▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb:                     mape █▅▃▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb:                     rmse █▅▃▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb:         time_this_iter_s █▅▂▁▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb:                timestamp ▁▄▅▅▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb:                      mae 36.55576\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb:                     mape 68804795.40931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb:                     rmse 119.0745\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb:       time_since_restore 7.9603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb:         time_this_iter_s 0.91584\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb:             time_total_s 7.9603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb:                timestamp 1689687968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: 🚀 View run FSR_Trainable_904e5216 at: https://wandb.ai/seokjin/FSR-prediction/runs/904e5216\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244395)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224605-904e5216/logs\n",
      "2023-07-18 22:46:14,081\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.691 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:46:14,085\tWARNING util.py:315 -- The `process_trial_result` operation took 1.696 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:46:14,086\tWARNING util.py:315 -- Processing trial results took 1.697 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:46:14,087\tWARNING util.py:315 -- The `process_trial_result` operation took 1.698 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb:                      mae █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb:                     mape █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb:                     rmse █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb:         time_this_iter_s ▅▃▃▂▂▂▂▃▂▂▅▅▆▇█▇▄▂▂▁▂▃█▇▄▃▃▄▃▄▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224544-e43793b5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_77db7443_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-45-59/wandb/run-20230718_224616-77db7443\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: Syncing run FSR_Trainable_77db7443\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/77db7443\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb:                      mae 60.73296\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb:                     mape 171976266.79795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb:                     rmse 196.11173\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb:       time_since_restore 2.0785\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb:         time_this_iter_s 0.94212\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb:             time_total_s 2.0785\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb:                timestamp 1689687975\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: 🚀 View run FSR_Trainable_77db7443 at: https://wandb.ai/seokjin/FSR-prediction/runs/77db7443\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224616-77db7443/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244625)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-18 22:46:23,853\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.793 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:46:23,856\tWARNING util.py:315 -- The `process_trial_result` operation took 1.797 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:46:23,859\tWARNING util.py:315 -- Processing trial results took 1.800 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:46:23,862\tWARNING util.py:315 -- The `process_trial_result` operation took 1.803 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_aa9978a5_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-46-11/wandb/run-20230718_224626-aa9978a5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: Syncing run FSR_Trainable_aa9978a5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/aa9978a5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb:                      mae 82.47361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb:                     mape 4894262113841028.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb:                     rmse 275.28364\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb:       time_since_restore 1.34193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb:         time_this_iter_s 1.34193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb:             time_total_s 1.34193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb:                timestamp 1689687982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: 🚀 View run FSR_Trainable_aa9978a5 at: https://wandb.ai/seokjin/FSR-prediction/runs/aa9978a5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=244858)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224626-aa9978a5/logs\n",
      "2023-07-18 22:46:32,955\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.844 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:46:32,958\tWARNING util.py:315 -- The `process_trial_result` operation took 1.848 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:46:32,960\tWARNING util.py:315 -- Processing trial results took 1.850 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:46:32,961\tWARNING util.py:315 -- The `process_trial_result` operation took 1.851 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb:                      mae █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb:                     mape █▃▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb:                     rmse █▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb:         time_this_iter_s █▃▂▁▂▁▂▆▃▂▃▃▃▄▅▅▆▃▃▂▄▆▄▃▄▃▃▄▄▃▁▂▁▁▂▃▂▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224532-ddc57f21/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_908e38b8_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-46-20/wandb/run-20230718_224634-908e38b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: Syncing run FSR_Trainable_908e38b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/908e38b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/908e38b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=243735)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:46:39,918\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.600 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:46:39,920\tWARNING util.py:315 -- The `process_trial_result` operation took 1.603 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:46:39,922\tWARNING util.py:315 -- Processing trial results took 1.605 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:46:39,926\tWARNING util.py:315 -- The `process_trial_result` operation took 1.610 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb:                      mae 58.86807\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb:                     mape 5735313262608246.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb:                     rmse 210.2948\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb:       time_since_restore 3.31753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb:         time_this_iter_s 1.208\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb:             time_total_s 3.31753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb:                timestamp 1689687994\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: 🚀 View run FSR_Trainable_908e38b8 at: https://wandb.ai/seokjin/FSR-prediction/runs/908e38b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224634-908e38b8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_c3cab307_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-46-29/wandb/run-20230718_224642-c3cab307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: Syncing run FSR_Trainable_c3cab307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c3cab307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245083)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:46:46,907\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.619 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:46:46,909\tWARNING util.py:315 -- The `process_trial_result` operation took 1.622 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:46:46,912\tWARNING util.py:315 -- Processing trial results took 1.624 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:46:46,914\tWARNING util.py:315 -- The `process_trial_result` operation took 1.627 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb:                      mae █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb:                     mape █▄▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb:                     rmse █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb:         time_this_iter_s █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb:                timestamp ▁▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb:                      mae 39.5362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb:                     mape 37396559.39243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb:                     rmse 138.80365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb:       time_since_restore 4.58418\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb:         time_this_iter_s 0.87924\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb:             time_total_s 4.58418\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb:                timestamp 1689688002\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: 🚀 View run FSR_Trainable_c3cab307 at: https://wandb.ai/seokjin/FSR-prediction/runs/c3cab307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224642-c3cab307/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245316)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_98a3ec20_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-46-36/wandb/run-20230718_224649-98a3ec20\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: Syncing run FSR_Trainable_98a3ec20\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/98a3ec20\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:46:54,161\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.758 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:46:54,164\tWARNING util.py:315 -- The `process_trial_result` operation took 1.762 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:46:54,165\tWARNING util.py:315 -- Processing trial results took 1.763 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:46:54,166\tWARNING util.py:315 -- The `process_trial_result` operation took 1.765 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb:                      mae 49.43973\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb:                     mape 107829953.58591\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb:                     rmse 166.17699\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb:       time_since_restore 1.91308\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb:         time_this_iter_s 0.72709\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb:             time_total_s 1.91308\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb:                timestamp 1689688007\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: 🚀 View run FSR_Trainable_98a3ec20 at: https://wandb.ai/seokjin/FSR-prediction/runs/98a3ec20\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245538)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224649-98a3ec20/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:46:59,642\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.889 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:46:59,647\tWARNING util.py:315 -- The `process_trial_result` operation took 1.894 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:46:59,649\tWARNING util.py:315 -- Processing trial results took 1.897 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:46:59,651\tWARNING util.py:315 -- The `process_trial_result` operation took 1.899 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224655-fa43b5fc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224655-fa43b5fc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245699)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_78122c6c_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-46-50/wandb/run-20230718_224702-78122c6c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: Syncing run FSR_Trainable_78122c6c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/78122c6c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:47:06,441\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.508 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:47:06,444\tWARNING util.py:315 -- The `process_trial_result` operation took 1.511 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:47:06,446\tWARNING util.py:315 -- Processing trial results took 1.513 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:47:06,447\tWARNING util.py:315 -- The `process_trial_result` operation took 1.515 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb:                      mae 63.63706\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb:                     mape 155500955.4502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb:                     rmse 195.93479\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb:       time_since_restore 2.03912\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb:         time_this_iter_s 0.82526\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb:             time_total_s 2.03912\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb:                timestamp 1689688020\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: 🚀 View run FSR_Trainable_78122c6c at: https://wandb.ai/seokjin/FSR-prediction/runs/78122c6c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224702-78122c6c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=245905)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_37ed06cd_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-46-56/wandb/run-20230718_224708-37ed06cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: Syncing run FSR_Trainable_37ed06cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/37ed06cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:47:15,381\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.689 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:47:15,384\tWARNING util.py:315 -- The `process_trial_result` operation took 1.693 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:47:15,385\tWARNING util.py:315 -- Processing trial results took 1.694 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:47:15,387\tWARNING util.py:315 -- The `process_trial_result` operation took 1.695 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb:                      mae █▅▃▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb:                     mape █▅▃▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb:                     rmse █▄▃▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb:         time_this_iter_s █▃▃▂▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb:                timestamp ▁▄▅▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb:                      mae 35.67907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb:                     mape 54092806.17117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb:                     rmse 117.08028\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb:       time_since_restore 5.94674\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb:         time_this_iter_s 0.56021\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb:             time_total_s 5.94674\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb:                timestamp 1689688031\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: 🚀 View run FSR_Trainable_37ed06cd at: https://wandb.ai/seokjin/FSR-prediction/runs/37ed06cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246090)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224708-37ed06cd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_f3d4c294_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-47-03/wandb/run-20230718_224717-f3d4c294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: Syncing run FSR_Trainable_f3d4c294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f3d4c294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:47:23,031\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.900 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:47:23,033\tWARNING util.py:315 -- The `process_trial_result` operation took 1.903 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:47:23,036\tWARNING util.py:315 -- Processing trial results took 1.906 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:47:23,039\tWARNING util.py:315 -- The `process_trial_result` operation took 1.909 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb:                      mae █▄▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb:                     mape █▁▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb:                     rmse █▄▃▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb:         time_this_iter_s █▃▂▂▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb:                timestamp ▁▄▄▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb:                      mae 36.96302\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb:                     mape 68325241.17129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb:                     rmse 118.66398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb:       time_since_restore 6.01144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb:         time_this_iter_s 0.64315\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb:             time_total_s 6.01144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb:                timestamp 1689688040\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: 🚀 View run FSR_Trainable_f3d4c294 at: https://wandb.ai/seokjin/FSR-prediction/runs/f3d4c294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246312)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224717-f3d4c294/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_56d1e4fa_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-47-12/wandb/run-20230718_224725-56d1e4fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: Syncing run FSR_Trainable_56d1e4fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/56d1e4fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:47:29,652\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.783 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:47:29,657\tWARNING util.py:315 -- The `process_trial_result` operation took 1.788 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:47:29,658\tWARNING util.py:315 -- Processing trial results took 1.790 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:47:29,661\tWARNING util.py:315 -- The `process_trial_result` operation took 1.792 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb:                      mae █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb:                     mape █▂▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb:                     rmse █▅▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb:         time_this_iter_s █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb:                      mae 38.31405\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb:                     mape 63008087.98012\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb:                     rmse 129.49985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb:       time_since_restore 3.19647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb:         time_this_iter_s 0.64628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb:             time_total_s 3.19647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb:                timestamp 1689688045\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: 🚀 View run FSR_Trainable_56d1e4fa at: https://wandb.ai/seokjin/FSR-prediction/runs/56d1e4fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246530)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224725-56d1e4fa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_347b06e3_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-47-20/wandb/run-20230718_224732-347b06e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: Syncing run FSR_Trainable_347b06e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/347b06e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-07-18 22:47:36,698\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.698 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:47:36,700\tWARNING util.py:315 -- The `process_trial_result` operation took 1.701 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:47:36,703\tWARNING util.py:315 -- Processing trial results took 1.704 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:47:36,705\tWARNING util.py:315 -- The `process_trial_result` operation took 1.705 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb:                      mae █▆▄▃▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb:                     mape █▅▇▃▂▁▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb:                     rmse █▅▄▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb:         time_this_iter_s █▃▂▂▁▁▂▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb:                timestamp ▁▄▅▅▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb:                      mae 35.48967\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb:                     mape 43765458.95758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb:                     rmse 119.61656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb:       time_since_restore 6.1719\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb:         time_this_iter_s 0.92951\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb:             time_total_s 6.1719\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb:                timestamp 1689688054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: 🚀 View run FSR_Trainable_347b06e3 at: https://wandb.ai/seokjin/FSR-prediction/runs/347b06e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246703)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224732-347b06e3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_92c39305_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-47-26/wandb/run-20230718_224739-92c39305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: Syncing run FSR_Trainable_92c39305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/92c39305\n",
      "2023-07-18 22:47:43,902\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.631 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:47:43,904\tWARNING util.py:315 -- The `process_trial_result` operation took 1.634 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:47:43,906\tWARNING util.py:315 -- Processing trial results took 1.636 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:47:43,907\tWARNING util.py:315 -- The `process_trial_result` operation took 1.637 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_a2319f85_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-47-33/wandb/run-20230718_224746-a2319f85\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: Syncing run FSR_Trainable_a2319f85\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a2319f85\n",
      "2023-07-18 22:47:55,884\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.972 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:47:55,887\tWARNING util.py:315 -- The `process_trial_result` operation took 1.977 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:47:55,888\tWARNING util.py:315 -- Processing trial results took 1.978 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:47:55,889\tWARNING util.py:315 -- The `process_trial_result` operation took 1.979 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_f1f0a561_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-47-40/wandb/run-20230718_224758-f1f0a561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: Syncing run FSR_Trainable_f1f0a561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f1f0a561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb:                      mae 84.97045\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb:                     mape 263930540.828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb:                     rmse 251.19377\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb:       time_since_restore 2.39752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb:         time_this_iter_s 2.39752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb:             time_total_s 2.39752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb:                timestamp 1689688073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: 🚀 View run FSR_Trainable_f1f0a561 at: https://wandb.ai/seokjin/FSR-prediction/runs/f1f0a561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247296)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224758-f1f0a561/logs\n",
      "2023-07-18 22:48:05,726\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.081 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:48:05,731\tWARNING util.py:315 -- The `process_trial_result` operation took 2.087 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:48:05,733\tWARNING util.py:315 -- Processing trial results took 2.089 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:48:05,734\tWARNING util.py:315 -- The `process_trial_result` operation took 2.090 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_d754af28_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-47-51/wandb/run-20230718_224808-d754af28\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: Syncing run FSR_Trainable_d754af28\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d754af28\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:48:15,112\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.009 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:48:15,115\tWARNING util.py:315 -- The `process_trial_result` operation took 2.013 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:48:15,116\tWARNING util.py:315 -- Processing trial results took 2.014 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:48:15,118\tWARNING util.py:315 -- The `process_trial_result` operation took 2.016 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb:                      mae █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb:                     mape █▂▁▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb:                     rmse █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb:         time_this_iter_s █▅▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb:                      mae 43.76443\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb:                     mape 116035129.43324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb:                     rmse 143.23207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb:       time_since_restore 8.76566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb:         time_this_iter_s 1.93224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb:             time_total_s 8.76566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb:                timestamp 1689688091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: 🚀 View run FSR_Trainable_d754af28 at: https://wandb.ai/seokjin/FSR-prediction/runs/d754af28\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247512)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224808-d754af28/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_cc55eb29_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-48-01/wandb/run-20230718_224819-cc55eb29\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: Syncing run FSR_Trainable_cc55eb29\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cc55eb29\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb:                      mae █▄▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb:                     mape █▄▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb:                     rmse █▄▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb:         time_this_iter_s ▄▄█▂▃▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb:                timestamp ▁▃▄▅▅▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb:                      mae 36.15175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb:                     mape 72994488.49089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb:                     rmse 117.69486\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb:       time_since_restore 10.28317\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb:         time_this_iter_s 1.09767\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb:             time_total_s 10.28317\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb:                timestamp 1689688104\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: 🚀 View run FSR_Trainable_cc55eb29 at: https://wandb.ai/seokjin/FSR-prediction/runs/cc55eb29\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247731)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224819-cc55eb29/logs\n",
      "2023-07-18 22:48:29,729\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.839 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:48:29,732\tWARNING util.py:315 -- The `process_trial_result` operation took 1.843 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:48:29,734\tWARNING util.py:315 -- Processing trial results took 1.845 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:48:29,736\tWARNING util.py:315 -- The `process_trial_result` operation took 1.847 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_c9a07f03_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-48-11/wandb/run-20230718_224832-c9a07f03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Syncing run FSR_Trainable_c9a07f03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c9a07f03\n",
      "2023-07-18 22:48:42,087\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.900 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:48:42,091\tWARNING util.py:315 -- The `process_trial_result` operation took 1.905 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:48:42,092\tWARNING util.py:315 -- Processing trial results took 1.907 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:48:42,094\tWARNING util.py:315 -- The `process_trial_result` operation took 1.909 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_8fd06241_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-48-26/wandb/run-20230718_224845-8fd06241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: Syncing run FSR_Trainable_8fd06241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8fd06241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb:                      mae 80.34969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb:                     mape 1.0142489544801374e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb:                     rmse 259.51386\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb:       time_since_restore 1.2673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb:         time_this_iter_s 1.2673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb:             time_total_s 1.2673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb:                timestamp 1689688120\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: 🚀 View run FSR_Trainable_8fd06241 at: https://wandb.ai/seokjin/FSR-prediction/runs/8fd06241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248192)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224845-8fd06241/logs\n",
      "2023-07-18 22:48:58,066\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.947 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:48:58,068\tWARNING util.py:315 -- The `process_trial_result` operation took 1.951 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:48:58,070\tWARNING util.py:315 -- Processing trial results took 1.952 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:48:58,071\tWARNING util.py:315 -- The `process_trial_result` operation took 1.954 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_9220cbaa_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-48-38/wandb/run-20230718_224901-9220cbaa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: Syncing run FSR_Trainable_9220cbaa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9220cbaa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb:                      mae 73.90989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb:                     mape 7.868147513483557e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb:                     rmse 247.27448\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb:       time_since_restore 1.6706\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb:         time_this_iter_s 1.6706\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb:             time_total_s 1.6706\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb:                timestamp 1689688136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: 🚀 View run FSR_Trainable_9220cbaa at: https://wandb.ai/seokjin/FSR-prediction/runs/9220cbaa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248420)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224901-9220cbaa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224901-9220cbaa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb:                      mae █▃▃▂▂▁▁▁▁▁▁▁▁▁▁▂▁▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb:                     mape █▂▂▂▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▃▃▃▄▃▄▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb:                     rmse █▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▂▁▁▁▂▁▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb:         time_this_iter_s ▃▂▂▁▁▁▁▂▂▂▁▃▂▂▂▁▂▂▁▁▂▂▃▁▂▃▂▃▄█▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: \n",
      "2023-07-18 22:49:16,420\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.764 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:49:16,422\tWARNING util.py:315 -- The `process_trial_result` operation took 1.766 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:49:16,424\tWARNING util.py:315 -- Processing trial results took 1.769 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:49:16,426\tWARNING util.py:315 -- The `process_trial_result` operation took 1.771 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247969)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_0994684c_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-48-54/wandb/run-20230718_224919-0994684c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: Syncing run FSR_Trainable_0994684c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0994684c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb:                      mae █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb:                     mape █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb:                     rmse █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb:         time_this_iter_s █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb:                      mae 38.68472\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb:                     mape 68226426.55814\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb:                     rmse 129.21111\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb:       time_since_restore 4.59197\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb:         time_this_iter_s 0.89454\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb:             time_total_s 4.59197\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb:                timestamp 1689688159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: 🚀 View run FSR_Trainable_0994684c at: https://wandb.ai/seokjin/FSR-prediction/runs/0994684c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248663)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224919-0994684c/logs\n",
      "2023-07-18 22:49:27,790\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.764 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:49:27,793\tWARNING util.py:315 -- The `process_trial_result` operation took 1.768 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:49:27,794\tWARNING util.py:315 -- Processing trial results took 1.770 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:49:27,795\tWARNING util.py:315 -- The `process_trial_result` operation took 1.771 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_09d43379_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-49-13/wandb/run-20230718_224931-09d43379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: Syncing run FSR_Trainable_09d43379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/09d43379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb:                      mae █▄▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb:                     mape █▃▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb:                     rmse █▄▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb:         time_this_iter_s ██▄▁▁▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb:                timestamp ▁▄▅▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb:                      mae 36.33929\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb:                     mape 54114505.74557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb:                     rmse 118.40703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb:       time_since_restore 7.99837\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb:         time_this_iter_s 0.85818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb:             time_total_s 7.99837\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb:                timestamp 1689688174\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: 🚀 View run FSR_Trainable_09d43379 at: https://wandb.ai/seokjin/FSR-prediction/runs/09d43379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248885)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224931-09d43379/logs\n",
      "2023-07-18 22:49:39,681\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.768 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:49:39,682\tWARNING util.py:315 -- The `process_trial_result` operation took 1.770 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:49:39,685\tWARNING util.py:315 -- Processing trial results took 1.773 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:49:39,686\tWARNING util.py:315 -- The `process_trial_result` operation took 1.774 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_f4cceab3_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-49-24/wandb/run-20230718_224942-f4cceab3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: Syncing run FSR_Trainable_f4cceab3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f4cceab3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb:                      mae █▅▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb:                     mape █▆▃▁▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb:                     rmse █▅▃▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb:         time_this_iter_s █▅▃▃▁▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb:                timestamp ▁▃▄▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb:                      mae 36.72341\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb:                     mape 71299532.11174\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb:                     rmse 119.43204\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb:       time_since_restore 7.85445\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb:         time_this_iter_s 0.85661\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb:             time_total_s 7.85445\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb:                timestamp 1689688186\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: 🚀 View run FSR_Trainable_f4cceab3 at: https://wandb.ai/seokjin/FSR-prediction/runs/f4cceab3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249110)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224942-f4cceab3/logs\n",
      "2023-07-18 22:49:51,821\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.873 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:49:51,822\tWARNING util.py:315 -- The `process_trial_result` operation took 1.876 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:49:51,824\tWARNING util.py:315 -- Processing trial results took 1.877 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:49:51,826\tWARNING util.py:315 -- The `process_trial_result` operation took 1.880 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_18431645_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-49-36/wandb/run-20230718_224955-18431645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: Syncing run FSR_Trainable_18431645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/18431645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb:                      mae 53.63375\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb:                     mape 141533410.81254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb:                     rmse 165.46805\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb:       time_since_restore 2.23627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb:         time_this_iter_s 1.05103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb:             time_total_s 2.23627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb:                timestamp 1689688192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: 🚀 View run FSR_Trainable_18431645 at: https://wandb.ai/seokjin/FSR-prediction/runs/18431645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249340)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224955-18431645/logs\n",
      "2023-07-18 22:50:03,031\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.589 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:50:03,034\tWARNING util.py:315 -- The `process_trial_result` operation took 1.593 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:50:03,036\tWARNING util.py:315 -- Processing trial results took 1.596 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:50:03,038\tWARNING util.py:315 -- The `process_trial_result` operation took 1.597 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_534340b0_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-49-48/wandb/run-20230718_225006-534340b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: Syncing run FSR_Trainable_534340b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/534340b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb:                      mae 90.8299\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb:                     mape 311750626.23467\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb:                     rmse 263.35786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb:       time_since_restore 1.55631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb:         time_this_iter_s 1.55631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb:             time_total_s 1.55631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb:                timestamp 1689688201\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: 🚀 View run FSR_Trainable_534340b0 at: https://wandb.ai/seokjin/FSR-prediction/runs/534340b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249566)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225006-534340b0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225006-534340b0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb:                      mae █▃▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb:                     mape █▂▂▁▁▁▁▁▂▂▁▂▁▁▁▁▁▁▁▂▁▁▂▂▂▂▂▁▂▁▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb:                     rmse █▃▂▂▂▂▂▂▁▂▂▂▁▁▂▂▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb:         time_this_iter_s ▆▁▃▃▂▂▄▂▄▄▄▆█▆▃▄▃▄▄▄▃▄▅█▅▇▃▃▅▄▃▃▄▄▃▅▁▂▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "2023-07-18 22:50:13,478\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.616 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:50:13,481\tWARNING util.py:315 -- The `process_trial_result` operation took 1.620 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:50:13,484\tWARNING util.py:315 -- Processing trial results took 1.622 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:50:13,485\tWARNING util.py:315 -- The `process_trial_result` operation took 1.623 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: \\ Waiting for wandb.init()...799)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_359145d9_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-49-59/wandb/run-20230718_225015-359145d9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Syncing run FSR_Trainable_359145d9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/359145d9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246899)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb:                      mae █▃▂▂▂▂▂▂▂▂▁▁▂▂▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb:                     mape █▃▂▂▁▁▁▁▂▁▁▂▂▁▂▂▁▂▂▂▁▁▂▁▂▂▂▁▂▁▁▁▁▂▂▂▂▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb:                     rmse █▃▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb:         time_this_iter_s ▄▂▂▂▂▂▅▅▂▄▇▅▄▃▃▅▄▃▄▇▇█▃▃▃▂▄▃▃▂▃▃▃▄▁▂▄▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb:                      mae 31.88381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb:                     mape 58257471.07338\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb:                     rmse 105.52282\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb:       time_since_restore 130.44854\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb:         time_this_iter_s 1.10685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb:             time_total_s 130.44854\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb:                timestamp 1689688214\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: 🚀 View run FSR_Trainable_a2319f85 at: https://wandb.ai/seokjin/FSR-prediction/runs/a2319f85\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247084)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_224746-a2319f85/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225015-359145d9/logs\n",
      "2023-07-18 22:50:23,119\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.741 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:50:23,121\tWARNING util.py:315 -- The `process_trial_result` operation took 1.743 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:50:23,122\tWARNING util.py:315 -- Processing trial results took 1.745 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:50:23,125\tWARNING util.py:315 -- The `process_trial_result` operation took 1.747 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_0ece50a3_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-50-10/wandb/run-20230718_225024-0ece50a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: Syncing run FSR_Trainable_0ece50a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0ece50a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:50:29,851\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.628 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:50:29,854\tWARNING util.py:315 -- The `process_trial_result` operation took 1.631 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:50:29,855\tWARNING util.py:315 -- Processing trial results took 1.632 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:50:29,857\tWARNING util.py:315 -- The `process_trial_result` operation took 1.634 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb:                      mae 49.62882\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb:                     mape 91669742.81866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb:                     rmse 158.60745\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb:       time_since_restore 3.81802\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb:         time_this_iter_s 1.55942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb:             time_total_s 3.81802\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb:                timestamp 1689688224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: 🚀 View run FSR_Trainable_0ece50a3 at: https://wandb.ai/seokjin/FSR-prediction/runs/0ece50a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225024-0ece50a3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250036)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_3d56171b_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-50-19/wandb/run-20230718_225032-3d56171b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: Syncing run FSR_Trainable_3d56171b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3d56171b\n",
      "2023-07-18 22:50:37,850\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.910 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:50:37,853\tWARNING util.py:315 -- The `process_trial_result` operation took 1.914 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:50:37,854\tWARNING util.py:315 -- Processing trial results took 1.916 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:50:37,857\tWARNING util.py:315 -- The `process_trial_result` operation took 1.918 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_56a68b84_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-50-26/wandb/run-20230718_225040-56a68b84\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: Syncing run FSR_Trainable_56a68b84\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/56a68b84\n",
      "2023-07-18 22:50:46,663\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.356 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:50:46,666\tWARNING util.py:315 -- The `process_trial_result` operation took 2.360 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:50:46,668\tWARNING util.py:315 -- Processing trial results took 2.362 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:50:46,670\tWARNING util.py:315 -- The `process_trial_result` operation took 2.364 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: \\ Waiting for wandb.init()...655)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_0c6a6284_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-50-34/wandb/run-20230718_225050-0c6a6284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: Syncing run FSR_Trainable_0c6a6284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0c6a6284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb:                      mae █▄▃▃▂▁▁▂▁▁▁▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb:                     mape █▂▄▅▂▂▂▁▁▃▁▁▁▁▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb:                     rmse █▄▃▃▂▁▂▂▁▂▁▂▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▄▄▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb:         time_this_iter_s ▇▃▂▁▁▁▄▄▄▃▃▃▆▆█▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▃▃▄▄▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb:                timestamp ▁▂▂▂▃▃▃▄▅▅▅▆▆▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb:                      mae 36.501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb:                     mape 70098209.04204\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb:                     rmse 121.61132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb:       time_since_restore 18.95577\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb:         time_this_iter_s 1.33293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb:             time_total_s 18.95577\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb:                timestamp 1689688249\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: 🚀 View run FSR_Trainable_3d56171b at: https://wandb.ai/seokjin/FSR-prediction/runs/3d56171b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250263)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225032-3d56171b/logs\n",
      "2023-07-18 22:50:57,545\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.895 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:50:57,551\tWARNING util.py:315 -- The `process_trial_result` operation took 1.902 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:50:57,552\tWARNING util.py:315 -- Processing trial results took 1.903 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:50:57,554\tWARNING util.py:315 -- The `process_trial_result` operation took 1.904 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_48d3be65_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-50-42/wandb/run-20230718_225100-48d3be65\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: Syncing run FSR_Trainable_48d3be65\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/48d3be65\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 22:51:07,754\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.141 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:51:07,757\tWARNING util.py:315 -- The `process_trial_result` operation took 2.145 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:51:07,760\tWARNING util.py:315 -- Processing trial results took 2.147 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:51:07,761\tWARNING util.py:315 -- The `process_trial_result` operation took 2.149 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb:                      mae 52.17878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb:                     mape 132189436.45346\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb:                     rmse 163.37909\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb:       time_since_restore 3.31753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb:         time_this_iter_s 1.50393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb:             time_total_s 3.31753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb:                timestamp 1689688259\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: 🚀 View run FSR_Trainable_48d3be65 at: https://wandb.ai/seokjin/FSR-prediction/runs/48d3be65\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250877)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225100-48d3be65/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_fc48ca5d_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-50-53/wandb/run-20230718_225111-fc48ca5d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: Syncing run FSR_Trainable_fc48ca5d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fc48ca5d\n",
      "2023-07-18 22:51:19,861\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.936 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:51:19,865\tWARNING util.py:315 -- The `process_trial_result` operation took 1.941 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:51:19,866\tWARNING util.py:315 -- Processing trial results took 1.942 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:51:19,868\tWARNING util.py:315 -- The `process_trial_result` operation took 1.944 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...331)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_6238d631_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-51-03/wandb/run-20230718_225123-6238d631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: Syncing run FSR_Trainable_6238d631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6238d631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb:                      mae █▅▃▂▂▂▃▂▂▂▂▂▁▁▁▁▂▂▁▁▁▃▂▂▂▁▁▂▂▁▂▂▂▁▁▁▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb:                     mape █▄▅▂▂▁▆▄▂▂▃▄▃▃▂▂▃▃▂▂▃▄▃▃▃▃▂▂▃▄▄▅▃▃▄▃▃▅▇▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb:                     rmse █▅▃▃▂▂▃▂▂▂▂▂▂▁▁▂▂▂▁▁▁▃▂▁▂▁▁▂▂▁▂▂▁▁▁▁▂▂▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb:         time_this_iter_s ▅▄▁▂▆▅▄▂▅▅▁▂█▆▅▇█▆▄▅▄▃▅▃▂▄▄▄▃▂▃▃▄▂▃▄▃▅▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb:                      mae 33.15026\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb:                     mape 80905480.43854\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb:                     rmse 110.64431\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb:       time_since_restore 83.89326\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb:         time_this_iter_s 1.29116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb:             time_total_s 83.89326\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb:                timestamp 1689688329\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: 🚀 View run FSR_Trainable_56a68b84 at: https://wandb.ai/seokjin/FSR-prediction/runs/56a68b84\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250480)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225040-56a68b84/logs\n",
      "2023-07-18 22:52:23,199\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.663 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:52:23,201\tWARNING util.py:315 -- The `process_trial_result` operation took 1.667 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:52:23,203\tWARNING util.py:315 -- Processing trial results took 1.669 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:52:23,204\tWARNING util.py:315 -- The `process_trial_result` operation took 1.670 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_89f4577e_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-51-16/wandb/run-20230718_225226-89f4577e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: Syncing run FSR_Trainable_89f4577e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/89f4577e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb:                      mae █▃▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▂▁▁▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb:                     mape █▃▁▁▁▁▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▃▂▂▂▃▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb:                     rmse █▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▂▁▁▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb:         time_this_iter_s █▆▁▅▁▁▇▄▆▅▄▄▄▂▃▄▃▃▃▃▄▂▃▄▃▄▃▅▃▄▃▃▃▃▄▄▃▄▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb:                      mae 30.40564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb:                     mape 56648317.95314\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb:                     rmse 101.46257\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb:       time_since_restore 129.69879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb:         time_this_iter_s 1.33805\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb:             time_total_s 129.69879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb:                timestamp 1689688384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: 🚀 View run FSR_Trainable_0c6a6284 at: https://wandb.ai/seokjin/FSR-prediction/runs/0c6a6284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250655)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225050-0c6a6284/logs\n",
      "2023-07-18 22:53:20,007\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.862 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:53:20,009\tWARNING util.py:315 -- The `process_trial_result` operation took 1.865 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:53:20,010\tWARNING util.py:315 -- Processing trial results took 1.865 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:53:20,010\tWARNING util.py:315 -- The `process_trial_result` operation took 1.866 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...939)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-35-35/FSR_Trainable_e9a68070_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_Simple_2023-07-18_22-52-19/wandb/run-20230718_225323-e9a68070\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: Syncing run FSR_Trainable_e9a68070\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e9a68070\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb:                      mae █▂▂▂▂▂▂▁▂▁▁▂▂▁▁▂▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb:                     mape █▂▁▁▂▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb:                     rmse █▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb:         time_this_iter_s █▆▆▄▃▄▂▂▄▂▃▂▂▃▃▂▄▂▄▃▁▆▄▄▃▃▂▂▁▂▃▃▃▄▁▂▂▃▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb:                      mae 29.8088\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb:                     mape 53840840.46247\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb:                     rmse 97.2674\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb:       time_since_restore 131.79371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb:         time_this_iter_s 1.41811\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb:             time_total_s 131.79371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb:                timestamp 1689688405\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: 🚀 View run FSR_Trainable_fc48ca5d at: https://wandb.ai/seokjin/FSR-prediction/runs/fc48ca5d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251103)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225111-fc48ca5d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb:                      mae █▃▂▂▂▂▂▂▂▁▂▁▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb:                     mape █▂▂▁▁▂▁▁▂▁▁▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▂▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb:                     rmse █▃▂▂▂▂▂▂▂▁▂▁▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb:         time_this_iter_s █▅▃▂▃▂▃▃▃▃▄▃▃▃▂▄▃▃▂▅▂▃▃▂▃▃▃▅▄▂▃▃▂▃▃█▄▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb:                      mae 32.11003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb:                     mape 58622690.42455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb:                     rmse 103.12793\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb:       time_since_restore 128.65497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb:         time_this_iter_s 1.06753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb:             time_total_s 128.65497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb:                timestamp 1689688413\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: 🚀 View run FSR_Trainable_6238d631 at: https://wandb.ai/seokjin/FSR-prediction/runs/6238d631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251331)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225123-6238d631/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb:                      mae █▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▂▂▁▁▁▁▁▂▂▂▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb:                     mape █▃▁▁▁▂▂▁▁▁▂▂▂▂▃▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▃▂▂▃▄▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb:                     rmse █▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▂▂▁▁▁▁▁▁▂▂▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb:         time_this_iter_s ▄▄▃▂▃▂▃▂▃▃▃▃▂▃▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▃█▃▁▁▂▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb:                      mae 32.44798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb:                     mape 56135810.56393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb:                     rmse 102.8691\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb:       time_since_restore 117.3037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb:         time_this_iter_s 0.89053\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb:             time_total_s 117.3037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb:                timestamp 1689688462\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: 🚀 View run FSR_Trainable_89f4577e at: https://wandb.ai/seokjin/FSR-prediction/runs/89f4577e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251600)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225226-89f4577e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb:                      mae █▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb:                     mape █▂▁▁▁▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb:                     rmse █▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb:         time_this_iter_s █▅▄▃▃▃▂▃▂▂▂▂▂▂▇▃▆▂▃▂▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb:                      mae 29.81467\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb:                     mape 50101648.5481\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb:                     rmse 98.97938\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb:       time_since_restore 90.41857\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb:         time_this_iter_s 0.60177\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb:             time_total_s 90.41857\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb:                timestamp 1689688490\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: 🚀 View run FSR_Trainable_e9a68070 at: https://wandb.ai/seokjin/FSR-prediction/runs/e9a68070\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251939)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225323-e9a68070/logs\n",
      "2023-07-18 22:54:54,817\tINFO tune.py:1111 -- Total run time: 1155.62 seconds (1151.38 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
