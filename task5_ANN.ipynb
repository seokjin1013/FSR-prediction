{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task5\n",
    "\n",
    "Index_X = FSR_for_force\n",
    "\n",
    "Index_y = force\n",
    "\n",
    "Data = Splited by Subject\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-07-19_09-38-35/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-07-19_09-38-35\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "184.365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.ANN'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': 'FSR_for_force',\n",
    "        'index_y': 'force',\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_subject'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-19 09:38:35,737] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 09:38:37,781\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-07-19 09:38:39,239\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-19 09:59:08</td></tr>\n",
       "<tr><td>Running for: </td><td>00:20:28.98        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.0/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=100<br>Bracket: Iter 64.000: -187.92928498454611 | Iter 32.000: -189.77752238741462 | Iter 16.000: -191.16754252719653 | Iter 8.000: -195.826644782191 | Iter 4.000: -234.1734578258139 | Iter 2.000: -285.96841781078035 | Iter 1.000: -352.61116081618036<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>index_X      </th><th>index_y  </th><th>model        </th><th style=\"text-align: right;\">    model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">  model_args/num_layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">    mae</th><th style=\"text-align: right;\">       mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_5a2a7e89</td><td>TERMINATED</td><td>172.26.215.93:510220</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00477892 </td><td>sklearn.preproc_8690</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       84.1486  </td><td style=\"text-align: right;\">1266.08 </td><td style=\"text-align: right;\">556.833</td><td style=\"text-align: right;\">1.21431e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_cf91cc1a</td><td>TERMINATED</td><td>172.26.215.93:510290</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0145024  </td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        7.40789 </td><td style=\"text-align: right;\"> 380.481</td><td style=\"text-align: right;\">251.92 </td><td style=\"text-align: right;\">1.99009e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_edc3e95b</td><td>TERMINATED</td><td>172.26.215.93:510461</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        7.2076e-05 </td><td>sklearn.preproc_8690</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.45104 </td><td style=\"text-align: right;\"> 549.013</td><td style=\"text-align: right;\">258.637</td><td style=\"text-align: right;\">8.39168e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_3d510e11</td><td>TERMINATED</td><td>172.26.215.93:510635</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000105617</td><td>sklearn.preproc_8690</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.22106 </td><td style=\"text-align: right;\"> 505.261</td><td style=\"text-align: right;\">240.061</td><td style=\"text-align: right;\">9.34939e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_d95d72ea</td><td>TERMINATED</td><td>172.26.215.93:510960</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        5.63426e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       88.4398  </td><td style=\"text-align: right;\"> 187.604</td><td style=\"text-align: right;\">108.37 </td><td style=\"text-align: right;\">5.0805e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_24c65055</td><td>TERMINATED</td><td>172.26.215.93:511189</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        7.30769e-05</td><td>sklearn.preproc_8690</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        4.59278 </td><td style=\"text-align: right;\"> 402.031</td><td style=\"text-align: right;\">195.081</td><td style=\"text-align: right;\">1.24332e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_208d51c4</td><td>TERMINATED</td><td>172.26.215.93:511402</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.64971e-05</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.51161 </td><td style=\"text-align: right;\"> 431.618</td><td style=\"text-align: right;\">239.749</td><td style=\"text-align: right;\">3.3003e+17 </td></tr>\n",
       "<tr><td>FSR_Trainable_5134dae4</td><td>TERMINATED</td><td>172.26.215.93:511630</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0390587  </td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.563251</td><td style=\"text-align: right;\"> 713.043</td><td style=\"text-align: right;\">322.831</td><td style=\"text-align: right;\">1.78541e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_0bc3160d</td><td>TERMINATED</td><td>172.26.215.93:511860</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.62911e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       30.7671  </td><td style=\"text-align: right;\"> 281.204</td><td style=\"text-align: right;\">166.87 </td><td style=\"text-align: right;\">1.45238e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_3662243b</td><td>TERMINATED</td><td>172.26.215.93:512083</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        5.41022e-05</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.554877</td><td style=\"text-align: right;\"> 446.227</td><td style=\"text-align: right;\">230.19 </td><td style=\"text-align: right;\">2.22036e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_e0d18344</td><td>TERMINATED</td><td>172.26.215.93:512315</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0803097  </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        4.35872 </td><td style=\"text-align: right;\"> 906.537</td><td style=\"text-align: right;\">635.734</td><td style=\"text-align: right;\">1.49198e+18</td></tr>\n",
       "<tr><td>FSR_Trainable_e476ad54</td><td>TERMINATED</td><td>172.26.215.93:512547</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        5.86973e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       71.7844  </td><td style=\"text-align: right;\"> 185.686</td><td style=\"text-align: right;\">107.189</td><td style=\"text-align: right;\">5.03803e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_2aab3b83</td><td>TERMINATED</td><td>172.26.215.93:512786</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00017174 </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       45.9975  </td><td style=\"text-align: right;\"> 192.543</td><td style=\"text-align: right;\">118.254</td><td style=\"text-align: right;\">1.80658e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_7fb2604a</td><td>TERMINATED</td><td>172.26.215.93:513011</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000807999</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.372649</td><td style=\"text-align: right;\"> 464.075</td><td style=\"text-align: right;\">261.525</td><td style=\"text-align: right;\">1.70589e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_262570e3</td><td>TERMINATED</td><td>172.26.215.93:513241</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        1.19231e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.569806</td><td style=\"text-align: right;\"> 414.839</td><td style=\"text-align: right;\">261.719</td><td style=\"text-align: right;\">1.37487e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_972ebba7</td><td>TERMINATED</td><td>172.26.215.93:513468</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.92557e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.93868 </td><td style=\"text-align: right;\"> 383.587</td><td style=\"text-align: right;\">236.778</td><td style=\"text-align: right;\">1.46707e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_ae1bac87</td><td>TERMINATED</td><td>172.26.215.93:513698</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000368954</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        4.30881 </td><td style=\"text-align: right;\"> 381.856</td><td style=\"text-align: right;\">242.758</td><td style=\"text-align: right;\">1.45975e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_b5fc5518</td><td>TERMINATED</td><td>172.26.215.93:513929</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00037332 </td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       67.0176  </td><td style=\"text-align: right;\"> 213.149</td><td style=\"text-align: right;\">121.818</td><td style=\"text-align: right;\">5.50608e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_77aaede5</td><td>TERMINATED</td><td>172.26.215.93:514167</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000329575</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      124.179   </td><td style=\"text-align: right;\"> 207.502</td><td style=\"text-align: right;\">123.386</td><td style=\"text-align: right;\">1.83657e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_be852e93</td><td>TERMINATED</td><td>172.26.215.93:514389</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00029703 </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       81.1037  </td><td style=\"text-align: right;\"> 213.206</td><td style=\"text-align: right;\">128.62 </td><td style=\"text-align: right;\">1.99125e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_ff32dc77</td><td>TERMINATED</td><td>172.26.215.93:514615</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000386173</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       64.2135  </td><td style=\"text-align: right;\"> 213.55 </td><td style=\"text-align: right;\">122.287</td><td style=\"text-align: right;\">5.7989e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_fe9934f5</td><td>TERMINATED</td><td>172.26.215.93:515604</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000301984</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      102.991   </td><td style=\"text-align: right;\"> 195.328</td><td style=\"text-align: right;\">114.364</td><td style=\"text-align: right;\">5.75342e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_c3683c7b</td><td>TERMINATED</td><td>172.26.215.93:515849</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        3.65133e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      100.068   </td><td style=\"text-align: right;\"> 184.365</td><td style=\"text-align: right;\">106.384</td><td style=\"text-align: right;\">5.01604e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_c335853a</td><td>TERMINATED</td><td>172.26.215.93:516050</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        4.38512e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       87.5677  </td><td style=\"text-align: right;\"> 184.729</td><td style=\"text-align: right;\">106.757</td><td style=\"text-align: right;\">5.0723e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_574934e9</td><td>TERMINATED</td><td>172.26.215.93:516323</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00141069 </td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       14.5681  </td><td style=\"text-align: right;\"> 211.774</td><td style=\"text-align: right;\">126.54 </td><td style=\"text-align: right;\">9.14135e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_a1901920</td><td>TERMINATED</td><td>172.26.215.93:516572</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        3.91817e-05</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.831791</td><td style=\"text-align: right;\"> 399.67 </td><td style=\"text-align: right;\">270.911</td><td style=\"text-align: right;\">5.21376e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_b5ad3a46</td><td>TERMINATED</td><td>172.26.215.93:516771</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        4.78164e-05</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.700626</td><td style=\"text-align: right;\"> 666.975</td><td style=\"text-align: right;\">421.333</td><td style=\"text-align: right;\">5.07914e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_fb53367b</td><td>TERMINATED</td><td>172.26.215.93:517003</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        3.83654e-05</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.630741</td><td style=\"text-align: right;\"> 639.643</td><td style=\"text-align: right;\">407.841</td><td style=\"text-align: right;\">6.04399e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_ffdc5c38</td><td>TERMINATED</td><td>172.26.215.93:517230</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000154098</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.669514</td><td style=\"text-align: right;\"> 601.003</td><td style=\"text-align: right;\">500.533</td><td style=\"text-align: right;\">1.18927e+18</td></tr>\n",
       "<tr><td>FSR_Trainable_d150faff</td><td>TERMINATED</td><td>172.26.215.93:517479</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        2.55105e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.53426 </td><td style=\"text-align: right;\"> 398.095</td><td style=\"text-align: right;\">227.394</td><td style=\"text-align: right;\">4.62401e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_95d0f2ef</td><td>TERMINATED</td><td>172.26.215.93:517692</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        2.41403e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.05627 </td><td style=\"text-align: right;\"> 359.551</td><td style=\"text-align: right;\">216.196</td><td style=\"text-align: right;\">5.2533e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_4409b5e9</td><td>TERMINATED</td><td>172.26.215.93:517785</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.1796e-05 </td><td>sklearn.preproc_8690</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.66916 </td><td style=\"text-align: right;\"> 341.403</td><td style=\"text-align: right;\">168.713</td><td style=\"text-align: right;\">8.88954e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_6af8e003</td><td>TERMINATED</td><td>172.26.215.93:518099</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        1.09818e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.81063 </td><td style=\"text-align: right;\"> 361.654</td><td style=\"text-align: right;\">224.321</td><td style=\"text-align: right;\">1.14799e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_acd3f5d3</td><td>TERMINATED</td><td>172.26.215.93:518190</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        8.92829e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       92.2076  </td><td style=\"text-align: right;\"> 192.557</td><td style=\"text-align: right;\">111.095</td><td style=\"text-align: right;\">5.20531e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_969ec62b</td><td>TERMINATED</td><td>172.26.215.93:518505</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        8.64086e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       94.1257  </td><td style=\"text-align: right;\"> 191.472</td><td style=\"text-align: right;\">110.268</td><td style=\"text-align: right;\">5.13003e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_83b58a49</td><td>TERMINATED</td><td>172.26.215.93:518592</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        8.26022e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       95.756   </td><td style=\"text-align: right;\"> 191.061</td><td style=\"text-align: right;\">110.233</td><td style=\"text-align: right;\">5.19771e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_1ffc41fd</td><td>TERMINATED</td><td>172.26.215.93:518895</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        3.04769e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        5.46966 </td><td style=\"text-align: right;\"> 283.144</td><td style=\"text-align: right;\">155.179</td><td style=\"text-align: right;\">6.09643e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_88a24ed6</td><td>TERMINATED</td><td>172.26.215.93:519156</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000108049</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       88.1338  </td><td style=\"text-align: right;\"> 195.094</td><td style=\"text-align: right;\">112.475</td><td style=\"text-align: right;\">5.30245e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_a33e0728</td><td>TERMINATED</td><td>172.26.215.93:519445</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        7.29574e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       69.426   </td><td style=\"text-align: right;\"> 187.002</td><td style=\"text-align: right;\">108.058</td><td style=\"text-align: right;\">5.13143e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ddefe89d</td><td>TERMINATED</td><td>172.26.215.93:519643</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        5.53792e-05</td><td>sklearn.preproc_8690</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.461459</td><td style=\"text-align: right;\"> 390.469</td><td style=\"text-align: right;\">180.231</td><td style=\"text-align: right;\">8.91752e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_995f7ebd</td><td>TERMINATED</td><td>172.26.215.93:519876</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        4.52172e-05</td><td>sklearn.preproc_8690</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.370386</td><td style=\"text-align: right;\"> 431.29 </td><td style=\"text-align: right;\">209.267</td><td style=\"text-align: right;\">4.14349e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_899586ad</td><td>TERMINATED</td><td>172.26.215.93:520103</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000132105</td><td>sklearn.preproc_8690</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.606609</td><td style=\"text-align: right;\"> 449.633</td><td style=\"text-align: right;\">206.197</td><td style=\"text-align: right;\">6.87544e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_44cad1cf</td><td>TERMINATED</td><td>172.26.215.93:520332</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000139625</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       40.9538  </td><td style=\"text-align: right;\"> 185.166</td><td style=\"text-align: right;\">106.979</td><td style=\"text-align: right;\">4.99794e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_8eb4940a</td><td>TERMINATED</td><td>172.26.215.93:520569</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        6.60235e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       75.4805  </td><td style=\"text-align: right;\"> 187.014</td><td style=\"text-align: right;\">108.038</td><td style=\"text-align: right;\">5.11277e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_89d7bea9</td><td>TERMINATED</td><td>172.26.215.93:520784</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        7.02962e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       73.5195  </td><td style=\"text-align: right;\"> 188.158</td><td style=\"text-align: right;\">108.713</td><td style=\"text-align: right;\">5.18333e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_a516f150</td><td>TERMINATED</td><td>172.26.215.93:521071</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        6.78337e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       67.0738  </td><td style=\"text-align: right;\"> 187.615</td><td style=\"text-align: right;\">108.261</td><td style=\"text-align: right;\">5.11415e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_5ab2b76c</td><td>TERMINATED</td><td>172.26.215.93:521271</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.70603e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.530025</td><td style=\"text-align: right;\"> 390.777</td><td style=\"text-align: right;\">248.599</td><td style=\"text-align: right;\">1.46053e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_aa6d1ad1</td><td>TERMINATED</td><td>172.26.215.93:521502</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        3.3963e-05 </td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.682003</td><td style=\"text-align: right;\"> 394.754</td><td style=\"text-align: right;\">241.338</td><td style=\"text-align: right;\">1.43499e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_9a1e6356</td><td>TERMINATED</td><td>172.26.215.93:521731</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        5.8559e-05 </td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.10606 </td><td style=\"text-align: right;\"> 321.313</td><td style=\"text-align: right;\">204.195</td><td style=\"text-align: right;\">1.19575e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_376b673f</td><td>TERMINATED</td><td>172.26.215.93:521972</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        6.33158e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.05669 </td><td style=\"text-align: right;\"> 322.196</td><td style=\"text-align: right;\">198.395</td><td style=\"text-align: right;\">8.14248e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_8111e724</td><td>TERMINATED</td><td>172.26.215.93:522204</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000174954</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.528618</td><td style=\"text-align: right;\"> 381.628</td><td style=\"text-align: right;\">214.705</td><td style=\"text-align: right;\">7.2123e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_8c44c95a</td><td>TERMINATED</td><td>172.26.215.93:522432</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.95611e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.329724</td><td style=\"text-align: right;\"> 388.351</td><td style=\"text-align: right;\">233.999</td><td style=\"text-align: right;\">9.90945e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ee3b5ce8</td><td>TERMINATED</td><td>172.26.215.93:522663</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.73778e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.419208</td><td style=\"text-align: right;\"> 375.551</td><td style=\"text-align: right;\">223.053</td><td style=\"text-align: right;\">7.19333e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e95decd0</td><td>TERMINATED</td><td>172.26.215.93:522885</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000113553</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       55.5889  </td><td style=\"text-align: right;\"> 189.366</td><td style=\"text-align: right;\">109.399</td><td style=\"text-align: right;\">5.09973e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_03b91ca1</td><td>TERMINATED</td><td>172.26.215.93:523115</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        3.67232e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.29487 </td><td style=\"text-align: right;\"> 303.918</td><td style=\"text-align: right;\">184.06 </td><td style=\"text-align: right;\">8.76312e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_871e547f</td><td>TERMINATED</td><td>172.26.215.93:523333</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000120348</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       55.1888  </td><td style=\"text-align: right;\"> 188.687</td><td style=\"text-align: right;\">108.712</td><td style=\"text-align: right;\">5.05852e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_fbdcd699</td><td>TERMINATED</td><td>172.26.215.93:523557</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000120609</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       24.8615  </td><td style=\"text-align: right;\"> 191.263</td><td style=\"text-align: right;\">111.079</td><td style=\"text-align: right;\">5.37775e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_c92d0ac4</td><td>TERMINATED</td><td>172.26.215.93:523780</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000127977</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       12.6308  </td><td style=\"text-align: right;\"> 192.133</td><td style=\"text-align: right;\">111.534</td><td style=\"text-align: right;\">5.52091e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_066e4b36</td><td>TERMINATED</td><td>172.26.215.93:524056</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000114667</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       10.8394  </td><td style=\"text-align: right;\"> 193.138</td><td style=\"text-align: right;\">112.182</td><td style=\"text-align: right;\">5.59801e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_8cb6ee6c</td><td>TERMINATED</td><td>172.26.215.93:524267</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000189318</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.610253</td><td style=\"text-align: right;\"> 364.208</td><td style=\"text-align: right;\">219.636</td><td style=\"text-align: right;\">1.02228e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_23856c50</td><td>TERMINATED</td><td>172.26.215.93:524483</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000202924</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.03001 </td><td style=\"text-align: right;\"> 301.708</td><td style=\"text-align: right;\">185.759</td><td style=\"text-align: right;\">7.68243e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_76db5144</td><td>TERMINATED</td><td>172.26.215.93:524730</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        4.2415e-05 </td><td>sklearn.preproc_8690</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.560733</td><td style=\"text-align: right;\"> 381.349</td><td style=\"text-align: right;\">190.604</td><td style=\"text-align: right;\">4.58407e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_1593ef3c</td><td>TERMINATED</td><td>172.26.215.93:524952</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        4.37376e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.74129 </td><td style=\"text-align: right;\"> 302.235</td><td style=\"text-align: right;\">178.086</td><td style=\"text-align: right;\">7.36189e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_32f780ed</td><td>TERMINATED</td><td>172.26.215.93:525037</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        5.3739e-05 </td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.84825 </td><td style=\"text-align: right;\"> 297.657</td><td style=\"text-align: right;\">160.94 </td><td style=\"text-align: right;\">5.85257e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_11ca36ec</td><td>TERMINATED</td><td>172.26.215.93:525218</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        2.4917e-05 </td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.721361</td><td style=\"text-align: right;\"> 385.332</td><td style=\"text-align: right;\">239.569</td><td style=\"text-align: right;\">1.23369e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_b7d4a193</td><td>TERMINATED</td><td>172.26.215.93:525476</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        2.47285e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.66003 </td><td style=\"text-align: right;\"> 386.811</td><td style=\"text-align: right;\">235.191</td><td style=\"text-align: right;\">1.0595e+08 </td></tr>\n",
       "<tr><td>FSR_Trainable_4714a00e</td><td>TERMINATED</td><td>172.26.215.93:525789</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        8.50412e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       11.402   </td><td style=\"text-align: right;\"> 195.12 </td><td style=\"text-align: right;\">112.692</td><td style=\"text-align: right;\">5.56e+07   </td></tr>\n",
       "<tr><td>FSR_Trainable_9e5d2527</td><td>TERMINATED</td><td>172.26.215.93:525879</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        7.54059e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.52739 </td><td style=\"text-align: right;\"> 247.359</td><td style=\"text-align: right;\">137.389</td><td style=\"text-align: right;\">5.54972e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_66b6fb88</td><td>TERMINATED</td><td>172.26.215.93:526186</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        7.96949e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       11.1101  </td><td style=\"text-align: right;\"> 192.866</td><td style=\"text-align: right;\">111.293</td><td style=\"text-align: right;\">5.52045e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_01769188</td><td>TERMINATED</td><td>172.26.215.93:526416</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.45516e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.64573 </td><td style=\"text-align: right;\"> 411.261</td><td style=\"text-align: right;\">252.559</td><td style=\"text-align: right;\">1.54333e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_c8b18d92</td><td>TERMINATED</td><td>172.26.215.93:526641</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.43015e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.755255</td><td style=\"text-align: right;\"> 391.93 </td><td style=\"text-align: right;\">244.718</td><td style=\"text-align: right;\">1.15085e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_ddfa8c6f</td><td>TERMINATED</td><td>172.26.215.93:526875</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        5.57821e-05</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.99031 </td><td style=\"text-align: right;\"> 378.104</td><td style=\"text-align: right;\">234.771</td><td style=\"text-align: right;\">4.73254e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_e59245c0</td><td>TERMINATED</td><td>172.26.215.93:526954</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        3.36659e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.55291 </td><td style=\"text-align: right;\"> 317.39 </td><td style=\"text-align: right;\">198.733</td><td style=\"text-align: right;\">1.10439e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_20cf1756</td><td>TERMINATED</td><td>172.26.215.93:527134</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        3.55128e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.999027</td><td style=\"text-align: right;\"> 372.302</td><td style=\"text-align: right;\">223.501</td><td style=\"text-align: right;\">1.55078e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_af997dd1</td><td>TERMINATED</td><td>172.26.215.93:527318</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        6.41699e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       57.5012  </td><td style=\"text-align: right;\"> 184.82 </td><td style=\"text-align: right;\">107.022</td><td style=\"text-align: right;\">5.10928e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e29404c6</td><td>TERMINATED</td><td>172.26.215.93:527630</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        6.72675e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       59.1306  </td><td style=\"text-align: right;\"> 185.389</td><td style=\"text-align: right;\">107.284</td><td style=\"text-align: right;\">5.10564e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_5fb24b8c</td><td>TERMINATED</td><td>172.26.215.93:527851</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        4.47687e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.46181 </td><td style=\"text-align: right;\"> 296.575</td><td style=\"text-align: right;\">172.039</td><td style=\"text-align: right;\">8.20083e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ea213d18</td><td>TERMINATED</td><td>172.26.215.93:528065</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        6.81644e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       60.0564  </td><td style=\"text-align: right;\"> 185.368</td><td style=\"text-align: right;\">107.295</td><td style=\"text-align: right;\">5.06327e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_4ca7fa1d</td><td>TERMINATED</td><td>172.26.215.93:528294</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        6.80138e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       67.6201  </td><td style=\"text-align: right;\"> 187.078</td><td style=\"text-align: right;\">108.087</td><td style=\"text-align: right;\">5.0983e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_778b0968</td><td>TERMINATED</td><td>172.26.215.93:528589</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        6.79202e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.447   </td><td style=\"text-align: right;\"> 236.25 </td><td style=\"text-align: right;\">131.838</td><td style=\"text-align: right;\">5.11174e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_c2226ee4</td><td>TERMINATED</td><td>172.26.215.93:528784</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        8.93194e-05</td><td>sklearn.preproc_8690</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.655505</td><td style=\"text-align: right;\"> 379.48 </td><td style=\"text-align: right;\">185.111</td><td style=\"text-align: right;\">9.41892e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_41e9d702</td><td>TERMINATED</td><td>172.26.215.93:529022</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        2.9767e-05 </td><td>sklearn.preproc_8690</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.399777</td><td style=\"text-align: right;\"> 455.668</td><td style=\"text-align: right;\">204.894</td><td style=\"text-align: right;\">1.23734e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_e652c47e</td><td>TERMINATED</td><td>172.26.215.93:529257</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000254656</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.338415</td><td style=\"text-align: right;\"> 404.051</td><td style=\"text-align: right;\">232.338</td><td style=\"text-align: right;\">1.29288e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_0166525e</td><td>TERMINATED</td><td>172.26.215.93:529487</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        5.19057e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.26319 </td><td style=\"text-align: right;\"> 295.776</td><td style=\"text-align: right;\">174.928</td><td style=\"text-align: right;\">8.70461e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_8425873d</td><td>TERMINATED</td><td>172.26.215.93:529707</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        5.11129e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.3126  </td><td style=\"text-align: right;\"> 236.692</td><td style=\"text-align: right;\">131.578</td><td style=\"text-align: right;\">5.34429e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_bbc12f41</td><td>TERMINATED</td><td>172.26.215.93:529943</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000148154</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       15.6126  </td><td style=\"text-align: right;\"> 190.625</td><td style=\"text-align: right;\">110.248</td><td style=\"text-align: right;\">5.4506e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_49d67090</td><td>TERMINATED</td><td>172.26.215.93:530030</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        8.97956e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.368312</td><td style=\"text-align: right;\"> 414.952</td><td style=\"text-align: right;\">261.066</td><td style=\"text-align: right;\">1.95265e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_7b5b79a8</td><td>TERMINATED</td><td>172.26.215.93:530209</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        9.22222e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.607288</td><td style=\"text-align: right;\"> 420.788</td><td style=\"text-align: right;\">270.743</td><td style=\"text-align: right;\">1.59256e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_6e8dd620</td><td>TERMINATED</td><td>172.26.215.93:530517</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        9.82735e-05</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.05329 </td><td style=\"text-align: right;\"> 306.582</td><td style=\"text-align: right;\">190.282</td><td style=\"text-align: right;\">3.65533e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_ec6ac595</td><td>TERMINATED</td><td>172.26.215.93:530749</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        2.12435e-05</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.76722 </td><td style=\"text-align: right;\"> 383.498</td><td style=\"text-align: right;\">236.526</td><td style=\"text-align: right;\">4.31953e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_7b58649c</td><td>TERMINATED</td><td>172.26.215.93:530842</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        4.15778e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.92705 </td><td style=\"text-align: right;\"> 392.328</td><td style=\"text-align: right;\">247.844</td><td style=\"text-align: right;\">1.56473e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_7c3324ee</td><td>TERMINATED</td><td>172.26.215.93:531153</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        2.97247e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.771473</td><td style=\"text-align: right;\"> 379.839</td><td style=\"text-align: right;\">238.45 </td><td style=\"text-align: right;\">1.55681e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_5c9a2b18</td><td>TERMINATED</td><td>172.26.215.93:531242</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        6.73252e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       78.6868  </td><td style=\"text-align: right;\"> 186.901</td><td style=\"text-align: right;\">108.013</td><td style=\"text-align: right;\">5.17904e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_5136c4dd</td><td>TERMINATED</td><td>172.26.215.93:531553</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        6.68571e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       77.858   </td><td style=\"text-align: right;\"> 187.28 </td><td style=\"text-align: right;\">108.165</td><td style=\"text-align: right;\">5.06315e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_4d848c95</td><td>TERMINATED</td><td>172.26.215.93:531646</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        6.68962e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.63689 </td><td style=\"text-align: right;\"> 234.443</td><td style=\"text-align: right;\">133.644</td><td style=\"text-align: right;\">6.29752e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_44f0d5ac</td><td>TERMINATED</td><td>172.26.215.93:531933</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        6.51877e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.51228 </td><td style=\"text-align: right;\"> 242.457</td><td style=\"text-align: right;\">138.258</td><td style=\"text-align: right;\">5.16278e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_68033e63</td><td>TERMINATED</td><td>172.26.215.93:532183</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        5.65659e-05</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.60055 </td><td style=\"text-align: right;\"> 243.337</td><td style=\"text-align: right;\">134.136</td><td style=\"text-align: right;\">5.37329e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_34cfda8e</td><td>TERMINATED</td><td>172.26.215.93:532411</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000147586</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       14.4404  </td><td style=\"text-align: right;\"> 191.812</td><td style=\"text-align: right;\">111.898</td><td style=\"text-align: right;\">5.60283e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_77190088</td><td>TERMINATED</td><td>172.26.215.93:532631</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000139907</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       24.1772  </td><td style=\"text-align: right;\"> 189.778</td><td style=\"text-align: right;\">110.612</td><td style=\"text-align: right;\">5.55118e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e3450325</td><td>TERMINATED</td><td>172.26.215.93:532889</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000155561</td><td>sklearn.preproc_84b0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       19.3772  </td><td style=\"text-align: right;\"> 190.365</td><td style=\"text-align: right;\">110.672</td><td style=\"text-align: right;\">5.4779e+07 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 09:38:39,288\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">    mae</th><th style=\"text-align: right;\">       mape</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_0166525e</td><td>2023-07-19_09-56-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">174.928</td><td style=\"text-align: right;\">8.70461e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">529487</td><td style=\"text-align: right;\"> 295.776</td><td style=\"text-align: right;\">            1.26319 </td><td style=\"text-align: right;\">          0.599463</td><td style=\"text-align: right;\">      1.26319 </td><td style=\"text-align: right;\"> 1689728179</td><td style=\"text-align: right;\">                   2</td><td>0166525e  </td></tr>\n",
       "<tr><td>FSR_Trainable_01769188</td><td>2023-07-19_09-53-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">252.559</td><td style=\"text-align: right;\">1.54333e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">526416</td><td style=\"text-align: right;\"> 411.261</td><td style=\"text-align: right;\">            0.64573 </td><td style=\"text-align: right;\">          0.64573 </td><td style=\"text-align: right;\">      0.64573 </td><td style=\"text-align: right;\"> 1689728023</td><td style=\"text-align: right;\">                   1</td><td>01769188  </td></tr>\n",
       "<tr><td>FSR_Trainable_03b91ca1</td><td>2023-07-19_09-51-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">184.06 </td><td style=\"text-align: right;\">8.76312e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">523115</td><td style=\"text-align: right;\"> 303.918</td><td style=\"text-align: right;\">            1.29487 </td><td style=\"text-align: right;\">          0.577222</td><td style=\"text-align: right;\">      1.29487 </td><td style=\"text-align: right;\"> 1689727883</td><td style=\"text-align: right;\">                   2</td><td>03b91ca1  </td></tr>\n",
       "<tr><td>FSR_Trainable_066e4b36</td><td>2023-07-19_09-52-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">112.182</td><td style=\"text-align: right;\">5.59801e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">524056</td><td style=\"text-align: right;\"> 193.138</td><td style=\"text-align: right;\">           10.8394  </td><td style=\"text-align: right;\">          0.777228</td><td style=\"text-align: right;\">     10.8394  </td><td style=\"text-align: right;\"> 1689727951</td><td style=\"text-align: right;\">                  16</td><td>066e4b36  </td></tr>\n",
       "<tr><td>FSR_Trainable_0bc3160d</td><td>2023-07-19_09-40-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">166.87 </td><td style=\"text-align: right;\">1.45238e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">511860</td><td style=\"text-align: right;\"> 281.204</td><td style=\"text-align: right;\">           30.7671  </td><td style=\"text-align: right;\">          0.277191</td><td style=\"text-align: right;\">     30.7671  </td><td style=\"text-align: right;\"> 1689727243</td><td style=\"text-align: right;\">                 100</td><td>0bc3160d  </td></tr>\n",
       "<tr><td>FSR_Trainable_11ca36ec</td><td>2023-07-19_09-53-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">239.569</td><td style=\"text-align: right;\">1.23369e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">525218</td><td style=\"text-align: right;\"> 385.332</td><td style=\"text-align: right;\">            0.721361</td><td style=\"text-align: right;\">          0.721361</td><td style=\"text-align: right;\">      0.721361</td><td style=\"text-align: right;\"> 1689727984</td><td style=\"text-align: right;\">                   1</td><td>11ca36ec  </td></tr>\n",
       "<tr><td>FSR_Trainable_1593ef3c</td><td>2023-07-19_09-52-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">178.086</td><td style=\"text-align: right;\">7.36189e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">524952</td><td style=\"text-align: right;\"> 302.235</td><td style=\"text-align: right;\">            1.74129 </td><td style=\"text-align: right;\">          0.683483</td><td style=\"text-align: right;\">      1.74129 </td><td style=\"text-align: right;\"> 1689727974</td><td style=\"text-align: right;\">                   2</td><td>1593ef3c  </td></tr>\n",
       "<tr><td>FSR_Trainable_1ffc41fd</td><td>2023-07-19_09-46-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">155.179</td><td style=\"text-align: right;\">6.09643e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">518895</td><td style=\"text-align: right;\"> 283.144</td><td style=\"text-align: right;\">            5.46966 </td><td style=\"text-align: right;\">          1.34345 </td><td style=\"text-align: right;\">      5.46966 </td><td style=\"text-align: right;\"> 1689727609</td><td style=\"text-align: right;\">                   4</td><td>1ffc41fd  </td></tr>\n",
       "<tr><td>FSR_Trainable_208d51c4</td><td>2023-07-19_09-39-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">239.749</td><td style=\"text-align: right;\">3.3003e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">511402</td><td style=\"text-align: right;\"> 431.618</td><td style=\"text-align: right;\">            1.51161 </td><td style=\"text-align: right;\">          0.323457</td><td style=\"text-align: right;\">      1.51161 </td><td style=\"text-align: right;\"> 1689727184</td><td style=\"text-align: right;\">                   4</td><td>208d51c4  </td></tr>\n",
       "<tr><td>FSR_Trainable_20cf1756</td><td>2023-07-19_09-54-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">223.501</td><td style=\"text-align: right;\">1.55078e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">527134</td><td style=\"text-align: right;\"> 372.302</td><td style=\"text-align: right;\">            0.999027</td><td style=\"text-align: right;\">          0.999027</td><td style=\"text-align: right;\">      0.999027</td><td style=\"text-align: right;\"> 1689728051</td><td style=\"text-align: right;\">                   1</td><td>20cf1756  </td></tr>\n",
       "<tr><td>FSR_Trainable_23856c50</td><td>2023-07-19_09-52-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">185.759</td><td style=\"text-align: right;\">7.68243e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">524483</td><td style=\"text-align: right;\"> 301.708</td><td style=\"text-align: right;\">            1.03001 </td><td style=\"text-align: right;\">          0.292524</td><td style=\"text-align: right;\">      1.03001 </td><td style=\"text-align: right;\"> 1689727958</td><td style=\"text-align: right;\">                   2</td><td>23856c50  </td></tr>\n",
       "<tr><td>FSR_Trainable_24c65055</td><td>2023-07-19_09-39-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">195.081</td><td style=\"text-align: right;\">1.24332e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">511189</td><td style=\"text-align: right;\"> 402.031</td><td style=\"text-align: right;\">            4.59278 </td><td style=\"text-align: right;\">          0.378993</td><td style=\"text-align: right;\">      4.59278 </td><td style=\"text-align: right;\"> 1689727173</td><td style=\"text-align: right;\">                   8</td><td>24c65055  </td></tr>\n",
       "<tr><td>FSR_Trainable_262570e3</td><td>2023-07-19_09-41-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">261.719</td><td style=\"text-align: right;\">1.37487e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">513241</td><td style=\"text-align: right;\"> 414.839</td><td style=\"text-align: right;\">            0.569806</td><td style=\"text-align: right;\">          0.230705</td><td style=\"text-align: right;\">      0.569806</td><td style=\"text-align: right;\"> 1689727276</td><td style=\"text-align: right;\">                   2</td><td>262570e3  </td></tr>\n",
       "<tr><td>FSR_Trainable_2aab3b83</td><td>2023-07-19_09-41-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">118.254</td><td style=\"text-align: right;\">1.80658e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">512786</td><td style=\"text-align: right;\"> 192.543</td><td style=\"text-align: right;\">           45.9975  </td><td style=\"text-align: right;\">          0.469964</td><td style=\"text-align: right;\">     45.9975  </td><td style=\"text-align: right;\"> 1689727309</td><td style=\"text-align: right;\">                 100</td><td>2aab3b83  </td></tr>\n",
       "<tr><td>FSR_Trainable_32f780ed</td><td>2023-07-19_09-53-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">160.94 </td><td style=\"text-align: right;\">5.85257e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">525037</td><td style=\"text-align: right;\"> 297.657</td><td style=\"text-align: right;\">            1.84825 </td><td style=\"text-align: right;\">          0.724876</td><td style=\"text-align: right;\">      1.84825 </td><td style=\"text-align: right;\"> 1689727980</td><td style=\"text-align: right;\">                   2</td><td>32f780ed  </td></tr>\n",
       "<tr><td>FSR_Trainable_34cfda8e</td><td>2023-07-19_09-58-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">111.898</td><td style=\"text-align: right;\">5.60283e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">532411</td><td style=\"text-align: right;\"> 191.812</td><td style=\"text-align: right;\">           14.4404  </td><td style=\"text-align: right;\">          1.18834 </td><td style=\"text-align: right;\">     14.4404  </td><td style=\"text-align: right;\"> 1689728313</td><td style=\"text-align: right;\">                  16</td><td>34cfda8e  </td></tr>\n",
       "<tr><td>FSR_Trainable_3662243b</td><td>2023-07-19_09-40-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">230.19 </td><td style=\"text-align: right;\">2.22036e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">512083</td><td style=\"text-align: right;\"> 446.227</td><td style=\"text-align: right;\">            0.554877</td><td style=\"text-align: right;\">          0.554877</td><td style=\"text-align: right;\">      0.554877</td><td style=\"text-align: right;\"> 1689727215</td><td style=\"text-align: right;\">                   1</td><td>3662243b  </td></tr>\n",
       "<tr><td>FSR_Trainable_376b673f</td><td>2023-07-19_09-50-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">198.395</td><td style=\"text-align: right;\">8.14248e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">521972</td><td style=\"text-align: right;\"> 322.196</td><td style=\"text-align: right;\">            1.05669 </td><td style=\"text-align: right;\">          0.409636</td><td style=\"text-align: right;\">      1.05669 </td><td style=\"text-align: right;\"> 1689727836</td><td style=\"text-align: right;\">                   2</td><td>376b673f  </td></tr>\n",
       "<tr><td>FSR_Trainable_3d510e11</td><td>2023-07-19_09-39-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">240.061</td><td style=\"text-align: right;\">9.34939e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">510635</td><td style=\"text-align: right;\"> 505.261</td><td style=\"text-align: right;\">            1.22106 </td><td style=\"text-align: right;\">          0.316211</td><td style=\"text-align: right;\">      1.22106 </td><td style=\"text-align: right;\"> 1689727149</td><td style=\"text-align: right;\">                   4</td><td>3d510e11  </td></tr>\n",
       "<tr><td>FSR_Trainable_41e9d702</td><td>2023-07-19_09-55-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">204.894</td><td style=\"text-align: right;\">1.23734e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">529022</td><td style=\"text-align: right;\"> 455.668</td><td style=\"text-align: right;\">            0.399777</td><td style=\"text-align: right;\">          0.399777</td><td style=\"text-align: right;\">      0.399777</td><td style=\"text-align: right;\"> 1689728158</td><td style=\"text-align: right;\">                   1</td><td>41e9d702  </td></tr>\n",
       "<tr><td>FSR_Trainable_4409b5e9</td><td>2023-07-19_09-46-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">168.713</td><td style=\"text-align: right;\">8.88954e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">517785</td><td style=\"text-align: right;\"> 341.403</td><td style=\"text-align: right;\">            1.66916 </td><td style=\"text-align: right;\">          0.306529</td><td style=\"text-align: right;\">      1.66916 </td><td style=\"text-align: right;\"> 1689727561</td><td style=\"text-align: right;\">                   4</td><td>4409b5e9  </td></tr>\n",
       "<tr><td>FSR_Trainable_44cad1cf</td><td>2023-07-19_09-49-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">106.979</td><td style=\"text-align: right;\">4.99794e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">520332</td><td style=\"text-align: right;\"> 185.166</td><td style=\"text-align: right;\">           40.9538  </td><td style=\"text-align: right;\">          0.484072</td><td style=\"text-align: right;\">     40.9538  </td><td style=\"text-align: right;\"> 1689727776</td><td style=\"text-align: right;\">                 100</td><td>44cad1cf  </td></tr>\n",
       "<tr><td>FSR_Trainable_44f0d5ac</td><td>2023-07-19_09-57-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">138.258</td><td style=\"text-align: right;\">5.16278e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">531933</td><td style=\"text-align: right;\"> 242.457</td><td style=\"text-align: right;\">            3.51228 </td><td style=\"text-align: right;\">          0.822965</td><td style=\"text-align: right;\">      3.51228 </td><td style=\"text-align: right;\"> 1689728276</td><td style=\"text-align: right;\">                   4</td><td>44f0d5ac  </td></tr>\n",
       "<tr><td>FSR_Trainable_4714a00e</td><td>2023-07-19_09-53-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">112.692</td><td style=\"text-align: right;\">5.56e+07   </td><td>172.26.215.93</td><td style=\"text-align: right;\">525789</td><td style=\"text-align: right;\"> 195.12 </td><td style=\"text-align: right;\">           11.402   </td><td style=\"text-align: right;\">          0.564514</td><td style=\"text-align: right;\">     11.402   </td><td style=\"text-align: right;\"> 1689728012</td><td style=\"text-align: right;\">                  16</td><td>4714a00e  </td></tr>\n",
       "<tr><td>FSR_Trainable_49d67090</td><td>2023-07-19_09-56-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">261.066</td><td style=\"text-align: right;\">1.95265e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">530030</td><td style=\"text-align: right;\"> 414.952</td><td style=\"text-align: right;\">            0.368312</td><td style=\"text-align: right;\">          0.368312</td><td style=\"text-align: right;\">      0.368312</td><td style=\"text-align: right;\"> 1689728199</td><td style=\"text-align: right;\">                   1</td><td>49d67090  </td></tr>\n",
       "<tr><td>FSR_Trainable_4ca7fa1d</td><td>2023-07-19_09-56-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">108.087</td><td style=\"text-align: right;\">5.0983e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">528294</td><td style=\"text-align: right;\"> 187.078</td><td style=\"text-align: right;\">           67.6201  </td><td style=\"text-align: right;\">          0.598347</td><td style=\"text-align: right;\">     67.6201  </td><td style=\"text-align: right;\"> 1689728179</td><td style=\"text-align: right;\">                 100</td><td>4ca7fa1d  </td></tr>\n",
       "<tr><td>FSR_Trainable_4d848c95</td><td>2023-07-19_09-57-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">133.644</td><td style=\"text-align: right;\">6.29752e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">531646</td><td style=\"text-align: right;\"> 234.443</td><td style=\"text-align: right;\">            3.63689 </td><td style=\"text-align: right;\">          0.855568</td><td style=\"text-align: right;\">      3.63689 </td><td style=\"text-align: right;\"> 1689728265</td><td style=\"text-align: right;\">                   4</td><td>4d848c95  </td></tr>\n",
       "<tr><td>FSR_Trainable_5134dae4</td><td>2023-07-19_09-39-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">322.831</td><td style=\"text-align: right;\">1.78541e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">511630</td><td style=\"text-align: right;\"> 713.043</td><td style=\"text-align: right;\">            0.563251</td><td style=\"text-align: right;\">          0.563251</td><td style=\"text-align: right;\">      0.563251</td><td style=\"text-align: right;\"> 1689727192</td><td style=\"text-align: right;\">                   1</td><td>5134dae4  </td></tr>\n",
       "<tr><td>FSR_Trainable_5136c4dd</td><td>2023-07-19_09-59-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">108.165</td><td style=\"text-align: right;\">5.06315e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">531553</td><td style=\"text-align: right;\"> 187.28 </td><td style=\"text-align: right;\">           77.858   </td><td style=\"text-align: right;\">          0.651114</td><td style=\"text-align: right;\">     77.858   </td><td style=\"text-align: right;\"> 1689728345</td><td style=\"text-align: right;\">                 100</td><td>5136c4dd  </td></tr>\n",
       "<tr><td>FSR_Trainable_574934e9</td><td>2023-07-19_09-44-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">126.54 </td><td style=\"text-align: right;\">9.14135e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">516323</td><td style=\"text-align: right;\"> 211.774</td><td style=\"text-align: right;\">           14.5681  </td><td style=\"text-align: right;\">          0.786197</td><td style=\"text-align: right;\">     14.5681  </td><td style=\"text-align: right;\"> 1689727485</td><td style=\"text-align: right;\">                  16</td><td>574934e9  </td></tr>\n",
       "<tr><td>FSR_Trainable_5a2a7e89</td><td>2023-07-19_09-40-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">556.833</td><td style=\"text-align: right;\">1.21431e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">510220</td><td style=\"text-align: right;\">1266.08 </td><td style=\"text-align: right;\">           84.1486  </td><td style=\"text-align: right;\">          0.968982</td><td style=\"text-align: right;\">     84.1486  </td><td style=\"text-align: right;\"> 1689727229</td><td style=\"text-align: right;\">                 100</td><td>5a2a7e89  </td></tr>\n",
       "<tr><td>FSR_Trainable_5ab2b76c</td><td>2023-07-19_09-49-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">248.599</td><td style=\"text-align: right;\">1.46053e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">521271</td><td style=\"text-align: right;\"> 390.777</td><td style=\"text-align: right;\">            0.530025</td><td style=\"text-align: right;\">          0.530025</td><td style=\"text-align: right;\">      0.530025</td><td style=\"text-align: right;\"> 1689727792</td><td style=\"text-align: right;\">                   1</td><td>5ab2b76c  </td></tr>\n",
       "<tr><td>FSR_Trainable_5c9a2b18</td><td>2023-07-19_09-58-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">108.013</td><td style=\"text-align: right;\">5.17904e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">531242</td><td style=\"text-align: right;\"> 186.901</td><td style=\"text-align: right;\">           78.6868  </td><td style=\"text-align: right;\">          0.941146</td><td style=\"text-align: right;\">     78.6868  </td><td style=\"text-align: right;\"> 1689728337</td><td style=\"text-align: right;\">                 100</td><td>5c9a2b18  </td></tr>\n",
       "<tr><td>FSR_Trainable_5fb24b8c</td><td>2023-07-19_09-54-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">172.039</td><td style=\"text-align: right;\">8.20083e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">527851</td><td style=\"text-align: right;\"> 296.575</td><td style=\"text-align: right;\">            1.46181 </td><td style=\"text-align: right;\">          0.67016 </td><td style=\"text-align: right;\">      1.46181 </td><td style=\"text-align: right;\"> 1689728078</td><td style=\"text-align: right;\">                   2</td><td>5fb24b8c  </td></tr>\n",
       "<tr><td>FSR_Trainable_66b6fb88</td><td>2023-07-19_09-53-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">111.293</td><td style=\"text-align: right;\">5.52045e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">526186</td><td style=\"text-align: right;\"> 192.866</td><td style=\"text-align: right;\">           11.1101  </td><td style=\"text-align: right;\">          0.66449 </td><td style=\"text-align: right;\">     11.1101  </td><td style=\"text-align: right;\"> 1689728028</td><td style=\"text-align: right;\">                  16</td><td>66b6fb88  </td></tr>\n",
       "<tr><td>FSR_Trainable_68033e63</td><td>2023-07-19_09-58-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">134.136</td><td style=\"text-align: right;\">5.37329e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">532183</td><td style=\"text-align: right;\"> 243.337</td><td style=\"text-align: right;\">            3.60055 </td><td style=\"text-align: right;\">          0.98926 </td><td style=\"text-align: right;\">      3.60055 </td><td style=\"text-align: right;\"> 1689728287</td><td style=\"text-align: right;\">                   4</td><td>68033e63  </td></tr>\n",
       "<tr><td>FSR_Trainable_6af8e003</td><td>2023-07-19_09-46-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">224.321</td><td style=\"text-align: right;\">1.14799e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">518099</td><td style=\"text-align: right;\"> 361.654</td><td style=\"text-align: right;\">            1.81063 </td><td style=\"text-align: right;\">          0.739454</td><td style=\"text-align: right;\">      1.81063 </td><td style=\"text-align: right;\"> 1689727569</td><td style=\"text-align: right;\">                   2</td><td>6af8e003  </td></tr>\n",
       "<tr><td>FSR_Trainable_6e8dd620</td><td>2023-07-19_09-56-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">190.282</td><td style=\"text-align: right;\">3.65533e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">530517</td><td style=\"text-align: right;\"> 306.582</td><td style=\"text-align: right;\">            1.05329 </td><td style=\"text-align: right;\">          0.331224</td><td style=\"text-align: right;\">      1.05329 </td><td style=\"text-align: right;\"> 1689728218</td><td style=\"text-align: right;\">                   2</td><td>6e8dd620  </td></tr>\n",
       "<tr><td>FSR_Trainable_76db5144</td><td>2023-07-19_09-52-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">190.604</td><td style=\"text-align: right;\">4.58407e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">524730</td><td style=\"text-align: right;\"> 381.349</td><td style=\"text-align: right;\">            0.560733</td><td style=\"text-align: right;\">          0.560733</td><td style=\"text-align: right;\">      0.560733</td><td style=\"text-align: right;\"> 1689727964</td><td style=\"text-align: right;\">                   1</td><td>76db5144  </td></tr>\n",
       "<tr><td>FSR_Trainable_77190088</td><td>2023-07-19_09-58-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">110.612</td><td style=\"text-align: right;\">5.55118e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">532631</td><td style=\"text-align: right;\"> 189.778</td><td style=\"text-align: right;\">           24.1772  </td><td style=\"text-align: right;\">          0.742794</td><td style=\"text-align: right;\">     24.1772  </td><td style=\"text-align: right;\"> 1689728337</td><td style=\"text-align: right;\">                  32</td><td>77190088  </td></tr>\n",
       "<tr><td>FSR_Trainable_778b0968</td><td>2023-07-19_09-55-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">131.838</td><td style=\"text-align: right;\">5.11174e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">528589</td><td style=\"text-align: right;\"> 236.25 </td><td style=\"text-align: right;\">            3.447   </td><td style=\"text-align: right;\">          0.822465</td><td style=\"text-align: right;\">      3.447   </td><td style=\"text-align: right;\"> 1689728141</td><td style=\"text-align: right;\">                   4</td><td>778b0968  </td></tr>\n",
       "<tr><td>FSR_Trainable_77aaede5</td><td>2023-07-19_09-44-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">123.386</td><td style=\"text-align: right;\">1.83657e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">514167</td><td style=\"text-align: right;\"> 207.502</td><td style=\"text-align: right;\">          124.179   </td><td style=\"text-align: right;\">          1.1466  </td><td style=\"text-align: right;\">    124.179   </td><td style=\"text-align: right;\"> 1689727455</td><td style=\"text-align: right;\">                 100</td><td>77aaede5  </td></tr>\n",
       "<tr><td>FSR_Trainable_7b58649c</td><td>2023-07-19_09-57-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">247.844</td><td style=\"text-align: right;\">1.56473e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">530842</td><td style=\"text-align: right;\"> 392.328</td><td style=\"text-align: right;\">            0.92705 </td><td style=\"text-align: right;\">          0.92705 </td><td style=\"text-align: right;\">      0.92705 </td><td style=\"text-align: right;\"> 1689728229</td><td style=\"text-align: right;\">                   1</td><td>7b58649c  </td></tr>\n",
       "<tr><td>FSR_Trainable_7b5b79a8</td><td>2023-07-19_09-56-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">270.743</td><td style=\"text-align: right;\">1.59256e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">530209</td><td style=\"text-align: right;\"> 420.788</td><td style=\"text-align: right;\">            0.607288</td><td style=\"text-align: right;\">          0.607288</td><td style=\"text-align: right;\">      0.607288</td><td style=\"text-align: right;\"> 1689728206</td><td style=\"text-align: right;\">                   1</td><td>7b5b79a8  </td></tr>\n",
       "<tr><td>FSR_Trainable_7c3324ee</td><td>2023-07-19_09-57-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">238.45 </td><td style=\"text-align: right;\">1.55681e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">531153</td><td style=\"text-align: right;\"> 379.839</td><td style=\"text-align: right;\">            0.771473</td><td style=\"text-align: right;\">          0.771473</td><td style=\"text-align: right;\">      0.771473</td><td style=\"text-align: right;\"> 1689728237</td><td style=\"text-align: right;\">                   1</td><td>7c3324ee  </td></tr>\n",
       "<tr><td>FSR_Trainable_7fb2604a</td><td>2023-07-19_09-41-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">261.525</td><td style=\"text-align: right;\">1.70589e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">513011</td><td style=\"text-align: right;\"> 464.075</td><td style=\"text-align: right;\">            0.372649</td><td style=\"text-align: right;\">          0.372649</td><td style=\"text-align: right;\">      0.372649</td><td style=\"text-align: right;\"> 1689727262</td><td style=\"text-align: right;\">                   1</td><td>7fb2604a  </td></tr>\n",
       "<tr><td>FSR_Trainable_8111e724</td><td>2023-07-19_09-50-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">214.705</td><td style=\"text-align: right;\">7.2123e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">522204</td><td style=\"text-align: right;\"> 381.628</td><td style=\"text-align: right;\">            0.528618</td><td style=\"text-align: right;\">          0.528618</td><td style=\"text-align: right;\">      0.528618</td><td style=\"text-align: right;\"> 1689727843</td><td style=\"text-align: right;\">                   1</td><td>8111e724  </td></tr>\n",
       "<tr><td>FSR_Trainable_83b58a49</td><td>2023-07-19_09-48-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">110.233</td><td style=\"text-align: right;\">5.19771e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">518592</td><td style=\"text-align: right;\"> 191.061</td><td style=\"text-align: right;\">           95.756   </td><td style=\"text-align: right;\">          0.967904</td><td style=\"text-align: right;\">     95.756   </td><td style=\"text-align: right;\"> 1689727693</td><td style=\"text-align: right;\">                 100</td><td>83b58a49  </td></tr>\n",
       "<tr><td>FSR_Trainable_8425873d</td><td>2023-07-19_09-56-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">131.578</td><td style=\"text-align: right;\">5.34429e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">529707</td><td style=\"text-align: right;\"> 236.692</td><td style=\"text-align: right;\">            2.3126  </td><td style=\"text-align: right;\">          0.504037</td><td style=\"text-align: right;\">      2.3126  </td><td style=\"text-align: right;\"> 1689728187</td><td style=\"text-align: right;\">                   4</td><td>8425873d  </td></tr>\n",
       "<tr><td>FSR_Trainable_871e547f</td><td>2023-07-19_09-52-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">108.712</td><td style=\"text-align: right;\">5.05852e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">523333</td><td style=\"text-align: right;\"> 188.687</td><td style=\"text-align: right;\">           55.1888  </td><td style=\"text-align: right;\">          0.367992</td><td style=\"text-align: right;\">     55.1888  </td><td style=\"text-align: right;\"> 1689727955</td><td style=\"text-align: right;\">                 100</td><td>871e547f  </td></tr>\n",
       "<tr><td>FSR_Trainable_88a24ed6</td><td>2023-07-19_09-48-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">112.475</td><td style=\"text-align: right;\">5.30245e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">519156</td><td style=\"text-align: right;\"> 195.094</td><td style=\"text-align: right;\">           88.1338  </td><td style=\"text-align: right;\">          0.780897</td><td style=\"text-align: right;\">     88.1338  </td><td style=\"text-align: right;\"> 1689727723</td><td style=\"text-align: right;\">                 100</td><td>88a24ed6  </td></tr>\n",
       "<tr><td>FSR_Trainable_899586ad</td><td>2023-07-19_09-48-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">206.197</td><td style=\"text-align: right;\">6.87544e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">520103</td><td style=\"text-align: right;\"> 449.633</td><td style=\"text-align: right;\">            0.606609</td><td style=\"text-align: right;\">          0.606609</td><td style=\"text-align: right;\">      0.606609</td><td style=\"text-align: right;\"> 1689727718</td><td style=\"text-align: right;\">                   1</td><td>899586ad  </td></tr>\n",
       "<tr><td>FSR_Trainable_89d7bea9</td><td>2023-07-19_09-50-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">108.713</td><td style=\"text-align: right;\">5.18333e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">520784</td><td style=\"text-align: right;\"> 188.158</td><td style=\"text-align: right;\">           73.5195  </td><td style=\"text-align: right;\">          0.637583</td><td style=\"text-align: right;\">     73.5195  </td><td style=\"text-align: right;\"> 1689727833</td><td style=\"text-align: right;\">                 100</td><td>89d7bea9  </td></tr>\n",
       "<tr><td>FSR_Trainable_8c44c95a</td><td>2023-07-19_09-50-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">233.999</td><td style=\"text-align: right;\">9.90945e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">522432</td><td style=\"text-align: right;\"> 388.351</td><td style=\"text-align: right;\">            0.329724</td><td style=\"text-align: right;\">          0.329724</td><td style=\"text-align: right;\">      0.329724</td><td style=\"text-align: right;\"> 1689727852</td><td style=\"text-align: right;\">                   1</td><td>8c44c95a  </td></tr>\n",
       "<tr><td>FSR_Trainable_8cb6ee6c</td><td>2023-07-19_09-52-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">219.636</td><td style=\"text-align: right;\">1.02228e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">524267</td><td style=\"text-align: right;\"> 364.208</td><td style=\"text-align: right;\">            0.610253</td><td style=\"text-align: right;\">          0.610253</td><td style=\"text-align: right;\">      0.610253</td><td style=\"text-align: right;\"> 1689727947</td><td style=\"text-align: right;\">                   1</td><td>8cb6ee6c  </td></tr>\n",
       "<tr><td>FSR_Trainable_8eb4940a</td><td>2023-07-19_09-50-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">108.038</td><td style=\"text-align: right;\">5.11277e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">520569</td><td style=\"text-align: right;\"> 187.014</td><td style=\"text-align: right;\">           75.4805  </td><td style=\"text-align: right;\">          0.802354</td><td style=\"text-align: right;\">     75.4805  </td><td style=\"text-align: right;\"> 1689727825</td><td style=\"text-align: right;\">                 100</td><td>8eb4940a  </td></tr>\n",
       "<tr><td>FSR_Trainable_95d0f2ef</td><td>2023-07-19_09-45-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">216.196</td><td style=\"text-align: right;\">5.2533e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">517692</td><td style=\"text-align: right;\"> 359.551</td><td style=\"text-align: right;\">            1.05627 </td><td style=\"text-align: right;\">          0.378564</td><td style=\"text-align: right;\">      1.05627 </td><td style=\"text-align: right;\"> 1689727555</td><td style=\"text-align: right;\">                   2</td><td>95d0f2ef  </td></tr>\n",
       "<tr><td>FSR_Trainable_969ec62b</td><td>2023-07-19_09-48-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">110.268</td><td style=\"text-align: right;\">5.13003e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">518505</td><td style=\"text-align: right;\"> 191.472</td><td style=\"text-align: right;\">           94.1257  </td><td style=\"text-align: right;\">          0.873177</td><td style=\"text-align: right;\">     94.1257  </td><td style=\"text-align: right;\"> 1689727685</td><td style=\"text-align: right;\">                 100</td><td>969ec62b  </td></tr>\n",
       "<tr><td>FSR_Trainable_972ebba7</td><td>2023-07-19_09-41-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">236.778</td><td style=\"text-align: right;\">1.46707e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">513468</td><td style=\"text-align: right;\"> 383.587</td><td style=\"text-align: right;\">            3.93868 </td><td style=\"text-align: right;\">          0.937828</td><td style=\"text-align: right;\">      3.93868 </td><td style=\"text-align: right;\"> 1689727289</td><td style=\"text-align: right;\">                   4</td><td>972ebba7  </td></tr>\n",
       "<tr><td>FSR_Trainable_995f7ebd</td><td>2023-07-19_09-48-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">209.267</td><td style=\"text-align: right;\">4.14349e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">519876</td><td style=\"text-align: right;\"> 431.29 </td><td style=\"text-align: right;\">            0.370386</td><td style=\"text-align: right;\">          0.370386</td><td style=\"text-align: right;\">      0.370386</td><td style=\"text-align: right;\"> 1689727708</td><td style=\"text-align: right;\">                   1</td><td>995f7ebd  </td></tr>\n",
       "<tr><td>FSR_Trainable_9a1e6356</td><td>2023-07-19_09-50-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">204.195</td><td style=\"text-align: right;\">1.19575e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">521731</td><td style=\"text-align: right;\"> 321.313</td><td style=\"text-align: right;\">            1.10606 </td><td style=\"text-align: right;\">          0.415493</td><td style=\"text-align: right;\">      1.10606 </td><td style=\"text-align: right;\"> 1689727823</td><td style=\"text-align: right;\">                   2</td><td>9a1e6356  </td></tr>\n",
       "<tr><td>FSR_Trainable_9e5d2527</td><td>2023-07-19_09-53-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">137.389</td><td style=\"text-align: right;\">5.54972e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">525879</td><td style=\"text-align: right;\"> 247.359</td><td style=\"text-align: right;\">            3.52739 </td><td style=\"text-align: right;\">          0.794385</td><td style=\"text-align: right;\">      3.52739 </td><td style=\"text-align: right;\"> 1689728009</td><td style=\"text-align: right;\">                   4</td><td>9e5d2527  </td></tr>\n",
       "<tr><td>FSR_Trainable_a1901920</td><td>2023-07-19_09-45-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">270.911</td><td style=\"text-align: right;\">5.21376e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">516572</td><td style=\"text-align: right;\"> 399.67 </td><td style=\"text-align: right;\">            0.831791</td><td style=\"text-align: right;\">          0.831791</td><td style=\"text-align: right;\">      0.831791</td><td style=\"text-align: right;\"> 1689727504</td><td style=\"text-align: right;\">                   1</td><td>a1901920  </td></tr>\n",
       "<tr><td>FSR_Trainable_a33e0728</td><td>2023-07-19_09-49-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">108.058</td><td style=\"text-align: right;\">5.13143e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">519445</td><td style=\"text-align: right;\"> 187.002</td><td style=\"text-align: right;\">           69.426   </td><td style=\"text-align: right;\">          0.820419</td><td style=\"text-align: right;\">     69.426   </td><td style=\"text-align: right;\"> 1689727770</td><td style=\"text-align: right;\">                 100</td><td>a33e0728  </td></tr>\n",
       "<tr><td>FSR_Trainable_a516f150</td><td>2023-07-19_09-51-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">108.261</td><td style=\"text-align: right;\">5.11415e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">521071</td><td style=\"text-align: right;\"> 187.615</td><td style=\"text-align: right;\">           67.0738  </td><td style=\"text-align: right;\">          0.593313</td><td style=\"text-align: right;\">     67.0738  </td><td style=\"text-align: right;\"> 1689727863</td><td style=\"text-align: right;\">                 100</td><td>a516f150  </td></tr>\n",
       "<tr><td>FSR_Trainable_aa6d1ad1</td><td>2023-07-19_09-50-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">241.338</td><td style=\"text-align: right;\">1.43499e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">521502</td><td style=\"text-align: right;\"> 394.754</td><td style=\"text-align: right;\">            0.682003</td><td style=\"text-align: right;\">          0.682003</td><td style=\"text-align: right;\">      0.682003</td><td style=\"text-align: right;\"> 1689727806</td><td style=\"text-align: right;\">                   1</td><td>aa6d1ad1  </td></tr>\n",
       "<tr><td>FSR_Trainable_acd3f5d3</td><td>2023-07-19_09-47-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">111.095</td><td style=\"text-align: right;\">5.20531e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">518190</td><td style=\"text-align: right;\"> 192.557</td><td style=\"text-align: right;\">           92.2076  </td><td style=\"text-align: right;\">          0.847869</td><td style=\"text-align: right;\">     92.2076  </td><td style=\"text-align: right;\"> 1689727674</td><td style=\"text-align: right;\">                 100</td><td>acd3f5d3  </td></tr>\n",
       "<tr><td>FSR_Trainable_ae1bac87</td><td>2023-07-19_09-41-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">242.758</td><td style=\"text-align: right;\">1.45975e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">513698</td><td style=\"text-align: right;\"> 381.856</td><td style=\"text-align: right;\">            4.30881 </td><td style=\"text-align: right;\">          1.14018 </td><td style=\"text-align: right;\">      4.30881 </td><td style=\"text-align: right;\"> 1689727302</td><td style=\"text-align: right;\">                   4</td><td>ae1bac87  </td></tr>\n",
       "<tr><td>FSR_Trainable_af997dd1</td><td>2023-07-19_09-55-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">107.022</td><td style=\"text-align: right;\">5.10928e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">527318</td><td style=\"text-align: right;\"> 184.82 </td><td style=\"text-align: right;\">           57.5012  </td><td style=\"text-align: right;\">          0.579187</td><td style=\"text-align: right;\">     57.5012  </td><td style=\"text-align: right;\"> 1689728125</td><td style=\"text-align: right;\">                 100</td><td>af997dd1  </td></tr>\n",
       "<tr><td>FSR_Trainable_b5ad3a46</td><td>2023-07-19_09-45-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">421.333</td><td style=\"text-align: right;\">5.07914e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">516771</td><td style=\"text-align: right;\"> 666.975</td><td style=\"text-align: right;\">            0.700626</td><td style=\"text-align: right;\">          0.700626</td><td style=\"text-align: right;\">      0.700626</td><td style=\"text-align: right;\"> 1689727518</td><td style=\"text-align: right;\">                   1</td><td>b5ad3a46  </td></tr>\n",
       "<tr><td>FSR_Trainable_b5fc5518</td><td>2023-07-19_09-43-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">121.818</td><td style=\"text-align: right;\">5.50608e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">513929</td><td style=\"text-align: right;\"> 213.149</td><td style=\"text-align: right;\">           67.0176  </td><td style=\"text-align: right;\">          0.967144</td><td style=\"text-align: right;\">     67.0176  </td><td style=\"text-align: right;\"> 1689727383</td><td style=\"text-align: right;\">                  64</td><td>b5fc5518  </td></tr>\n",
       "<tr><td>FSR_Trainable_b7d4a193</td><td>2023-07-19_09-53-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">235.191</td><td style=\"text-align: right;\">1.0595e+08 </td><td>172.26.215.93</td><td style=\"text-align: right;\">525476</td><td style=\"text-align: right;\"> 386.811</td><td style=\"text-align: right;\">            0.66003 </td><td style=\"text-align: right;\">          0.66003 </td><td style=\"text-align: right;\">      0.66003 </td><td style=\"text-align: right;\"> 1689727990</td><td style=\"text-align: right;\">                   1</td><td>b7d4a193  </td></tr>\n",
       "<tr><td>FSR_Trainable_bbc12f41</td><td>2023-07-19_09-56-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">110.248</td><td style=\"text-align: right;\">5.4506e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">529943</td><td style=\"text-align: right;\"> 190.625</td><td style=\"text-align: right;\">           15.6126  </td><td style=\"text-align: right;\">          0.469158</td><td style=\"text-align: right;\">     15.6126  </td><td style=\"text-align: right;\"> 1689728214</td><td style=\"text-align: right;\">                  32</td><td>bbc12f41  </td></tr>\n",
       "<tr><td>FSR_Trainable_be852e93</td><td>2023-07-19_09-43-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">128.62 </td><td style=\"text-align: right;\">1.99125e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">514389</td><td style=\"text-align: right;\"> 213.206</td><td style=\"text-align: right;\">           81.1037  </td><td style=\"text-align: right;\">          1.10047 </td><td style=\"text-align: right;\">     81.1037  </td><td style=\"text-align: right;\"> 1689727419</td><td style=\"text-align: right;\">                  64</td><td>be852e93  </td></tr>\n",
       "<tr><td>FSR_Trainable_c2226ee4</td><td>2023-07-19_09-55-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">185.111</td><td style=\"text-align: right;\">9.41892e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">528784</td><td style=\"text-align: right;\"> 379.48 </td><td style=\"text-align: right;\">            0.655505</td><td style=\"text-align: right;\">          0.655505</td><td style=\"text-align: right;\">      0.655505</td><td style=\"text-align: right;\"> 1689728147</td><td style=\"text-align: right;\">                   1</td><td>c2226ee4  </td></tr>\n",
       "<tr><td>FSR_Trainable_c335853a</td><td>2023-07-19_09-45-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">106.757</td><td style=\"text-align: right;\">5.0723e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">516050</td><td style=\"text-align: right;\"> 184.729</td><td style=\"text-align: right;\">           87.5677  </td><td style=\"text-align: right;\">          0.650166</td><td style=\"text-align: right;\">     87.5677  </td><td style=\"text-align: right;\"> 1689727535</td><td style=\"text-align: right;\">                 100</td><td>c335853a  </td></tr>\n",
       "<tr><td>FSR_Trainable_c3683c7b</td><td>2023-07-19_09-45-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">106.384</td><td style=\"text-align: right;\">5.01604e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">515849</td><td style=\"text-align: right;\"> 184.365</td><td style=\"text-align: right;\">          100.068   </td><td style=\"text-align: right;\">          0.656042</td><td style=\"text-align: right;\">    100.068   </td><td style=\"text-align: right;\"> 1689727535</td><td style=\"text-align: right;\">                 100</td><td>c3683c7b  </td></tr>\n",
       "<tr><td>FSR_Trainable_c8b18d92</td><td>2023-07-19_09-53-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">244.718</td><td style=\"text-align: right;\">1.15085e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">526641</td><td style=\"text-align: right;\"> 391.93 </td><td style=\"text-align: right;\">            0.755255</td><td style=\"text-align: right;\">          0.755255</td><td style=\"text-align: right;\">      0.755255</td><td style=\"text-align: right;\"> 1689728031</td><td style=\"text-align: right;\">                   1</td><td>c8b18d92  </td></tr>\n",
       "<tr><td>FSR_Trainable_c92d0ac4</td><td>2023-07-19_09-52-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">111.534</td><td style=\"text-align: right;\">5.52091e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">523780</td><td style=\"text-align: right;\"> 192.133</td><td style=\"text-align: right;\">           12.6308  </td><td style=\"text-align: right;\">          0.688823</td><td style=\"text-align: right;\">     12.6308  </td><td style=\"text-align: right;\"> 1689727926</td><td style=\"text-align: right;\">                  16</td><td>c92d0ac4  </td></tr>\n",
       "<tr><td>FSR_Trainable_cf91cc1a</td><td>2023-07-19_09-39-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">251.92 </td><td style=\"text-align: right;\">1.99009e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">510290</td><td style=\"text-align: right;\"> 380.481</td><td style=\"text-align: right;\">            7.40789 </td><td style=\"text-align: right;\">          0.449334</td><td style=\"text-align: right;\">      7.40789 </td><td style=\"text-align: right;\"> 1689727143</td><td style=\"text-align: right;\">                  16</td><td>cf91cc1a  </td></tr>\n",
       "<tr><td>FSR_Trainable_d150faff</td><td>2023-07-19_09-45-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">227.394</td><td style=\"text-align: right;\">4.62401e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">517479</td><td style=\"text-align: right;\"> 398.095</td><td style=\"text-align: right;\">            0.53426 </td><td style=\"text-align: right;\">          0.53426 </td><td style=\"text-align: right;\">      0.53426 </td><td style=\"text-align: right;\"> 1689727545</td><td style=\"text-align: right;\">                   1</td><td>d150faff  </td></tr>\n",
       "<tr><td>FSR_Trainable_d95d72ea</td><td>2023-07-19_09-41-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">108.37 </td><td style=\"text-align: right;\">5.0805e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">510960</td><td style=\"text-align: right;\"> 187.604</td><td style=\"text-align: right;\">           88.4398  </td><td style=\"text-align: right;\">          0.779449</td><td style=\"text-align: right;\">     88.4398  </td><td style=\"text-align: right;\"> 1689727261</td><td style=\"text-align: right;\">                 100</td><td>d95d72ea  </td></tr>\n",
       "<tr><td>FSR_Trainable_ddefe89d</td><td>2023-07-19_09-48-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">180.231</td><td style=\"text-align: right;\">8.91752e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">519643</td><td style=\"text-align: right;\"> 390.469</td><td style=\"text-align: right;\">            0.461459</td><td style=\"text-align: right;\">          0.461459</td><td style=\"text-align: right;\">      0.461459</td><td style=\"text-align: right;\"> 1689727697</td><td style=\"text-align: right;\">                   1</td><td>ddefe89d  </td></tr>\n",
       "<tr><td>FSR_Trainable_ddfa8c6f</td><td>2023-07-19_09-53-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">234.771</td><td style=\"text-align: right;\">4.73254e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">526875</td><td style=\"text-align: right;\"> 378.104</td><td style=\"text-align: right;\">            0.99031 </td><td style=\"text-align: right;\">          0.99031 </td><td style=\"text-align: right;\">      0.99031 </td><td style=\"text-align: right;\"> 1689728039</td><td style=\"text-align: right;\">                   1</td><td>ddfa8c6f  </td></tr>\n",
       "<tr><td>FSR_Trainable_e0d18344</td><td>2023-07-19_09-40-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">635.734</td><td style=\"text-align: right;\">1.49198e+18</td><td>172.26.215.93</td><td style=\"text-align: right;\">512315</td><td style=\"text-align: right;\"> 906.537</td><td style=\"text-align: right;\">            4.35872 </td><td style=\"text-align: right;\">          0.443914</td><td style=\"text-align: right;\">      4.35872 </td><td style=\"text-align: right;\"> 1689727236</td><td style=\"text-align: right;\">                   8</td><td>e0d18344  </td></tr>\n",
       "<tr><td>FSR_Trainable_e29404c6</td><td>2023-07-19_09-55-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">107.284</td><td style=\"text-align: right;\">5.10564e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">527630</td><td style=\"text-align: right;\"> 185.389</td><td style=\"text-align: right;\">           59.1306  </td><td style=\"text-align: right;\">          0.559005</td><td style=\"text-align: right;\">     59.1306  </td><td style=\"text-align: right;\"> 1689728134</td><td style=\"text-align: right;\">                 100</td><td>e29404c6  </td></tr>\n",
       "<tr><td>FSR_Trainable_e3450325</td><td>2023-07-19_09-59-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">110.672</td><td style=\"text-align: right;\">5.4779e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">532889</td><td style=\"text-align: right;\"> 190.365</td><td style=\"text-align: right;\">           19.3772  </td><td style=\"text-align: right;\">          0.330273</td><td style=\"text-align: right;\">     19.3772  </td><td style=\"text-align: right;\"> 1689728348</td><td style=\"text-align: right;\">                  32</td><td>e3450325  </td></tr>\n",
       "<tr><td>FSR_Trainable_e476ad54</td><td>2023-07-19_09-42-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">107.189</td><td style=\"text-align: right;\">5.03803e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">512547</td><td style=\"text-align: right;\"> 185.686</td><td style=\"text-align: right;\">           71.7844  </td><td style=\"text-align: right;\">          0.771385</td><td style=\"text-align: right;\">     71.7844  </td><td style=\"text-align: right;\"> 1689727327</td><td style=\"text-align: right;\">                 100</td><td>e476ad54  </td></tr>\n",
       "<tr><td>FSR_Trainable_e59245c0</td><td>2023-07-19_09-54-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">198.733</td><td style=\"text-align: right;\">1.10439e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">526954</td><td style=\"text-align: right;\"> 317.39 </td><td style=\"text-align: right;\">            1.55291 </td><td style=\"text-align: right;\">          0.573229</td><td style=\"text-align: right;\">      1.55291 </td><td style=\"text-align: right;\"> 1689728047</td><td style=\"text-align: right;\">                   2</td><td>e59245c0  </td></tr>\n",
       "<tr><td>FSR_Trainable_e652c47e</td><td>2023-07-19_09-56-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">232.338</td><td style=\"text-align: right;\">1.29288e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">529257</td><td style=\"text-align: right;\"> 404.051</td><td style=\"text-align: right;\">            0.338415</td><td style=\"text-align: right;\">          0.338415</td><td style=\"text-align: right;\">      0.338415</td><td style=\"text-align: right;\"> 1689728167</td><td style=\"text-align: right;\">                   1</td><td>e652c47e  </td></tr>\n",
       "<tr><td>FSR_Trainable_e95decd0</td><td>2023-07-19_09-52-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">109.399</td><td style=\"text-align: right;\">5.09973e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">522885</td><td style=\"text-align: right;\"> 189.366</td><td style=\"text-align: right;\">           55.5889  </td><td style=\"text-align: right;\">          0.539719</td><td style=\"text-align: right;\">     55.5889  </td><td style=\"text-align: right;\"> 1689727936</td><td style=\"text-align: right;\">                 100</td><td>e95decd0  </td></tr>\n",
       "<tr><td>FSR_Trainable_ea213d18</td><td>2023-07-19_09-55-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">107.295</td><td style=\"text-align: right;\">5.06327e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">528065</td><td style=\"text-align: right;\"> 185.368</td><td style=\"text-align: right;\">           60.0564  </td><td style=\"text-align: right;\">          0.523615</td><td style=\"text-align: right;\">     60.0564  </td><td style=\"text-align: right;\"> 1689728156</td><td style=\"text-align: right;\">                 100</td><td>ea213d18  </td></tr>\n",
       "<tr><td>FSR_Trainable_ec6ac595</td><td>2023-07-19_09-57-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">236.526</td><td style=\"text-align: right;\">4.31953e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">530749</td><td style=\"text-align: right;\"> 383.498</td><td style=\"text-align: right;\">            0.76722 </td><td style=\"text-align: right;\">          0.76722 </td><td style=\"text-align: right;\">      0.76722 </td><td style=\"text-align: right;\"> 1689728223</td><td style=\"text-align: right;\">                   1</td><td>ec6ac595  </td></tr>\n",
       "<tr><td>FSR_Trainable_edc3e95b</td><td>2023-07-19_09-39-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">258.637</td><td style=\"text-align: right;\">8.39168e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">510461</td><td style=\"text-align: right;\"> 549.013</td><td style=\"text-align: right;\">            1.45104 </td><td style=\"text-align: right;\">          0.277796</td><td style=\"text-align: right;\">      1.45104 </td><td style=\"text-align: right;\"> 1689727142</td><td style=\"text-align: right;\">                   4</td><td>edc3e95b  </td></tr>\n",
       "<tr><td>FSR_Trainable_ee3b5ce8</td><td>2023-07-19_09-51-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">223.053</td><td style=\"text-align: right;\">7.19333e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">522663</td><td style=\"text-align: right;\"> 375.551</td><td style=\"text-align: right;\">            0.419208</td><td style=\"text-align: right;\">          0.419208</td><td style=\"text-align: right;\">      0.419208</td><td style=\"text-align: right;\"> 1689727861</td><td style=\"text-align: right;\">                   1</td><td>ee3b5ce8  </td></tr>\n",
       "<tr><td>FSR_Trainable_fb53367b</td><td>2023-07-19_09-45-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">407.841</td><td style=\"text-align: right;\">6.04399e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">517003</td><td style=\"text-align: right;\"> 639.643</td><td style=\"text-align: right;\">            0.630741</td><td style=\"text-align: right;\">          0.630741</td><td style=\"text-align: right;\">      0.630741</td><td style=\"text-align: right;\"> 1689727529</td><td style=\"text-align: right;\">                   1</td><td>fb53367b  </td></tr>\n",
       "<tr><td>FSR_Trainable_fbdcd699</td><td>2023-07-19_09-52-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">111.079</td><td style=\"text-align: right;\">5.37775e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">523557</td><td style=\"text-align: right;\"> 191.263</td><td style=\"text-align: right;\">           24.8615  </td><td style=\"text-align: right;\">          0.6751  </td><td style=\"text-align: right;\">     24.8615  </td><td style=\"text-align: right;\"> 1689727928</td><td style=\"text-align: right;\">                  32</td><td>fbdcd699  </td></tr>\n",
       "<tr><td>FSR_Trainable_fe9934f5</td><td>2023-07-19_09-45-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">114.364</td><td style=\"text-align: right;\">5.75342e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">515604</td><td style=\"text-align: right;\"> 195.328</td><td style=\"text-align: right;\">          102.991   </td><td style=\"text-align: right;\">          1.00122 </td><td style=\"text-align: right;\">    102.991   </td><td style=\"text-align: right;\"> 1689727508</td><td style=\"text-align: right;\">                 100</td><td>fe9934f5  </td></tr>\n",
       "<tr><td>FSR_Trainable_ff32dc77</td><td>2023-07-19_09-43-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">122.287</td><td style=\"text-align: right;\">5.7989e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">514615</td><td style=\"text-align: right;\"> 213.55 </td><td style=\"text-align: right;\">           64.2135  </td><td style=\"text-align: right;\">          0.829618</td><td style=\"text-align: right;\">     64.2135  </td><td style=\"text-align: right;\"> 1689727413</td><td style=\"text-align: right;\">                  64</td><td>ff32dc77  </td></tr>\n",
       "<tr><td>FSR_Trainable_ffdc5c38</td><td>2023-07-19_09-45-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">500.533</td><td style=\"text-align: right;\">1.18927e+18</td><td>172.26.215.93</td><td style=\"text-align: right;\">517230</td><td style=\"text-align: right;\"> 601.003</td><td style=\"text-align: right;\">            0.669514</td><td style=\"text-align: right;\">          0.669514</td><td style=\"text-align: right;\">      0.669514</td><td style=\"text-align: right;\"> 1689727538</td><td style=\"text-align: right;\">                   1</td><td>ffdc5c38  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_5a2a7e89_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_09-38-39/wandb/run-20230719_093849-5a2a7e89\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Syncing run FSR_Trainable_5a2a7e89\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5a2a7e89\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_cf91cc1a_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_09-38-44/wandb/run-20230719_093856-cf91cc1a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: Syncing run FSR_Trainable_cf91cc1a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cf91cc1a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_edc3e95b_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_09-38-51/wandb/run-20230719_093904-edc3e95b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: Syncing run FSR_Trainable_edc3e95b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/edc3e95b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb:                      mae ▄▃▄▃▃▁▅▄▄▅▅█▆▄▆▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb:                     mape ▁▁▁▂▂▄▇▇▃▅▄▇▅▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb:                     rmse ▄▂▄▃▃▁▃▄▃▅▄█▇▅▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb:         time_this_iter_s ▅▃▂▃▄▂▂▃▁▃▃▁█▄▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb:                timestamp ▁▃▃▃▄▄▄▅▅▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb:                      mae 251.92048\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb:                     mape 199009154.79715\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb:                     rmse 380.48134\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb:       time_since_restore 7.40789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb:         time_this_iter_s 0.44933\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb:             time_total_s 7.40789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb:                timestamp 1689727143\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: 🚀 View run FSR_Trainable_cf91cc1a at: https://wandb.ai/seokjin/FSR-prediction/runs/cf91cc1a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510460)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_093856-cf91cc1a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb:                      mae █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb:                     mape █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb:                     rmse █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb:         time_this_iter_s █▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb:                timestamp ▁███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_093904-edc3e95b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_3d510e11_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_09-38-58/wandb/run-20230719_093911-3d510e11\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: Syncing run FSR_Trainable_3d510e11\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3d510e11\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510634)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb:                      mae █▆▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb:                     mape █▆▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb:                     rmse █▆▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb:       time_since_restore ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb:         time_this_iter_s █▃▁▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb:             time_total_s ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb:                timestamp ▁███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb:                      mae 240.06066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb:                     mape 9349392439409340.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb:                     rmse 505.26112\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb:       time_since_restore 1.22106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb:         time_this_iter_s 0.31621\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb:             time_total_s 1.22106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb:                timestamp 1689727149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: 🚀 View run FSR_Trainable_3d510e11 at: https://wandb.ai/seokjin/FSR-prediction/runs/3d510e11\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_093911-3d510e11/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510822)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_d95d72ea_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_09-39-06/wandb/run-20230719_093920-d95d72ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: Syncing run FSR_Trainable_d95d72ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d95d72ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_24c65055_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_09-39-15/wandb/run-20230719_093933-24c65055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: Syncing run FSR_Trainable_24c65055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/24c65055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb:                      mae █▇▆▅▄▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb:                     mape █▆▅▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb:                     rmse █▇▆▅▄▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb:         time_this_iter_s █▂▇▆▃▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb:                timestamp ▁▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb:                      mae 195.08087\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb:                     mape 1.243317704776809e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb:                     rmse 402.03093\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb:       time_since_restore 4.59278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb:         time_this_iter_s 0.37899\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb:             time_total_s 4.59278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb:                timestamp 1689727173\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: 🚀 View run FSR_Trainable_24c65055 at: https://wandb.ai/seokjin/FSR-prediction/runs/24c65055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511273)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_093933-24c65055/logs\n",
      "2023-07-19 09:39:43,270\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.561 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:39:43,273\tWARNING util.py:315 -- The `process_trial_result` operation took 2.564 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:39:43,275\tWARNING util.py:315 -- Processing trial results took 2.567 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:39:43,277\tWARNING util.py:315 -- The `process_trial_result` operation took 2.568 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_208d51c4_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_09-39-26/wandb/run-20230719_093946-208d51c4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: Syncing run FSR_Trainable_208d51c4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/208d51c4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb:                      mae █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb:                     mape ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb:                     rmse █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb:         time_this_iter_s █▁▇▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb:                timestamp ▁▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb:                      mae 239.74915\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb:                     mape 3.300295095568491e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb:                     rmse 431.61795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb:       time_since_restore 1.51161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb:         time_this_iter_s 0.32346\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb:             time_total_s 1.51161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb:                timestamp 1689727184\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: 🚀 View run FSR_Trainable_208d51c4 at: https://wandb.ai/seokjin/FSR-prediction/runs/208d51c4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511501)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_093946-208d51c4/logs\n",
      "2023-07-19 09:39:54,970\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.349 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:39:54,974\tWARNING util.py:315 -- The `process_trial_result` operation took 2.354 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:39:54,975\tWARNING util.py:315 -- Processing trial results took 2.355 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:39:54,980\tWARNING util.py:315 -- The `process_trial_result` operation took 2.359 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_5134dae4_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_09-39-40/wandb/run-20230719_093957-5134dae4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: Syncing run FSR_Trainable_5134dae4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5134dae4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)04 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb:                      mae 322.83099\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb:                     mape 178540757.70718\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb:                     rmse 713.0426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb:       time_since_restore 0.56325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb:         time_this_iter_s 0.56325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb:             time_total_s 0.56325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb:                timestamp 1689727192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: 🚀 View run FSR_Trainable_5134dae4 at: https://wandb.ai/seokjin/FSR-prediction/runs/5134dae4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511723)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_093957-5134dae4/logs\n",
      "2023-07-19 09:40:05,736\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.330 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:40:05,740\tWARNING util.py:315 -- The `process_trial_result` operation took 2.334 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:40:05,745\tWARNING util.py:315 -- Processing trial results took 2.340 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:40:05,747\tWARNING util.py:315 -- The `process_trial_result` operation took 2.341 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_0bc3160d_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_09-39-52/wandb/run-20230719_094009-0bc3160d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: Syncing run FSR_Trainable_0bc3160d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0bc3160d\n",
      "2023-07-19 09:40:17,974\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.980 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:40:17,983\tWARNING util.py:315 -- The `process_trial_result` operation took 1.990 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:40:17,986\tWARNING util.py:315 -- Processing trial results took 1.993 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:40:17,987\tWARNING util.py:315 -- The `process_trial_result` operation took 1.994 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_3662243b_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-40-02/wandb/run-20230719_094020-3662243b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: Syncing run FSR_Trainable_3662243b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3662243b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb:                      mae 230.18966\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb:                     mape 2.2203627173145104e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb:                     rmse 446.22745\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb:       time_since_restore 0.55488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb:         time_this_iter_s 0.55488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb:             time_total_s 0.55488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb:                timestamp 1689727215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: 🚀 View run FSR_Trainable_3662243b at: https://wandb.ai/seokjin/FSR-prediction/runs/3662243b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512174)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094020-3662243b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:40:32,585\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.239 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:40:32,590\tWARNING util.py:315 -- The `process_trial_result` operation took 2.244 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:40:32,591\tWARNING util.py:315 -- Processing trial results took 2.246 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:40:32,592\tWARNING util.py:315 -- The `process_trial_result` operation took 2.247 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb:                      mae ▂▂▅▂▅▂▁▁▁▁▁▂█▄▃▃▃▄▅▆▆▇▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb:                     mape ▂▁▂▃▂▄▃▂▄▃▃▃▇▅▅▅▅▆▆▇▇████████▇▇▇▇▆▆▆▆▆▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb:                     rmse ▂▂▆▂▇▂▁▁▁▁▁▂█▄▄▃▃▄▄▅▆▆▇▇▇▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb:         time_this_iter_s ▆▃▂▄▃▂▄▂▃▃▁▂▂▄▄▃▇▇█▄▄▅▆▃▄▃▃▃▅▃▅█▆▅▃▃▅▄▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=510288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_e0d18344_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-40-15/wandb/run-20230719_094035-e0d18344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: Syncing run FSR_Trainable_e0d18344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e0d18344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb:                      mae ▁▁▁▁▁█▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb:                     mape ▁▁▁▁▁█▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb:                     rmse ▁▁▁▁▁█▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb:         time_this_iter_s █▂▃▄▄▂▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb:                timestamp ▁▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb:                      mae 635.73359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb:                     mape 1.491984109633264e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb:                     rmse 906.53669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb:       time_since_restore 4.35872\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb:         time_this_iter_s 0.44391\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb:             time_total_s 4.35872\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb:                timestamp 1689727236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: 🚀 View run FSR_Trainable_e0d18344 at: https://wandb.ai/seokjin/FSR-prediction/runs/e0d18344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094035-e0d18344/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512413)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 09:40:43,298\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.830 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:40:43,303\tWARNING util.py:315 -- The `process_trial_result` operation took 1.835 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:40:43,305\tWARNING util.py:315 -- Processing trial results took 1.837 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:40:43,306\tWARNING util.py:315 -- The `process_trial_result` operation took 1.838 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_e476ad54_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-40-29/wandb/run-20230719_094046-e476ad54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: Syncing run FSR_Trainable_e476ad54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e476ad54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb:                      mae ████▇▇▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb:                     mape █████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb:                     rmse ████▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb:         time_this_iter_s █▄▆▅▄▄▃▄▂▃▂▂▅▃▅▃▅▇▂▄▃▅▃▃▃▄▇▃▄▃▅▂▂▁▁▂▂▁▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb:                      mae 166.86959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb:                     mape 145237880.27108\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb:                     rmse 281.20421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb:       time_since_restore 30.76708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb:         time_this_iter_s 0.27719\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb:             time_total_s 30.76708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb:                timestamp 1689727243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: 🚀 View run FSR_Trainable_0bc3160d at: https://wandb.ai/seokjin/FSR-prediction/runs/0bc3160d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511950)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094009-0bc3160d/logs\n",
      "2023-07-19 09:40:53,566\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.173 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:40:53,569\tWARNING util.py:315 -- The `process_trial_result` operation took 2.177 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:40:53,571\tWARNING util.py:315 -- Processing trial results took 2.180 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:40:53,572\tWARNING util.py:315 -- The `process_trial_result` operation took 2.181 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_2aab3b83_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-40-40/wandb/run-20230719_094056-2aab3b83\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Syncing run FSR_Trainable_2aab3b83\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2aab3b83\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb:                      mae █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb:                     mape █▁▁▁▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb:                     rmse █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb:         time_this_iter_s ▄▄▂▂▂█▃▄▂▅▃▂▃▅▃▂▁▂▄▂▃▂▃▂▄▂▃▄▃▃▁▁▃▂▂▁▃▃▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb:                      mae 108.3701\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb:                     mape 50804986.24547\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb:                     rmse 187.60443\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb:       time_since_restore 88.43983\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb:         time_this_iter_s 0.77945\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb:             time_total_s 88.43983\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb:                timestamp 1689727261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: 🚀 View run FSR_Trainable_d95d72ea at: https://wandb.ai/seokjin/FSR-prediction/runs/d95d72ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=511054)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_093920-d95d72ea/logs\n",
      "2023-07-19 09:41:05,436\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.429 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:41:05,443\tWARNING util.py:315 -- The `process_trial_result` operation took 2.438 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:41:05,445\tWARNING util.py:315 -- Processing trial results took 2.439 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:41:05,446\tWARNING util.py:315 -- The `process_trial_result` operation took 2.441 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_7fb2604a_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-40-50/wandb/run-20230719_094108-7fb2604a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: Syncing run FSR_Trainable_7fb2604a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7fb2604a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb:                      mae 261.52537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb:                     mape 170588978.50565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb:                     rmse 464.07472\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb:       time_since_restore 0.37265\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb:         time_this_iter_s 0.37265\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb:             time_total_s 0.37265\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb:                timestamp 1689727262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: 🚀 View run FSR_Trainable_7fb2604a at: https://wandb.ai/seokjin/FSR-prediction/runs/7fb2604a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513103)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094108-7fb2604a/logs\n",
      "2023-07-19 09:41:16,332\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.476 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:41:16,334\tWARNING util.py:315 -- The `process_trial_result` operation took 2.479 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:41:16,335\tWARNING util.py:315 -- Processing trial results took 2.480 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:41:16,336\tWARNING util.py:315 -- The `process_trial_result` operation took 2.481 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_262570e3_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-41-02/wandb/run-20230719_094119-262570e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: Syncing run FSR_Trainable_262570e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/262570e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb:                      mae 261.71885\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb:                     mape 137487280.29654\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb:                     rmse 414.83929\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb:       time_since_restore 0.56981\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb:         time_this_iter_s 0.2307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb:             time_total_s 0.56981\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb:                timestamp 1689727276\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: 🚀 View run FSR_Trainable_262570e3 at: https://wandb.ai/seokjin/FSR-prediction/runs/262570e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513336)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094119-262570e3/logs\n",
      "2023-07-19 09:41:26,652\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.709 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:41:26,655\tWARNING util.py:315 -- The `process_trial_result` operation took 1.713 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:41:26,657\tWARNING util.py:315 -- Processing trial results took 1.714 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:41:26,657\tWARNING util.py:315 -- The `process_trial_result` operation took 1.715 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_972ebba7_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-41-13/wandb/run-20230719_094129-972ebba7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: Syncing run FSR_Trainable_972ebba7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/972ebba7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb:                      mae █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb:                     mape █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb:                     rmse █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb:         time_this_iter_s █▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb:                      mae 236.77791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb:                     mape 146707287.26141\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb:                     rmse 383.58653\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb:       time_since_restore 3.93868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb:         time_this_iter_s 0.93783\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb:             time_total_s 3.93868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb:                timestamp 1689727289\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: 🚀 View run FSR_Trainable_972ebba7 at: https://wandb.ai/seokjin/FSR-prediction/runs/972ebba7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513564)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094129-972ebba7/logs\n",
      "2023-07-19 09:41:39,326\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.154 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:41:39,330\tWARNING util.py:315 -- The `process_trial_result` operation took 2.159 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:41:39,335\tWARNING util.py:315 -- Processing trial results took 2.165 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:41:39,337\tWARNING util.py:315 -- The `process_trial_result` operation took 2.167 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_ae1bac87_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-41-23/wandb/run-20230719_094142-ae1bac87\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: Syncing run FSR_Trainable_ae1bac87\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ae1bac87\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb:                      mae █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb:                     mape █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb:                     rmse █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb:         time_this_iter_s ▆▄▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb:                      mae 242.75848\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb:                     mape 145974547.31683\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb:                     rmse 381.85616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb:       time_since_restore 4.30881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb:         time_this_iter_s 1.14018\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb:             time_total_s 4.30881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb:                timestamp 1689727302\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: 🚀 View run FSR_Trainable_ae1bac87 at: https://wandb.ai/seokjin/FSR-prediction/runs/ae1bac87\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=513800)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094142-ae1bac87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:41:52,847\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.728 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:41:52,850\tWARNING util.py:315 -- The `process_trial_result` operation took 1.732 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:41:52,851\tWARNING util.py:315 -- Processing trial results took 1.733 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:41:52,852\tWARNING util.py:315 -- The `process_trial_result` operation took 1.734 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb:                      mae ██▄▅▅▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb:                     mape ▇█▄▅▅▅▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb:                     rmse █▇▅▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb:         time_this_iter_s ▆▄▄▆▅▂▂▃▅▄▅▂▄▁▂▂▄▂▂▂▃▂▂▂▆▅▃▄▆▅▃▁▆▆▆▄█▃▆▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094056-2aab3b83/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512882)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_b5fc5518_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-41-36/wandb/run-20230719_094155-b5fc5518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: Syncing run FSR_Trainable_b5fc5518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b5fc5518\n",
      "2023-07-19 09:42:03,922\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.552 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:42:03,924\tWARNING util.py:315 -- The `process_trial_result` operation took 1.555 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:42:03,927\tWARNING util.py:315 -- Processing trial results took 1.557 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:42:03,928\tWARNING util.py:315 -- The `process_trial_result` operation took 1.559 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_77aaede5_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-41-49/wandb/run-20230719_094207-77aaede5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: Syncing run FSR_Trainable_77aaede5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/77aaede5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb:                      mae █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb:                     mape █▇▃▂▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb:                     rmse █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb:         time_this_iter_s █▅▂▃▂▃▅▆▇▃▆▆▄▅▁▃▃▂▂▄▂▄▅▅▅▃▆▇▆█▇▄▇▄▃▂▄▄▆▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb:                      mae 107.18891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb:                     mape 50380272.89509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb:                     rmse 185.68644\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb:       time_since_restore 71.78436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb:         time_this_iter_s 0.77139\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb:             time_total_s 71.78436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb:                timestamp 1689727327\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: 🚀 View run FSR_Trainable_e476ad54 at: https://wandb.ai/seokjin/FSR-prediction/runs/e476ad54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=512651)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094046-e476ad54/logs\n",
      "2023-07-19 09:42:15,361\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.480 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:42:15,365\tWARNING util.py:315 -- The `process_trial_result` operation took 1.485 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:42:15,367\tWARNING util.py:315 -- Processing trial results took 1.487 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:42:15,368\tWARNING util.py:315 -- The `process_trial_result` operation took 1.488 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_be852e93_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-42-00/wandb/run-20230719_094218-be852e93\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: Syncing run FSR_Trainable_be852e93\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/be852e93\n",
      "2023-07-19 09:42:27,467\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.839 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:42:27,470\tWARNING util.py:315 -- The `process_trial_result` operation took 1.843 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:42:27,472\tWARNING util.py:315 -- Processing trial results took 1.846 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:42:27,474\tWARNING util.py:315 -- The `process_trial_result` operation took 1.847 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_ff32dc77_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-42-12/wandb/run-20230719_094230-ff32dc77\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: Syncing run FSR_Trainable_ff32dc77\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ff32dc77\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb:                      mae █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb:                     mape █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb:                     rmse █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb:         time_this_iter_s ▇▂▃▂▂▂▂▅▅▄▃▁▁▄▄▄▃▃▂▃▃▄▄▆▆▅▄▅▄▄▅▆▆▇█▅▇▆▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb:                      mae 121.818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb:                     mape 55060771.09566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb:                     rmse 213.14914\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb:       time_since_restore 67.01755\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb:         time_this_iter_s 0.96714\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb:             time_total_s 67.01755\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb:                timestamp 1689727383\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: 🚀 View run FSR_Trainable_b5fc5518 at: https://wandb.ai/seokjin/FSR-prediction/runs/b5fc5518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514034)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094155-b5fc5518/logs\n",
      "2023-07-19 09:43:18,459\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.008 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:43:18,462\tWARNING util.py:315 -- The `process_trial_result` operation took 2.012 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:43:18,463\tWARNING util.py:315 -- Processing trial results took 2.013 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:43:18,464\tWARNING util.py:315 -- The `process_trial_result` operation took 2.014 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_fe9934f5_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-42-24/wandb/run-20230719_094321-fe9934f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: Syncing run FSR_Trainable_fe9934f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fe9934f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb:                      mae █▆▄▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb:                     mape █▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb:                     rmse ██▅▂▁▁▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb:         time_this_iter_s ▅▃▄▅▄▃▅▅▄▄▄▆▇██▅▄▆▅▄▃▁▂▃▂▂▂▄▂▆▃▄▂▃▃▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb:                      mae 122.28653\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb:                     mape 57988955.46789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb:                     rmse 213.55044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb:       time_since_restore 64.2135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb:         time_this_iter_s 0.82962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb:             time_total_s 64.2135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb:                timestamp 1689727413\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: 🚀 View run FSR_Trainable_ff32dc77 at: https://wandb.ai/seokjin/FSR-prediction/runs/ff32dc77\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514700)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094230-ff32dc77/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb:                      mae █▃▃▂▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb:                     mape █▄▄▂▂▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb:                     rmse █▄▂▂▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb:         time_this_iter_s ▅▄▂▃▂▂▄▃▅▄▄▄▄▄▅▆█▆▄▆▄▂▁▂▃▂▃▂▅▄▂▃▂▂▂▂▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb:                      mae 128.61956\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb:                     mape 1.991246382171591e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb:                     rmse 213.20572\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb:       time_since_restore 81.1037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb:         time_this_iter_s 1.10047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb:             time_total_s 81.1037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb:                timestamp 1689727419\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: 🚀 View run FSR_Trainable_be852e93 at: https://wandb.ai/seokjin/FSR-prediction/runs/be852e93\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514485)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094218-be852e93/logs\n",
      "2023-07-19 09:43:46,802\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.787 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:43:46,805\tWARNING util.py:315 -- The `process_trial_result` operation took 1.791 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:43:46,815\tWARNING util.py:315 -- Processing trial results took 1.802 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:43:46,819\tWARNING util.py:315 -- The `process_trial_result` operation took 1.805 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_c3683c7b_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-43-15/wandb/run-20230719_094349-c3683c7b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: Syncing run FSR_Trainable_c3683c7b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c3683c7b\n",
      "2023-07-19 09:43:58,722\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.856 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:43:58,726\tWARNING util.py:315 -- The `process_trial_result` operation took 1.861 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:43:58,729\tWARNING util.py:315 -- Processing trial results took 1.864 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:43:58,730\tWARNING util.py:315 -- The `process_trial_result` operation took 1.865 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516134)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516134)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516134)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516134)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516134)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_c335853a_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-43-43/wandb/run-20230719_094401-c335853a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516134)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516134)\u001b[0m wandb: Syncing run FSR_Trainable_c335853a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516134)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516134)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c335853a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb:                      mae █▄▂▂▄▃▂▂▂▂▁▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb:                     mape █▅▃▃▅▃▂▁▁▂▁▂▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▂▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb:                     rmse █▄▂▂▃▃▃▃▂▂▂▂▁▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb:         time_this_iter_s ▇▄▂▁▄▃▃▄▄▆▅▅▅▇█▇▆▂▃▃▃▅▄▄▄▃▄▃▃▁▃▄▄▃▄▄▅▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb:                      mae 123.38569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb:                     mape 1.8365679772126576e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb:                     rmse 207.50234\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb:       time_since_restore 124.1794\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb:         time_this_iter_s 1.1466\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb:             time_total_s 124.1794\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb:                timestamp 1689727455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: 🚀 View run FSR_Trainable_77aaede5 at: https://wandb.ai/seokjin/FSR-prediction/runs/77aaede5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=514259)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094207-77aaede5/logs\n",
      "2023-07-19 09:44:31,470\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.034 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:44:31,474\tWARNING util.py:315 -- The `process_trial_result` operation took 2.039 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:44:31,475\tWARNING util.py:315 -- Processing trial results took 2.041 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:44:31,477\tWARNING util.py:315 -- The `process_trial_result` operation took 2.042 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_574934e9_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-43-55/wandb/run-20230719_094435-574934e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: Syncing run FSR_Trainable_574934e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/574934e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb:                      mae █▅▅▅▄▃▃▃▂▂▂▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb:                     mape █▅▄▄▄▃▃▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb:                     rmse █▄▄▄▄▃▃▃▂▂▂▂▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb:       time_since_restore ▁▁▂▃▃▄▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb:         time_this_iter_s ▆▄█▄▅▃▄▁▃▄▄▂▃▄▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb:             time_total_s ▁▁▂▃▃▄▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb:                timestamp ▁▂▃▃▄▄▅▅▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb:                      mae 126.54016\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb:                     mape 91413456.99026\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb:                     rmse 211.7744\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb:       time_since_restore 14.56812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb:         time_this_iter_s 0.7862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb:             time_total_s 14.56812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb:                timestamp 1689727485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: 🚀 View run FSR_Trainable_574934e9 at: https://wandb.ai/seokjin/FSR-prediction/runs/574934e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516385)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094435-574934e9/logs\n",
      "2023-07-19 09:45:06,459\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.227 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:45:06,464\tWARNING util.py:315 -- The `process_trial_result` operation took 2.233 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:45:06,468\tWARNING util.py:315 -- Processing trial results took 2.237 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:45:06,470\tWARNING util.py:315 -- The `process_trial_result` operation took 2.239 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_a1901920_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-44-28/wandb/run-20230719_094509-a1901920\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Syncing run FSR_Trainable_a1901920\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a1901920\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb:                      mae █▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb:                     mape █▇▆▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb:                     rmse █▅▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb:         time_this_iter_s ▄▃▂▂▂▂▂▁▁▂▁▂▂▂▂▂▂▂▃▃▂▂▂▂▂▂▂▃▂▄▃▃▃▂▅▅█▄▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb:                      mae 114.36444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb:                     mape 57534226.70829\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb:                     rmse 195.32821\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb:       time_since_restore 102.99148\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb:         time_this_iter_s 1.00122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb:             time_total_s 102.99148\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb:                timestamp 1689727508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: 🚀 View run FSR_Trainable_fe9934f5 at: https://wandb.ai/seokjin/FSR-prediction/runs/fe9934f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515661)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094321-fe9934f5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb:       training_iteration ▁\n",
      "2023-07-19 09:45:20,671\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.119 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:45:20,689\tWARNING util.py:315 -- The `process_trial_result` operation took 2.138 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:45:20,690\tWARNING util.py:315 -- Processing trial results took 2.139 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:45:20,693\tWARNING util.py:315 -- The `process_trial_result` operation took 2.142 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516632)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_b5ad3a46_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-45-03/wandb/run-20230719_094523-b5ad3a46\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: Syncing run FSR_Trainable_b5ad3a46\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b5ad3a46\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb:                      mae 421.33322\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb:                     mape 5.07913691758801e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb:                     rmse 666.97512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb:       time_since_restore 0.70063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb:         time_this_iter_s 0.70063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb:             time_total_s 0.70063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb:                timestamp 1689727518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: 🚀 View run FSR_Trainable_b5ad3a46 at: https://wandb.ai/seokjin/FSR-prediction/runs/b5ad3a46\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516873)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094523-b5ad3a46/logs\n",
      "2023-07-19 09:45:31,195\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.096 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:45:31,208\tWARNING util.py:315 -- The `process_trial_result` operation took 2.110 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:45:31,210\tWARNING util.py:315 -- Processing trial results took 2.112 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:45:31,211\tWARNING util.py:315 -- The `process_trial_result` operation took 2.113 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_fb53367b_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-45-17/wandb/run-20230719_094533-fb53367b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: Syncing run FSR_Trainable_fb53367b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fb53367b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb:                      mae 407.84135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb:                     mape 6.043994345627444e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb:                     rmse 639.64341\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb:       time_since_restore 0.63074\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb:         time_this_iter_s 0.63074\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb:             time_total_s 0.63074\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb:                timestamp 1689727529\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: 🚀 View run FSR_Trainable_fb53367b at: https://wandb.ai/seokjin/FSR-prediction/runs/fb53367b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517095)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094533-fb53367b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516134)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516134)\u001b[0m wandb:                      mae █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516134)\u001b[0m wandb:                     mape █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516134)\u001b[0m wandb:                     rmse █▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516134)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516134)\u001b[0m wandb:         time_this_iter_s ▂▂▂▂▂▂▁▂▂▄▁▂▂▃▃▂▂▂▂▂▂▃▄█▃▂▂▂▂▃▁▁▁▂▁▁▁▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516134)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516134)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=516134)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb:                     mape █▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb:                     rmse █▅▄▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb:         time_this_iter_s ▃▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▃▂▄▃▂▂▂▃▄█▃▂▃▂▃▁▁▂▁▁▁▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: Waiting for W&B process to finish... (success).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2023-07-19 09:45:40,502\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.977 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:45:40,506\tWARNING util.py:315 -- The `process_trial_result` operation took 1.981 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:45:40,507\tWARNING util.py:315 -- Processing trial results took 1.983 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:45:40,509\tWARNING util.py:315 -- The `process_trial_result` operation took 1.985 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: iterations_since_restore 100\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb:                      mae 106.38374\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb:                     mape 50160434.04546\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb:                     rmse 184.36508\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb:       time_since_restore 100.06846\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb:         time_this_iter_s 0.65604\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb:             time_total_s 100.06846\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb:                timestamp 1689727535\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb:       training_iteration 100\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: 🚀 View run FSR_Trainable_c3683c7b at: https://wandb.ai/seokjin/FSR-prediction/runs/c3683c7b\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094349-c3683c7b/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_ffdc5c38_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-45-28/wandb/run-20230719_094542-ffdc5c38\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Syncing run FSR_Trainable_ffdc5c38\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ffdc5c38\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=515913)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:45:47,641\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.817 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:45:47,649\tWARNING util.py:315 -- The `process_trial_result` operation took 1.825 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:45:47,651\tWARNING util.py:315 -- Processing trial results took 1.827 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:45:47,652\tWARNING util.py:315 -- The `process_trial_result` operation took 1.829 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094542-ffdc5c38/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094542-ffdc5c38/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094542-ffdc5c38/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094542-ffdc5c38/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094542-ffdc5c38/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094542-ffdc5c38/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094542-ffdc5c38/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094542-ffdc5c38/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094542-ffdc5c38/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094542-ffdc5c38/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094542-ffdc5c38/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094542-ffdc5c38/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094542-ffdc5c38/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517331)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094542-ffdc5c38/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_d150faff_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-45-37/wandb/run-20230719_094549-d150faff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Syncing run FSR_Trainable_d150faff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d150faff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb:                      mae 227.39365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb:                     mape 46240083.53157\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb:                     rmse 398.09461\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb:       time_since_restore 0.53426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb:         time_this_iter_s 0.53426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb:             time_total_s 0.53426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb:                timestamp 1689727545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: 🚀 View run FSR_Trainable_d150faff at: https://wandb.ai/seokjin/FSR-prediction/runs/d150faff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094549-d150faff/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094549-d150faff/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094549-d150faff/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094549-d150faff/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094549-d150faff/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094549-d150faff/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094549-d150faff/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094549-d150faff/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094549-d150faff/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094549-d150faff/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517565)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 09:45:55,096\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.035 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:45:55,100\tWARNING util.py:315 -- The `process_trial_result` operation took 2.039 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:45:55,102\tWARNING util.py:315 -- Processing trial results took 2.041 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:45:55,111\tWARNING util.py:315 -- The `process_trial_result` operation took 2.051 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_95d0f2ef_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-45-45/wandb/run-20230719_094557-95d0f2ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: Syncing run FSR_Trainable_95d0f2ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/95d0f2ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:46:00,647\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.692 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:46:00,649\tWARNING util.py:315 -- The `process_trial_result` operation took 1.695 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:46:00,650\tWARNING util.py:315 -- Processing trial results took 1.696 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:46:00,652\tWARNING util.py:315 -- The `process_trial_result` operation took 1.699 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb:                      mae 216.19627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb:                     mape 52532953.87465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb:                     rmse 359.55107\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb:       time_since_restore 1.05627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb:         time_this_iter_s 0.37856\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb:             time_total_s 1.05627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb:                timestamp 1689727555\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: 🚀 View run FSR_Trainable_95d0f2ef at: https://wandb.ai/seokjin/FSR-prediction/runs/95d0f2ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517784)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094557-95d0f2ef/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_4409b5e9_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-45-52/wandb/run-20230719_094602-4409b5e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Syncing run FSR_Trainable_4409b5e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4409b5e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb:                      mae █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb:                     mape █▅▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb:                     rmse █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb:         time_this_iter_s █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb:                timestamp ▁███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094602-4409b5e9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094602-4409b5e9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094602-4409b5e9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094602-4409b5e9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094602-4409b5e9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094602-4409b5e9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094602-4409b5e9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094602-4409b5e9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094602-4409b5e9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094602-4409b5e9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094602-4409b5e9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094602-4409b5e9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094602-4409b5e9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=517966)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094602-4409b5e9/logs\n",
      "2023-07-19 09:46:08,281\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.719 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:46:08,285\tWARNING util.py:315 -- The `process_trial_result` operation took 1.723 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:46:08,286\tWARNING util.py:315 -- Processing trial results took 1.725 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:46:08,288\tWARNING util.py:315 -- The `process_trial_result` operation took 1.727 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_6af8e003_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-45-58/wandb/run-20230719_094610-6af8e003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: Syncing run FSR_Trainable_6af8e003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6af8e003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:46:14,337\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.796 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:46:14,341\tWARNING util.py:315 -- The `process_trial_result` operation took 1.801 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:46:14,344\tWARNING util.py:315 -- Processing trial results took 1.803 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:46:14,349\tWARNING util.py:315 -- The `process_trial_result` operation took 1.809 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb:                      mae 224.32122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb:                     mape 114798518.41103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb:                     rmse 361.65445\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb:       time_since_restore 1.81063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb:         time_this_iter_s 0.73945\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb:             time_total_s 1.81063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb:                timestamp 1689727569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: 🚀 View run FSR_Trainable_6af8e003 at: https://wandb.ai/seokjin/FSR-prediction/runs/6af8e003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518189)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094610-6af8e003/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_acd3f5d3_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-46-05/wandb/run-20230719_094616-acd3f5d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: Syncing run FSR_Trainable_acd3f5d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/acd3f5d3\n",
      "2023-07-19 09:46:23,857\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.064 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:46:23,860\tWARNING util.py:315 -- The `process_trial_result` operation took 2.068 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:46:23,866\tWARNING util.py:315 -- Processing trial results took 2.074 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:46:23,868\tWARNING util.py:315 -- The `process_trial_result` operation took 2.076 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_969ec62b_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-46-11/wandb/run-20230719_094626-969ec62b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: Syncing run FSR_Trainable_969ec62b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/969ec62b\n",
      "2023-07-19 09:46:32,065\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.841 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:46:32,066\tWARNING util.py:315 -- The `process_trial_result` operation took 1.843 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:46:32,068\tWARNING util.py:315 -- Processing trial results took 1.845 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:46:32,086\tWARNING util.py:315 -- The `process_trial_result` operation took 1.863 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_83b58a49_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-46-20/wandb/run-20230719_094635-83b58a49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: Syncing run FSR_Trainable_83b58a49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/83b58a49\n",
      "2023-07-19 09:46:45,705\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.872 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:46:45,708\tWARNING util.py:315 -- The `process_trial_result` operation took 1.876 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:46:45,714\tWARNING util.py:315 -- Processing trial results took 1.882 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:46:45,716\tWARNING util.py:315 -- The `process_trial_result` operation took 1.885 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_1ffc41fd_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-46-28/wandb/run-20230719_094650-1ffc41fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: Syncing run FSR_Trainable_1ffc41fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1ffc41fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb:                      mae █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb:                     mape █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb:                     rmse █▅▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb:         time_this_iter_s ▅▁█▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb:                      mae 155.17928\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb:                     mape 60964287.01921\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb:                     rmse 283.14405\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb:       time_since_restore 5.46966\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb:         time_this_iter_s 1.34345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb:             time_total_s 5.46966\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb:                timestamp 1689727609\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: 🚀 View run FSR_Trainable_1ffc41fd at: https://wandb.ai/seokjin/FSR-prediction/runs/1ffc41fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518983)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094650-1ffc41fd/logs\n",
      "2023-07-19 09:47:06,169\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.117 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:47:06,172\tWARNING util.py:315 -- The `process_trial_result` operation took 2.121 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:47:06,173\tWARNING util.py:315 -- Processing trial results took 2.122 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:47:06,175\tWARNING util.py:315 -- The `process_trial_result` operation took 2.124 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_88a24ed6_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-46-42/wandb/run-20230719_094709-88a24ed6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: Syncing run FSR_Trainable_88a24ed6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/88a24ed6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb:                      mae █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb:                     mape ▅█▅▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb:                     rmse █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb:       time_since_restore ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb:         time_this_iter_s ▇▃▂▁▁▃▄▃▆▅▄▄▇█▅▅▄▄▅▆▅▄▄▃▄▄▄▄▄▄▄▃▄▄▄▄▅▃▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb:             time_total_s ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb:                      mae 111.09477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb:                     mape 52053139.34745\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb:                     rmse 192.55651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb:       time_since_restore 92.20759\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb:         time_this_iter_s 0.84787\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb:             time_total_s 92.20759\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb:                timestamp 1689727674\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: 🚀 View run FSR_Trainable_acd3f5d3 at: https://wandb.ai/seokjin/FSR-prediction/runs/acd3f5d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518352)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094616-acd3f5d3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:48:09,100\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.212 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:48:09,103\tWARNING util.py:315 -- The `process_trial_result` operation took 2.216 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:48:09,104\tWARNING util.py:315 -- Processing trial results took 2.217 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:48:09,106\tWARNING util.py:315 -- The `process_trial_result` operation took 2.219 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb:                      mae █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb:                     mape █▆▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb:                     rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb:         time_this_iter_s ▃▂▁▅▃▄▃▃█▂▃▃▃▃▄▄▃▂▂▂▂▁▂▂▂▂▁▁▂▂▂▂▂▃▂▂▃▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb:                      mae 110.26751\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb:                     mape 51300307.01807\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb:                     rmse 191.47236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb:       time_since_restore 94.12569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb:         time_this_iter_s 0.87318\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb:             time_total_s 94.12569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb:                timestamp 1689727685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: 🚀 View run FSR_Trainable_969ec62b at: https://wandb.ai/seokjin/FSR-prediction/runs/969ec62b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518591)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094626-969ec62b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_a33e0728_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-47-02/wandb/run-20230719_094812-a33e0728\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: Syncing run FSR_Trainable_a33e0728\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a33e0728\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb:                      mae █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb:                     mape █▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb:                     rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb:         time_this_iter_s █▆▅▄▄▅▃▄▄▃▄▆▅▄▁▁▁▂▂▃▂▂▁▂▃▂▂▂▂▂▂▁▁▅▃▃▁▂▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb:                      mae 110.23288\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb:                     mape 51977090.07034\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb:                     rmse 191.06064\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb:       time_since_restore 95.75603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb:         time_this_iter_s 0.9679\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb:             time_total_s 95.75603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb:                timestamp 1689727693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: 🚀 View run FSR_Trainable_83b58a49 at: https://wandb.ai/seokjin/FSR-prediction/runs/83b58a49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=518755)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094635-83b58a49/logs\n",
      "2023-07-19 09:48:20,253\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.373 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:48:20,256\tWARNING util.py:315 -- The `process_trial_result` operation took 2.376 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:48:20,260\tWARNING util.py:315 -- Processing trial results took 2.380 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:48:20,263\tWARNING util.py:315 -- The `process_trial_result` operation took 2.383 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_ddefe89d_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-48-05/wandb/run-20230719_094822-ddefe89d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: Syncing run FSR_Trainable_ddefe89d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ddefe89d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb:                      mae 180.23065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb:                     mape 8917516815320861.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb:                     rmse 390.46917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb:       time_since_restore 0.46146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb:         time_this_iter_s 0.46146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb:             time_total_s 0.46146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb:                timestamp 1689727697\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: 🚀 View run FSR_Trainable_ddefe89d at: https://wandb.ai/seokjin/FSR-prediction/runs/ddefe89d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519742)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094822-ddefe89d/logs\n",
      "2023-07-19 09:48:30,375\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.224 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:48:30,378\tWARNING util.py:315 -- The `process_trial_result` operation took 2.228 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:48:30,380\tWARNING util.py:315 -- Processing trial results took 2.230 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:48:30,381\tWARNING util.py:315 -- The `process_trial_result` operation took 2.231 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_995f7ebd_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-48-17/wandb/run-20230719_094833-995f7ebd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: Syncing run FSR_Trainable_995f7ebd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/995f7ebd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb:                      mae 209.26654\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb:                     mape 4143490048227572.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb:                     rmse 431.29041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb:       time_since_restore 0.37039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb:         time_this_iter_s 0.37039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb:             time_total_s 0.37039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb:                timestamp 1689727708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: 🚀 View run FSR_Trainable_995f7ebd at: https://wandb.ai/seokjin/FSR-prediction/runs/995f7ebd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519968)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094833-995f7ebd/logs\n",
      "2023-07-19 09:48:40,838\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.082 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:48:40,841\tWARNING util.py:315 -- The `process_trial_result` operation took 2.086 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:48:40,842\tWARNING util.py:315 -- Processing trial results took 2.087 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:48:40,844\tWARNING util.py:315 -- The `process_trial_result` operation took 2.088 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_899586ad_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-48-27/wandb/run-20230719_094843-899586ad\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Syncing run FSR_Trainable_899586ad\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/899586ad\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb:                      mae █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb:                     mape ▄█▇▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb:                     rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb:         time_this_iter_s █▅▅▃▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▃▄▃▃▂▄▃▁▂▂▁▁▂▂▂▁▁▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb:                      mae 112.4753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb:                     mape 53024484.94121\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb:                     rmse 195.09427\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb:       time_since_restore 88.13384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb:         time_this_iter_s 0.7809\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb:             time_total_s 88.13384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb:                timestamp 1689727723\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: 🚀 View run FSR_Trainable_88a24ed6 at: https://wandb.ai/seokjin/FSR-prediction/runs/88a24ed6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519219)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094709-88a24ed6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb:       training_iteration ▁\n",
      "2023-07-19 09:48:50,268\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.155 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:48:50,287\tWARNING util.py:315 -- The `process_trial_result` operation took 2.175 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:48:50,289\tWARNING util.py:315 -- Processing trial results took 2.178 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:48:50,291\tWARNING util.py:315 -- The `process_trial_result` operation took 2.179 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_44cad1cf_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-48-38/wandb/run-20230719_094853-44cad1cf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Syncing run FSR_Trainable_44cad1cf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/44cad1cf\n",
      "2023-07-19 09:49:00,457\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.773 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:49:00,461\tWARNING util.py:315 -- The `process_trial_result` operation took 1.778 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:49:00,461\tWARNING util.py:315 -- Processing trial results took 1.779 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:49:00,463\tWARNING util.py:315 -- The `process_trial_result` operation took 1.780 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_8eb4940a_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-48-47/wandb/run-20230719_094903-8eb4940a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: Syncing run FSR_Trainable_8eb4940a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8eb4940a\n",
      "2023-07-19 09:49:12,205\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.734 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:49:12,206\tWARNING util.py:315 -- The `process_trial_result` operation took 1.737 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:49:12,208\tWARNING util.py:315 -- Processing trial results took 1.739 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:49:12,211\tWARNING util.py:315 -- The `process_trial_result` operation took 1.741 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_89d7bea9_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-48-57/wandb/run-20230719_094915-89d7bea9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: Syncing run FSR_Trainable_89d7bea9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/89d7bea9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb:                      mae █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb:                     mape █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb:                     rmse █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb:         time_this_iter_s █▆▄▂▄▃▅▂▂▃▄▄▃▃▂▄▃▁▁▁▄▃▂▂▂▆▅▄▃▃▆▆▅▅▄▅▄▄▄▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb:                      mae 108.0583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb:                     mape 51314337.44828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb:                     rmse 187.00151\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb:       time_since_restore 69.42604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb:         time_this_iter_s 0.82042\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb:             time_total_s 69.42604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb:                timestamp 1689727770\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: 🚀 View run FSR_Trainable_a33e0728 at: https://wandb.ai/seokjin/FSR-prediction/runs/a33e0728\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=519511)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094812-a33e0728/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb:                      mae █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb:                     mape █▃▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb:                     rmse █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb:         time_this_iter_s ▅▃▃▂▁▄▃▁▂▆▅▆▆▆▂▅▂▄█▇▅▄▄▄▆▄▆▄▃▄▃▅▄▃▅▃▄▃▃▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094853-44cad1cf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094853-44cad1cf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094853-44cad1cf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094853-44cad1cf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094853-44cad1cf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094853-44cad1cf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094853-44cad1cf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094853-44cad1cf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094853-44cad1cf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094853-44cad1cf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094853-44cad1cf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094853-44cad1cf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094853-44cad1cf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094853-44cad1cf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094853-44cad1cf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520429)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094853-44cad1cf/logs\n",
      "2023-07-19 09:49:43,081\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.866 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:49:43,083\tWARNING util.py:315 -- The `process_trial_result` operation took 1.869 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:49:43,084\tWARNING util.py:315 -- Processing trial results took 1.870 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:49:43,087\tWARNING util.py:315 -- The `process_trial_result` operation took 1.873 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_a516f150_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-49-09/wandb/run-20230719_094946-a516f150\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Syncing run FSR_Trainable_a516f150\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a516f150\n",
      "2023-07-19 09:49:54,953\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.087 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:49:54,955\tWARNING util.py:315 -- The `process_trial_result` operation took 2.090 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:49:54,957\tWARNING util.py:315 -- Processing trial results took 2.092 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:49:54,958\tWARNING util.py:315 -- The `process_trial_result` operation took 2.093 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_5ab2b76c_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-49-40/wandb/run-20230719_094957-5ab2b76c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: Syncing run FSR_Trainable_5ab2b76c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5ab2b76c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb:                      mae 248.59892\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb:                     mape 146053221.64725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb:                     rmse 390.77685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb:       time_since_restore 0.53003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb:         time_this_iter_s 0.53003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb:             time_total_s 0.53003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb:                timestamp 1689727792\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: 🚀 View run FSR_Trainable_5ab2b76c at: https://wandb.ai/seokjin/FSR-prediction/runs/5ab2b76c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521359)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094957-5ab2b76c/logs\n",
      "2023-07-19 09:50:09,083\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.101 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:50:09,086\tWARNING util.py:315 -- The `process_trial_result` operation took 2.104 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:50:09,090\tWARNING util.py:315 -- Processing trial results took 2.108 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:50:09,091\tWARNING util.py:315 -- The `process_trial_result` operation took 2.109 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_aa6d1ad1_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-49-52/wandb/run-20230719_095012-aa6d1ad1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: Syncing run FSR_Trainable_aa6d1ad1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/aa6d1ad1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb:                      mae 241.33762\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb:                     mape 143498551.33557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb:                     rmse 394.75357\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb:       time_since_restore 0.682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb:         time_this_iter_s 0.682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb:             time_total_s 0.682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb:                timestamp 1689727806\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: 🚀 View run FSR_Trainable_aa6d1ad1 at: https://wandb.ai/seokjin/FSR-prediction/runs/aa6d1ad1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521591)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095012-aa6d1ad1/logs\n",
      "2023-07-19 09:50:23,535\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.081 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:50:23,537\tWARNING util.py:315 -- The `process_trial_result` operation took 2.084 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:50:23,539\tWARNING util.py:315 -- Processing trial results took 2.086 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:50:23,541\tWARNING util.py:315 -- The `process_trial_result` operation took 2.088 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_9a1e6356_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-50-06/wandb/run-20230719_095026-9a1e6356\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: Syncing run FSR_Trainable_9a1e6356\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9a1e6356\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb:                      mae █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb:                     mape █▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb:                     rmse █▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb:         time_this_iter_s █▆▄▃▃▄▆▄▄▅▅▃▄▄▃▃▃▄▄▂▁▅▅▄▄▄▄▅▅▅▅▄▄▆▄▅▆▃▆▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb:                      mae 108.03779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb:                     mape 51127675.46502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb:                     rmse 187.01367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb:       time_since_restore 75.48047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb:         time_this_iter_s 0.80235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb:             time_total_s 75.48047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb:                timestamp 1689727825\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: 🚀 View run FSR_Trainable_8eb4940a at: https://wandb.ai/seokjin/FSR-prediction/runs/8eb4940a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520654)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094903-8eb4940a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521825)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:50:36,410\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.000 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:50:36,412\tWARNING util.py:315 -- The `process_trial_result` operation took 2.003 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:50:36,413\tWARNING util.py:315 -- Processing trial results took 2.004 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:50:36,415\tWARNING util.py:315 -- The `process_trial_result` operation took 2.006 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb:                      mae █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb:                     mape █▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb:                     rmse █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb:         time_this_iter_s █▅▃▃▃▄▃▂▃▃▂▃▃▂▁▂▃▄▄▃▄▂▅▅▃▃▃▄▅▃▂▄▄▄▄▃▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb:                      mae 108.71302\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb:                     mape 51833294.42403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb:                     rmse 188.15827\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb:       time_since_restore 73.51952\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb:         time_this_iter_s 0.63758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb:             time_total_s 73.51952\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb:                timestamp 1689727833\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: 🚀 View run FSR_Trainable_89d7bea9 at: https://wandb.ai/seokjin/FSR-prediction/runs/89d7bea9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094915-89d7bea9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=520874)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_376b673f_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-50-20/wandb/run-20230719_095038-376b673f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: Syncing run FSR_Trainable_376b673f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/376b673f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:50:45,551\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.157 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:50:45,554\tWARNING util.py:315 -- The `process_trial_result` operation took 2.161 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:50:45,555\tWARNING util.py:315 -- Processing trial results took 2.162 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:50:45,556\tWARNING util.py:315 -- The `process_trial_result` operation took 2.163 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb:                      mae 198.39502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb:                     mape 81424826.16646\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb:                     rmse 322.19598\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb:       time_since_restore 1.05669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb:         time_this_iter_s 0.40964\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb:             time_total_s 1.05669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb:                timestamp 1689727836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: 🚀 View run FSR_Trainable_376b673f at: https://wandb.ai/seokjin/FSR-prediction/runs/376b673f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095038-376b673f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522068)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_8111e724_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-50-33/wandb/run-20230719_095048-8111e724\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: Syncing run FSR_Trainable_8111e724\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8111e724\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb:                      mae 214.70538\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb:                     mape 72122963.08675\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb:                     rmse 381.62808\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb:       time_since_restore 0.52862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb:         time_this_iter_s 0.52862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb:             time_total_s 0.52862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb:                timestamp 1689727843\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: 🚀 View run FSR_Trainable_8111e724 at: https://wandb.ai/seokjin/FSR-prediction/runs/8111e724\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095048-8111e724/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522301)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 09:50:54,458\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.201 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:50:54,461\tWARNING util.py:315 -- The `process_trial_result` operation took 2.205 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:50:54,467\tWARNING util.py:315 -- Processing trial results took 2.210 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:50:54,468\tWARNING util.py:315 -- The `process_trial_result` operation took 2.211 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_8c44c95a_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-50-42/wandb/run-20230719_095057-8c44c95a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: Syncing run FSR_Trainable_8c44c95a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8c44c95a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb:                      mae 233.99883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb:                     mape 99094487.63725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb:                     rmse 388.35106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb:       time_since_restore 0.32972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb:         time_this_iter_s 0.32972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb:             time_total_s 0.32972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb:                timestamp 1689727852\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: 🚀 View run FSR_Trainable_8c44c95a at: https://wandb.ai/seokjin/FSR-prediction/runs/8c44c95a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522529)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095057-8c44c95a/logs\n",
      "2023-07-19 09:51:03,325\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.095 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:51:03,327\tWARNING util.py:315 -- The `process_trial_result` operation took 2.098 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:51:03,329\tWARNING util.py:315 -- Processing trial results took 2.099 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:51:03,330\tWARNING util.py:315 -- The `process_trial_result` operation took 2.101 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_ee3b5ce8_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-50-51/wandb/run-20230719_095105-ee3b5ce8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: Syncing run FSR_Trainable_ee3b5ce8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ee3b5ce8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb:                      mae █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb:                     mape █▇█▇▆▆▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb:                     rmse █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb:         time_this_iter_s █▆▄▄▄▄▅▄▃▄▅▄▃▅▅▆▄▅▅▄▂▃▂▂▄▂▁▂▂▃▄▂▁▂▃▄▁▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094946-a516f150/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094946-a516f150/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094946-a516f150/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094946-a516f150/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094946-a516f150/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094946-a516f150/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094946-a516f150/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094946-a516f150/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094946-a516f150/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094946-a516f150/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094946-a516f150/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094946-a516f150/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094946-a516f150/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=521138)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_094946-a516f150/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: / 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: / 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: / 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:51:12,376\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.773 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:51:12,379\tWARNING util.py:315 -- The `process_trial_result` operation took 1.777 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:51:12,380\tWARNING util.py:315 -- Processing trial results took 1.778 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:51:12,382\tWARNING util.py:315 -- The `process_trial_result` operation took 1.780 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb:                      mae 223.05299\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb:                     mape 71933306.68854\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb:                     rmse 375.55079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb:       time_since_restore 0.41921\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb:         time_this_iter_s 0.41921\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb:             time_total_s 0.41921\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb:                timestamp 1689727861\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: 🚀 View run FSR_Trainable_ee3b5ce8 at: https://wandb.ai/seokjin/FSR-prediction/runs/ee3b5ce8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095105-ee3b5ce8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522752)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_e95decd0_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-51-00/wandb/run-20230719_095114-e95decd0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: Syncing run FSR_Trainable_e95decd0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e95decd0\n",
      "2023-07-19 09:51:22,650\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.201 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:51:22,653\tWARNING util.py:315 -- The `process_trial_result` operation took 2.204 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:51:22,654\tWARNING util.py:315 -- Processing trial results took 2.205 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:51:22,655\tWARNING util.py:315 -- The `process_trial_result` operation took 2.206 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...204)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_03b91ca1_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-51-09/wandb/run-20230719_095125-03b91ca1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: Syncing run FSR_Trainable_03b91ca1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/03b91ca1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:51:31,760\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.014 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:51:31,762\tWARNING util.py:315 -- The `process_trial_result` operation took 2.017 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:51:31,765\tWARNING util.py:315 -- Processing trial results took 2.019 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:51:31,765\tWARNING util.py:315 -- The `process_trial_result` operation took 2.020 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb:                      mae 184.05954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb:                     mape 87631246.53578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb:                     rmse 303.91837\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb:       time_since_restore 1.29487\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb:         time_this_iter_s 0.57722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb:             time_total_s 1.29487\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb:                timestamp 1689727883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: 🚀 View run FSR_Trainable_03b91ca1 at: https://wandb.ai/seokjin/FSR-prediction/runs/03b91ca1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523204)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095125-03b91ca1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_871e547f_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-51-19/wandb/run-20230719_095134-871e547f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: Syncing run FSR_Trainable_871e547f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/871e547f\n",
      "2023-07-19 09:51:42,446\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.885 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:51:42,449\tWARNING util.py:315 -- The `process_trial_result` operation took 1.889 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:51:42,451\tWARNING util.py:315 -- Processing trial results took 1.891 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:51:42,452\tWARNING util.py:315 -- The `process_trial_result` operation took 1.892 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_fbdcd699_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-51-29/wandb/run-20230719_095145-fbdcd699\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Syncing run FSR_Trainable_fbdcd699\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fbdcd699\n",
      "2023-07-19 09:51:54,374\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.840 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:51:54,379\tWARNING util.py:315 -- The `process_trial_result` operation took 1.845 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:51:54,380\tWARNING util.py:315 -- Processing trial results took 1.846 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:51:54,397\tWARNING util.py:315 -- The `process_trial_result` operation took 1.863 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_c92d0ac4_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-51-39/wandb/run-20230719_095157-c92d0ac4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: Syncing run FSR_Trainable_c92d0ac4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c92d0ac4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb:                      mae █▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb:                     mape ▁▃▄▄▆▇████████▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb:                     rmse █▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb:       time_since_restore ▁▁▂▃▃▄▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb:         time_this_iter_s █▃▆▆▅▄▅▂▂▄▁▂▁▃▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb:             time_total_s ▁▁▂▃▃▄▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb:                timestamp ▁▃▃▃▃▄▅▅▅▅▆▇▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb:                      mae 111.53439\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb:                     mape 55209069.42408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb:                     rmse 192.13343\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb:       time_since_restore 12.63084\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb:         time_this_iter_s 0.68882\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb:             time_total_s 12.63084\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb:                timestamp 1689727926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: 🚀 View run FSR_Trainable_c92d0ac4 at: https://wandb.ai/seokjin/FSR-prediction/runs/c92d0ac4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523866)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095157-c92d0ac4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb:                      mae █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb:                     mape ▇█▄▁▁▃▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb:                     rmse █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb:         time_this_iter_s █▂▅▃▄▂▄▃▂▂▄▁▂▄▂▄▅▃▃▅▂▃▃▁▂▂▃▂▂▁▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb:                timestamp ▁▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095145-fbdcd699/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523649)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:52:18,978\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.828 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:52:18,980\tWARNING util.py:315 -- The `process_trial_result` operation took 1.832 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:52:18,981\tWARNING util.py:315 -- Processing trial results took 1.833 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:52:18,982\tWARNING util.py:315 -- The `process_trial_result` operation took 1.834 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb:                      mae █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb:                     mape █▁▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb:                     rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb:         time_this_iter_s █▂▃▃▃▂▄▃▃▁▃▁▁▃▄▃▄▂▃▅▆▄▅▄▅▅▆▅▃▄▃▄▄▃▄▄▂▂▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb:                      mae 109.39894\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb:                     mape 50997287.47962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb:                     rmse 189.36643\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb:       time_since_restore 55.58894\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb:         time_this_iter_s 0.53972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb:             time_total_s 55.58894\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb:                timestamp 1689727936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: 🚀 View run FSR_Trainable_e95decd0 at: https://wandb.ai/seokjin/FSR-prediction/runs/e95decd0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095114-e95decd0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=522984)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_066e4b36_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-51-51/wandb/run-20230719_095221-066e4b36\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: Syncing run FSR_Trainable_066e4b36\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/066e4b36\n",
      "2023-07-19 09:52:29,575\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.014 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:52:29,577\tWARNING util.py:315 -- The `process_trial_result` operation took 2.017 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:52:29,578\tWARNING util.py:315 -- Processing trial results took 2.018 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:52:29,582\tWARNING util.py:315 -- The `process_trial_result` operation took 2.022 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524354)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524354)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524354)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524354)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524354)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_8cb6ee6c_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-52-16/wandb/run-20230719_095232-8cb6ee6c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524354)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524354)\u001b[0m wandb: Syncing run FSR_Trainable_8cb6ee6c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524354)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524354)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8cb6ee6c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb:                      mae █▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb:                     mape █▂▁▂▃▃▃▃▃▃▃▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb:                     rmse █▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb:         time_this_iter_s █▃▄▃▃▃▄▁▃▂▂▂▁▆▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb:                timestamp ▁▂▃▃▃▃▄▄▅▅▅▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb:                      mae 112.18206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb:                     mape 55980063.51166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb:                     rmse 193.13754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb:       time_since_restore 10.83939\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb:         time_this_iter_s 0.77723\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb:             time_total_s 10.83939\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb:                timestamp 1689727951\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: 🚀 View run FSR_Trainable_066e4b36 at: https://wandb.ai/seokjin/FSR-prediction/runs/066e4b36\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524130)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095221-066e4b36/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: Waiting for W&B process to finish... (success).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524354)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524354)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524354)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524354)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524354)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524354)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524354)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524354)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524354)\u001b[0m wandb:       training_iteration ▁\n",
      "2023-07-19 09:52:38,687\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.820 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:52:38,690\tWARNING util.py:315 -- The `process_trial_result` operation took 1.824 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:52:38,692\tWARNING util.py:315 -- Processing trial results took 1.826 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:52:38,693\tWARNING util.py:315 -- The `process_trial_result` operation took 1.827 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb:                      mae █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb:                     mape █▂▁▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb:                     rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb:         time_this_iter_s ▇▅▃▃▄▃▅▅▆▆▅▅▇██▅▇▇▄▅▅▅▄▅▃▂▃▂▆▅▄▄▄▃▂▄▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: iterations_since_restore 100\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb:                      mae 108.71233\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb:                     mape 50585202.64518\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb:                     rmse 188.68685\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb:       time_since_restore 55.18882\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb:         time_this_iter_s 0.36799\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb:             time_total_s 55.18882\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb:                timestamp 1689727955\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb:       training_iteration 100\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: 🚀 View run FSR_Trainable_871e547f at: https://wandb.ai/seokjin/FSR-prediction/runs/871e547f\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=523423)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095134-871e547f/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_23856c50_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-52-26/wandb/run-20230719_095240-23856c50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Syncing run FSR_Trainable_23856c50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/23856c50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095240-23856c50/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095240-23856c50/logs\n",
      "2023-07-19 09:52:46,209\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.869 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:52:46,211\tWARNING util.py:315 -- The `process_trial_result` operation took 1.872 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:52:46,213\tWARNING util.py:315 -- Processing trial results took 1.873 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:52:46,214\tWARNING util.py:315 -- The `process_trial_result` operation took 1.875 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524589)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_76db5144_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-52-36/wandb/run-20230719_095248-76db5144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: Syncing run FSR_Trainable_76db5144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/76db5144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb:                      mae 190.60441\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb:                     mape 4584068006470287.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb:                     rmse 381.34858\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb:       time_since_restore 0.56073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb:         time_this_iter_s 0.56073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb:             time_total_s 0.56073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb:                timestamp 1689727964\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: 🚀 View run FSR_Trainable_76db5144 at: https://wandb.ai/seokjin/FSR-prediction/runs/76db5144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095248-76db5144/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=524817)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 09:52:53,955\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.726 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:52:53,958\tWARNING util.py:315 -- The `process_trial_result` operation took 1.728 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:52:53,960\tWARNING util.py:315 -- Processing trial results took 1.731 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:52:53,961\tWARNING util.py:315 -- The `process_trial_result` operation took 1.732 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_1593ef3c_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-52-43/wandb/run-20230719_095256-1593ef3c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: Syncing run FSR_Trainable_1593ef3c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1593ef3c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:53:00,236\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.937 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:53:00,238\tWARNING util.py:315 -- The `process_trial_result` operation took 1.940 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:53:00,242\tWARNING util.py:315 -- Processing trial results took 1.944 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:53:00,243\tWARNING util.py:315 -- The `process_trial_result` operation took 1.946 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb:                      mae 178.08617\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb:                     mape 73618943.84833\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb:                     rmse 302.23486\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb:       time_since_restore 1.74129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb:         time_this_iter_s 0.68348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb:             time_total_s 1.74129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb:                timestamp 1689727974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: 🚀 View run FSR_Trainable_1593ef3c at: https://wandb.ai/seokjin/FSR-prediction/runs/1593ef3c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525036)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095256-1593ef3c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_32f780ed_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-52-51/wandb/run-20230719_095302-32f780ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Syncing run FSR_Trainable_32f780ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/32f780ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:53:06,523\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.319 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:53:06,528\tWARNING util.py:315 -- The `process_trial_result` operation took 2.325 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:53:06,530\tWARNING util.py:315 -- Processing trial results took 2.327 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:53:06,532\tWARNING util.py:315 -- The `process_trial_result` operation took 2.329 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525217)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095302-32f780ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_11ca36ec_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-52-57/wandb/run-20230719_095308-11ca36ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: Syncing run FSR_Trainable_11ca36ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/11ca36ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:53:12,707\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.880 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:53:12,709\tWARNING util.py:315 -- The `process_trial_result` operation took 1.883 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:53:12,710\tWARNING util.py:315 -- Processing trial results took 1.885 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:53:12,711\tWARNING util.py:315 -- The `process_trial_result` operation took 1.886 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb:                      mae 239.56883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb:                     mape 123369206.4227\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb:                     rmse 385.33203\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb:       time_since_restore 0.72136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb:         time_this_iter_s 0.72136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb:             time_total_s 0.72136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb:                timestamp 1689727984\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: 🚀 View run FSR_Trainable_11ca36ec at: https://wandb.ai/seokjin/FSR-prediction/runs/11ca36ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525475)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095308-11ca36ec/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095314-b7d4a193/logs\n",
      "2023-07-19 09:53:20,424\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.826 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:53:20,428\tWARNING util.py:315 -- The `process_trial_result` operation took 1.831 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:53:20,429\tWARNING util.py:315 -- Processing trial results took 1.833 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:53:20,431\tWARNING util.py:315 -- The `process_trial_result` operation took 1.834 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_4714a00e_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-53-10/wandb/run-20230719_095322-4714a00e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Syncing run FSR_Trainable_4714a00e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4714a00e\n",
      "2023-07-19 09:53:27,015\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.611 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:53:27,019\tWARNING util.py:315 -- The `process_trial_result` operation took 1.616 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:53:27,020\tWARNING util.py:315 -- Processing trial results took 1.617 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:53:27,021\tWARNING util.py:315 -- The `process_trial_result` operation took 1.618 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_9e5d2527_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-53-17/wandb/run-20230719_095329-9e5d2527\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: Syncing run FSR_Trainable_9e5d2527\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9e5d2527\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb:                      mae █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb:                     mape █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb:                     rmse █▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb:         time_this_iter_s █▁▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb:                      mae 137.38916\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb:                     mape 55497187.25579\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb:                     rmse 247.35867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb:       time_since_restore 3.52739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb:         time_this_iter_s 0.79439\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb:             time_total_s 3.52739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb:                timestamp 1689728009\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: 🚀 View run FSR_Trainable_9e5d2527 at: https://wandb.ai/seokjin/FSR-prediction/runs/9e5d2527\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526057)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095329-9e5d2527/logs\n",
      "2023-07-19 09:53:36,359\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.525 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:53:36,363\tWARNING util.py:315 -- The `process_trial_result` operation took 1.529 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:53:36,364\tWARNING util.py:315 -- Processing trial results took 1.530 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:53:36,365\tWARNING util.py:315 -- The `process_trial_result` operation took 1.531 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb:                      mae █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb:                     mape █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb:                     rmse █▇▄▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb:         time_this_iter_s █▃▃▂▂▁▄▇▄▅▅▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb:                timestamp ▁▃▃▃▃▃▄▅▅▆▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095322-4714a00e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_66b6fb88_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-53-24/wandb/run-20230719_095338-66b6fb88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb: Syncing run FSR_Trainable_66b6fb88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/66b6fb88\n",
      "2023-07-19 09:53:45,257\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.999 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:53:45,260\tWARNING util.py:315 -- The `process_trial_result` operation took 2.003 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:53:45,262\tWARNING util.py:315 -- Processing trial results took 2.005 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:53:45,263\tWARNING util.py:315 -- The `process_trial_result` operation took 2.007 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=525878)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_01769188_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-53-33/wandb/run-20230719_095347-01769188\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: Syncing run FSR_Trainable_01769188\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/01769188\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb:                      mae █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb:                     mape █▁▂▂▂▂▂▂▃▃▃▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb:                     rmse █▆▄▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb:       time_since_restore ▁▂▂▂▃▃▄▄▅▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb:         time_this_iter_s █▃▃▂▂▃▂▁▂▁▃▄▃▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb:             time_total_s ▁▂▂▂▃▃▄▄▅▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb:                timestamp ▁▃▃▃▃▃▄▄▅▅▅▅▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb:                      mae 111.29251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb:                     mape 55204535.91054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb:                     rmse 192.86572\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb:       time_since_restore 11.11006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb:         time_this_iter_s 0.66449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb:             time_total_s 11.11006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb:                timestamp 1689728028\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb: 🚀 View run FSR_Trainable_66b6fb88 at: https://wandb.ai/seokjin/FSR-prediction/runs/66b6fb88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526280)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095338-66b6fb88/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb:       training_iteration ▁\n",
      "2023-07-19 09:53:53,603\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.779 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:53:53,607\tWARNING util.py:315 -- The `process_trial_result` operation took 1.784 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:53:53,609\tWARNING util.py:315 -- Processing trial results took 1.785 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:53:53,610\tWARNING util.py:315 -- The `process_trial_result` operation took 1.786 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_c8b18d92_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-53-42/wandb/run-20230719_095355-c8b18d92\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: Syncing run FSR_Trainable_c8b18d92\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c8b18d92\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526506)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb:                      mae 244.71798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb:                     mape 115085427.363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb:                     rmse 391.92958\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb:       time_since_restore 0.75525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb:         time_this_iter_s 0.75525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb:             time_total_s 0.75525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb:                timestamp 1689728031\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: 🚀 View run FSR_Trainable_c8b18d92 at: https://wandb.ai/seokjin/FSR-prediction/runs/c8b18d92\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095355-c8b18d92/logs\n",
      "2023-07-19 09:54:01,368\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.781 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:54:01,372\tWARNING util.py:315 -- The `process_trial_result` operation took 1.785 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:54:01,373\tWARNING util.py:315 -- Processing trial results took 1.786 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:54:01,374\tWARNING util.py:315 -- The `process_trial_result` operation took 1.788 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526730)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_ddfa8c6f_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-53-51/wandb/run-20230719_095403-ddfa8c6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: Syncing run FSR_Trainable_ddfa8c6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ddfa8c6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:54:07,240\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.884 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:54:07,243\tWARNING util.py:315 -- The `process_trial_result` operation took 1.888 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:54:07,245\tWARNING util.py:315 -- Processing trial results took 1.890 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:54:07,246\tWARNING util.py:315 -- The `process_trial_result` operation took 1.891 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb:                      mae 234.77114\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb:                     mape 4.73254094769693e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb:                     rmse 378.10441\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb:       time_since_restore 0.99031\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb:         time_this_iter_s 0.99031\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb:             time_total_s 0.99031\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb:                timestamp 1689728039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: 🚀 View run FSR_Trainable_ddfa8c6f at: https://wandb.ai/seokjin/FSR-prediction/runs/ddfa8c6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095403-ddfa8c6f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=526953)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_e59245c0_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-53-58/wandb/run-20230719_095409-e59245c0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Syncing run FSR_Trainable_e59245c0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e59245c0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:54:13,369\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.883 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:54:13,373\tWARNING util.py:315 -- The `process_trial_result` operation took 1.888 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:54:13,375\tWARNING util.py:315 -- Processing trial results took 1.890 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:54:13,377\tWARNING util.py:315 -- The `process_trial_result` operation took 1.892 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095409-e59245c0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095409-e59245c0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095409-e59245c0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095409-e59245c0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095409-e59245c0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095409-e59245c0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095409-e59245c0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095409-e59245c0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095409-e59245c0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095409-e59245c0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095409-e59245c0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095409-e59245c0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095409-e59245c0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095409-e59245c0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527131)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:54:18,964\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.724 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:54:18,966\tWARNING util.py:315 -- The `process_trial_result` operation took 1.727 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:54:18,969\tWARNING util.py:315 -- Processing trial results took 1.730 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:54:18,970\tWARNING util.py:315 -- The `process_trial_result` operation took 1.731 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb:                      mae 223.50066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb:                     mape 155077740.33093\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb:                     rmse 372.30204\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb:       time_since_restore 0.99903\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb:         time_this_iter_s 0.99903\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb:             time_total_s 0.99903\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb:                timestamp 1689728051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: 🚀 View run FSR_Trainable_20cf1756 at: https://wandb.ai/seokjin/FSR-prediction/runs/20cf1756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095415-20cf1756/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527316)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_af997dd1_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-54-10/wandb/run-20230719_095421-af997dd1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: Syncing run FSR_Trainable_af997dd1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/af997dd1\n",
      "2023-07-19 09:54:27,824\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.986 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:54:27,826\tWARNING util.py:315 -- The `process_trial_result` operation took 1.988 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:54:27,828\tWARNING util.py:315 -- Processing trial results took 1.990 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:54:27,832\tWARNING util.py:315 -- The `process_trial_result` operation took 1.994 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_e29404c6_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-54-16/wandb/run-20230719_095430-e29404c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: Syncing run FSR_Trainable_e29404c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e29404c6\n",
      "2023-07-19 09:54:38,055\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.894 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:54:38,060\tWARNING util.py:315 -- The `process_trial_result` operation took 1.900 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:54:38,061\tWARNING util.py:315 -- Processing trial results took 1.902 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:54:38,063\tWARNING util.py:315 -- The `process_trial_result` operation took 1.903 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_5fb24b8c_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-54-25/wandb/run-20230719_095441-5fb24b8c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: Syncing run FSR_Trainable_5fb24b8c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5fb24b8c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb:                      mae 172.03932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb:                     mape 82008298.42201\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb:                     rmse 296.57504\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb:       time_since_restore 1.46181\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb:         time_this_iter_s 0.67016\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb:             time_total_s 1.46181\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb:                timestamp 1689728078\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: 🚀 View run FSR_Trainable_5fb24b8c at: https://wandb.ai/seokjin/FSR-prediction/runs/5fb24b8c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527936)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095441-5fb24b8c/logs\n",
      "2023-07-19 09:54:48,636\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.935 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:54:48,638\tWARNING util.py:315 -- The `process_trial_result` operation took 1.938 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:54:48,639\tWARNING util.py:315 -- Processing trial results took 1.939 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:54:48,640\tWARNING util.py:315 -- The `process_trial_result` operation took 1.940 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_ea213d18_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-54-35/wandb/run-20230719_095451-ea213d18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Syncing run FSR_Trainable_ea213d18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ea213d18\n",
      "2023-07-19 09:55:00,813\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.933 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:55:00,814\tWARNING util.py:315 -- The `process_trial_result` operation took 1.936 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:55:00,819\tWARNING util.py:315 -- Processing trial results took 1.940 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:55:00,820\tWARNING util.py:315 -- The `process_trial_result` operation took 1.941 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_4ca7fa1d_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-54-45/wandb/run-20230719_095504-4ca7fa1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb: Syncing run FSR_Trainable_4ca7fa1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4ca7fa1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb:                      mae █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb:                     mape █▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb:                     rmse █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb:         time_this_iter_s █▂▂▁▁▁▅▃▃▂▃▃▃▅▃▃▂▃▂▅▄▅▄▄▅▅▅▄▄▅▄▄▄▅▄▄▄▅▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb:                      mae 107.02229\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb:                     mape 51092828.37482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb:                     rmse 184.81966\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb:       time_since_restore 57.50119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb:         time_this_iter_s 0.57919\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb:             time_total_s 57.50119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb:                timestamp 1689728125\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: 🚀 View run FSR_Trainable_af997dd1 at: https://wandb.ai/seokjin/FSR-prediction/runs/af997dd1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527499)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095421-af997dd1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-07-19 09:55:38,947\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.942 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:55:38,950\tWARNING util.py:315 -- The `process_trial_result` operation took 1.947 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:55:38,952\tWARNING util.py:315 -- Processing trial results took 1.948 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:55:38,953\tWARNING util.py:315 -- The `process_trial_result` operation took 1.950 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_778b0968_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-54-57/wandb/run-20230719_095542-778b0968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Syncing run FSR_Trainable_778b0968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/778b0968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb:                      mae █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb:                     mape █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb:                     rmse █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb:         time_this_iter_s ▇▃▂▃▃▁▇▄▃▂▁▃▂▆▆▅▄▄▅▅█▄▄▄▄▃▅▆▄▄▆▄▄▅▃▄▄▅▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb:                      mae 107.28396\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb:                     mape 51056402.22171\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb:                     rmse 185.38925\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb:       time_since_restore 59.1306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb:         time_this_iter_s 0.55901\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb:             time_total_s 59.1306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb:                timestamp 1689728134\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: 🚀 View run FSR_Trainable_e29404c6 at: https://wandb.ai/seokjin/FSR-prediction/runs/e29404c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=527720)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095430-e29404c6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb:                      mae █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb:                     mape █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb:                     rmse █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb:         time_this_iter_s █▁▅▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "2023-07-19 09:55:50,097\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.147 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:55:50,099\tWARNING util.py:315 -- The `process_trial_result` operation took 2.150 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:55:50,100\tWARNING util.py:315 -- Processing trial results took 2.151 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:55:50,102\tWARNING util.py:315 -- The `process_trial_result` operation took 2.153 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528655)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_c2226ee4_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-55-36/wandb/run-20230719_095553-c2226ee4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: Syncing run FSR_Trainable_c2226ee4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c2226ee4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb:                      mae 185.1115\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb:                     mape 9418924324722710.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb:                     rmse 379.48029\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb:       time_since_restore 0.65551\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb:         time_this_iter_s 0.65551\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb:             time_total_s 0.65551\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb:                timestamp 1689728147\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: 🚀 View run FSR_Trainable_c2226ee4 at: https://wandb.ai/seokjin/FSR-prediction/runs/c2226ee4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528884)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095553-c2226ee4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:56:00,417\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.198 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:56:00,421\tWARNING util.py:315 -- The `process_trial_result` operation took 2.202 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:56:00,422\tWARNING util.py:315 -- Processing trial results took 2.203 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:56:00,423\tWARNING util.py:315 -- The `process_trial_result` operation took 2.205 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb:                      mae █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb:                     mape █▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb:                     rmse █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb:         time_this_iter_s █▅▃▅▃▄▅▅▃▃▂▄▄▃▃▃▂▃▆▄▄▄▂▄▂▄▄▄▃▄▄▁▁▁▂▆▃▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_41e9d702_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-55-47/wandb/run-20230719_095602-41e9d702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: Syncing run FSR_Trainable_41e9d702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/41e9d702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb:                      mae 204.89429\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb:                     mape 1.2373369563924094e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb:                     rmse 455.66766\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb:       time_since_restore 0.39978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb:         time_this_iter_s 0.39978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb:             time_total_s 0.39978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb:                timestamp 1689728158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: 🚀 View run FSR_Trainable_41e9d702 at: https://wandb.ai/seokjin/FSR-prediction/runs/41e9d702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095602-41e9d702/logs\n",
      "2023-07-19 09:56:09,361\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.107 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:56:09,363\tWARNING util.py:315 -- The `process_trial_result` operation took 2.111 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:56:09,365\tWARNING util.py:315 -- Processing trial results took 2.112 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:56:09,366\tWARNING util.py:315 -- The `process_trial_result` operation took 2.114 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529121)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_e652c47e_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-55-57/wandb/run-20230719_095611-e652c47e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: Syncing run FSR_Trainable_e652c47e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e652c47e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb:                      mae 232.33825\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb:                     mape 129287661.37231\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb:                     rmse 404.05128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb:       time_since_restore 0.33842\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb:         time_this_iter_s 0.33842\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb:             time_total_s 0.33842\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb:                timestamp 1689728167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: 🚀 View run FSR_Trainable_e652c47e at: https://wandb.ai/seokjin/FSR-prediction/runs/e652c47e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095611-e652c47e/logs\n",
      "2023-07-19 09:56:18,391\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.064 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:56:18,394\tWARNING util.py:315 -- The `process_trial_result` operation took 2.068 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:56:18,396\tWARNING util.py:315 -- Processing trial results took 2.070 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:56:18,398\tWARNING util.py:315 -- The `process_trial_result` operation took 2.072 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529349)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_0166525e_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-56-06/wandb/run-20230719_095620-0166525e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Syncing run FSR_Trainable_0166525e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0166525e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb:                      mae █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb:                     mape █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb:                     rmse █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb:         time_this_iter_s ▇█▄▅▅▅▅▆▄▅▄▅▆▅▅▅▄▅▃▆▆▃▅▄▄▃▅▃▁▃▂▁▁▂▄▁▂▂▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb:                      mae 108.08671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb:                     mape 50983011.58646\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb:                     rmse 187.0781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb:       time_since_restore 67.62011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb:         time_this_iter_s 0.59835\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb:             time_total_s 67.62011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb:                timestamp 1689728179\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb: 🚀 View run FSR_Trainable_4ca7fa1d at: https://wandb.ai/seokjin/FSR-prediction/runs/4ca7fa1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=528385)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095504-4ca7fa1d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2023-07-19 09:56:26,492\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.770 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:56:26,494\tWARNING util.py:315 -- The `process_trial_result` operation took 1.773 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:56:26,496\tWARNING util.py:315 -- Processing trial results took 1.775 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:56:26,498\tWARNING util.py:315 -- The `process_trial_result` operation took 1.776 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_8425873d_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-56-15/wandb/run-20230719_095628-8425873d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: Syncing run FSR_Trainable_8425873d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8425873d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529576)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb:                      mae █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb:                     mape █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb:                     rmse █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb:         time_this_iter_s █▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb:                timestamp ▁▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb:                      mae 131.57773\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb:                     mape 53442898.97929\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb:                     rmse 236.69182\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb:       time_since_restore 2.3126\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb:         time_this_iter_s 0.50404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb:             time_total_s 2.3126\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb:                timestamp 1689728187\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: 🚀 View run FSR_Trainable_8425873d at: https://wandb.ai/seokjin/FSR-prediction/runs/8425873d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=529803)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095628-8425873d/logs\n",
      "2023-07-19 09:56:34,783\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.226 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:56:34,787\tWARNING util.py:315 -- The `process_trial_result` operation took 2.231 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:56:34,792\tWARNING util.py:315 -- Processing trial results took 2.236 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:56:34,794\tWARNING util.py:315 -- The `process_trial_result` operation took 2.238 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_bbc12f41_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-56-23/wandb/run-20230719_095637-bbc12f41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: Syncing run FSR_Trainable_bbc12f41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/bbc12f41\n",
      "2023-07-19 09:56:42,018\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.416 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:56:42,023\tWARNING util.py:315 -- The `process_trial_result` operation took 2.422 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:56:42,025\tWARNING util.py:315 -- Processing trial results took 2.424 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:56:42,027\tWARNING util.py:315 -- The `process_trial_result` operation took 2.426 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_49d67090_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-56-31/wandb/run-20230719_095644-49d67090\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: Syncing run FSR_Trainable_49d67090\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/49d67090\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:56:49,110\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.323 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:56:49,113\tWARNING util.py:315 -- The `process_trial_result` operation took 2.326 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:56:49,114\tWARNING util.py:315 -- Processing trial results took 2.327 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:56:49,115\tWARNING util.py:315 -- The `process_trial_result` operation took 2.328 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb:                      mae 261.06573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb:                     mape 195264719.3356\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb:                     rmse 414.95243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb:       time_since_restore 0.36831\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb:         time_this_iter_s 0.36831\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb:             time_total_s 0.36831\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb:                timestamp 1689728199\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: 🚀 View run FSR_Trainable_49d67090 at: https://wandb.ai/seokjin/FSR-prediction/runs/49d67090\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530207)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095644-49d67090/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_7b5b79a8_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-56-39/wandb/run-20230719_095651-7b5b79a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: Syncing run FSR_Trainable_7b5b79a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7b5b79a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb:                      mae 270.74337\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb:                     mape 159256013.76645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb:                     rmse 420.7876\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb:       time_since_restore 0.60729\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb:         time_this_iter_s 0.60729\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb:             time_total_s 0.60729\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb:                timestamp 1689728206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: 🚀 View run FSR_Trainable_7b5b79a8 at: https://wandb.ai/seokjin/FSR-prediction/runs/7b5b79a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530386)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095651-7b5b79a8/logs\n",
      "2023-07-19 09:56:57,946\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.937 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:56:57,949\tWARNING util.py:315 -- The `process_trial_result` operation took 1.941 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:56:57,950\tWARNING util.py:315 -- Processing trial results took 1.942 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:56:57,951\tWARNING util.py:315 -- The `process_trial_result` operation took 1.944 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb:                      mae █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb:                     mape █▅▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb:                     rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb:         time_this_iter_s █▄▆▃▂▁▁▁▂▂▃▂▂▃▃▂▁▁▂▅▆▂▃▂▂▁▁▁▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb:                timestamp ▁▂▂▂▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095637-bbc12f41/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_6e8dd620_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-56-46/wandb/run-20230719_095700-6e8dd620\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: Syncing run FSR_Trainable_6e8dd620\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6e8dd620\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530028)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb:                      mae 190.28186\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb:                     mape 3.6553275308135795e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb:                     rmse 306.58203\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb:       time_since_restore 1.05329\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb:         time_this_iter_s 0.33122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb:             time_total_s 1.05329\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb:                timestamp 1689728218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: 🚀 View run FSR_Trainable_6e8dd620 at: https://wandb.ai/seokjin/FSR-prediction/runs/6e8dd620\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095700-6e8dd620/logs\n",
      "2023-07-19 09:57:05,748\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.020 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:57:05,751\tWARNING util.py:315 -- The `process_trial_result` operation took 2.024 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:57:05,752\tWARNING util.py:315 -- Processing trial results took 2.025 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:57:05,753\tWARNING util.py:315 -- The `process_trial_result` operation took 2.026 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_ec6ac595_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-56-55/wandb/run-20230719_095708-ec6ac595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: Syncing run FSR_Trainable_ec6ac595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ec6ac595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530614)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 09:57:11,525\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.732 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:57:11,529\tWARNING util.py:315 -- The `process_trial_result` operation took 1.736 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:57:11,530\tWARNING util.py:315 -- Processing trial results took 1.738 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:57:11,531\tWARNING util.py:315 -- The `process_trial_result` operation took 1.739 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb:                      mae 236.52635\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb:                     mape 4.319528688003384e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb:                     rmse 383.49803\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb:       time_since_restore 0.76722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb:         time_this_iter_s 0.76722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb:             time_total_s 0.76722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb:                timestamp 1689728223\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: 🚀 View run FSR_Trainable_ec6ac595 at: https://wandb.ai/seokjin/FSR-prediction/runs/ec6ac595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=530841)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095708-ec6ac595/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:57:19,563\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.251 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:57:19,569\tWARNING util.py:315 -- The `process_trial_result` operation took 2.257 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:57:19,572\tWARNING util.py:315 -- Processing trial results took 2.260 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:57:19,574\tWARNING util.py:315 -- The `process_trial_result` operation took 2.262 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_7c3324ee_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-57-08/wandb/run-20230719_095721-7c3324ee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: Syncing run FSR_Trainable_7c3324ee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7c3324ee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:57:25,400\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.656 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:57:25,402\tWARNING util.py:315 -- The `process_trial_result` operation took 1.659 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:57:25,403\tWARNING util.py:315 -- Processing trial results took 1.660 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:57:25,405\tWARNING util.py:315 -- The `process_trial_result` operation took 1.662 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb:                      mae 238.44962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb:                     mape 155681008.2664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb:                     rmse 379.83858\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb:       time_since_restore 0.77147\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb:         time_this_iter_s 0.77147\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb:             time_total_s 0.77147\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb:                timestamp 1689728237\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: 🚀 View run FSR_Trainable_7c3324ee at: https://wandb.ai/seokjin/FSR-prediction/runs/7c3324ee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531241)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095721-7c3324ee/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_5c9a2b18_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-57-16/wandb/run-20230719_095727-5c9a2b18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: Syncing run FSR_Trainable_5c9a2b18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5c9a2b18\n",
      "2023-07-19 09:57:34,430\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.183 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:57:34,432\tWARNING util.py:315 -- The `process_trial_result` operation took 2.186 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:57:34,433\tWARNING util.py:315 -- Processing trial results took 2.188 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:57:34,434\tWARNING util.py:315 -- The `process_trial_result` operation took 2.189 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "2023-07-19 09:57:42,403\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.017 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:57:42,407\tWARNING util.py:315 -- The `process_trial_result` operation took 2.022 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:57:42,408\tWARNING util.py:315 -- Processing trial results took 2.023 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:57:42,409\tWARNING util.py:315 -- The `process_trial_result` operation took 2.024 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_5136c4dd_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-57-22/wandb/run-20230719_095741-5136c4dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: Syncing run FSR_Trainable_5136c4dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5136c4dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb:                      mae █▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb:                     mape █▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb:                     rmse █▆▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb:         time_this_iter_s █▁▆▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb:                      mae 133.64411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb:                     mape 62975240.00288\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb:                     rmse 234.44316\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb:       time_since_restore 3.63689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb:         time_this_iter_s 0.85557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb:             time_total_s 3.63689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb:                timestamp 1689728265\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: 🚀 View run FSR_Trainable_4d848c95 at: https://wandb.ai/seokjin/FSR-prediction/runs/4d848c95\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531793)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095745-4d848c95/logs\n",
      "2023-07-19 09:57:54,009\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.932 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:57:54,012\tWARNING util.py:315 -- The `process_trial_result` operation took 1.936 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:57:54,013\tWARNING util.py:315 -- Processing trial results took 1.937 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:57:54,014\tWARNING util.py:315 -- The `process_trial_result` operation took 1.938 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_44f0d5ac_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-57-39/wandb/run-20230719_095757-44f0d5ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: Syncing run FSR_Trainable_44f0d5ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/44f0d5ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb:                      mae █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb:                     mape █▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb:                     rmse █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb:         time_this_iter_s █▁▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb:                      mae 138.2581\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb:                     mape 51627754.46745\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb:                     rmse 242.45712\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb:       time_since_restore 3.51228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb:         time_this_iter_s 0.82296\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb:             time_total_s 3.51228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb:                timestamp 1689728276\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: 🚀 View run FSR_Trainable_44f0d5ac at: https://wandb.ai/seokjin/FSR-prediction/runs/44f0d5ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532046)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095757-44f0d5ac/logs\n",
      "2023-07-19 09:58:05,240\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.017 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:58:05,242\tWARNING util.py:315 -- The `process_trial_result` operation took 2.020 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:58:05,243\tWARNING util.py:315 -- Processing trial results took 2.021 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:58:05,244\tWARNING util.py:315 -- The `process_trial_result` operation took 2.022 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_68033e63_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-57-51/wandb/run-20230719_095808-68033e63\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: Syncing run FSR_Trainable_68033e63\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/68033e63\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-07-19 09:58:17,783\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.061 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:58:17,786\tWARNING util.py:315 -- The `process_trial_result` operation took 2.065 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:58:17,787\tWARNING util.py:315 -- Processing trial results took 2.066 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:58:17,788\tWARNING util.py:315 -- The `process_trial_result` operation took 2.067 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb:                      mae █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb:                     mape █▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb:                     rmse █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb:       time_since_restore ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb:         time_this_iter_s ▆▂▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb:             time_total_s ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb:                timestamp ▁▆▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb:                      mae 134.13647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb:                     mape 53732872.89522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb:                     rmse 243.33666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb:       time_since_restore 3.60055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb:         time_this_iter_s 0.98926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb:             time_total_s 3.60055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb:                timestamp 1689728287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: 🚀 View run FSR_Trainable_68033e63 at: https://wandb.ai/seokjin/FSR-prediction/runs/68033e63\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532276)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095808-68033e63/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_34cfda8e_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-58-02/wandb/run-20230719_095821-34cfda8e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: Syncing run FSR_Trainable_34cfda8e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/34cfda8e\n",
      "2023-07-19 09:58:31,214\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.114 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:58:31,217\tWARNING util.py:315 -- The `process_trial_result` operation took 2.119 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:58:31,219\tWARNING util.py:315 -- Processing trial results took 2.121 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:58:31,220\tWARNING util.py:315 -- The `process_trial_result` operation took 2.122 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_77190088_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_09-58-14/wandb/run-20230719_095834-77190088\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: Syncing run FSR_Trainable_77190088\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/77190088\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb:                      mae █▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb:                     mape █▁▂▃▄▄▄▄▃▃▃▃▃▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb:                     rmse █▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb:         time_this_iter_s ▇▄▄▃▄▇▅▁▃▂▂▁▄▄▄█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb:                timestamp ▁▂▃▃▃▄▄▅▅▅▅▆▆▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb:                      mae 111.89786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb:                     mape 56028289.47282\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb:                     rmse 191.81232\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb:       time_since_restore 14.44036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb:         time_this_iter_s 1.18834\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb:             time_total_s 14.44036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb:                timestamp 1689728313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: 🚀 View run FSR_Trainable_34cfda8e at: https://wandb.ai/seokjin/FSR-prediction/runs/34cfda8e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532502)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095821-34cfda8e/logs\n",
      "2023-07-19 09:58:48,558\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.391 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:58:48,563\tWARNING util.py:315 -- The `process_trial_result` operation took 2.397 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:58:48,566\tWARNING util.py:315 -- Processing trial results took 2.400 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:58:48,568\tWARNING util.py:315 -- The `process_trial_result` operation took 2.401 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_09-38-35/FSR_Trainable_e3450325_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,inde_2023-07-19_09-58-28/wandb/run-20230719_095853-e3450325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb: Syncing run FSR_Trainable_e3450325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e3450325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb:                      mae █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb:                     mape ▅█▃▁▃▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb:                     rmse █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb:         time_this_iter_s ▃▂▃▄▃▂▃▃▁▄▃▂▁▁▁▂▁▁▁▁▂▂██▄▂▂▃▂▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb:                timestamp ▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb:                      mae 110.61234\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb:                     mape 55511842.93951\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb:                     rmse 189.77752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb:       time_since_restore 24.17718\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb:         time_this_iter_s 0.74279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb:             time_total_s 24.17718\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb:                timestamp 1689728337\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: 🚀 View run FSR_Trainable_77190088 at: https://wandb.ai/seokjin/FSR-prediction/runs/77190088\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532726)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095834-77190088/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb:                      mae █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb:                     mape █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb:                     rmse █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb:       time_since_restore ▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb:         time_this_iter_s ▄▁▁▁▁▂▂▂▁▂▃▂▂▂▃▃▂▂▂▃▃▂▂▂▃▃▅▂▃▃▅▃▄▂▃▂▂█▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb:             time_total_s ▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: Waiting for W&B process to finish... (success).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531421)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb:                      mae █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb:                     mape █▁▁▁▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb:                     rmse █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb:         time_this_iter_s ▄▄▂▂▄▄▃▃▂▅▃▂▃▂▄▆▂▂▃▇▅█▅▃▆█▆▇▃▃▃▅▄▅▅▃▂▂▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇█████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb:                      mae 108.16529\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb:                     mape 50631500.29349\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb:                     rmse 187.28006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb:       time_since_restore 77.85796\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb:         time_this_iter_s 0.65111\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb:             time_total_s 77.85796\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb:                timestamp 1689728345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: 🚀 View run FSR_Trainable_5136c4dd at: https://wandb.ai/seokjin/FSR-prediction/runs/5136c4dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=531645)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095741-5136c4dd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb:                      mae █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb:                     mape █▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb:                     rmse █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb:         time_this_iter_s ▅▇█▇▃▄▄▄▃▃▄▂▂▂▂▃▂▂▂▂▂▂▁▂▂▂▄▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb:                timestamp ▁▂▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_095853-e3450325/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=532965)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2023-07-19 09:59:13,723\tINFO tune.py:1111 -- Total run time: 1234.48 seconds (1228.96 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
