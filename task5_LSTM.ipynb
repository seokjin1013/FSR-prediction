{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task5\n",
    "\n",
    "Index_X = FSR_for_force\n",
    "\n",
    "Index_y = force\n",
    "\n",
    "Data = Splited by Subject\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-07-19_11-36-24/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-07-19_11-36-24\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "188.688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.LSTM'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': 'FSR_for_force',\n",
    "        'index_y': 'force',\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_subject'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-19 11:36:24,891] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 11:36:27,096\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "2023-07-19 11:36:28,516\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-19 12:09:03</td></tr>\n",
       "<tr><td>Running for: </td><td>00:32:35.10        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.8/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=100<br>Bracket: Iter 64.000: -197.75219676845356 | Iter 32.000: -193.06511335403982 | Iter 16.000: -192.86325138587623 | Iter 8.000: -196.4428253810569 | Iter 4.000: -201.2730062950329 | Iter 2.000: -207.68553539215264 | Iter 1.000: -240.95941630065636<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>index_X      </th><th>index_y  </th><th>model         </th><th style=\"text-align: right;\">    model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">  model_args/num_layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">       mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_e628ae96</td><td>TERMINATED</td><td>172.26.215.93:561746</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000358319</td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      189.202   </td><td style=\"text-align: right;\">387.396</td><td style=\"text-align: right;\">241.414 </td><td style=\"text-align: right;\">1.38803e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_59cddc2b</td><td>TERMINATED</td><td>172.26.215.93:561819</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000106029</td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      327.326   </td><td style=\"text-align: right;\">271.485</td><td style=\"text-align: right;\">145.739 </td><td style=\"text-align: right;\">4.18275e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_7623c5cc</td><td>TERMINATED</td><td>172.26.215.93:561993</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000460718</td><td>sklearn.preproc_0330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      572.246   </td><td style=\"text-align: right;\">266.701</td><td style=\"text-align: right;\">150.591 </td><td style=\"text-align: right;\">2.20392e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_cedcf763</td><td>TERMINATED</td><td>172.26.215.93:562168</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00126912 </td><td>sklearn.preproc_0750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        5.21457 </td><td style=\"text-align: right;\">393.183</td><td style=\"text-align: right;\">197.986 </td><td style=\"text-align: right;\">1.82152e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_9d2e5769</td><td>TERMINATED</td><td>172.26.215.93:562514</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0080904  </td><td>sklearn.preproc_0330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">     1308.08    </td><td style=\"text-align: right;\">194.499</td><td style=\"text-align: right;\"> 97.8343</td><td style=\"text-align: right;\">6.95811e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_eab59ae0</td><td>TERMINATED</td><td>172.26.215.93:562838</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.1696e-05 </td><td>sklearn.preproc_0330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.23095 </td><td style=\"text-align: right;\">823.25 </td><td style=\"text-align: right;\">475.246 </td><td style=\"text-align: right;\">6.56315e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_7c4e4b2a</td><td>TERMINATED</td><td>172.26.215.93:563036</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00278154 </td><td>sklearn.preproc_0750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.56874 </td><td style=\"text-align: right;\">458.901</td><td style=\"text-align: right;\">258.255 </td><td style=\"text-align: right;\">5.8531e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_d303bd48</td><td>TERMINATED</td><td>172.26.215.93:563263</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        6.89597e-05</td><td>sklearn.preproc_0330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       15.4521  </td><td style=\"text-align: right;\">450.097</td><td style=\"text-align: right;\">240.418 </td><td style=\"text-align: right;\">2.70154e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_f476ee4d</td><td>TERMINATED</td><td>172.26.215.93:563531</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00632062 </td><td>sklearn.preproc_0330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.99444 </td><td style=\"text-align: right;\">420.939</td><td style=\"text-align: right;\">261.123 </td><td style=\"text-align: right;\">4.49431e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_4ad2657e</td><td>TERMINATED</td><td>172.26.215.93:563727</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        6.24482e-05</td><td>sklearn.preproc_0330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.53678 </td><td style=\"text-align: right;\">450.468</td><td style=\"text-align: right;\">330.747 </td><td style=\"text-align: right;\">6.94809e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_2a2f496d</td><td>TERMINATED</td><td>172.26.215.93:563953</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00956835 </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       13.485   </td><td style=\"text-align: right;\">342.465</td><td style=\"text-align: right;\">180.029 </td><td style=\"text-align: right;\">4.25533e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_0f299666</td><td>TERMINATED</td><td>172.26.215.93:564223</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0056096  </td><td>sklearn.preproc_0750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      106.196   </td><td style=\"text-align: right;\">250.928</td><td style=\"text-align: right;\">133.836 </td><td style=\"text-align: right;\">2.41661e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_b07bb1b2</td><td>TERMINATED</td><td>172.26.215.93:564456</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0209731  </td><td>sklearn.preproc_0750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">       28.1421  </td><td style=\"text-align: right;\">386.348</td><td style=\"text-align: right;\">234.115 </td><td style=\"text-align: right;\">9.92124e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_a2d1aeb7</td><td>TERMINATED</td><td>172.26.215.93:564704</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00226968 </td><td>sklearn.preproc_0330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.20365 </td><td style=\"text-align: right;\">424.793</td><td style=\"text-align: right;\">245.981 </td><td style=\"text-align: right;\">3.21758e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_48b95fcb</td><td>TERMINATED</td><td>172.26.215.93:564901</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0981854  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      105.039   </td><td style=\"text-align: right;\">204.054</td><td style=\"text-align: right;\"> 99.4635</td><td style=\"text-align: right;\">2.25678e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_1acabe1b</td><td>TERMINATED</td><td>172.26.215.93:565190</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0514621  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      155.002   </td><td style=\"text-align: right;\">292.503</td><td style=\"text-align: right;\">160.304 </td><td style=\"text-align: right;\">8.62857e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_8ac6cd16</td><td>TERMINATED</td><td>172.26.215.93:565468</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0878153  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      104.811   </td><td style=\"text-align: right;\">199.278</td><td style=\"text-align: right;\"> 99.9243</td><td style=\"text-align: right;\">2.38344e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_5cb6c374</td><td>TERMINATED</td><td>172.26.215.93:565748</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0963016  </td><td>sklearn.preproc_0750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      100.614   </td><td style=\"text-align: right;\">230.361</td><td style=\"text-align: right;\">114.757 </td><td style=\"text-align: right;\">1.7527e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_b3de54a8</td><td>TERMINATED</td><td>172.26.215.93:565992</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0964736  </td><td>sklearn.preproc_0750</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        4.82186 </td><td style=\"text-align: right;\">321.615</td><td style=\"text-align: right;\">159.508 </td><td style=\"text-align: right;\">2.04376e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_a2f42b1f</td><td>TERMINATED</td><td>172.26.215.93:566224</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0315581  </td><td>sklearn.preproc_0750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       66.791   </td><td style=\"text-align: right;\">215.841</td><td style=\"text-align: right;\">115.772 </td><td style=\"text-align: right;\">2.18896e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_29617618</td><td>TERMINATED</td><td>172.26.215.93:566427</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0344314  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       64.5837  </td><td style=\"text-align: right;\">199.008</td><td style=\"text-align: right;\">102.219 </td><td style=\"text-align: right;\">3.27968e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_1957c775</td><td>TERMINATED</td><td>172.26.215.93:566743</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0266689  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       67.8572  </td><td style=\"text-align: right;\">195.164</td><td style=\"text-align: right;\">101.07  </td><td style=\"text-align: right;\">2.79805e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_52b3b738</td><td>TERMINATED</td><td>172.26.215.93:566942</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0241688  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       67.8855  </td><td style=\"text-align: right;\">193.149</td><td style=\"text-align: right;\"> 96.75  </td><td style=\"text-align: right;\">2.50216e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_cc4614e8</td><td>TERMINATED</td><td>172.26.215.93:567177</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0204384  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        3.59375 </td><td style=\"text-align: right;\">331.745</td><td style=\"text-align: right;\">179.16  </td><td style=\"text-align: right;\">8.08442e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_07549cff</td><td>TERMINATED</td><td>172.26.215.93:567419</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0152641  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      136.751   </td><td style=\"text-align: right;\">206.378</td><td style=\"text-align: right;\">105.439 </td><td style=\"text-align: right;\">2.95234e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_426f126e</td><td>TERMINATED</td><td>172.26.215.93:567712</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0409689  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       65.9079  </td><td style=\"text-align: right;\">197.163</td><td style=\"text-align: right;\"> 99.4801</td><td style=\"text-align: right;\">2.80729e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_d8976afa</td><td>TERMINATED</td><td>172.26.215.93:567915</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0139902  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       65.3051  </td><td style=\"text-align: right;\">196.542</td><td style=\"text-align: right;\">104.756 </td><td style=\"text-align: right;\">2.692e+07  </td></tr>\n",
       "<tr><td>FSR_Trainable_e5b649e8</td><td>TERMINATED</td><td>172.26.215.93:568240</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.016462   </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       68.3205  </td><td style=\"text-align: right;\">195.227</td><td style=\"text-align: right;\"> 99.2893</td><td style=\"text-align: right;\">2.36394e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ad20764b</td><td>TERMINATED</td><td>172.26.215.93:568435</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0111599  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">       12.5462  </td><td style=\"text-align: right;\">222.284</td><td style=\"text-align: right;\">115.198 </td><td style=\"text-align: right;\">4.43534e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_86dbc305</td><td>TERMINATED</td><td>172.26.215.93:568671</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0129986  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       19.42    </td><td style=\"text-align: right;\">205.704</td><td style=\"text-align: right;\">105.333 </td><td style=\"text-align: right;\">3.96385e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_545478ec</td><td>TERMINATED</td><td>172.26.215.93:568883</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00879526 </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        6.35684 </td><td style=\"text-align: right;\">207.799</td><td style=\"text-align: right;\">111.813 </td><td style=\"text-align: right;\">4.69687e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_a9e45cc6</td><td>TERMINATED</td><td>172.26.215.93:569168</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00650421 </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        6.19803 </td><td style=\"text-align: right;\">214.259</td><td style=\"text-align: right;\">114.873 </td><td style=\"text-align: right;\">4.10034e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_0e6d32df</td><td>TERMINATED</td><td>172.26.215.93:569366</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00490065 </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.70178 </td><td style=\"text-align: right;\">332.793</td><td style=\"text-align: right;\">193.475 </td><td style=\"text-align: right;\">5.76929e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_83b47f12</td><td>TERMINATED</td><td>172.26.215.93:569594</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00406533 </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.59686 </td><td style=\"text-align: right;\">347.025</td><td style=\"text-align: right;\">192.573 </td><td style=\"text-align: right;\">5.87047e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_7bf4a9dc</td><td>TERMINATED</td><td>172.26.215.93:569837</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0207108  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      102.234   </td><td style=\"text-align: right;\">203.647</td><td style=\"text-align: right;\">104.377 </td><td style=\"text-align: right;\">2.90635e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_53e69005</td><td>TERMINATED</td><td>172.26.215.93:570061</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.020856   </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      101.886   </td><td style=\"text-align: right;\">195.915</td><td style=\"text-align: right;\"> 97.658 </td><td style=\"text-align: right;\">2.29628e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ff916a9d</td><td>TERMINATED</td><td>172.26.215.93:570274</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0242604  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       64.7629  </td><td style=\"text-align: right;\">205.851</td><td style=\"text-align: right;\">103.237 </td><td style=\"text-align: right;\">3.08395e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_9ff2fafb</td><td>TERMINATED</td><td>172.26.215.93:570601</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0220843  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      107.08    </td><td style=\"text-align: right;\">198.134</td><td style=\"text-align: right;\"> 99.618 </td><td style=\"text-align: right;\">2.46262e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_1f2a53fe</td><td>TERMINATED</td><td>172.26.215.93:570825</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.016505   </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.656   </td><td style=\"text-align: right;\">207.154</td><td style=\"text-align: right;\">113.217 </td><td style=\"text-align: right;\">4.13507e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_12caccc0</td><td>TERMINATED</td><td>172.26.215.93:571041</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0487264  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.77442 </td><td style=\"text-align: right;\">221.505</td><td style=\"text-align: right;\">131.178 </td><td style=\"text-align: right;\">7.83256e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_6fad082a</td><td>TERMINATED</td><td>172.26.215.93:571268</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0463673  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       17.7394  </td><td style=\"text-align: right;\">205.962</td><td style=\"text-align: right;\">107.384 </td><td style=\"text-align: right;\">3.51123e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_cfde22d1</td><td>TERMINATED</td><td>172.26.215.93:571479</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00117998 </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       17.7899  </td><td style=\"text-align: right;\">387.809</td><td style=\"text-align: right;\">240.588 </td><td style=\"text-align: right;\">1.47781e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_539f358c</td><td>TERMINATED</td><td>172.26.215.93:571737</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0017374  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        5.2251  </td><td style=\"text-align: right;\">349.815</td><td style=\"text-align: right;\">222.083 </td><td style=\"text-align: right;\">1.35474e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_2f623f46</td><td>TERMINATED</td><td>172.26.215.93:571938</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00201087 </td><td>sklearn.preproc_0330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.58888 </td><td style=\"text-align: right;\">411.745</td><td style=\"text-align: right;\">261.695 </td><td style=\"text-align: right;\">4.0885e+17 </td></tr>\n",
       "<tr><td>FSR_Trainable_1b6cfdf3</td><td>TERMINATED</td><td>172.26.215.93:572190</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00819466 </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.45511 </td><td style=\"text-align: right;\">211.226</td><td style=\"text-align: right;\">116.72  </td><td style=\"text-align: right;\">4.48802e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_21ca597b</td><td>TERMINATED</td><td>172.26.215.93:572418</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0100494  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.85316 </td><td style=\"text-align: right;\">227.281</td><td style=\"text-align: right;\">122.084 </td><td style=\"text-align: right;\">5.07295e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_a9c0ec65</td><td>TERMINATED</td><td>172.26.215.93:572649</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0113571  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.08912 </td><td style=\"text-align: right;\">209.123</td><td style=\"text-align: right;\">114.076 </td><td style=\"text-align: right;\">4.30756e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_64cfa2c5</td><td>TERMINATED</td><td>172.26.215.93:572889</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0140406  </td><td>sklearn.preproc_0330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.92954 </td><td style=\"text-align: right;\">396.525</td><td style=\"text-align: right;\">224.826 </td><td style=\"text-align: right;\">3.9413e+17 </td></tr>\n",
       "<tr><td>FSR_Trainable_a17695c7</td><td>TERMINATED</td><td>172.26.215.93:573099</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0616179  </td><td>sklearn.preproc_0330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.80991 </td><td style=\"text-align: right;\">432.585</td><td style=\"text-align: right;\">223.419 </td><td style=\"text-align: right;\">2.37929e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_8873a54e</td><td>TERMINATED</td><td>172.26.215.93:573313</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00305281 </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.27176 </td><td style=\"text-align: right;\">406.613</td><td style=\"text-align: right;\">257.82  </td><td style=\"text-align: right;\">1.30664e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_e25d6d8d</td><td>TERMINATED</td><td>172.26.215.93:573563</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00335292 </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.46832 </td><td style=\"text-align: right;\">366.17 </td><td style=\"text-align: right;\">213.51  </td><td style=\"text-align: right;\">8.03345e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_449c1ee7</td><td>TERMINATED</td><td>172.26.215.93:573788</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0293863  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">      102.718   </td><td style=\"text-align: right;\">205.551</td><td style=\"text-align: right;\"> 98.3475</td><td style=\"text-align: right;\">2.00671e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_2a4efb97</td><td>TERMINATED</td><td>172.26.215.93:573990</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0672223  </td><td>sklearn.preproc_0750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        3.86082 </td><td style=\"text-align: right;\">251.728</td><td style=\"text-align: right;\">125.035 </td><td style=\"text-align: right;\">1.88964e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_2f10f206</td><td>TERMINATED</td><td>172.26.215.93:574208</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0316567  </td><td>sklearn.preproc_0750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.31052 </td><td style=\"text-align: right;\">308.995</td><td style=\"text-align: right;\">156.165 </td><td style=\"text-align: right;\">3.38366e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_ca99d759</td><td>TERMINATED</td><td>172.26.215.93:574426</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0327563  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       68.7363  </td><td style=\"text-align: right;\">196.536</td><td style=\"text-align: right;\">101.126 </td><td style=\"text-align: right;\">2.54869e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e9bebd71</td><td>TERMINATED</td><td>172.26.215.93:574674</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.035913   </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       65.6664  </td><td style=\"text-align: right;\">198.898</td><td style=\"text-align: right;\"> 99.8435</td><td style=\"text-align: right;\">2.47584e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_48a57263</td><td>TERMINATED</td><td>172.26.215.93:574964</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0314995  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       63.973   </td><td style=\"text-align: right;\">202.798</td><td style=\"text-align: right;\">104.105 </td><td style=\"text-align: right;\">2.82163e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_1af8c14e</td><td>TERMINATED</td><td>172.26.215.93:575228</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0293712  </td><td>sklearn.preproc_0330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.28131 </td><td style=\"text-align: right;\">254.3  </td><td style=\"text-align: right;\">142.498 </td><td style=\"text-align: right;\">1.98824e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_2ed487a6</td><td>TERMINATED</td><td>172.26.215.93:575429</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0271468  </td><td>sklearn.preproc_0330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       13.6808  </td><td style=\"text-align: right;\">381.844</td><td style=\"text-align: right;\">200.143 </td><td style=\"text-align: right;\">2.83211e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_8fa2c49f</td><td>TERMINATED</td><td>172.26.215.93:575650</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0178765  </td><td>sklearn.preproc_0330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        9.91708 </td><td style=\"text-align: right;\">367.159</td><td style=\"text-align: right;\">219.928 </td><td style=\"text-align: right;\">3.99222e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_1acc5a57</td><td>TERMINATED</td><td>172.26.215.93:575859</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00648792 </td><td>sklearn.preproc_0330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       13.4941  </td><td style=\"text-align: right;\">379.768</td><td style=\"text-align: right;\">225.349 </td><td style=\"text-align: right;\">4.25041e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_6a3b5f4a</td><td>TERMINATED</td><td>172.26.215.93:576100</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00701725 </td><td>sklearn.preproc_0330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       14.9242  </td><td style=\"text-align: right;\">389.733</td><td style=\"text-align: right;\">238.767 </td><td style=\"text-align: right;\">4.82693e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_60033bfd</td><td>TERMINATED</td><td>172.26.215.93:576320</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00713423 </td><td>sklearn.preproc_0330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.42166 </td><td style=\"text-align: right;\">393.934</td><td style=\"text-align: right;\">261.579 </td><td style=\"text-align: right;\">5.69257e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_5ccec0ba</td><td>TERMINATED</td><td>172.26.215.93:576549</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0124108  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       65.5574  </td><td style=\"text-align: right;\">193.105</td><td style=\"text-align: right;\">102.73  </td><td style=\"text-align: right;\">2.92568e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_c47111ed</td><td>TERMINATED</td><td>172.26.215.93:576794</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0146767  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       66.8119  </td><td style=\"text-align: right;\">193.979</td><td style=\"text-align: right;\">101.292 </td><td style=\"text-align: right;\">2.81301e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_26e3e47a</td><td>TERMINATED</td><td>172.26.215.93:577024</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0147465  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       67.5037  </td><td style=\"text-align: right;\">189.723</td><td style=\"text-align: right;\"> 99.4837</td><td style=\"text-align: right;\">2.68803e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e5ea828b</td><td>TERMINATED</td><td>172.26.215.93:577242</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0152469  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.38027 </td><td style=\"text-align: right;\">204.031</td><td style=\"text-align: right;\">109.858 </td><td style=\"text-align: right;\">5.20684e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_abc8a58e</td><td>TERMINATED</td><td>172.26.215.93:577487</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.013776   </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       11.3499  </td><td style=\"text-align: right;\">200.53 </td><td style=\"text-align: right;\">108.941 </td><td style=\"text-align: right;\">3.83449e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_b6fe67e4</td><td>TERMINATED</td><td>172.26.215.93:577765</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0128678  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       58.577   </td><td style=\"text-align: right;\">190.545</td><td style=\"text-align: right;\">100.03  </td><td style=\"text-align: right;\">2.34234e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_457c8add</td><td>TERMINATED</td><td>172.26.215.93:577970</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0202191  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.40007 </td><td style=\"text-align: right;\">380.829</td><td style=\"text-align: right;\">235.523 </td><td style=\"text-align: right;\">1.5129e+08 </td></tr>\n",
       "<tr><td>FSR_Trainable_0b7bdef0</td><td>TERMINATED</td><td>172.26.215.93:578211</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00991509 </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.73836 </td><td style=\"text-align: right;\">381.256</td><td style=\"text-align: right;\">230.739 </td><td style=\"text-align: right;\">1.56708e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_43a1d0fd</td><td>TERMINATED</td><td>172.26.215.93:578422</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00972945 </td><td>sklearn.preproc_0750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.9324  </td><td style=\"text-align: right;\">351.467</td><td style=\"text-align: right;\">172.273 </td><td style=\"text-align: right;\">1.79295e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_553408e6</td><td>TERMINATED</td><td>172.26.215.93:578654</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0051803  </td><td>sklearn.preproc_0750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.84599 </td><td style=\"text-align: right;\">248.617</td><td style=\"text-align: right;\">132.604 </td><td style=\"text-align: right;\">2.97767e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_6399e6a9</td><td>TERMINATED</td><td>172.26.215.93:578902</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00473434 </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.781013</td><td style=\"text-align: right;\">308.396</td><td style=\"text-align: right;\">173.773 </td><td style=\"text-align: right;\">5.7268e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_fcec382d</td><td>TERMINATED</td><td>172.26.215.93:579132</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0192006  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        9.36924 </td><td style=\"text-align: right;\">193.293</td><td style=\"text-align: right;\">101.235 </td><td style=\"text-align: right;\">3.11275e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_d41e2bd6</td><td>TERMINATED</td><td>172.26.215.93:579358</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0231344  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       65.05    </td><td style=\"text-align: right;\">199.339</td><td style=\"text-align: right;\">101.41  </td><td style=\"text-align: right;\">2.40855e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_826407ff</td><td>TERMINATED</td><td>172.26.215.93:579593</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0173361  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       21.0648  </td><td style=\"text-align: right;\">195.418</td><td style=\"text-align: right;\">104.655 </td><td style=\"text-align: right;\">3.23541e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_741520de</td><td>TERMINATED</td><td>172.26.215.93:579818</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0407705  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       64.8726  </td><td style=\"text-align: right;\">202.891</td><td style=\"text-align: right;\">103.004 </td><td style=\"text-align: right;\">2.84806e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_5a6b36a8</td><td>TERMINATED</td><td>172.26.215.93:580035</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0123831  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       11.4506  </td><td style=\"text-align: right;\">194.687</td><td style=\"text-align: right;\">105.398 </td><td style=\"text-align: right;\">3.64715e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_be819b00</td><td>TERMINATED</td><td>172.26.215.93:580259</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0116431  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.50384 </td><td style=\"text-align: right;\">203.604</td><td style=\"text-align: right;\">109.732 </td><td style=\"text-align: right;\">3.96669e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_7a6d0a42</td><td>TERMINATED</td><td>172.26.215.93:580497</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0120702  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       11.2824  </td><td style=\"text-align: right;\">194.721</td><td style=\"text-align: right;\">106.921 </td><td style=\"text-align: right;\">4.15804e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_b05cbfe8</td><td>TERMINATED</td><td>172.26.215.93:580739</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00833967 </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.96364 </td><td style=\"text-align: right;\">326.715</td><td style=\"text-align: right;\">186.806 </td><td style=\"text-align: right;\">7.1031e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_a1543e18</td><td>TERMINATED</td><td>172.26.215.93:580964</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00873591 </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.51264 </td><td style=\"text-align: right;\">217.354</td><td style=\"text-align: right;\">118.887 </td><td style=\"text-align: right;\">5.74591e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_77225a5e</td><td>TERMINATED</td><td>172.26.215.93:581211</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0128227  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        5.02499 </td><td style=\"text-align: right;\">198.812</td><td style=\"text-align: right;\">107.056 </td><td style=\"text-align: right;\">4.66814e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e7a05b2f</td><td>TERMINATED</td><td>172.26.215.93:581439</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0122298  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       60.7138  </td><td style=\"text-align: right;\">191.04 </td><td style=\"text-align: right;\">101.074 </td><td style=\"text-align: right;\">2.66258e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_5043e67a</td><td>TERMINATED</td><td>172.26.215.93:581678</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.016926   </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.660958</td><td style=\"text-align: right;\">258.359</td><td style=\"text-align: right;\">151.703 </td><td style=\"text-align: right;\">8.24247e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ae71f7ed</td><td>TERMINATED</td><td>172.26.215.93:581891</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00610947 </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.732858</td><td style=\"text-align: right;\">364.963</td><td style=\"text-align: right;\">210.83  </td><td style=\"text-align: right;\">3.91157e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_0d84e070</td><td>TERMINATED</td><td>172.26.215.93:582116</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0240589  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       61.3308  </td><td style=\"text-align: right;\">193.19 </td><td style=\"text-align: right;\"> 99.2406</td><td style=\"text-align: right;\">2.64201e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_70fc4d25</td><td>TERMINATED</td><td>172.26.215.93:582346</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0111984  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.87543 </td><td style=\"text-align: right;\">203.845</td><td style=\"text-align: right;\">109.356 </td><td style=\"text-align: right;\">4.19815e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_d26b524c</td><td>TERMINATED</td><td>172.26.215.93:582566</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0119297  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.57122 </td><td style=\"text-align: right;\">290.556</td><td style=\"text-align: right;\">161.152 </td><td style=\"text-align: right;\">6.09749e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e4b7bac0</td><td>TERMINATED</td><td>172.26.215.93:582783</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0233621  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.92244 </td><td style=\"text-align: right;\">304.34 </td><td style=\"text-align: right;\">174.762 </td><td style=\"text-align: right;\">5.34859e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_439eadae</td><td>TERMINATED</td><td>172.26.215.93:583007</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.023463   </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        4.75452 </td><td style=\"text-align: right;\">202.535</td><td style=\"text-align: right;\">108.01  </td><td style=\"text-align: right;\">4.30778e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f3a1c266</td><td>TERMINATED</td><td>172.26.215.93:583266</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00414655 </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.06651 </td><td style=\"text-align: right;\">377.764</td><td style=\"text-align: right;\">233.9   </td><td style=\"text-align: right;\">1.77532e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_78b2c5a7</td><td>TERMINATED</td><td>172.26.215.93:583497</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0147155  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.4464  </td><td style=\"text-align: right;\">222.378</td><td style=\"text-align: right;\">121.444 </td><td style=\"text-align: right;\">5.07927e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_9416fb3d</td><td>TERMINATED</td><td>172.26.215.93:583723</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0141808  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        8.7429  </td><td style=\"text-align: right;\">193.739</td><td style=\"text-align: right;\">103.735 </td><td style=\"text-align: right;\">3.87691e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_69bb004d</td><td>TERMINATED</td><td>172.26.215.93:583960</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0178812  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       54.0916  </td><td style=\"text-align: right;\">193.231</td><td style=\"text-align: right;\">100.672 </td><td style=\"text-align: right;\">3.10853e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_769e78c7</td><td>TERMINATED</td><td>172.26.215.93:584189</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0185459  </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.91037 </td><td style=\"text-align: right;\">215.951</td><td style=\"text-align: right;\">119.466 </td><td style=\"text-align: right;\">6.14987e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_37f4b8f9</td><td>TERMINATED</td><td>172.26.215.93:584278</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.019626   </td><td>sklearn.preproc_03f0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        5.5123  </td><td style=\"text-align: right;\">198.984</td><td style=\"text-align: right;\">105.08  </td><td style=\"text-align: right;\">4.67594e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_10d553c3</td><td>TERMINATED</td><td>172.26.215.93:584592</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0411394  </td><td>sklearn.preproc_0330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       45.3697  </td><td style=\"text-align: right;\">188.688</td><td style=\"text-align: right;\">106.066 </td><td style=\"text-align: right;\">1.1448e+17 </td></tr>\n",
       "<tr><td>FSR_Trainable_ac03a684</td><td>TERMINATED</td><td>172.26.215.93:584825</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_07b0</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0392863  </td><td>sklearn.preproc_0330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.23497 </td><td style=\"text-align: right;\">269.331</td><td style=\"text-align: right;\">180.755 </td><td style=\"text-align: right;\">3.45915e+17</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 11:36:28,568\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">       mape</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_07549cff</td><td>2023-07-19_11-51-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">105.439 </td><td style=\"text-align: right;\">2.95234e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">567419</td><td style=\"text-align: right;\">206.378</td><td style=\"text-align: right;\">          136.751   </td><td style=\"text-align: right;\">          1.50546 </td><td style=\"text-align: right;\">    136.751   </td><td style=\"text-align: right;\"> 1689735117</td><td style=\"text-align: right;\">                 100</td><td>07549cff  </td></tr>\n",
       "<tr><td>FSR_Trainable_0b7bdef0</td><td>2023-07-19_12-03-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">230.739 </td><td style=\"text-align: right;\">1.56708e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">578211</td><td style=\"text-align: right;\">381.256</td><td style=\"text-align: right;\">            2.73836 </td><td style=\"text-align: right;\">          2.73836 </td><td style=\"text-align: right;\">      2.73836 </td><td style=\"text-align: right;\"> 1689735781</td><td style=\"text-align: right;\">                   1</td><td>0b7bdef0  </td></tr>\n",
       "<tr><td>FSR_Trainable_0d84e070</td><td>2023-07-19_12-07-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 99.2406</td><td style=\"text-align: right;\">2.64201e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">582116</td><td style=\"text-align: right;\">193.19 </td><td style=\"text-align: right;\">           61.3308  </td><td style=\"text-align: right;\">          0.536478</td><td style=\"text-align: right;\">     61.3308  </td><td style=\"text-align: right;\"> 1689736049</td><td style=\"text-align: right;\">                 100</td><td>0d84e070  </td></tr>\n",
       "<tr><td>FSR_Trainable_0e6d32df</td><td>2023-07-19_11-52-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">193.475 </td><td style=\"text-align: right;\">5.76929e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">569366</td><td style=\"text-align: right;\">332.793</td><td style=\"text-align: right;\">            1.70178 </td><td style=\"text-align: right;\">          1.70178 </td><td style=\"text-align: right;\">      1.70178 </td><td style=\"text-align: right;\"> 1689735177</td><td style=\"text-align: right;\">                   1</td><td>0e6d32df  </td></tr>\n",
       "<tr><td>FSR_Trainable_0f299666</td><td>2023-07-19_11-44-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">133.836 </td><td style=\"text-align: right;\">2.41661e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">564223</td><td style=\"text-align: right;\">250.928</td><td style=\"text-align: right;\">          106.196   </td><td style=\"text-align: right;\">          0.991985</td><td style=\"text-align: right;\">    106.196   </td><td style=\"text-align: right;\"> 1689734646</td><td style=\"text-align: right;\">                 100</td><td>0f299666  </td></tr>\n",
       "<tr><td>FSR_Trainable_10d553c3</td><td>2023-07-19_12-09-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">106.066 </td><td style=\"text-align: right;\">1.1448e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">584592</td><td style=\"text-align: right;\">188.688</td><td style=\"text-align: right;\">           45.3697  </td><td style=\"text-align: right;\">          0.369972</td><td style=\"text-align: right;\">     45.3697  </td><td style=\"text-align: right;\"> 1689736143</td><td style=\"text-align: right;\">                 100</td><td>10d553c3  </td></tr>\n",
       "<tr><td>FSR_Trainable_12caccc0</td><td>2023-07-19_11-55-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">131.178 </td><td style=\"text-align: right;\">7.83256e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">571041</td><td style=\"text-align: right;\">221.505</td><td style=\"text-align: right;\">            1.77442 </td><td style=\"text-align: right;\">          0.900398</td><td style=\"text-align: right;\">      1.77442 </td><td style=\"text-align: right;\"> 1689735340</td><td style=\"text-align: right;\">                   2</td><td>12caccc0  </td></tr>\n",
       "<tr><td>FSR_Trainable_1957c775</td><td>2023-07-19_11-50-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">101.07  </td><td style=\"text-align: right;\">2.79805e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">566743</td><td style=\"text-align: right;\">195.164</td><td style=\"text-align: right;\">           67.8572  </td><td style=\"text-align: right;\">          0.652041</td><td style=\"text-align: right;\">     67.8572  </td><td style=\"text-align: right;\"> 1689735003</td><td style=\"text-align: right;\">                 100</td><td>1957c775  </td></tr>\n",
       "<tr><td>FSR_Trainable_1acabe1b</td><td>2023-07-19_11-47-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">160.304 </td><td style=\"text-align: right;\">8.62857e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">565190</td><td style=\"text-align: right;\">292.503</td><td style=\"text-align: right;\">          155.002   </td><td style=\"text-align: right;\">          1.73536 </td><td style=\"text-align: right;\">    155.002   </td><td style=\"text-align: right;\"> 1689734821</td><td style=\"text-align: right;\">                 100</td><td>1acabe1b  </td></tr>\n",
       "<tr><td>FSR_Trainable_1acc5a57</td><td>2023-07-19_12-00-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">225.349 </td><td style=\"text-align: right;\">4.25041e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">575859</td><td style=\"text-align: right;\">379.768</td><td style=\"text-align: right;\">           13.4941  </td><td style=\"text-align: right;\">         13.4941  </td><td style=\"text-align: right;\">     13.4941  </td><td style=\"text-align: right;\"> 1689735656</td><td style=\"text-align: right;\">                   1</td><td>1acc5a57  </td></tr>\n",
       "<tr><td>FSR_Trainable_1af8c14e</td><td>2023-07-19_12-00-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">142.498 </td><td style=\"text-align: right;\">1.98824e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">575228</td><td style=\"text-align: right;\">254.3  </td><td style=\"text-align: right;\">            2.28131 </td><td style=\"text-align: right;\">          2.28131 </td><td style=\"text-align: right;\">      2.28131 </td><td style=\"text-align: right;\"> 1689735612</td><td style=\"text-align: right;\">                   1</td><td>1af8c14e  </td></tr>\n",
       "<tr><td>FSR_Trainable_1b6cfdf3</td><td>2023-07-19_11-56-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">116.72  </td><td style=\"text-align: right;\">4.48802e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">572190</td><td style=\"text-align: right;\">211.226</td><td style=\"text-align: right;\">            3.45511 </td><td style=\"text-align: right;\">          0.807832</td><td style=\"text-align: right;\">      3.45511 </td><td style=\"text-align: right;\"> 1689735413</td><td style=\"text-align: right;\">                   4</td><td>1b6cfdf3  </td></tr>\n",
       "<tr><td>FSR_Trainable_1f2a53fe</td><td>2023-07-19_11-55-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">113.217 </td><td style=\"text-align: right;\">4.13507e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">570825</td><td style=\"text-align: right;\">207.154</td><td style=\"text-align: right;\">            3.656   </td><td style=\"text-align: right;\">          0.812092</td><td style=\"text-align: right;\">      3.656   </td><td style=\"text-align: right;\"> 1689735329</td><td style=\"text-align: right;\">                   4</td><td>1f2a53fe  </td></tr>\n",
       "<tr><td>FSR_Trainable_21ca597b</td><td>2023-07-19_11-57-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">122.084 </td><td style=\"text-align: right;\">5.07295e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">572418</td><td style=\"text-align: right;\">227.281</td><td style=\"text-align: right;\">            1.85316 </td><td style=\"text-align: right;\">          0.840187</td><td style=\"text-align: right;\">      1.85316 </td><td style=\"text-align: right;\"> 1689735424</td><td style=\"text-align: right;\">                   2</td><td>21ca597b  </td></tr>\n",
       "<tr><td>FSR_Trainable_26e3e47a</td><td>2023-07-19_12-02-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 99.4837</td><td style=\"text-align: right;\">2.68803e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">577024</td><td style=\"text-align: right;\">189.723</td><td style=\"text-align: right;\">           67.5037  </td><td style=\"text-align: right;\">          0.517962</td><td style=\"text-align: right;\">     67.5037  </td><td style=\"text-align: right;\"> 1689735775</td><td style=\"text-align: right;\">                 100</td><td>26e3e47a  </td></tr>\n",
       "<tr><td>FSR_Trainable_29617618</td><td>2023-07-19_11-48-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">102.219 </td><td style=\"text-align: right;\">3.27968e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">566427</td><td style=\"text-align: right;\">199.008</td><td style=\"text-align: right;\">           64.5837  </td><td style=\"text-align: right;\">          0.622448</td><td style=\"text-align: right;\">     64.5837  </td><td style=\"text-align: right;\"> 1689734937</td><td style=\"text-align: right;\">                 100</td><td>29617618  </td></tr>\n",
       "<tr><td>FSR_Trainable_2a2f496d</td><td>2023-07-19_11-41-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">180.029 </td><td style=\"text-align: right;\">4.25533e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">563953</td><td style=\"text-align: right;\">342.465</td><td style=\"text-align: right;\">           13.485   </td><td style=\"text-align: right;\">          1.59722 </td><td style=\"text-align: right;\">     13.485   </td><td style=\"text-align: right;\"> 1689734517</td><td style=\"text-align: right;\">                   8</td><td>2a2f496d  </td></tr>\n",
       "<tr><td>FSR_Trainable_2a4efb97</td><td>2023-07-19_11-58-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">125.035 </td><td style=\"text-align: right;\">1.88964e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">573990</td><td style=\"text-align: right;\">251.728</td><td style=\"text-align: right;\">            3.86082 </td><td style=\"text-align: right;\">          1.8673  </td><td style=\"text-align: right;\">      3.86082 </td><td style=\"text-align: right;\"> 1689735503</td><td style=\"text-align: right;\">                   2</td><td>2a4efb97  </td></tr>\n",
       "<tr><td>FSR_Trainable_2ed487a6</td><td>2023-07-19_12-00-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">200.143 </td><td style=\"text-align: right;\">2.83211e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">575429</td><td style=\"text-align: right;\">381.844</td><td style=\"text-align: right;\">           13.6808  </td><td style=\"text-align: right;\">         13.6808  </td><td style=\"text-align: right;\">     13.6808  </td><td style=\"text-align: right;\"> 1689735633</td><td style=\"text-align: right;\">                   1</td><td>2ed487a6  </td></tr>\n",
       "<tr><td>FSR_Trainable_2f10f206</td><td>2023-07-19_11-58-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">156.165 </td><td style=\"text-align: right;\">3.38366e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">574208</td><td style=\"text-align: right;\">308.995</td><td style=\"text-align: right;\">            2.31052 </td><td style=\"text-align: right;\">          2.31052 </td><td style=\"text-align: right;\">      2.31052 </td><td style=\"text-align: right;\"> 1689735511</td><td style=\"text-align: right;\">                   1</td><td>2f10f206  </td></tr>\n",
       "<tr><td>FSR_Trainable_2f623f46</td><td>2023-07-19_11-56-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">261.695 </td><td style=\"text-align: right;\">4.0885e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">571938</td><td style=\"text-align: right;\">411.745</td><td style=\"text-align: right;\">            1.58888 </td><td style=\"text-align: right;\">          1.58888 </td><td style=\"text-align: right;\">      1.58888 </td><td style=\"text-align: right;\"> 1689735395</td><td style=\"text-align: right;\">                   1</td><td>2f623f46  </td></tr>\n",
       "<tr><td>FSR_Trainable_37f4b8f9</td><td>2023-07-19_12-08-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">105.08  </td><td style=\"text-align: right;\">4.67594e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">584278</td><td style=\"text-align: right;\">198.984</td><td style=\"text-align: right;\">            5.5123  </td><td style=\"text-align: right;\">          0.616123</td><td style=\"text-align: right;\">      5.5123  </td><td style=\"text-align: right;\"> 1689736088</td><td style=\"text-align: right;\">                   8</td><td>37f4b8f9  </td></tr>\n",
       "<tr><td>FSR_Trainable_426f126e</td><td>2023-07-19_11-51-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 99.4801</td><td style=\"text-align: right;\">2.80729e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">567712</td><td style=\"text-align: right;\">197.163</td><td style=\"text-align: right;\">           65.9079  </td><td style=\"text-align: right;\">          0.590255</td><td style=\"text-align: right;\">     65.9079  </td><td style=\"text-align: right;\"> 1689735088</td><td style=\"text-align: right;\">                 100</td><td>426f126e  </td></tr>\n",
       "<tr><td>FSR_Trainable_439eadae</td><td>2023-07-19_12-07-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">108.01  </td><td style=\"text-align: right;\">4.30778e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">583007</td><td style=\"text-align: right;\">202.535</td><td style=\"text-align: right;\">            4.75452 </td><td style=\"text-align: right;\">          0.952281</td><td style=\"text-align: right;\">      4.75452 </td><td style=\"text-align: right;\"> 1689736024</td><td style=\"text-align: right;\">                   4</td><td>439eadae  </td></tr>\n",
       "<tr><td>FSR_Trainable_43a1d0fd</td><td>2023-07-19_12-03-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">172.273 </td><td style=\"text-align: right;\">1.79295e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">578422</td><td style=\"text-align: right;\">351.467</td><td style=\"text-align: right;\">            2.9324  </td><td style=\"text-align: right;\">          2.9324  </td><td style=\"text-align: right;\">      2.9324  </td><td style=\"text-align: right;\"> 1689735791</td><td style=\"text-align: right;\">                   1</td><td>43a1d0fd  </td></tr>\n",
       "<tr><td>FSR_Trainable_449c1ee7</td><td>2023-07-19_12-00-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 98.3475</td><td style=\"text-align: right;\">2.00671e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">573788</td><td style=\"text-align: right;\">205.551</td><td style=\"text-align: right;\">          102.718   </td><td style=\"text-align: right;\">          1.42339 </td><td style=\"text-align: right;\">    102.718   </td><td style=\"text-align: right;\"> 1689735603</td><td style=\"text-align: right;\">                  64</td><td>449c1ee7  </td></tr>\n",
       "<tr><td>FSR_Trainable_457c8add</td><td>2023-07-19_12-02-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">235.523 </td><td style=\"text-align: right;\">1.5129e+08 </td><td>172.26.215.93</td><td style=\"text-align: right;\">577970</td><td style=\"text-align: right;\">380.829</td><td style=\"text-align: right;\">            1.40007 </td><td style=\"text-align: right;\">          1.40007 </td><td style=\"text-align: right;\">      1.40007 </td><td style=\"text-align: right;\"> 1689735767</td><td style=\"text-align: right;\">                   1</td><td>457c8add  </td></tr>\n",
       "<tr><td>FSR_Trainable_48a57263</td><td>2023-07-19_12-00-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">104.105 </td><td style=\"text-align: right;\">2.82163e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">574964</td><td style=\"text-align: right;\">202.798</td><td style=\"text-align: right;\">           63.973   </td><td style=\"text-align: right;\">          0.67195 </td><td style=\"text-align: right;\">     63.973   </td><td style=\"text-align: right;\"> 1689735651</td><td style=\"text-align: right;\">                 100</td><td>48a57263  </td></tr>\n",
       "<tr><td>FSR_Trainable_48b95fcb</td><td>2023-07-19_11-45-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 99.4635</td><td style=\"text-align: right;\">2.25678e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">564901</td><td style=\"text-align: right;\">204.054</td><td style=\"text-align: right;\">          105.039   </td><td style=\"text-align: right;\">          1.13997 </td><td style=\"text-align: right;\">    105.039   </td><td style=\"text-align: right;\"> 1689734721</td><td style=\"text-align: right;\">                 100</td><td>48b95fcb  </td></tr>\n",
       "<tr><td>FSR_Trainable_4ad2657e</td><td>2023-07-19_11-41-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">330.747 </td><td style=\"text-align: right;\">6.94809e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">563727</td><td style=\"text-align: right;\">450.468</td><td style=\"text-align: right;\">            2.53678 </td><td style=\"text-align: right;\">          2.53678 </td><td style=\"text-align: right;\">      2.53678 </td><td style=\"text-align: right;\"> 1689734487</td><td style=\"text-align: right;\">                   1</td><td>4ad2657e  </td></tr>\n",
       "<tr><td>FSR_Trainable_5043e67a</td><td>2023-07-19_12-05-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">151.703 </td><td style=\"text-align: right;\">8.24247e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">581678</td><td style=\"text-align: right;\">258.359</td><td style=\"text-align: right;\">            0.660958</td><td style=\"text-align: right;\">          0.660958</td><td style=\"text-align: right;\">      0.660958</td><td style=\"text-align: right;\"> 1689735950</td><td style=\"text-align: right;\">                   1</td><td>5043e67a  </td></tr>\n",
       "<tr><td>FSR_Trainable_52b3b738</td><td>2023-07-19_11-50-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 96.75  </td><td style=\"text-align: right;\">2.50216e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">566942</td><td style=\"text-align: right;\">193.149</td><td style=\"text-align: right;\">           67.8855  </td><td style=\"text-align: right;\">          0.601923</td><td style=\"text-align: right;\">     67.8855  </td><td style=\"text-align: right;\"> 1689735014</td><td style=\"text-align: right;\">                 100</td><td>52b3b738  </td></tr>\n",
       "<tr><td>FSR_Trainable_539f358c</td><td>2023-07-19_11-56-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">222.083 </td><td style=\"text-align: right;\">1.35474e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">571737</td><td style=\"text-align: right;\">349.815</td><td style=\"text-align: right;\">            5.2251  </td><td style=\"text-align: right;\">          5.2251  </td><td style=\"text-align: right;\">      5.2251  </td><td style=\"text-align: right;\"> 1689735386</td><td style=\"text-align: right;\">                   1</td><td>539f358c  </td></tr>\n",
       "<tr><td>FSR_Trainable_53e69005</td><td>2023-07-19_11-55-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 97.658 </td><td style=\"text-align: right;\">2.29628e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">570061</td><td style=\"text-align: right;\">195.915</td><td style=\"text-align: right;\">          101.886   </td><td style=\"text-align: right;\">          0.99856 </td><td style=\"text-align: right;\">    101.886   </td><td style=\"text-align: right;\"> 1689735320</td><td style=\"text-align: right;\">                 100</td><td>53e69005  </td></tr>\n",
       "<tr><td>FSR_Trainable_545478ec</td><td>2023-07-19_11-52-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">111.813 </td><td style=\"text-align: right;\">4.69687e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">568883</td><td style=\"text-align: right;\">207.799</td><td style=\"text-align: right;\">            6.35684 </td><td style=\"text-align: right;\">          0.795212</td><td style=\"text-align: right;\">      6.35684 </td><td style=\"text-align: right;\"> 1689735151</td><td style=\"text-align: right;\">                   8</td><td>545478ec  </td></tr>\n",
       "<tr><td>FSR_Trainable_553408e6</td><td>2023-07-19_12-03-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">132.604 </td><td style=\"text-align: right;\">2.97767e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">578654</td><td style=\"text-align: right;\">248.617</td><td style=\"text-align: right;\">            1.84599 </td><td style=\"text-align: right;\">          0.857708</td><td style=\"text-align: right;\">      1.84599 </td><td style=\"text-align: right;\"> 1689735801</td><td style=\"text-align: right;\">                   2</td><td>553408e6  </td></tr>\n",
       "<tr><td>FSR_Trainable_59cddc2b</td><td>2023-07-19_11-42-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">145.739 </td><td style=\"text-align: right;\">4.18275e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">561819</td><td style=\"text-align: right;\">271.485</td><td style=\"text-align: right;\">          327.326   </td><td style=\"text-align: right;\">          3.39915 </td><td style=\"text-align: right;\">    327.326   </td><td style=\"text-align: right;\"> 1689734540</td><td style=\"text-align: right;\">                 100</td><td>59cddc2b  </td></tr>\n",
       "<tr><td>FSR_Trainable_5a6b36a8</td><td>2023-07-19_12-04-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">105.398 </td><td style=\"text-align: right;\">3.64715e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">580035</td><td style=\"text-align: right;\">194.687</td><td style=\"text-align: right;\">           11.4506  </td><td style=\"text-align: right;\">          0.643749</td><td style=\"text-align: right;\">     11.4506  </td><td style=\"text-align: right;\"> 1689735880</td><td style=\"text-align: right;\">                  16</td><td>5a6b36a8  </td></tr>\n",
       "<tr><td>FSR_Trainable_5cb6c374</td><td>2023-07-19_11-48-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">114.757 </td><td style=\"text-align: right;\">1.7527e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">565748</td><td style=\"text-align: right;\">230.361</td><td style=\"text-align: right;\">          100.614   </td><td style=\"text-align: right;\">          0.990676</td><td style=\"text-align: right;\">    100.614   </td><td style=\"text-align: right;\"> 1689734913</td><td style=\"text-align: right;\">                 100</td><td>5cb6c374  </td></tr>\n",
       "<tr><td>FSR_Trainable_5ccec0ba</td><td>2023-07-19_12-02-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">102.73  </td><td style=\"text-align: right;\">2.92568e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">576549</td><td style=\"text-align: right;\">193.105</td><td style=\"text-align: right;\">           65.5574  </td><td style=\"text-align: right;\">          0.68948 </td><td style=\"text-align: right;\">     65.5574  </td><td style=\"text-align: right;\"> 1689735750</td><td style=\"text-align: right;\">                 100</td><td>5ccec0ba  </td></tr>\n",
       "<tr><td>FSR_Trainable_60033bfd</td><td>2023-07-19_12-01-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">261.579 </td><td style=\"text-align: right;\">5.69257e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">576320</td><td style=\"text-align: right;\">393.934</td><td style=\"text-align: right;\">            3.42166 </td><td style=\"text-align: right;\">          3.42166 </td><td style=\"text-align: right;\">      3.42166 </td><td style=\"text-align: right;\"> 1689735667</td><td style=\"text-align: right;\">                   1</td><td>60033bfd  </td></tr>\n",
       "<tr><td>FSR_Trainable_6399e6a9</td><td>2023-07-19_12-03-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">173.773 </td><td style=\"text-align: right;\">5.7268e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">578902</td><td style=\"text-align: right;\">308.396</td><td style=\"text-align: right;\">            0.781013</td><td style=\"text-align: right;\">          0.781013</td><td style=\"text-align: right;\">      0.781013</td><td style=\"text-align: right;\"> 1689735809</td><td style=\"text-align: right;\">                   1</td><td>6399e6a9  </td></tr>\n",
       "<tr><td>FSR_Trainable_64cfa2c5</td><td>2023-07-19_11-57-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">224.826 </td><td style=\"text-align: right;\">3.9413e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">572889</td><td style=\"text-align: right;\">396.525</td><td style=\"text-align: right;\">            1.92954 </td><td style=\"text-align: right;\">          1.92954 </td><td style=\"text-align: right;\">      1.92954 </td><td style=\"text-align: right;\"> 1689735446</td><td style=\"text-align: right;\">                   1</td><td>64cfa2c5  </td></tr>\n",
       "<tr><td>FSR_Trainable_69bb004d</td><td>2023-07-19_12-08-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">100.672 </td><td style=\"text-align: right;\">3.10853e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">583960</td><td style=\"text-align: right;\">193.231</td><td style=\"text-align: right;\">           54.0916  </td><td style=\"text-align: right;\">          0.504614</td><td style=\"text-align: right;\">     54.0916  </td><td style=\"text-align: right;\"> 1689736127</td><td style=\"text-align: right;\">                 100</td><td>69bb004d  </td></tr>\n",
       "<tr><td>FSR_Trainable_6a3b5f4a</td><td>2023-07-19_12-01-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">238.767 </td><td style=\"text-align: right;\">4.82693e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">576100</td><td style=\"text-align: right;\">389.733</td><td style=\"text-align: right;\">           14.9242  </td><td style=\"text-align: right;\">         14.9242  </td><td style=\"text-align: right;\">     14.9242  </td><td style=\"text-align: right;\"> 1689735668</td><td style=\"text-align: right;\">                   1</td><td>6a3b5f4a  </td></tr>\n",
       "<tr><td>FSR_Trainable_6fad082a</td><td>2023-07-19_11-56-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">107.384 </td><td style=\"text-align: right;\">3.51123e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">571268</td><td style=\"text-align: right;\">205.962</td><td style=\"text-align: right;\">           17.7394  </td><td style=\"text-align: right;\">          2.18113 </td><td style=\"text-align: right;\">     17.7394  </td><td style=\"text-align: right;\"> 1689735369</td><td style=\"text-align: right;\">                   8</td><td>6fad082a  </td></tr>\n",
       "<tr><td>FSR_Trainable_70fc4d25</td><td>2023-07-19_12-06-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">109.356 </td><td style=\"text-align: right;\">4.19815e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">582346</td><td style=\"text-align: right;\">203.845</td><td style=\"text-align: right;\">            3.87543 </td><td style=\"text-align: right;\">          0.715822</td><td style=\"text-align: right;\">      3.87543 </td><td style=\"text-align: right;\"> 1689735988</td><td style=\"text-align: right;\">                   4</td><td>70fc4d25  </td></tr>\n",
       "<tr><td>FSR_Trainable_741520de</td><td>2023-07-19_12-05-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">103.004 </td><td style=\"text-align: right;\">2.84806e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">579818</td><td style=\"text-align: right;\">202.891</td><td style=\"text-align: right;\">           64.8726  </td><td style=\"text-align: right;\">          0.692668</td><td style=\"text-align: right;\">     64.8726  </td><td style=\"text-align: right;\"> 1689735934</td><td style=\"text-align: right;\">                 100</td><td>741520de  </td></tr>\n",
       "<tr><td>FSR_Trainable_7623c5cc</td><td>2023-07-19_11-46-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">150.591 </td><td style=\"text-align: right;\">2.20392e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">561993</td><td style=\"text-align: right;\">266.701</td><td style=\"text-align: right;\">          572.246   </td><td style=\"text-align: right;\">          5.95439 </td><td style=\"text-align: right;\">    572.246   </td><td style=\"text-align: right;\"> 1689734791</td><td style=\"text-align: right;\">                 100</td><td>7623c5cc  </td></tr>\n",
       "<tr><td>FSR_Trainable_769e78c7</td><td>2023-07-19_12-07-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">119.466 </td><td style=\"text-align: right;\">6.14987e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">584189</td><td style=\"text-align: right;\">215.951</td><td style=\"text-align: right;\">            1.91037 </td><td style=\"text-align: right;\">          0.932136</td><td style=\"text-align: right;\">      1.91037 </td><td style=\"text-align: right;\"> 1689736076</td><td style=\"text-align: right;\">                   2</td><td>769e78c7  </td></tr>\n",
       "<tr><td>FSR_Trainable_77225a5e</td><td>2023-07-19_12-05-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">107.056 </td><td style=\"text-align: right;\">4.66814e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">581211</td><td style=\"text-align: right;\">198.812</td><td style=\"text-align: right;\">            5.02499 </td><td style=\"text-align: right;\">          0.516517</td><td style=\"text-align: right;\">      5.02499 </td><td style=\"text-align: right;\"> 1689735937</td><td style=\"text-align: right;\">                   8</td><td>77225a5e  </td></tr>\n",
       "<tr><td>FSR_Trainable_78b2c5a7</td><td>2023-07-19_12-07-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">121.444 </td><td style=\"text-align: right;\">5.07927e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">583497</td><td style=\"text-align: right;\">222.378</td><td style=\"text-align: right;\">            1.4464  </td><td style=\"text-align: right;\">          0.759586</td><td style=\"text-align: right;\">      1.4464  </td><td style=\"text-align: right;\"> 1689736044</td><td style=\"text-align: right;\">                   2</td><td>78b2c5a7  </td></tr>\n",
       "<tr><td>FSR_Trainable_7a6d0a42</td><td>2023-07-19_12-05-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">106.921 </td><td style=\"text-align: right;\">4.15804e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">580497</td><td style=\"text-align: right;\">194.721</td><td style=\"text-align: right;\">           11.2824  </td><td style=\"text-align: right;\">          0.671026</td><td style=\"text-align: right;\">     11.2824  </td><td style=\"text-align: right;\"> 1689735906</td><td style=\"text-align: right;\">                  16</td><td>7a6d0a42  </td></tr>\n",
       "<tr><td>FSR_Trainable_7bf4a9dc</td><td>2023-07-19_11-55-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">104.377 </td><td style=\"text-align: right;\">2.90635e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">569837</td><td style=\"text-align: right;\">203.647</td><td style=\"text-align: right;\">          102.234   </td><td style=\"text-align: right;\">          1.12689 </td><td style=\"text-align: right;\">    102.234   </td><td style=\"text-align: right;\"> 1689735310</td><td style=\"text-align: right;\">                 100</td><td>7bf4a9dc  </td></tr>\n",
       "<tr><td>FSR_Trainable_7c4e4b2a</td><td>2023-07-19_11-40-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">258.255 </td><td style=\"text-align: right;\">5.8531e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">563036</td><td style=\"text-align: right;\">458.901</td><td style=\"text-align: right;\">            2.56874 </td><td style=\"text-align: right;\">          2.56874 </td><td style=\"text-align: right;\">      2.56874 </td><td style=\"text-align: right;\"> 1689734422</td><td style=\"text-align: right;\">                   1</td><td>7c4e4b2a  </td></tr>\n",
       "<tr><td>FSR_Trainable_826407ff</td><td>2023-07-19_12-04-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">104.655 </td><td style=\"text-align: right;\">3.23541e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">579593</td><td style=\"text-align: right;\">195.418</td><td style=\"text-align: right;\">           21.0648  </td><td style=\"text-align: right;\">          0.81211 </td><td style=\"text-align: right;\">     21.0648  </td><td style=\"text-align: right;\"> 1689735866</td><td style=\"text-align: right;\">                  32</td><td>826407ff  </td></tr>\n",
       "<tr><td>FSR_Trainable_83b47f12</td><td>2023-07-19_11-53-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">192.573 </td><td style=\"text-align: right;\">5.87047e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">569594</td><td style=\"text-align: right;\">347.025</td><td style=\"text-align: right;\">            1.59686 </td><td style=\"text-align: right;\">          1.59686 </td><td style=\"text-align: right;\">      1.59686 </td><td style=\"text-align: right;\"> 1689735188</td><td style=\"text-align: right;\">                   1</td><td>83b47f12  </td></tr>\n",
       "<tr><td>FSR_Trainable_86dbc305</td><td>2023-07-19_11-52-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">105.333 </td><td style=\"text-align: right;\">3.96385e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">568671</td><td style=\"text-align: right;\">205.704</td><td style=\"text-align: right;\">           19.42    </td><td style=\"text-align: right;\">          2.29006 </td><td style=\"text-align: right;\">     19.42    </td><td style=\"text-align: right;\"> 1689735153</td><td style=\"text-align: right;\">                   8</td><td>86dbc305  </td></tr>\n",
       "<tr><td>FSR_Trainable_8873a54e</td><td>2023-07-19_11-57-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">257.82  </td><td style=\"text-align: right;\">1.30664e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">573313</td><td style=\"text-align: right;\">406.613</td><td style=\"text-align: right;\">            1.27176 </td><td style=\"text-align: right;\">          1.27176 </td><td style=\"text-align: right;\">      1.27176 </td><td style=\"text-align: right;\"> 1689735465</td><td style=\"text-align: right;\">                   1</td><td>8873a54e  </td></tr>\n",
       "<tr><td>FSR_Trainable_8ac6cd16</td><td>2023-07-19_11-47-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 99.9243</td><td style=\"text-align: right;\">2.38344e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">565468</td><td style=\"text-align: right;\">199.278</td><td style=\"text-align: right;\">          104.811   </td><td style=\"text-align: right;\">          1.06402 </td><td style=\"text-align: right;\">    104.811   </td><td style=\"text-align: right;\"> 1689734846</td><td style=\"text-align: right;\">                 100</td><td>8ac6cd16  </td></tr>\n",
       "<tr><td>FSR_Trainable_8fa2c49f</td><td>2023-07-19_12-00-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">219.928 </td><td style=\"text-align: right;\">3.99222e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">575650</td><td style=\"text-align: right;\">367.159</td><td style=\"text-align: right;\">            9.91708 </td><td style=\"text-align: right;\">          9.91708 </td><td style=\"text-align: right;\">      9.91708 </td><td style=\"text-align: right;\"> 1689735640</td><td style=\"text-align: right;\">                   1</td><td>8fa2c49f  </td></tr>\n",
       "<tr><td>FSR_Trainable_9416fb3d</td><td>2023-07-19_12-07-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">103.735 </td><td style=\"text-align: right;\">3.87691e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">583723</td><td style=\"text-align: right;\">193.739</td><td style=\"text-align: right;\">            8.7429  </td><td style=\"text-align: right;\">          0.614838</td><td style=\"text-align: right;\">      8.7429  </td><td style=\"text-align: right;\"> 1689736061</td><td style=\"text-align: right;\">                  16</td><td>9416fb3d  </td></tr>\n",
       "<tr><td>FSR_Trainable_9d2e5769</td><td>2023-07-19_11-59-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 97.8343</td><td style=\"text-align: right;\">6.95811e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">562514</td><td style=\"text-align: right;\">194.499</td><td style=\"text-align: right;\">         1308.08    </td><td style=\"text-align: right;\">         13.2169  </td><td style=\"text-align: right;\">   1308.08    </td><td style=\"text-align: right;\"> 1689735565</td><td style=\"text-align: right;\">                 100</td><td>9d2e5769  </td></tr>\n",
       "<tr><td>FSR_Trainable_9ff2fafb</td><td>2023-07-19_11-57-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 99.618 </td><td style=\"text-align: right;\">2.46262e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">570601</td><td style=\"text-align: right;\">198.134</td><td style=\"text-align: right;\">          107.08    </td><td style=\"text-align: right;\">          0.81289 </td><td style=\"text-align: right;\">    107.08    </td><td style=\"text-align: right;\"> 1689735432</td><td style=\"text-align: right;\">                 100</td><td>9ff2fafb  </td></tr>\n",
       "<tr><td>FSR_Trainable_a1543e18</td><td>2023-07-19_12-05-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">118.887 </td><td style=\"text-align: right;\">5.74591e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">580964</td><td style=\"text-align: right;\">217.354</td><td style=\"text-align: right;\">            2.51264 </td><td style=\"text-align: right;\">          1.24948 </td><td style=\"text-align: right;\">      2.51264 </td><td style=\"text-align: right;\"> 1689735923</td><td style=\"text-align: right;\">                   2</td><td>a1543e18  </td></tr>\n",
       "<tr><td>FSR_Trainable_a17695c7</td><td>2023-07-19_11-57-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">223.419 </td><td style=\"text-align: right;\">2.37929e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">573099</td><td style=\"text-align: right;\">432.585</td><td style=\"text-align: right;\">            1.80991 </td><td style=\"text-align: right;\">          1.80991 </td><td style=\"text-align: right;\">      1.80991 </td><td style=\"text-align: right;\"> 1689735456</td><td style=\"text-align: right;\">                   1</td><td>a17695c7  </td></tr>\n",
       "<tr><td>FSR_Trainable_a2d1aeb7</td><td>2023-07-19_11-43-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">245.981 </td><td style=\"text-align: right;\">3.21758e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">564704</td><td style=\"text-align: right;\">424.793</td><td style=\"text-align: right;\">            1.20365 </td><td style=\"text-align: right;\">          1.20365 </td><td style=\"text-align: right;\">      1.20365 </td><td style=\"text-align: right;\"> 1689734595</td><td style=\"text-align: right;\">                   1</td><td>a2d1aeb7  </td></tr>\n",
       "<tr><td>FSR_Trainable_a2f42b1f</td><td>2023-07-19_11-48-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">115.772 </td><td style=\"text-align: right;\">2.18896e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">566224</td><td style=\"text-align: right;\">215.841</td><td style=\"text-align: right;\">           66.791   </td><td style=\"text-align: right;\">          0.683162</td><td style=\"text-align: right;\">     66.791   </td><td style=\"text-align: right;\"> 1689734925</td><td style=\"text-align: right;\">                 100</td><td>a2f42b1f  </td></tr>\n",
       "<tr><td>FSR_Trainable_a9c0ec65</td><td>2023-07-19_11-57-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">114.076 </td><td style=\"text-align: right;\">4.30756e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">572649</td><td style=\"text-align: right;\">209.123</td><td style=\"text-align: right;\">            3.08912 </td><td style=\"text-align: right;\">          0.64406 </td><td style=\"text-align: right;\">      3.08912 </td><td style=\"text-align: right;\"> 1689735438</td><td style=\"text-align: right;\">                   4</td><td>a9c0ec65  </td></tr>\n",
       "<tr><td>FSR_Trainable_a9e45cc6</td><td>2023-07-19_11-52-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">114.873 </td><td style=\"text-align: right;\">4.10034e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">569168</td><td style=\"text-align: right;\">214.259</td><td style=\"text-align: right;\">            6.19803 </td><td style=\"text-align: right;\">          0.757753</td><td style=\"text-align: right;\">      6.19803 </td><td style=\"text-align: right;\"> 1689735170</td><td style=\"text-align: right;\">                   8</td><td>a9e45cc6  </td></tr>\n",
       "<tr><td>FSR_Trainable_abc8a58e</td><td>2023-07-19_12-02-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">108.941 </td><td style=\"text-align: right;\">3.83449e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">577487</td><td style=\"text-align: right;\">200.53 </td><td style=\"text-align: right;\">           11.3499  </td><td style=\"text-align: right;\">          0.640829</td><td style=\"text-align: right;\">     11.3499  </td><td style=\"text-align: right;\"> 1689735739</td><td style=\"text-align: right;\">                  16</td><td>abc8a58e  </td></tr>\n",
       "<tr><td>FSR_Trainable_ac03a684</td><td>2023-07-19_12-08-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">180.755 </td><td style=\"text-align: right;\">3.45915e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">584825</td><td style=\"text-align: right;\">269.331</td><td style=\"text-align: right;\">            2.23497 </td><td style=\"text-align: right;\">          2.23497 </td><td style=\"text-align: right;\">      2.23497 </td><td style=\"text-align: right;\"> 1689736106</td><td style=\"text-align: right;\">                   1</td><td>ac03a684  </td></tr>\n",
       "<tr><td>FSR_Trainable_ad20764b</td><td>2023-07-19_11-52-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">115.198 </td><td style=\"text-align: right;\">4.43534e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">568435</td><td style=\"text-align: right;\">222.284</td><td style=\"text-align: right;\">           12.5462  </td><td style=\"text-align: right;\">          2.76236 </td><td style=\"text-align: right;\">     12.5462  </td><td style=\"text-align: right;\"> 1689735129</td><td style=\"text-align: right;\">                   4</td><td>ad20764b  </td></tr>\n",
       "<tr><td>FSR_Trainable_ae71f7ed</td><td>2023-07-19_12-06-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">210.83  </td><td style=\"text-align: right;\">3.91157e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">581891</td><td style=\"text-align: right;\">364.963</td><td style=\"text-align: right;\">            0.732858</td><td style=\"text-align: right;\">          0.732858</td><td style=\"text-align: right;\">      0.732858</td><td style=\"text-align: right;\"> 1689735961</td><td style=\"text-align: right;\">                   1</td><td>ae71f7ed  </td></tr>\n",
       "<tr><td>FSR_Trainable_b05cbfe8</td><td>2023-07-19_12-05-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">186.806 </td><td style=\"text-align: right;\">7.1031e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">580739</td><td style=\"text-align: right;\">326.715</td><td style=\"text-align: right;\">            1.96364 </td><td style=\"text-align: right;\">          1.96364 </td><td style=\"text-align: right;\">      1.96364 </td><td style=\"text-align: right;\"> 1689735908</td><td style=\"text-align: right;\">                   1</td><td>b05cbfe8  </td></tr>\n",
       "<tr><td>FSR_Trainable_b07bb1b2</td><td>2023-07-19_11-43-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">234.115 </td><td style=\"text-align: right;\">9.92124e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">564456</td><td style=\"text-align: right;\">386.348</td><td style=\"text-align: right;\">           28.1421  </td><td style=\"text-align: right;\">         13.4699  </td><td style=\"text-align: right;\">     28.1421  </td><td style=\"text-align: right;\"> 1689734582</td><td style=\"text-align: right;\">                   2</td><td>b07bb1b2  </td></tr>\n",
       "<tr><td>FSR_Trainable_b3de54a8</td><td>2023-07-19_11-47-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">159.508 </td><td style=\"text-align: right;\">2.04376e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">565992</td><td style=\"text-align: right;\">321.615</td><td style=\"text-align: right;\">            4.82186 </td><td style=\"text-align: right;\">          0.977888</td><td style=\"text-align: right;\">      4.82186 </td><td style=\"text-align: right;\"> 1689734840</td><td style=\"text-align: right;\">                   4</td><td>b3de54a8  </td></tr>\n",
       "<tr><td>FSR_Trainable_b6fe67e4</td><td>2023-07-19_12-03-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">100.03  </td><td style=\"text-align: right;\">2.34234e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">577765</td><td style=\"text-align: right;\">190.545</td><td style=\"text-align: right;\">           58.577   </td><td style=\"text-align: right;\">          0.557596</td><td style=\"text-align: right;\">     58.577   </td><td style=\"text-align: right;\"> 1689735827</td><td style=\"text-align: right;\">                 100</td><td>b6fe67e4  </td></tr>\n",
       "<tr><td>FSR_Trainable_be819b00</td><td>2023-07-19_12-04-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">109.732 </td><td style=\"text-align: right;\">3.96669e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">580259</td><td style=\"text-align: right;\">203.604</td><td style=\"text-align: right;\">            3.50384 </td><td style=\"text-align: right;\">          0.750514</td><td style=\"text-align: right;\">      3.50384 </td><td style=\"text-align: right;\"> 1689735885</td><td style=\"text-align: right;\">                   4</td><td>be819b00  </td></tr>\n",
       "<tr><td>FSR_Trainable_c47111ed</td><td>2023-07-19_12-02-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">101.292 </td><td style=\"text-align: right;\">2.81301e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">576794</td><td style=\"text-align: right;\">193.979</td><td style=\"text-align: right;\">           66.8119  </td><td style=\"text-align: right;\">          0.612086</td><td style=\"text-align: right;\">     66.8119  </td><td style=\"text-align: right;\"> 1689735763</td><td style=\"text-align: right;\">                 100</td><td>c47111ed  </td></tr>\n",
       "<tr><td>FSR_Trainable_ca99d759</td><td>2023-07-19_11-59-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">101.126 </td><td style=\"text-align: right;\">2.54869e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">574426</td><td style=\"text-align: right;\">196.536</td><td style=\"text-align: right;\">           68.7363  </td><td style=\"text-align: right;\">          0.705606</td><td style=\"text-align: right;\">     68.7363  </td><td style=\"text-align: right;\"> 1689735599</td><td style=\"text-align: right;\">                 100</td><td>ca99d759  </td></tr>\n",
       "<tr><td>FSR_Trainable_cc4614e8</td><td>2023-07-19_11-49-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">179.16  </td><td style=\"text-align: right;\">8.08442e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">567177</td><td style=\"text-align: right;\">331.745</td><td style=\"text-align: right;\">            3.59375 </td><td style=\"text-align: right;\">          1.73801 </td><td style=\"text-align: right;\">      3.59375 </td><td style=\"text-align: right;\"> 1689734957</td><td style=\"text-align: right;\">                   2</td><td>cc4614e8  </td></tr>\n",
       "<tr><td>FSR_Trainable_cedcf763</td><td>2023-07-19_11-37-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">197.986 </td><td style=\"text-align: right;\">1.82152e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">562168</td><td style=\"text-align: right;\">393.183</td><td style=\"text-align: right;\">            5.21457 </td><td style=\"text-align: right;\">          5.21457 </td><td style=\"text-align: right;\">      5.21457 </td><td style=\"text-align: right;\"> 1689734224</td><td style=\"text-align: right;\">                   1</td><td>cedcf763  </td></tr>\n",
       "<tr><td>FSR_Trainable_cfde22d1</td><td>2023-07-19_11-56-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">240.588 </td><td style=\"text-align: right;\">1.47781e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">571479</td><td style=\"text-align: right;\">387.809</td><td style=\"text-align: right;\">           17.7899  </td><td style=\"text-align: right;\">         17.7899  </td><td style=\"text-align: right;\">     17.7899  </td><td style=\"text-align: right;\"> 1689735379</td><td style=\"text-align: right;\">                   1</td><td>cfde22d1  </td></tr>\n",
       "<tr><td>FSR_Trainable_d26b524c</td><td>2023-07-19_12-06-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">161.152 </td><td style=\"text-align: right;\">6.09749e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">582566</td><td style=\"text-align: right;\">290.556</td><td style=\"text-align: right;\">            2.57122 </td><td style=\"text-align: right;\">          2.57122 </td><td style=\"text-align: right;\">      2.57122 </td><td style=\"text-align: right;\"> 1689735998</td><td style=\"text-align: right;\">                   1</td><td>d26b524c  </td></tr>\n",
       "<tr><td>FSR_Trainable_d303bd48</td><td>2023-07-19_11-40-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">240.418 </td><td style=\"text-align: right;\">2.70154e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">563263</td><td style=\"text-align: right;\">450.097</td><td style=\"text-align: right;\">           15.4521  </td><td style=\"text-align: right;\">         15.4521  </td><td style=\"text-align: right;\">     15.4521  </td><td style=\"text-align: right;\"> 1689734453</td><td style=\"text-align: right;\">                   1</td><td>d303bd48  </td></tr>\n",
       "<tr><td>FSR_Trainable_d41e2bd6</td><td>2023-07-19_12-05-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">101.41  </td><td style=\"text-align: right;\">2.40855e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">579358</td><td style=\"text-align: right;\">199.339</td><td style=\"text-align: right;\">           65.05    </td><td style=\"text-align: right;\">          0.759781</td><td style=\"text-align: right;\">     65.05    </td><td style=\"text-align: right;\"> 1689735912</td><td style=\"text-align: right;\">                 100</td><td>d41e2bd6  </td></tr>\n",
       "<tr><td>FSR_Trainable_d8976afa</td><td>2023-07-19_11-51-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">104.756 </td><td style=\"text-align: right;\">2.692e+07  </td><td>172.26.215.93</td><td style=\"text-align: right;\">567915</td><td style=\"text-align: right;\">196.542</td><td style=\"text-align: right;\">           65.3051  </td><td style=\"text-align: right;\">          0.647391</td><td style=\"text-align: right;\">     65.3051  </td><td style=\"text-align: right;\"> 1689735099</td><td style=\"text-align: right;\">                 100</td><td>d8976afa  </td></tr>\n",
       "<tr><td>FSR_Trainable_e25d6d8d</td><td>2023-07-19_11-57-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">213.51  </td><td style=\"text-align: right;\">8.03345e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">573563</td><td style=\"text-align: right;\">366.17 </td><td style=\"text-align: right;\">            1.46832 </td><td style=\"text-align: right;\">          1.46832 </td><td style=\"text-align: right;\">      1.46832 </td><td style=\"text-align: right;\"> 1689735476</td><td style=\"text-align: right;\">                   1</td><td>e25d6d8d  </td></tr>\n",
       "<tr><td>FSR_Trainable_e4b7bac0</td><td>2023-07-19_12-06-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">174.762 </td><td style=\"text-align: right;\">5.34859e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">582783</td><td style=\"text-align: right;\">304.34 </td><td style=\"text-align: right;\">            2.92244 </td><td style=\"text-align: right;\">          2.92244 </td><td style=\"text-align: right;\">      2.92244 </td><td style=\"text-align: right;\"> 1689736010</td><td style=\"text-align: right;\">                   1</td><td>e4b7bac0  </td></tr>\n",
       "<tr><td>FSR_Trainable_e5b649e8</td><td>2023-07-19_11-53-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 99.2893</td><td style=\"text-align: right;\">2.36394e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">568240</td><td style=\"text-align: right;\">195.227</td><td style=\"text-align: right;\">           68.3205  </td><td style=\"text-align: right;\">          0.648668</td><td style=\"text-align: right;\">     68.3205  </td><td style=\"text-align: right;\"> 1689735184</td><td style=\"text-align: right;\">                 100</td><td>e5b649e8  </td></tr>\n",
       "<tr><td>FSR_Trainable_e5ea828b</td><td>2023-07-19_12-01-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">109.858 </td><td style=\"text-align: right;\">5.20684e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">577242</td><td style=\"text-align: right;\">204.031</td><td style=\"text-align: right;\">            3.38027 </td><td style=\"text-align: right;\">          0.78972 </td><td style=\"text-align: right;\">      3.38027 </td><td style=\"text-align: right;\"> 1689735713</td><td style=\"text-align: right;\">                   4</td><td>e5ea828b  </td></tr>\n",
       "<tr><td>FSR_Trainable_e628ae96</td><td>2023-07-19_11-39-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">241.414 </td><td style=\"text-align: right;\">1.38803e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">561746</td><td style=\"text-align: right;\">387.396</td><td style=\"text-align: right;\">          189.202   </td><td style=\"text-align: right;\">          1.82564 </td><td style=\"text-align: right;\">    189.202   </td><td style=\"text-align: right;\"> 1689734391</td><td style=\"text-align: right;\">                 100</td><td>e628ae96  </td></tr>\n",
       "<tr><td>FSR_Trainable_e7a05b2f</td><td>2023-07-19_12-06-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">101.074 </td><td style=\"text-align: right;\">2.66258e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">581439</td><td style=\"text-align: right;\">191.04 </td><td style=\"text-align: right;\">           60.7138  </td><td style=\"text-align: right;\">          0.735189</td><td style=\"text-align: right;\">     60.7138  </td><td style=\"text-align: right;\"> 1689736017</td><td style=\"text-align: right;\">                 100</td><td>e7a05b2f  </td></tr>\n",
       "<tr><td>FSR_Trainable_e9bebd71</td><td>2023-07-19_12-00-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 99.8435</td><td style=\"text-align: right;\">2.47584e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">574674</td><td style=\"text-align: right;\">198.898</td><td style=\"text-align: right;\">           65.6664  </td><td style=\"text-align: right;\">          0.509471</td><td style=\"text-align: right;\">     65.6664  </td><td style=\"text-align: right;\"> 1689735607</td><td style=\"text-align: right;\">                 100</td><td>e9bebd71  </td></tr>\n",
       "<tr><td>FSR_Trainable_eab59ae0</td><td>2023-07-19_11-40-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">475.246 </td><td style=\"text-align: right;\">6.56315e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">562838</td><td style=\"text-align: right;\">823.25 </td><td style=\"text-align: right;\">            2.23095 </td><td style=\"text-align: right;\">          2.23095 </td><td style=\"text-align: right;\">      2.23095 </td><td style=\"text-align: right;\"> 1689734405</td><td style=\"text-align: right;\">                   1</td><td>eab59ae0  </td></tr>\n",
       "<tr><td>FSR_Trainable_f3a1c266</td><td>2023-07-19_12-07-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">233.9   </td><td style=\"text-align: right;\">1.77532e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">583266</td><td style=\"text-align: right;\">377.764</td><td style=\"text-align: right;\">            1.06651 </td><td style=\"text-align: right;\">          1.06651 </td><td style=\"text-align: right;\">      1.06651 </td><td style=\"text-align: right;\"> 1689736031</td><td style=\"text-align: right;\">                   1</td><td>f3a1c266  </td></tr>\n",
       "<tr><td>FSR_Trainable_f476ee4d</td><td>2023-07-19_11-41-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">261.123 </td><td style=\"text-align: right;\">4.49431e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">563531</td><td style=\"text-align: right;\">420.939</td><td style=\"text-align: right;\">            1.99444 </td><td style=\"text-align: right;\">          1.99444 </td><td style=\"text-align: right;\">      1.99444 </td><td style=\"text-align: right;\"> 1689734469</td><td style=\"text-align: right;\">                   1</td><td>f476ee4d  </td></tr>\n",
       "<tr><td>FSR_Trainable_fcec382d</td><td>2023-07-19_12-03-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">101.235 </td><td style=\"text-align: right;\">3.11275e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">579132</td><td style=\"text-align: right;\">193.293</td><td style=\"text-align: right;\">            9.36924 </td><td style=\"text-align: right;\">          0.802684</td><td style=\"text-align: right;\">      9.36924 </td><td style=\"text-align: right;\"> 1689735830</td><td style=\"text-align: right;\">                  16</td><td>fcec382d  </td></tr>\n",
       "<tr><td>FSR_Trainable_ff916a9d</td><td>2023-07-19_11-54-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">103.237 </td><td style=\"text-align: right;\">3.08395e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">570274</td><td style=\"text-align: right;\">205.851</td><td style=\"text-align: right;\">           64.7629  </td><td style=\"text-align: right;\">          1.07048 </td><td style=\"text-align: right;\">     64.7629  </td><td style=\"text-align: right;\"> 1689735292</td><td style=\"text-align: right;\">                  64</td><td>ff916a9d  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_e628ae96_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_11-36-28/wandb/run-20230719_113639-e628ae96\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: Syncing run FSR_Trainable_e628ae96\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e628ae96\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_59cddc2b_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_11-36-33/wandb/run-20230719_113647-59cddc2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: Syncing run FSR_Trainable_59cddc2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/59cddc2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_7623c5cc_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_11-36-41/wandb/run-20230719_113657-7623c5cc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: Syncing run FSR_Trainable_7623c5cc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7623c5cc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_cedcf763_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_11-36-49/wandb/run-20230719_113707-cedcf763\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: Syncing run FSR_Trainable_cedcf763\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cedcf763\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb:                      mae 197.98621\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb:                     mape 1.8215163157814484e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb:                     rmse 393.18313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb:       time_since_restore 5.21457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb:         time_this_iter_s 5.21457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb:             time_total_s 5.21457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb:                timestamp 1689734224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: 🚀 View run FSR_Trainable_cedcf763 at: https://wandb.ai/seokjin/FSR-prediction/runs/cedcf763\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562340)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_113707-cedcf763/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_9d2e5769_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_11-36-59/wandb/run-20230719_113727-9d2e5769\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: Syncing run FSR_Trainable_9d2e5769\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9d2e5769\n",
      "2023-07-19 11:37:36,088\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.242 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:37:36,093\tWARNING util.py:315 -- The `process_trial_result` operation took 2.247 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:37:36,094\tWARNING util.py:315 -- Processing trial results took 2.249 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:37:36,095\tWARNING util.py:315 -- The `process_trial_result` operation took 2.250 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb:                      mae ██▇▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb:                     mape ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb:                     rmse ██▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb:         time_this_iter_s ▆▁▅▄▆█▅▄▄▇▆▅▆▅▆▅▅▅▅▄▆▅▅▅▅▅▅▅▅▅▅▅▄▅▅▅▅▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb:                      mae 241.4139\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb:                     mape 138803193.61434\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb:                     rmse 387.39595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb:       time_since_restore 189.20184\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb:         time_this_iter_s 1.82564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb:             time_total_s 189.20184\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb:                timestamp 1689734391\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: 🚀 View run FSR_Trainable_e628ae96 at: https://wandb.ai/seokjin/FSR-prediction/runs/e628ae96\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561818)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_113639-e628ae96/logs\n",
      "2023-07-19 11:40:07,930\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.210 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:40:07,934\tWARNING util.py:315 -- The `process_trial_result` operation took 2.215 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:40:07,937\tWARNING util.py:315 -- Processing trial results took 2.218 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:40:07,941\tWARNING util.py:315 -- The `process_trial_result` operation took 2.222 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_eab59ae0_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_11-37-18/wandb/run-20230719_114010-eab59ae0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: Syncing run FSR_Trainable_eab59ae0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/eab59ae0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb:                      mae 475.24644\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb:                     mape 6.563150625847908e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb:                     rmse 823.24993\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb:       time_since_restore 2.23095\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb:         time_this_iter_s 2.23095\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb:             time_total_s 2.23095\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb:                timestamp 1689734405\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: 🚀 View run FSR_Trainable_eab59ae0 at: https://wandb.ai/seokjin/FSR-prediction/runs/eab59ae0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562894)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114010-eab59ae0/logs\n",
      "2023-07-19 11:40:25,499\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.688 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:40:25,504\tWARNING util.py:315 -- The `process_trial_result` operation took 2.695 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:40:25,506\tWARNING util.py:315 -- Processing trial results took 2.697 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:40:25,511\tWARNING util.py:315 -- The `process_trial_result` operation took 2.702 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_7c4e4b2a_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_11-40-03/wandb/run-20230719_114027-7c4e4b2a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: Syncing run FSR_Trainable_7c4e4b2a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7c4e4b2a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb:                      mae 258.25513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb:                     mape 5.853098406506424e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb:                     rmse 458.90065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb:       time_since_restore 2.56874\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb:         time_this_iter_s 2.56874\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb:             time_total_s 2.56874\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb:                timestamp 1689734422\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: 🚀 View run FSR_Trainable_7c4e4b2a at: https://wandb.ai/seokjin/FSR-prediction/runs/7c4e4b2a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563121)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114027-7c4e4b2a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_d303bd48_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_11-40-20/wandb/run-20230719_114046-d303bd48\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: Syncing run FSR_Trainable_d303bd48\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d303bd48\n",
      "2023-07-19 11:40:55,420\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.078 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:40:55,425\tWARNING util.py:315 -- The `process_trial_result` operation took 2.083 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:40:55,427\tWARNING util.py:315 -- Processing trial results took 2.086 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:40:55,429\tWARNING util.py:315 -- The `process_trial_result` operation took 2.087 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb:                      mae 240.41761\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb:                     mape 2.7015411122772976e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb:                     rmse 450.09712\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb:       time_since_restore 15.45213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb:         time_this_iter_s 15.45213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb:             time_total_s 15.45213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb:                timestamp 1689734453\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: 🚀 View run FSR_Trainable_d303bd48 at: https://wandb.ai/seokjin/FSR-prediction/runs/d303bd48\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563350)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114046-d303bd48/logs\n",
      "2023-07-19 11:41:12,243\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.645 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:41:12,255\tWARNING util.py:315 -- The `process_trial_result` operation took 2.657 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:41:12,264\tWARNING util.py:315 -- Processing trial results took 2.666 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:41:12,269\tWARNING util.py:315 -- The `process_trial_result` operation took 2.672 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_f476ee4d_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_11-40-37/wandb/run-20230719_114115-f476ee4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: Syncing run FSR_Trainable_f476ee4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f476ee4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb:                      mae 261.12298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb:                     mape 4.49431033643576e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb:                     rmse 420.93878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb:       time_since_restore 1.99444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb:         time_this_iter_s 1.99444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb:             time_total_s 1.99444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb:                timestamp 1689734469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: 🚀 View run FSR_Trainable_f476ee4d at: https://wandb.ai/seokjin/FSR-prediction/runs/f476ee4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563587)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114115-f476ee4d/logs\n",
      "2023-07-19 11:41:29,832\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.824 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:41:29,839\tWARNING util.py:315 -- The `process_trial_result` operation took 2.831 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:41:29,841\tWARNING util.py:315 -- Processing trial results took 2.833 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:41:29,858\tWARNING util.py:315 -- The `process_trial_result` operation took 2.851 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_4ad2657e_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-41-07/wandb/run-20230719_114133-4ad2657e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: Syncing run FSR_Trainable_4ad2657e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4ad2657e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb:                      mae 330.74654\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb:                     mape 6.948090663537596e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb:                     rmse 450.46779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb:       time_since_restore 2.53678\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb:         time_this_iter_s 2.53678\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb:             time_total_s 2.53678\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb:                timestamp 1689734487\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: 🚀 View run FSR_Trainable_4ad2657e at: https://wandb.ai/seokjin/FSR-prediction/runs/4ad2657e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=563814)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114133-4ad2657e/logs\n",
      "2023-07-19 11:41:46,227\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.436 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:41:46,232\tWARNING util.py:315 -- The `process_trial_result` operation took 2.443 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:41:46,234\tWARNING util.py:315 -- Processing trial results took 2.444 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:41:46,236\tWARNING util.py:315 -- The `process_trial_result` operation took 2.446 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_2a2f496d_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-41-24/wandb/run-20230719_114149-2a2f496d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: Syncing run FSR_Trainable_2a2f496d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2a2f496d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb:                      mae █▇▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb:                     mape █▇▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb:                     rmse █▆▁▂▁▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb:         time_this_iter_s █▆▄▄▂▃▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb:                timestamp ▁▃▄▅▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb:                      mae 180.02928\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb:                     mape 42553318.2237\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb:                     rmse 342.46485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb:       time_since_restore 13.48497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb:         time_this_iter_s 1.59722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb:             time_total_s 13.48497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb:                timestamp 1689734517\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: 🚀 View run FSR_Trainable_2a2f496d at: https://wandb.ai/seokjin/FSR-prediction/runs/2a2f496d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564043)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114149-2a2f496d/logs\n",
      "2023-07-19 11:42:13,527\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.661 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:42:13,532\tWARNING util.py:315 -- The `process_trial_result` operation took 2.667 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:42:13,533\tWARNING util.py:315 -- Processing trial results took 2.669 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:42:13,535\tWARNING util.py:315 -- The `process_trial_result` operation took 2.670 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_0f299666_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-41-41/wandb/run-20230719_114217-0f299666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: Syncing run FSR_Trainable_0f299666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0f299666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb:                      mae ██▆▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb:                     mape ██▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb:                     rmse ██▆▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb:         time_this_iter_s ▃▄▇▂█▃▃▂▂▃▂▅▂▁▁▂▃▂▂▂▂▃▁▅▃▆▆▅█▅█▁▅▃▆▃▄▃▃▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb:                      mae 145.73869\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb:                     mape 41827493.97398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb:                     rmse 271.48515\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb:       time_since_restore 327.32584\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb:         time_this_iter_s 3.39915\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb:             time_total_s 327.32584\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb:                timestamp 1689734540\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: 🚀 View run FSR_Trainable_59cddc2b at: https://wandb.ai/seokjin/FSR-prediction/runs/59cddc2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=561992)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_113647-59cddc2b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_b07bb1b2_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-42-09/wandb/run-20230719_114240-b07bb1b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: Syncing run FSR_Trainable_b07bb1b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b07bb1b2\n",
      "2023-07-19 11:42:49,162\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.113 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:42:49,165\tWARNING util.py:315 -- The `process_trial_result` operation took 2.117 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:42:49,166\tWARNING util.py:315 -- Processing trial results took 2.118 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:42:49,168\tWARNING util.py:315 -- The `process_trial_result` operation took 2.120 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb:                      mae 234.11492\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb:                     mape 9.921241823875093e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb:                     rmse 386.34795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb:       time_since_restore 28.14211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb:         time_this_iter_s 13.46992\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb:             time_total_s 28.14211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb:                timestamp 1689734582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: 🚀 View run FSR_Trainable_b07bb1b2 at: https://wandb.ai/seokjin/FSR-prediction/runs/b07bb1b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564513)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114240-b07bb1b2/logs\n",
      "2023-07-19 11:43:18,304\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.795 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:43:18,310\tWARNING util.py:315 -- The `process_trial_result` operation took 2.802 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:43:18,313\tWARNING util.py:315 -- Processing trial results took 2.805 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:43:18,316\tWARNING util.py:315 -- The `process_trial_result` operation took 2.808 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_a2d1aeb7_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-42-32/wandb/run-20230719_114321-a2d1aeb7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: Syncing run FSR_Trainable_a2d1aeb7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a2d1aeb7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb:                      mae 245.98131\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb:                     mape 3.2175765016621523e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb:                     rmse 424.7925\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb:       time_since_restore 1.20365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb:         time_this_iter_s 1.20365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb:             time_total_s 1.20365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb:                timestamp 1689734595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: 🚀 View run FSR_Trainable_a2d1aeb7 at: https://wandb.ai/seokjin/FSR-prediction/runs/a2d1aeb7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564762)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114321-a2d1aeb7/logs\n",
      "2023-07-19 11:43:34,735\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.648 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:43:34,741\tWARNING util.py:315 -- The `process_trial_result` operation took 2.655 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:43:34,743\tWARNING util.py:315 -- Processing trial results took 2.657 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:43:34,745\tWARNING util.py:315 -- The `process_trial_result` operation took 2.659 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_48b95fcb_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-43-14/wandb/run-20230719_114338-48b95fcb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: Syncing run FSR_Trainable_48b95fcb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/48b95fcb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb:                      mae █▆▆▆▅▅▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb:                     mape ▁▅▇█████████▇▇▆▆▆▆▆▆▆▇▆▆▆▇▆▆▆▆▆▆▆▆▆▆▅▇▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb:                     rmse █▆▆▅▄▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb:         time_this_iter_s █▃▃▃▃▃▂▃▄▆▄▂▃▃▄▁▂▃▄▃▂▁▃▄▃▂▂▄▄▂▃▃▂▃▁▁▃▄▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb:                      mae 133.83615\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb:                     mape 2.4166075555638304e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb:                     rmse 250.92811\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb:       time_since_restore 106.1958\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb:         time_this_iter_s 0.99198\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb:             time_total_s 106.1958\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb:                timestamp 1689734646\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: 🚀 View run FSR_Trainable_0f299666 at: https://wandb.ai/seokjin/FSR-prediction/runs/0f299666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564281)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114217-0f299666/logs\n",
      "2023-07-19 11:44:22,722\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.140 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:44:22,725\tWARNING util.py:315 -- The `process_trial_result` operation took 2.143 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:44:22,727\tWARNING util.py:315 -- Processing trial results took 2.145 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:44:22,730\tWARNING util.py:315 -- The `process_trial_result` operation took 2.149 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_1acabe1b_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-43-30/wandb/run-20230719_114426-1acabe1b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: Syncing run FSR_Trainable_1acabe1b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1acabe1b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb:                      mae █▃▃▃▃▃▂▂▂▁▁▁▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▂▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb:                     mape █▅▃▄▃▄▁▂▂▁▂▂▃▄▄▄▃▄▄▄▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb:                     rmse █▂▂▂▂▂▁▂▁▁▁▁▁▂▂▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▃▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb:         time_this_iter_s █▄▄▂▂▁▂▁▃▃▂▃▃▃▂▁▁▅▄▃▂▂▂▁▂▂▃▁▂▅▃▁▂▃▁▁▆▃▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb:                      mae 99.46345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb:                     mape 22567798.96676\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb:                     rmse 204.05439\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb:       time_since_restore 105.03944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb:         time_this_iter_s 1.13997\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb:             time_total_s 105.03944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb:                timestamp 1689734721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: 🚀 View run FSR_Trainable_48b95fcb at: https://wandb.ai/seokjin/FSR-prediction/runs/48b95fcb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=564990)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114338-48b95fcb/logs\n",
      "2023-07-19 11:45:37,677\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.640 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:45:37,681\tWARNING util.py:315 -- The `process_trial_result` operation took 2.645 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:45:37,686\tWARNING util.py:315 -- Processing trial results took 2.649 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:45:37,689\tWARNING util.py:315 -- The `process_trial_result` operation took 2.653 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_8ac6cd16_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-44-18/wandb/run-20230719_114541-8ac6cd16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Syncing run FSR_Trainable_8ac6cd16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8ac6cd16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb:                      mae ██▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb:                     mape ██▅▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb:                     rmse ██▅▅▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb:         time_this_iter_s ▄▁▆▂▂▁▂▂▁▂▁▁▁▂▄▃▆▄▅█▄▂▄▂▅▃▃▃▂▂▂▄▂▃▆▃▄▄▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb:                      mae 150.59119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb:                     mape 2.203915304158509e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb:                     rmse 266.70071\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb:       time_since_restore 572.24647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb:         time_this_iter_s 5.95439\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb:             time_total_s 572.24647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb:                timestamp 1689734791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: 🚀 View run FSR_Trainable_7623c5cc at: https://wandb.ai/seokjin/FSR-prediction/runs/7623c5cc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562167)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_113657-7623c5cc/logs\n",
      "2023-07-19 11:46:46,823\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.401 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:46:46,828\tWARNING util.py:315 -- The `process_trial_result` operation took 2.407 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:46:46,830\tWARNING util.py:315 -- Processing trial results took 2.409 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:46:46,831\tWARNING util.py:315 -- The `process_trial_result` operation took 2.411 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_5cb6c374_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-45-33/wandb/run-20230719_114650-5cb6c374\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: Syncing run FSR_Trainable_5cb6c374\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cb6c374\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb:                      mae ▆▄▇▆▅█▄▃█▄▂▃▁▃▃▁▂▃▃▂▂▂▄▃▂▂▁▂▂▂▂▃▁▂▃▂▁▂▃▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb:                     mape ▄▄▅▅▄▃▂▂█▂▂▂▂▃▃▁▁▂▂▂▂▂▁▂▂▁▁▂▂▂▁▂▁▁▁▁▁▁▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb:                     rmse ▅▄▆▅▄▇▃▂█▃▂▂▁▂▃▂▂▃▃▂▂▃▅▄▂▂▁▃▂▂▂▃▂▂▃▂▂▂▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb:         time_this_iter_s █▄▃▂▁▂▂▂▂▁▂▂▂▆▄▂▃▃▁▅▃▂▅▃▃▂▅▁▃▃▂▂▂▃▃▂▆▄▂▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb:                      mae 160.30437\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb:                     mape 86285696.92741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb:                     rmse 292.50266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb:       time_since_restore 155.00242\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb:         time_this_iter_s 1.73536\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb:             time_total_s 155.00242\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb:                timestamp 1689734821\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: 🚀 View run FSR_Trainable_1acabe1b at: https://wandb.ai/seokjin/FSR-prediction/runs/1acabe1b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565248)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114426-1acabe1b/logs\n",
      "2023-07-19 11:47:17,154\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.303 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:47:17,160\tWARNING util.py:315 -- The `process_trial_result` operation took 2.310 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:47:17,163\tWARNING util.py:315 -- Processing trial results took 2.313 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:47:17,165\tWARNING util.py:315 -- The `process_trial_result` operation took 2.315 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_b3de54a8_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-46-43/wandb/run-20230719_114720-b3de54a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: Syncing run FSR_Trainable_b3de54a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b3de54a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)03 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb:                      mae ▅▂▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb:                     mape █▁▄▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb:                     rmse █▅▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb:         time_this_iter_s █▃▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb:                timestamp ▁▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb:                      mae 159.50768\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb:                     mape 2.0437586340253676e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb:                     rmse 321.61477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb:       time_since_restore 4.82186\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb:         time_this_iter_s 0.97789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb:             time_total_s 4.82186\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb:                timestamp 1689734840\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: 🚀 View run FSR_Trainable_b3de54a8 at: https://wandb.ai/seokjin/FSR-prediction/runs/b3de54a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566052)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114720-b3de54a8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb:                      mae █▄▁▁▂▁▁▁▁▁▂▂▂▂▂▁▂▃▃▃▃▃▃▃▃▃▃▃▄▄▃▃▄▃▂▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb:                     mape █▆▄▄▄▂▂▂▂▂▂▁▂▂▂▃▂▃▃▃▃▃▃▂▂▂▃▃▃▃▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb:                     rmse █▂▁▁▂▂▂▂▂▂▂▂▂▃▃▁▂▃▃▃▄▄▃▃▄▄▃▃▄▄▄▃▄▄▃▃▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb:         time_this_iter_s █▃▄▂▅▄▂▃▁▂▂▅▂▂▃▃▂▂▂▁▁▂▂▁▁▄▃▃▁▂▂▁▃▂▃▂▃▂▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "2023-07-19 11:47:35,271\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.540 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:47:35,276\tWARNING util.py:315 -- The `process_trial_result` operation took 2.546 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:47:35,278\tWARNING util.py:315 -- Processing trial results took 2.548 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:47:35,279\tWARNING util.py:315 -- The `process_trial_result` operation took 2.549 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_a2f42b1f_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-47-13/wandb/run-20230719_114738-a2f42b1f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: Syncing run FSR_Trainable_a2f42b1f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a2f42b1f\n",
      "2023-07-19 11:47:48,967\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.650 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:47:48,971\tWARNING util.py:315 -- The `process_trial_result` operation took 2.656 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:47:48,975\tWARNING util.py:315 -- Processing trial results took 2.660 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:47:48,977\tWARNING util.py:315 -- The `process_trial_result` operation took 2.662 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_29617618_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-47-31/wandb/run-20230719_114752-29617618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: Syncing run FSR_Trainable_29617618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/29617618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb:                      mae █▄▅▅▂▂▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▃▃▃▃▂▃▃▂▂▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb:                     mape █▇▇█▆▆▃▂▂▃▂▃▁▁▁▁▁▂▁▂▂▂▁▂▁▂▁▁▂▃▂▃▂▃▂▂▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb:                     rmse █▃▅▄▂▂▁▁▁▂▂▃▂▂▂▂▂▂▂▂▂▂▃▃▂▃▂▃▃▃▃▃▃▃▂▂▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb:         time_this_iter_s █▄▄▃▃▃▂▂▂▂▂▇▄▃▃▁▁▄▅▃▃▂▄▃▂▃▄▂▂▃▃▂▃▃▂▂▃▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb:                      mae 114.75743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb:                     mape 1.7526952296487954e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb:                     rmse 230.36114\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb:       time_since_restore 100.61399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb:         time_this_iter_s 0.99068\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb:             time_total_s 100.61399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb:                timestamp 1689734913\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: 🚀 View run FSR_Trainable_5cb6c374 at: https://wandb.ai/seokjin/FSR-prediction/runs/5cb6c374\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=565805)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114650-5cb6c374/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 11:48:48,602\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.515 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:48:48,603\tWARNING util.py:315 -- The `process_trial_result` operation took 2.518 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:48:48,607\tWARNING util.py:315 -- Processing trial results took 2.522 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:48:48,610\tWARNING util.py:315 -- The `process_trial_result` operation took 2.525 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb:                      mae █▆▂▁▁▁▂▃▂▂▂▂▃▄▃▃▃▃▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb:                     mape ▃▇█▆▅▅▄▄▄▄▃▄▃▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb:                     rmse █▆▂▁▁▁▂▂▂▂▂▂▃▄▂▃▃▂▂▂▃▃▂▂▃▂▃▃▃▃▃▃▃▃▃▃▃▂▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb:         time_this_iter_s █▃▂▃▁▂▃█▃▂▂▁▂▂▁▂▁▁▂▁▁▁▂▂▁▁▂▁▂▂▁▃▂▁▂▂▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb:                      mae 115.77164\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb:                     mape 2.1889561961623892e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb:                     rmse 215.84057\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb:       time_since_restore 66.79096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb:         time_this_iter_s 0.68316\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb:             time_total_s 66.79096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb:                timestamp 1689734925\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: 🚀 View run FSR_Trainable_a2f42b1f at: https://wandb.ai/seokjin/FSR-prediction/runs/a2f42b1f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566297)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114738-a2f42b1f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_1957c775_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-47-45/wandb/run-20230719_114851-1957c775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: Syncing run FSR_Trainable_1957c775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1957c775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb:                      mae █▆▃▂▂▂▃▃▂▂▂▂▂▂▃▂▂▃▃▃▂▃▃▃▃▃▃▃▄▄▄▄▆▁▂▃▃▃▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb:                     mape █▆▅▄▄▄▃▃▂▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb:                     rmse ▄▄▂▁▁▂▄▅▄▄▃▃▃▄▅▄▄▆▅▅▅▅▅▅▅▅▅▆▆▆▆▆█▁▅▅▆▆▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb:         time_this_iter_s █▄▄▃▄▂▁▂▂▂▂▄▂▁▂▁▂▂▂▂▃▂▃▁▅▁▂▃▃▁▂▂▃▃▆▄▃▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb:                      mae 102.21929\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb:                     mape 32796815.5991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb:                     rmse 199.00828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb:       time_since_restore 64.58369\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb:         time_this_iter_s 0.62245\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb:             time_total_s 64.58369\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb:                timestamp 1689734937\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: 🚀 View run FSR_Trainable_29617618 at: https://wandb.ai/seokjin/FSR-prediction/runs/29617618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566515)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114752-29617618/logs\n",
      "2023-07-19 11:49:02,189\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.601 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:49:02,193\tWARNING util.py:315 -- The `process_trial_result` operation took 2.606 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:49:02,194\tWARNING util.py:315 -- Processing trial results took 2.608 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:49:02,197\tWARNING util.py:315 -- The `process_trial_result` operation took 2.611 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_52b3b738_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-48-45/wandb/run-20230719_114905-52b3b738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: Syncing run FSR_Trainable_52b3b738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/52b3b738\n",
      "2023-07-19 11:49:15,858\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.876 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:49:15,862\tWARNING util.py:315 -- The `process_trial_result` operation took 1.880 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:49:15,865\tWARNING util.py:315 -- Processing trial results took 1.883 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:49:15,867\tWARNING util.py:315 -- The `process_trial_result` operation took 1.885 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_cc4614e8_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-48-58/wandb/run-20230719_114919-cc4614e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: Syncing run FSR_Trainable_cc4614e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cc4614e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)04 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb:                      mae 179.16023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb:                     mape 80844185.40198\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb:                     rmse 331.74476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb:       time_since_restore 3.59375\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb:         time_this_iter_s 1.73801\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb:             time_total_s 3.59375\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb:                timestamp 1689734957\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: 🚀 View run FSR_Trainable_cc4614e8 at: https://wandb.ai/seokjin/FSR-prediction/runs/cc4614e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567266)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114919-cc4614e8/logs\n",
      "2023-07-19 11:49:33,364\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.134 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:49:33,368\tWARNING util.py:315 -- The `process_trial_result` operation took 2.139 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:49:33,372\tWARNING util.py:315 -- Processing trial results took 2.143 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:49:33,374\tWARNING util.py:315 -- The `process_trial_result` operation took 2.145 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_07549cff_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-49-12/wandb/run-20230719_114936-07549cff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: Syncing run FSR_Trainable_07549cff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/07549cff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb:                      mae █▇▆▂▁▁▂▂▃▃▃▃▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb:                     mape █▆▆▅▅▅▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb:                     rmse ▆▅▅▃▁▂▄▆▆▇▇▆▆▆▆▆▆▆▅▇▇▇▇▇▇▇▇▇▇▇▇▇█████▇▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb:         time_this_iter_s █▄▃▃▂▂▆▃▃▃▂▁▂▄▄▃▅▂▂▃▂▅▃▃▃▃▂▁▃▂▂▂▂▃▃▃▂▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb:                      mae 101.06968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb:                     mape 27980493.93916\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb:                     rmse 195.16408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb:       time_since_restore 67.85716\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb:         time_this_iter_s 0.65204\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb:             time_total_s 67.85716\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb:                timestamp 1689735003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: 🚀 View run FSR_Trainable_1957c775 at: https://wandb.ai/seokjin/FSR-prediction/runs/1957c775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=566804)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114851-1957c775/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb:                      mae █▅▅▄▄▄▃▃▃▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▁▁▁▂▁▄▂▁▁▁▁▁▂▂▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb:                     mape █▆▆▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb:                     rmse ▃▂▃▃▃▃▂▂▂▁▁▁▁▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▄▂▂▂▂▂▃▃▃█▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb:         time_this_iter_s █▃▂▃▃▁▃▃▃▂▂▂▁▁▂▅▃▁▂▂▂▁▂▁▂▂▁▂▂▂▁▃▂▂▃▃▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb:                      mae 96.75001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb:                     mape 25021566.49917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb:                     rmse 193.14867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb:       time_since_restore 67.88551\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb:         time_this_iter_s 0.60192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb:             time_total_s 67.88551\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb:                timestamp 1689735014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: 🚀 View run FSR_Trainable_52b3b738 at: https://wandb.ai/seokjin/FSR-prediction/runs/52b3b738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567039)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114905-52b3b738/logs\n",
      "2023-07-19 11:50:18,813\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.583 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:50:18,836\tWARNING util.py:315 -- The `process_trial_result` operation took 2.607 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:50:18,839\tWARNING util.py:315 -- Processing trial results took 2.610 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:50:18,846\tWARNING util.py:315 -- The `process_trial_result` operation took 2.617 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_426f126e_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-49-29/wandb/run-20230719_115022-426f126e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: Syncing run FSR_Trainable_426f126e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/426f126e\n",
      "2023-07-19 11:50:32,610\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.548 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:50:32,613\tWARNING util.py:315 -- The `process_trial_result` operation took 2.552 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:50:32,616\tWARNING util.py:315 -- Processing trial results took 2.556 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:50:32,619\tWARNING util.py:315 -- The `process_trial_result` operation took 2.559 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_d8976afa_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-50-15/wandb/run-20230719_115035-d8976afa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: Syncing run FSR_Trainable_d8976afa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d8976afa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb:                      mae █▅▂▁▂▂▂▁▂▃▃▂▄▄▄▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb:                     mape █▅▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb:                     rmse ▁▂▂▂▃▃▃▃▄▅▅▃▆▆▆▆▇▇▇▇▇█████████████████▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb:         time_this_iter_s █▃▃▄▃▂▄▄▃▂▂▃▁▃▂▂▃▂▃▂▂▂▂▂▂▁▂▂▃▁▁▃▂▂▂▃▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb:                      mae 99.48009\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb:                     mape 28072893.02138\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb:                     rmse 197.16274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb:       time_since_restore 65.90792\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb:         time_this_iter_s 0.59026\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb:             time_total_s 65.90792\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb:                timestamp 1689735088\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: 🚀 View run FSR_Trainable_426f126e at: https://wandb.ai/seokjin/FSR-prediction/runs/426f126e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567776)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115022-426f126e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 11:51:43,854\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.588 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:51:43,859\tWARNING util.py:315 -- The `process_trial_result` operation took 2.594 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:51:43,860\tWARNING util.py:315 -- Processing trial results took 2.596 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:51:43,862\tWARNING util.py:315 -- The `process_trial_result` operation took 2.598 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb:                      mae █▄▃▃▃▃▃▃▃▃▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb:                     mape █▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb:                     rmse █▃▂▂▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb:         time_this_iter_s █▄▃▄▂▄▂▃▂▂▁▃▂▃▃▁▁▂▃▄▂▂▂▂▂▂▁▁▂▁▁▂▃▂▂▃▃▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb:                      mae 104.7557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb:                     mape 26919975.0236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb:                     rmse 196.54165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb:       time_since_restore 65.30508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb:         time_this_iter_s 0.64739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb:             time_total_s 65.30508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb:                timestamp 1689735099\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: 🚀 View run FSR_Trainable_d8976afa at: https://wandb.ai/seokjin/FSR-prediction/runs/d8976afa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568001)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115035-d8976afa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_e5b649e8_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-50-29/wandb/run-20230719_115147-e5b649e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Syncing run FSR_Trainable_e5b649e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e5b649e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 11:52:00,276\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.466 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:52:00,280\tWARNING util.py:315 -- The `process_trial_result` operation took 2.470 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:52:00,282\tWARNING util.py:315 -- Processing trial results took 2.472 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:52:00,284\tWARNING util.py:315 -- The `process_trial_result` operation took 2.475 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb:                      mae █▄▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▂▁▂▂▂▂▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb:                     mape █▃▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb:                     rmse █▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▃▄▄▄▄▃▃▂▃▃▃▃▃▃▃▃▂▃▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb:         time_this_iter_s █▃▂▁▁▂▂▂▂▂▂▃▂▄▂▂▄▃▃▂▁▂▂▃▂▄▃▂▃▂▂▂▂▂▃▃▆▂▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb:                      mae 105.43944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb:                     mape 29523384.2633\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb:                     rmse 206.37778\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb:       time_since_restore 136.75145\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb:         time_this_iter_s 1.50546\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb:             time_total_s 136.75145\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb:                timestamp 1689735117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: 🚀 View run FSR_Trainable_07549cff at: https://wandb.ai/seokjin/FSR-prediction/runs/07549cff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=567501)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_114936-07549cff/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_ad20764b_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-51-40/wandb/run-20230719_115202-ad20764b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: Syncing run FSR_Trainable_ad20764b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ad20764b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb:                      mae █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb:                     mape █▁▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb:                     rmse █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb:         time_this_iter_s █▅▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb:                      mae 115.19841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb:                     mape 44353356.36504\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb:                     rmse 222.28374\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb:       time_since_restore 12.54623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb:         time_this_iter_s 2.76236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb:             time_total_s 12.54623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb:                timestamp 1689735129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: 🚀 View run FSR_Trainable_ad20764b at: https://wandb.ai/seokjin/FSR-prediction/runs/ad20764b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568530)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115202-ad20764b/logs\n",
      "2023-07-19 11:52:15,740\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.272 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:52:15,747\tWARNING util.py:315 -- The `process_trial_result` operation took 2.280 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:52:15,750\tWARNING util.py:315 -- Processing trial results took 2.282 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:52:15,753\tWARNING util.py:315 -- The `process_trial_result` operation took 2.286 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_86dbc305_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-51-54/wandb/run-20230719_115218-86dbc305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Syncing run FSR_Trainable_86dbc305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/86dbc305\n",
      "2023-07-19 11:52:26,142\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.638 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:52:26,146\tWARNING util.py:315 -- The `process_trial_result` operation took 2.643 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:52:26,148\tWARNING util.py:315 -- Processing trial results took 2.645 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:52:26,150\tWARNING util.py:315 -- The `process_trial_result` operation took 2.647 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_545478ec_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-52-10/wandb/run-20230719_115229-545478ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: Syncing run FSR_Trainable_545478ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/545478ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb:                      mae █▃▃▁▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb:                     mape █▂▃▃▂▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb:                     rmse █▄▂▁▄▃▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb:         time_this_iter_s █▅▃▄▄▂▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb:                timestamp ▁▅▅▅▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb:                      mae 111.8134\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb:                     mape 46968651.03988\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb:                     rmse 207.79915\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb:       time_since_restore 6.35684\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb:         time_this_iter_s 0.79521\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb:             time_total_s 6.35684\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb:                timestamp 1689735151\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: 🚀 View run FSR_Trainable_545478ec at: https://wandb.ai/seokjin/FSR-prediction/runs/545478ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568988)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115229-545478ec/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb:                      mae █▆▃▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb:                     mape █▂▇▄▅▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb:                     rmse █▇▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb:         time_this_iter_s █▃▂▁▁▃▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb:                timestamp ▁▃▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: \n",
      "2023-07-19 11:52:45,413\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.512 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:52:45,416\tWARNING util.py:315 -- The `process_trial_result` operation took 2.515 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:52:45,418\tWARNING util.py:315 -- Processing trial results took 2.517 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:52:45,422\tWARNING util.py:315 -- The `process_trial_result` operation took 2.521 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568767)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_a9e45cc6_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-52-22/wandb/run-20230719_115248-a9e45cc6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: Syncing run FSR_Trainable_a9e45cc6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9e45cc6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb:                      mae ▃█▇█▂▃▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb:                     mape ▆▅█▆▅▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb:                     rmse ▁▃▄▆▅█▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb:         time_this_iter_s █▄▂▂▂▁▆▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb:                timestamp ▁▅▅▅▆▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb:                      mae 114.87275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb:                     mape 41003402.31182\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb:                     rmse 214.25896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb:       time_since_restore 6.19803\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb:         time_this_iter_s 0.75775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb:             time_total_s 6.19803\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb:                timestamp 1689735170\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: 🚀 View run FSR_Trainable_a9e45cc6 at: https://wandb.ai/seokjin/FSR-prediction/runs/a9e45cc6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115248-a9e45cc6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569236)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 11:52:58,914\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.890 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:52:58,918\tWARNING util.py:315 -- The `process_trial_result` operation took 1.895 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:52:58,920\tWARNING util.py:315 -- Processing trial results took 1.897 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:52:58,922\tWARNING util.py:315 -- The `process_trial_result` operation took 1.899 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_0e6d32df_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-52-42/wandb/run-20230719_115302-0e6d32df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: Syncing run FSR_Trainable_0e6d32df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0e6d32df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb:                      mae 193.47455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb:                     mape 57692901.88741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb:                     rmse 332.79321\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb:       time_since_restore 1.70178\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb:         time_this_iter_s 1.70178\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb:             time_total_s 1.70178\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb:                timestamp 1689735177\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: 🚀 View run FSR_Trainable_0e6d32df at: https://wandb.ai/seokjin/FSR-prediction/runs/0e6d32df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569466)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115302-0e6d32df/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb:                      mae █▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb:                     mape █▆▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb:                     rmse █▆▄▃▂▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb:         time_this_iter_s █▄▂▆▃▂▃▅▄▃▄▂▃▃▇▇▄▃▂▄▅▄▃▄▂▁▄▁▃▂▅▄▅▃▃▁▅▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115147-e5b649e8/logs\n",
      "2023-07-19 11:53:10,856\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.973 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:53:10,860\tWARNING util.py:315 -- The `process_trial_result` operation took 1.979 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:53:10,862\tWARNING util.py:315 -- Processing trial results took 1.981 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:53:10,864\tWARNING util.py:315 -- The `process_trial_result` operation took 1.982 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=568302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_83b47f12_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-52-55/wandb/run-20230719_115313-83b47f12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: Syncing run FSR_Trainable_83b47f12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/83b47f12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb:                      mae 192.57275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb:                     mape 58704702.88138\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb:                     rmse 347.02536\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb:       time_since_restore 1.59686\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb:         time_this_iter_s 1.59686\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb:             time_total_s 1.59686\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb:                timestamp 1689735188\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: 🚀 View run FSR_Trainable_83b47f12 at: https://wandb.ai/seokjin/FSR-prediction/runs/83b47f12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115313-83b47f12/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569696)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 11:53:21,676\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.263 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:53:21,680\tWARNING util.py:315 -- The `process_trial_result` operation took 2.267 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:53:21,682\tWARNING util.py:315 -- Processing trial results took 2.269 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:53:21,684\tWARNING util.py:315 -- The `process_trial_result` operation took 2.271 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_7bf4a9dc_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-53-07/wandb/run-20230719_115324-7bf4a9dc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: Syncing run FSR_Trainable_7bf4a9dc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7bf4a9dc\n",
      "2023-07-19 11:53:33,842\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.146 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:53:33,844\tWARNING util.py:315 -- The `process_trial_result` operation took 2.149 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:53:33,847\tWARNING util.py:315 -- Processing trial results took 2.152 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:53:33,850\tWARNING util.py:315 -- The `process_trial_result` operation took 2.155 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_53e69005_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-53-18/wandb/run-20230719_115337-53e69005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: Syncing run FSR_Trainable_53e69005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/53e69005\n",
      "2023-07-19 11:53:47,444\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.223 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:53:47,448\tWARNING util.py:315 -- The `process_trial_result` operation took 2.228 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:53:47,455\tWARNING util.py:315 -- Processing trial results took 2.235 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:53:47,458\tWARNING util.py:315 -- The `process_trial_result` operation took 2.238 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_ff916a9d_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-53-30/wandb/run-20230719_115351-ff916a9d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: Syncing run FSR_Trainable_ff916a9d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ff916a9d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb:                      mae █▆▄▃▃▂▂▂▂▂▁▁▁▁▁▂▁▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb:                     mape █▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb:                     rmse █▆▄▃▂▂▂▁▁▁▁▁▂▂▂▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▅▄▅▅▅▅▆▆▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb:         time_this_iter_s █▅▄▂▂▃▃▃▁▃▂▃▃▃▃▃▂▂▂▂▂▂▂▃▂▂▂▂▃▂▂▃▂▂▁▃▂▂▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb:                      mae 103.23661\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb:                     mape 30839547.43935\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb:                     rmse 205.85129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb:       time_since_restore 64.7629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb:         time_this_iter_s 1.07048\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb:             time_total_s 64.7629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb:                timestamp 1689735292\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: 🚀 View run FSR_Trainable_ff916a9d at: https://wandb.ai/seokjin/FSR-prediction/runs/ff916a9d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570361)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115351-ff916a9d/logs\n",
      "2023-07-19 11:55:07,940\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.966 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:55:07,943\tWARNING util.py:315 -- The `process_trial_result` operation took 1.970 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:55:07,945\tWARNING util.py:315 -- Processing trial results took 1.973 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:55:07,947\tWARNING util.py:315 -- The `process_trial_result` operation took 1.974 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_9ff2fafb_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-53-43/wandb/run-20230719_115511-9ff2fafb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Syncing run FSR_Trainable_9ff2fafb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9ff2fafb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb:                      mae █▅▄▃▃▃▂▂▁▁▁▁▁▃▃▁▁▁▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb:                     mape █▄▃▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb:                     rmse █▆▅▅▄▃▂▂▁▁▁▁▁▃▄▂▁▂▄▃▃▃▄▄▄▄▅▄▅▅▅▅▅▅▆▅▆▆▅▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb:         time_this_iter_s █▃▆▁▅▇▃▃▂█▆▅▄▅▄▄▆▄▂▄▄▄▄▄▂▃▄▅▅▃▄▄▅▃▅▃▂▄▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb:                      mae 104.37704\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb:                     mape 29063537.07865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb:                     rmse 203.64684\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb:       time_since_restore 102.23442\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb:         time_this_iter_s 1.12689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb:             time_total_s 102.23442\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb:                timestamp 1689735310\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: 🚀 View run FSR_Trainable_7bf4a9dc at: https://wandb.ai/seokjin/FSR-prediction/runs/7bf4a9dc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=569923)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115324-7bf4a9dc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb:                      mae █▄▂▂▂▂▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb:                     mape █▆▄▃▃▂▂▂▂▂▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb:                     rmse █▃▁▂▂▂▁▁▂▂▄▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb:         time_this_iter_s █▃▄▂▄▆▃▁▃▃▂▂▃▄▂▁▃▃▃▄▂▂▂▂▁▂▃▃▅▂▃▂▃▂▃▄▃▄▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb:                      mae 97.65799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb:                     mape 22962781.25646\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb:                     rmse 195.91452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb:       time_since_restore 101.88575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb:         time_this_iter_s 0.99856\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb:             time_total_s 101.88575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb:                timestamp 1689735320\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: 🚀 View run FSR_Trainable_53e69005 at: https://wandb.ai/seokjin/FSR-prediction/runs/53e69005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570145)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115337-53e69005/logs\n",
      "2023-07-19 11:55:26,399\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.752 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:55:26,405\tWARNING util.py:315 -- The `process_trial_result` operation took 2.759 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:55:26,407\tWARNING util.py:315 -- Processing trial results took 2.761 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:55:26,409\tWARNING util.py:315 -- The `process_trial_result` operation took 2.763 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_1f2a53fe_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-55-04/wandb/run-20230719_115529-1f2a53fe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: Syncing run FSR_Trainable_1f2a53fe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1f2a53fe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb:                      mae █▁▆▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb:                     mape ▇█▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb:                     rmse █▁▆▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb:         time_this_iter_s █▇▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb:                timestamp ▁▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb:                      mae 113.21741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb:                     mape 41350663.06976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb:                     rmse 207.15429\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb:       time_since_restore 3.656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb:         time_this_iter_s 0.81209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb:             time_total_s 3.656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb:                timestamp 1689735329\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: 🚀 View run FSR_Trainable_1f2a53fe at: https://wandb.ai/seokjin/FSR-prediction/runs/1f2a53fe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570899)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115529-1f2a53fe/logs\n",
      "2023-07-19 11:55:39,357\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.985 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:55:39,360\tWARNING util.py:315 -- The `process_trial_result` operation took 2.989 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:55:39,362\tWARNING util.py:315 -- Processing trial results took 2.992 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:55:39,372\tWARNING util.py:315 -- The `process_trial_result` operation took 3.001 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_12caccc0_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-55-22/wandb/run-20230719_115543-12caccc0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: Syncing run FSR_Trainable_12caccc0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/12caccc0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb:                      mae 131.17756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb:                     mape 78325552.43985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb:                     rmse 221.5053\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb:       time_since_restore 1.77442\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb:         time_this_iter_s 0.9004\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb:             time_total_s 1.77442\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb:                timestamp 1689735340\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: 🚀 View run FSR_Trainable_12caccc0 at: https://wandb.ai/seokjin/FSR-prediction/runs/12caccc0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571130)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115543-12caccc0/logs\n",
      "2023-07-19 11:55:53,924\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.495 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:55:53,928\tWARNING util.py:315 -- The `process_trial_result` operation took 2.499 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:55:53,930\tWARNING util.py:315 -- Processing trial results took 2.501 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:55:53,932\tWARNING util.py:315 -- The `process_trial_result` operation took 2.503 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_6fad082a_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-55-35/wandb/run-20230719_115556-6fad082a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: Syncing run FSR_Trainable_6fad082a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6fad082a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_cfde22d1_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-55-48/wandb/run-20230719_115609-cfde22d1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: Syncing run FSR_Trainable_cfde22d1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cfde22d1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: / 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb:                      mae █▅▁▃▃▁▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb:                     mape ▅█▄▇▁▆▃▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb:                     rmse █▄▁▂▂▂▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb:         time_this_iter_s █▄▃▁▅▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb:                timestamp ▁▃▄▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb:                      mae 107.38447\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb:                     mape 35112260.36151\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb:                     rmse 205.96209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb:       time_since_restore 17.73945\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb:         time_this_iter_s 2.18113\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb:             time_total_s 17.73945\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb:                timestamp 1689735369\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: 🚀 View run FSR_Trainable_6fad082a at: https://wandb.ai/seokjin/FSR-prediction/runs/6fad082a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571365)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115556-6fad082a/logs\n",
      "2023-07-19 11:56:21,294\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.079 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:56:21,298\tWARNING util.py:315 -- The `process_trial_result` operation took 2.083 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:56:21,300\tWARNING util.py:315 -- Processing trial results took 2.085 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:56:21,307\tWARNING util.py:315 -- The `process_trial_result` operation took 2.093 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb:                      mae 240.58753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb:                     mape 147781029.61677\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb:                     rmse 387.80891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb:       time_since_restore 17.78989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb:         time_this_iter_s 17.78989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb:             time_total_s 17.78989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb:                timestamp 1689735379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: 🚀 View run FSR_Trainable_cfde22d1 at: https://wandb.ai/seokjin/FSR-prediction/runs/cfde22d1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571579)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115609-cfde22d1/logs\n",
      "2023-07-19 11:56:29,566\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.616 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:56:29,571\tWARNING util.py:315 -- The `process_trial_result` operation took 2.621 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:56:29,573\tWARNING util.py:315 -- Processing trial results took 2.623 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:56:29,575\tWARNING util.py:315 -- The `process_trial_result` operation took 2.626 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_539f358c_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-56-01/wandb/run-20230719_115630-539f358c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: Syncing run FSR_Trainable_539f358c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/539f358c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb:                      mae 222.08279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb:                     mape 135474478.12358\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb:                     rmse 349.8146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb:       time_since_restore 5.2251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb:         time_this_iter_s 5.2251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb:             time_total_s 5.2251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb:                timestamp 1689735386\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: 🚀 View run FSR_Trainable_539f358c at: https://wandb.ai/seokjin/FSR-prediction/runs/539f358c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=571822)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115630-539f358c/logs\n",
      "2023-07-19 11:56:37,863\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.260 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:56:37,867\tWARNING util.py:315 -- The `process_trial_result` operation took 2.265 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:56:37,869\tWARNING util.py:315 -- Processing trial results took 2.267 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:56:37,871\tWARNING util.py:315 -- The `process_trial_result` operation took 2.269 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_2f623f46_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-56-21/wandb/run-20230719_115641-2f623f46\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: Syncing run FSR_Trainable_2f623f46\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2f623f46\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb:                      mae 261.69501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb:                     mape 4.088497114014173e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb:                     rmse 411.74451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb:       time_since_restore 1.58888\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb:         time_this_iter_s 1.58888\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb:             time_total_s 1.58888\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb:                timestamp 1689735395\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: 🚀 View run FSR_Trainable_2f623f46 at: https://wandb.ai/seokjin/FSR-prediction/runs/2f623f46\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572053)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115641-2f623f46/logs\n",
      "2023-07-19 11:56:50,840\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.716 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:56:50,844\tWARNING util.py:315 -- The `process_trial_result` operation took 2.721 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:56:50,846\tWARNING util.py:315 -- Processing trial results took 2.723 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:56:50,850\tWARNING util.py:315 -- The `process_trial_result` operation took 2.727 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_1b6cfdf3_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-56-34/wandb/run-20230719_115654-1b6cfdf3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: Syncing run FSR_Trainable_1b6cfdf3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1b6cfdf3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb:                      mae █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb:                     mape █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb:                     rmse █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb:         time_this_iter_s █▅▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb:                      mae 116.72019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb:                     mape 44880151.68086\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb:                     rmse 211.22605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb:       time_since_restore 3.45511\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb:         time_this_iter_s 0.80783\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb:             time_total_s 3.45511\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb:                timestamp 1689735413\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: 🚀 View run FSR_Trainable_1b6cfdf3 at: https://wandb.ai/seokjin/FSR-prediction/runs/1b6cfdf3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572288)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115654-1b6cfdf3/logs\n",
      "2023-07-19 11:57:04,080\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.787 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:57:04,084\tWARNING util.py:315 -- The `process_trial_result` operation took 2.792 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:57:04,085\tWARNING util.py:315 -- Processing trial results took 2.794 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:57:04,086\tWARNING util.py:315 -- The `process_trial_result` operation took 2.795 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_21ca597b_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-56-47/wandb/run-20230719_115707-21ca597b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: Syncing run FSR_Trainable_21ca597b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/21ca597b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb:                      mae 122.08435\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb:                     mape 50729538.65298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb:                     rmse 227.28132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb:       time_since_restore 1.85316\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb:         time_this_iter_s 0.84019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb:             time_total_s 1.85316\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb:                timestamp 1689735424\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: 🚀 View run FSR_Trainable_21ca597b at: https://wandb.ai/seokjin/FSR-prediction/runs/21ca597b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572513)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115707-21ca597b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb:                      mae █▇▅▄▂▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb:                     mape █▄▅▅▄▄▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb:                     rmse █▇▄▄▂▂▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▂▂▃▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb:         time_this_iter_s █▅▄▅▂▃▅▄▃▃▅▄▃▁▇▆▄▅▆▅▅▅▆▃█▅▂▄▃▄▃▂▆▁▂▂▅▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115511-9ff2fafb/logs\n",
      "2023-07-19 11:57:16,798\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.636 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:57:16,803\tWARNING util.py:315 -- The `process_trial_result` operation took 2.642 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:57:16,805\tWARNING util.py:315 -- Processing trial results took 2.644 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:57:16,810\tWARNING util.py:315 -- The `process_trial_result` operation took 2.649 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=570659)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_a9c0ec65_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-57-00/wandb/run-20230719_115720-a9c0ec65\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: Syncing run FSR_Trainable_a9c0ec65\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9c0ec65\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb:                      mae █▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb:                     mape █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb:                     rmse █▆▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb:         time_this_iter_s █▆▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb:                timestamp ▁▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb:                      mae 114.07557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb:                     mape 43075599.31295\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb:                     rmse 209.12338\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb:       time_since_restore 3.08912\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb:         time_this_iter_s 0.64406\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb:             time_total_s 3.08912\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb:                timestamp 1689735438\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: 🚀 View run FSR_Trainable_a9c0ec65 at: https://wandb.ai/seokjin/FSR-prediction/runs/a9c0ec65\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115720-a9c0ec65/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572747)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 11:57:28,909\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.383 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:57:28,914\tWARNING util.py:315 -- The `process_trial_result` operation took 2.389 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:57:28,916\tWARNING util.py:315 -- Processing trial results took 2.391 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:57:28,919\tWARNING util.py:315 -- The `process_trial_result` operation took 2.394 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_64cfa2c5_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-57-13/wandb/run-20230719_115731-64cfa2c5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: Syncing run FSR_Trainable_64cfa2c5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/64cfa2c5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb:                      mae 224.82586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb:                     mape 3.9412957961575686e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb:                     rmse 396.5252\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb:       time_since_restore 1.92954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb:         time_this_iter_s 1.92954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb:             time_total_s 1.92954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb:                timestamp 1689735446\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: 🚀 View run FSR_Trainable_64cfa2c5 at: https://wandb.ai/seokjin/FSR-prediction/runs/64cfa2c5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=572979)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115731-64cfa2c5/logs\n",
      "2023-07-19 11:57:38,523\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.428 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:57:38,530\tWARNING util.py:315 -- The `process_trial_result` operation took 2.435 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:57:38,531\tWARNING util.py:315 -- Processing trial results took 2.437 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:57:38,532\tWARNING util.py:315 -- The `process_trial_result` operation took 2.438 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_a17695c7_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-57-24/wandb/run-20230719_115741-a17695c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: Syncing run FSR_Trainable_a17695c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a17695c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb:                      mae 223.41874\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb:                     mape 2.3792860427280454e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb:                     rmse 432.58483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb:       time_since_restore 1.80991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb:         time_this_iter_s 1.80991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb:             time_total_s 1.80991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb:                timestamp 1689735456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: 🚀 View run FSR_Trainable_a17695c7 at: https://wandb.ai/seokjin/FSR-prediction/runs/a17695c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573200)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115741-a17695c7/logs\n",
      "2023-07-19 11:57:47,522\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.304 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:57:47,527\tWARNING util.py:315 -- The `process_trial_result` operation took 2.310 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:57:47,531\tWARNING util.py:315 -- Processing trial results took 2.314 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:57:47,533\tWARNING util.py:315 -- The `process_trial_result` operation took 2.317 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_8873a54e_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-57-34/wandb/run-20230719_115750-8873a54e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: Syncing run FSR_Trainable_8873a54e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8873a54e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb:                      mae 257.81974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb:                     mape 130664168.39281\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb:                     rmse 406.61255\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb:       time_since_restore 1.27176\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb:         time_this_iter_s 1.27176\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb:             time_total_s 1.27176\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb:                timestamp 1689735465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: 🚀 View run FSR_Trainable_8873a54e at: https://wandb.ai/seokjin/FSR-prediction/runs/8873a54e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573421)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115750-8873a54e/logs\n",
      "2023-07-19 11:57:58,727\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.013 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:57:58,732\tWARNING util.py:315 -- The `process_trial_result` operation took 2.018 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:57:58,734\tWARNING util.py:315 -- Processing trial results took 2.020 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:57:58,736\tWARNING util.py:315 -- The `process_trial_result` operation took 2.022 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_e25d6d8d_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-57-44/wandb/run-20230719_115801-e25d6d8d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: Syncing run FSR_Trainable_e25d6d8d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e25d6d8d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb:                      mae 213.51049\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb:                     mape 80334547.81122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb:                     rmse 366.17041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb:       time_since_restore 1.46832\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb:         time_this_iter_s 1.46832\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb:             time_total_s 1.46832\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb:                timestamp 1689735476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: 🚀 View run FSR_Trainable_e25d6d8d at: https://wandb.ai/seokjin/FSR-prediction/runs/e25d6d8d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573648)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115801-e25d6d8d/logs\n",
      "2023-07-19 11:58:10,625\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.306 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:58:10,632\tWARNING util.py:315 -- The `process_trial_result` operation took 2.313 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:58:10,634\tWARNING util.py:315 -- Processing trial results took 2.315 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:58:10,640\tWARNING util.py:315 -- The `process_trial_result` operation took 2.322 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_449c1ee7_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-57-55/wandb/run-20230719_115813-449c1ee7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: Syncing run FSR_Trainable_449c1ee7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/449c1ee7\n",
      "2023-07-19 11:58:21,550\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.448 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:58:21,555\tWARNING util.py:315 -- The `process_trial_result` operation took 2.454 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:58:21,557\tWARNING util.py:315 -- Processing trial results took 2.456 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:58:21,559\tWARNING util.py:315 -- The `process_trial_result` operation took 2.458 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_2a4efb97_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-58-06/wandb/run-20230719_115824-2a4efb97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: Syncing run FSR_Trainable_2a4efb97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2a4efb97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb:                      mae 125.03479\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb:                     mape 1.889636813042737e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb:                     rmse 251.72779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb:       time_since_restore 3.86082\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb:         time_this_iter_s 1.8673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb:             time_total_s 3.86082\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb:                timestamp 1689735503\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: 🚀 View run FSR_Trainable_2a4efb97 at: https://wandb.ai/seokjin/FSR-prediction/runs/2a4efb97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574090)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115824-2a4efb97/logs\n",
      "2023-07-19 11:58:33,598\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.494 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:58:33,603\tWARNING util.py:315 -- The `process_trial_result` operation took 2.500 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:58:33,604\tWARNING util.py:315 -- Processing trial results took 2.502 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:58:33,606\tWARNING util.py:315 -- The `process_trial_result` operation took 2.503 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_2f10f206_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-58-17/wandb/run-20230719_115836-2f10f206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: Syncing run FSR_Trainable_2f10f206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2f10f206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb:                      mae 156.16482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb:                     mape 3.383662172562541e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb:                     rmse 308.99512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb:       time_since_restore 2.31052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb:         time_this_iter_s 2.31052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb:             time_total_s 2.31052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb:                timestamp 1689735511\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: 🚀 View run FSR_Trainable_2f10f206 at: https://wandb.ai/seokjin/FSR-prediction/runs/2f10f206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574310)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115836-2f10f206/logs\n",
      "2023-07-19 11:58:43,845\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.356 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:58:43,848\tWARNING util.py:315 -- The `process_trial_result` operation took 2.360 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:58:43,849\tWARNING util.py:315 -- Processing trial results took 2.361 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:58:43,851\tWARNING util.py:315 -- The `process_trial_result` operation took 2.362 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_ca99d759_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-58-28/wandb/run-20230719_115847-ca99d759\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: Syncing run FSR_Trainable_ca99d759\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ca99d759\n",
      "2023-07-19 11:58:57,904\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.574 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:58:57,905\tWARNING util.py:315 -- The `process_trial_result` operation took 2.577 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:58:57,907\tWARNING util.py:315 -- Processing trial results took 2.578 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:58:57,909\tWARNING util.py:315 -- The `process_trial_result` operation took 2.580 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_e9bebd71_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-58-40/wandb/run-20230719_115901-e9bebd71\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Syncing run FSR_Trainable_e9bebd71\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e9bebd71\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb:                      mae █▆▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb:                     mape █▆▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb:                     rmse █▅▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb:         time_this_iter_s █▅▄▄▄▅▆▅▅▅▅▆▆▅▆▅▅▄▄▄▄▅▄▄▄▄▅▃▁▅▄▅▄▆▄▄▂▁▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb:                      mae 97.8343\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb:                     mape 6.958108879169548e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb:                     rmse 194.49886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb:       time_since_restore 1308.0807\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb:         time_this_iter_s 13.21687\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb:             time_total_s 1308.0807\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb:                timestamp 1689735565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: 🚀 View run FSR_Trainable_9d2e5769 at: https://wandb.ai/seokjin/FSR-prediction/runs/9d2e5769\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=562572)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_113727-9d2e5769/logs\n",
      "2023-07-19 11:59:40,733\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.076 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:59:40,738\tWARNING util.py:315 -- The `process_trial_result` operation took 2.081 s, which may be a performance bottleneck.\n",
      "2023-07-19 11:59:40,739\tWARNING util.py:315 -- Processing trial results took 2.082 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 11:59:40,740\tWARNING util.py:315 -- The `process_trial_result` operation took 2.083 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_48a57263_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-58-54/wandb/run-20230719_115944-48a57263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: Syncing run FSR_Trainable_48a57263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/48a57263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb:                      mae █▅▄▄▃▃▃▃▂▁▂▂▂▂▂▃▃▄▄▃▃▃▃▄▃▃▃▃▃▃▃▃▃▃▄▄▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb:                     mape ▇█▇▇▇▆▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb:                     rmse █▂▂▄▁▂▂▃▂▁▄▃▃▃▃▄▆▇▇▇▆▆▆▆▆▆▆▆▆▆▆▇▆▆██▆▆▇▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb:         time_this_iter_s █▄▃▄▄▂▄▆▃▃▂▂▁▂▃▂▃▂▃▄▃▃▁▃▃▁▂▂▂▅▄▂▂▂▂▁▃▂▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb:                      mae 101.12609\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb:                     mape 25486863.18938\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb:                     rmse 196.53605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb:       time_since_restore 68.73626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb:         time_this_iter_s 0.70561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb:             time_total_s 68.73626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb:                timestamp 1689735599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: 🚀 View run FSR_Trainable_ca99d759 at: https://wandb.ai/seokjin/FSR-prediction/runs/ca99d759\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574536)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115847-ca99d759/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb:                      mae █▆▅▄▃▃▃▂▂▃▁▂▂▁▂▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▃▂▃▃▂▂▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb:                     mape █▆▆▅▄▃▃▂▂▃▂▂▂▂▁▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb:                     rmse █▆▅▅▄▄▄▂▂▃▁▂▃▂▃▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▄▅▅▅▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb:         time_this_iter_s █▅▂▁▅▃▂▆█▃▃▅▅▄▄▆▇▅▅▂▄▅▄▃▅▄▆▅▃▃▄▆▄▃▃▃▂▅▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb:                      mae 98.34747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb:                     mape 20067105.82777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb:                     rmse 205.55132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb:       time_since_restore 102.71814\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb:         time_this_iter_s 1.42339\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb:             time_total_s 102.71814\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb:                timestamp 1689735603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: 🚀 View run FSR_Trainable_449c1ee7 at: https://wandb.ai/seokjin/FSR-prediction/runs/449c1ee7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=573872)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115813-449c1ee7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb:                      mae █▅▄▃▃▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▅▅▄▄▄▃▃▃▂▃▃▂▃▃▃▂▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb:                     mape █▇▇▆▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb:                     rmse ▆▄▄▃▃▂▂▁▁▁▁▁▁▂▂▃▃▃▃▄▄██▇▆▆▅▅▅▄▅▅▄▅▆▆▅▆▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb:         time_this_iter_s █▄▃▅▄▂▂▃▃▄▃▃▃▄▃▃▃▄▄▂▂▃▄▄▅▂▅▃▂▃▃▃▂▅▄▂▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "2023-07-19 12:00:14,629\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.040 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:00:14,634\tWARNING util.py:315 -- The `process_trial_result` operation took 2.046 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:00:14,636\tWARNING util.py:315 -- Processing trial results took 2.047 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:00:14,638\tWARNING util.py:315 -- The `process_trial_result` operation took 2.049 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=574760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_1af8c14e_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_11-59-37/wandb/run-20230719_120016-1af8c14e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: Syncing run FSR_Trainable_1af8c14e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1af8c14e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb:                      mae 142.4976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb:                     mape 1.9882403610287347e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb:                     rmse 254.29983\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb:       time_since_restore 2.28131\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb:         time_this_iter_s 2.28131\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb:             time_total_s 2.28131\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb:                timestamp 1689735612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: 🚀 View run FSR_Trainable_1af8c14e at: https://wandb.ai/seokjin/FSR-prediction/runs/1af8c14e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120016-1af8c14e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575308)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_2ed487a6_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-00-10/wandb/run-20230719_120027-2ed487a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: Syncing run FSR_Trainable_2ed487a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2ed487a6\n",
      "2023-07-19 12:00:35,688\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.332 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:00:35,692\tWARNING util.py:315 -- The `process_trial_result` operation took 2.337 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:00:35,694\tWARNING util.py:315 -- Processing trial results took 2.339 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:00:35,696\tWARNING util.py:315 -- The `process_trial_result` operation took 2.341 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_8fa2c49f_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-00-19/wandb/run-20230719_120038-8fa2c49f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: Syncing run FSR_Trainable_8fa2c49f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8fa2c49f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb:                      mae 200.14345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb:                     mape 2.8321062395889094e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb:                     rmse 381.84448\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb:       time_since_restore 13.68079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb:         time_this_iter_s 13.68079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb:             time_total_s 13.68079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb:                timestamp 1689735633\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: 🚀 View run FSR_Trainable_2ed487a6 at: https://wandb.ai/seokjin/FSR-prediction/runs/2ed487a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575531)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120027-2ed487a6/logs\n",
      "2023-07-19 12:00:42,395\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.687 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:00:42,397\tWARNING util.py:315 -- The `process_trial_result` operation took 1.690 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:00:42,399\tWARNING util.py:315 -- Processing trial results took 1.692 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:00:42,400\tWARNING util.py:315 -- The `process_trial_result` operation took 1.693 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb:                      mae 219.92784\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb:                     mape 3.992223720706309e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb:                     rmse 367.1594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb:       time_since_restore 9.91708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb:         time_this_iter_s 9.91708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb:             time_total_s 9.91708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb:                timestamp 1689735640\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: 🚀 View run FSR_Trainable_8fa2c49f at: https://wandb.ai/seokjin/FSR-prediction/runs/8fa2c49f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575745)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120038-8fa2c49f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_1acc5a57_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-00-30/wandb/run-20230719_120050-1acc5a57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: Syncing run FSR_Trainable_1acc5a57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1acc5a57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb:                      mae █▃▁▁▁▁▂▂▂▃▇▃▃▂▂▃▃▃▄▄▃▅▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb:                     mape ▆██▇▅▄▃▃▃▃▃▂▂▂▂▁▂▂▂▂▂▂▂▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb:                     rmse ▃▁▁▁▁▁▃▃▄▄▇▄▅▄▅▆▆▅▇▆▆▇▆▆▆▆▇▇▇▇▇▇████████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb:         time_this_iter_s █▆▅▆▅▃▅▅▄▅▃▅▅▄▅▂▄▂▁▆▃▃▂▁▆▅▄▄▄▄▇▅▄▄▃▅▃▄▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb:                      mae 104.10464\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb:                     mape 28216300.02546\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb:                     rmse 202.79819\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb:       time_since_restore 63.97305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb:         time_this_iter_s 0.67195\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb:             time_total_s 63.97305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb:                timestamp 1689735651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: 🚀 View run FSR_Trainable_48a57263 at: https://wandb.ai/seokjin/FSR-prediction/runs/48a57263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575026)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_115944-48a57263/logs\n",
      "2023-07-19 12:00:58,619\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.537 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:00:58,621\tWARNING util.py:315 -- The `process_trial_result` operation took 2.541 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:00:58,624\tWARNING util.py:315 -- Processing trial results took 2.543 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:00:58,625\tWARNING util.py:315 -- The `process_trial_result` operation took 2.545 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_6a3b5f4a_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-00-42/wandb/run-20230719_120100-6a3b5f4a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: Syncing run FSR_Trainable_6a3b5f4a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6a3b5f4a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb:                      mae 225.34924\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb:                     mape 4.2504096197765254e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb:                     rmse 379.7685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb:       time_since_restore 13.49409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb:         time_this_iter_s 13.49409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb:             time_total_s 13.49409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb:                timestamp 1689735656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: 🚀 View run FSR_Trainable_1acc5a57 at: https://wandb.ai/seokjin/FSR-prediction/runs/1acc5a57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=575981)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120050-1acc5a57/logs\n",
      "2023-07-19 12:01:09,454\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.147 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:01:09,458\tWARNING util.py:315 -- The `process_trial_result` operation took 2.152 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:01:09,462\tWARNING util.py:315 -- Processing trial results took 2.155 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:01:09,463\tWARNING util.py:315 -- The `process_trial_result` operation took 2.157 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:01:11,295\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.771 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:01:11,297\tWARNING util.py:315 -- The `process_trial_result` operation took 1.774 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:01:11,299\tWARNING util.py:315 -- Processing trial results took 1.776 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:01:11,301\tWARNING util.py:315 -- The `process_trial_result` operation took 1.778 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_60033bfd_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-00-53/wandb/run-20230719_120113-60033bfd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Syncing run FSR_Trainable_60033bfd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/60033bfd\n",
      "2023-07-19 12:01:16,091\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.044 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:01:16,095\tWARNING util.py:315 -- The `process_trial_result` operation took 2.048 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:01:16,096\tWARNING util.py:315 -- Processing trial results took 2.049 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:01:16,098\tWARNING util.py:315 -- The `process_trial_result` operation took 2.051 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb:                      mae 238.76717\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb:                     mape 4.826931787397667e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb:                     rmse 389.73288\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb:       time_since_restore 14.92416\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb:         time_this_iter_s 14.92416\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb:             time_total_s 14.92416\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb:                timestamp 1689735668\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: 🚀 View run FSR_Trainable_6a3b5f4a at: https://wandb.ai/seokjin/FSR-prediction/runs/6a3b5f4a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576204)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120100-6a3b5f4a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120100-6a3b5f4a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5ccec0ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5ccec0ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5ccec0ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5ccec0ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5ccec0ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5ccec0ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5ccec0ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5ccec0ba\n",
      "2023-07-19 12:01:26,315\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.029 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:01:26,318\tWARNING util.py:315 -- The `process_trial_result` operation took 2.033 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:01:26,319\tWARNING util.py:315 -- Processing trial results took 2.034 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:01:26,320\tWARNING util.py:315 -- The `process_trial_result` operation took 2.035 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_c47111ed_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-01-12/wandb/run-20230719_120129-c47111ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: Syncing run FSR_Trainable_c47111ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c47111ed\n",
      "2023-07-19 12:01:38,022\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.181 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:01:38,026\tWARNING util.py:315 -- The `process_trial_result` operation took 2.186 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:01:38,029\tWARNING util.py:315 -- Processing trial results took 2.188 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:01:38,030\tWARNING util.py:315 -- The `process_trial_result` operation took 2.189 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_26e3e47a_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-01-23/wandb/run-20230719_120141-26e3e47a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: Syncing run FSR_Trainable_26e3e47a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/26e3e47a\n",
      "2023-07-19 12:01:51,266\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.077 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:01:51,269\tWARNING util.py:315 -- The `process_trial_result` operation took 2.082 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:01:51,272\tWARNING util.py:315 -- Processing trial results took 2.085 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:01:51,273\tWARNING util.py:315 -- The `process_trial_result` operation took 2.086 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_e5ea828b_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-01-34/wandb/run-20230719_120155-e5ea828b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: Syncing run FSR_Trainable_e5ea828b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e5ea828b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb:                      mae █▇▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb:                     mape █▆▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb:                     rmse ▇█▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb:         time_this_iter_s █▄▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb:                timestamp ▁▆▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb:                      mae 109.85797\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb:                     mape 52068401.52631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb:                     rmse 204.03107\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb:       time_since_restore 3.38027\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb:         time_this_iter_s 0.78972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb:             time_total_s 3.38027\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb:                timestamp 1689735713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: 🚀 View run FSR_Trainable_e5ea828b at: https://wandb.ai/seokjin/FSR-prediction/runs/e5ea828b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577334)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120155-e5ea828b/logs\n",
      "2023-07-19 12:02:09,147\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.147 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:02:09,151\tWARNING util.py:315 -- The `process_trial_result` operation took 2.151 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:02:09,152\tWARNING util.py:315 -- Processing trial results took 2.152 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:02:09,153\tWARNING util.py:315 -- The `process_trial_result` operation took 2.153 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_abc8a58e_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-01-48/wandb/run-20230719_120212-abc8a58e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: Syncing run FSR_Trainable_abc8a58e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/abc8a58e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb:                      mae █▆▄▃▂▁▁▁▁▁▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb:                     mape █▇▆▅▅▄▄▃▃▃▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb:                     rmse █▅▃▂▁▁▁▁▁▁▃▃▅▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb:       time_since_restore ▁▂▂▃▃▄▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb:         time_this_iter_s █▇▃▃▃▂▁▃▃▂▂▂▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb:             time_total_s ▁▂▂▃▃▄▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb:                timestamp ▁▃▃▄▄▄▅▅▆▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb:                      mae 108.94101\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb:                     mape 38344857.27505\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb:                     rmse 200.52992\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb:       time_since_restore 11.34989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb:         time_this_iter_s 0.64083\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb:             time_total_s 11.34989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb:                timestamp 1689735739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: 🚀 View run FSR_Trainable_abc8a58e at: https://wandb.ai/seokjin/FSR-prediction/runs/abc8a58e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577573)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120212-abc8a58e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb:                      mae █▅▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb:                     mape ██▆▆▅▅▅▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb:                     rmse █▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb:         time_this_iter_s █▃▁▁▂▂▂▃▂▂▂▂▂▄▄▅▃▂▃▅▄▃▄▄▃▃▃▄▄▃▄▃▃▃▃▄▃▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb:                      mae 102.73033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb:                     mape 29256758.62406\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb:                     rmse 193.105\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb:       time_since_restore 65.5574\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb:         time_this_iter_s 0.68948\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb:             time_total_s 65.5574\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb:                timestamp 1689735750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: 🚀 View run FSR_Trainable_5ccec0ba at: https://wandb.ai/seokjin/FSR-prediction/runs/5ccec0ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576636)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120118-5ccec0ba/logs\n",
      "2023-07-19 12:02:35,930\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.262 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:02:35,934\tWARNING util.py:315 -- The `process_trial_result` operation took 2.267 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:02:35,936\tWARNING util.py:315 -- Processing trial results took 2.269 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:02:35,938\tWARNING util.py:315 -- The `process_trial_result` operation took 2.271 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_b6fe67e4_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-02-06/wandb/run-20230719_120241-b6fe67e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: Syncing run FSR_Trainable_b6fe67e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b6fe67e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb:                      mae █▅▃▂▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb:                     mape █▆▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb:                     rmse █▅▄▃▂▂▁▁▁▁▂▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb:         time_this_iter_s █▅▃▃▁▃█▇▄▇▄▃▃█▇▄█▄▄▄▄▇▅▄▃▄▄▃▅▇▄▄▄▄▂▇▇▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb:                      mae 101.29244\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb:                     mape 28130141.7844\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb:                     rmse 193.97868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb:       time_since_restore 66.81194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb:         time_this_iter_s 0.61209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb:             time_total_s 66.81194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb:                timestamp 1689735763\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: 🚀 View run FSR_Trainable_c47111ed at: https://wandb.ai/seokjin/FSR-prediction/runs/c47111ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=576895)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120129-c47111ed/logs\n",
      "2023-07-19 12:02:49,774\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.144 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:02:49,777\tWARNING util.py:315 -- The `process_trial_result` operation took 2.148 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:02:49,778\tWARNING util.py:315 -- Processing trial results took 2.149 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:02:49,779\tWARNING util.py:315 -- The `process_trial_result` operation took 2.150 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_457c8add_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-02-32/wandb/run-20230719_120253-457c8add\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Syncing run FSR_Trainable_457c8add\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/457c8add\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)03 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb:                      mae █▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb:                     mape █▇▇▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb:                     rmse █▄▃▃▃▃▄▄▃▃▃▃▂▂▂▂▂▂▂▃▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb:         time_this_iter_s █▄▂▄▄▃▄▆▄▃▃▃▄▃▂▅▄▃▅▃▃▂▃▄▂▃▂▂▂▄▄▂▃▂▂▂▅▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb:                      mae 99.48368\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb:                     mape 26880298.74251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb:                     rmse 189.72256\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb:       time_since_restore 67.50375\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb:         time_this_iter_s 0.51796\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb:             time_total_s 67.50375\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb:                timestamp 1689735775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: 🚀 View run FSR_Trainable_26e3e47a at: https://wandb.ai/seokjin/FSR-prediction/runs/26e3e47a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120141-26e3e47a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb:       training_iteration ▁\n",
      "2023-07-19 12:03:03,475\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.971 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:03:03,479\tWARNING util.py:315 -- The `process_trial_result` operation took 1.976 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:03:03,481\tWARNING util.py:315 -- Processing trial results took 1.978 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:03:03,482\tWARNING util.py:315 -- The `process_trial_result` operation took 1.979 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577112)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578069)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_0b7bdef0_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-02-46/wandb/run-20230719_120305-0b7bdef0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: Syncing run FSR_Trainable_0b7bdef0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0b7bdef0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb:                      mae 230.73878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb:                     mape 156708275.96384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb:                     rmse 381.25561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb:       time_since_restore 2.73836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb:         time_this_iter_s 2.73836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb:             time_total_s 2.73836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb:                timestamp 1689735781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: 🚀 View run FSR_Trainable_0b7bdef0 at: https://wandb.ai/seokjin/FSR-prediction/runs/0b7bdef0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120305-0b7bdef0/logs\n",
      "2023-07-19 12:03:13,399\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.005 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:03:13,404\tWARNING util.py:315 -- The `process_trial_result` operation took 2.011 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:03:13,406\tWARNING util.py:315 -- Processing trial results took 2.013 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:03:13,409\tWARNING util.py:315 -- The `process_trial_result` operation took 2.016 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578307)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_43a1d0fd_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-02-58/wandb/run-20230719_120315-43a1d0fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: Syncing run FSR_Trainable_43a1d0fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/43a1d0fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb:                      mae 172.27328\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb:                     mape 1.792952513419394e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb:                     rmse 351.46667\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb:       time_since_restore 2.9324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb:         time_this_iter_s 2.9324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb:             time_total_s 2.9324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb:                timestamp 1689735791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: 🚀 View run FSR_Trainable_43a1d0fd at: https://wandb.ai/seokjin/FSR-prediction/runs/43a1d0fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120315-43a1d0fd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578539)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 12:03:21,014\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.065 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:03:21,018\tWARNING util.py:315 -- The `process_trial_result` operation took 2.070 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:03:21,019\tWARNING util.py:315 -- Processing trial results took 2.071 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:03:21,020\tWARNING util.py:315 -- The `process_trial_result` operation took 2.072 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_553408e6_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-03-08/wandb/run-20230719_120324-553408e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: Syncing run FSR_Trainable_553408e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/553408e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb:                      mae 132.60381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb:                     mape 2.977671648718685e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb:                     rmse 248.61705\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb:       time_since_restore 1.84599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb:         time_this_iter_s 0.85771\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb:             time_total_s 1.84599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb:                timestamp 1689735801\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: 🚀 View run FSR_Trainable_553408e6 at: https://wandb.ai/seokjin/FSR-prediction/runs/553408e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578766)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120324-553408e6/logs\n",
      "2023-07-19 12:03:31,518\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.215 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:03:31,521\tWARNING util.py:315 -- The `process_trial_result` operation took 2.219 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:03:31,523\tWARNING util.py:315 -- Processing trial results took 2.221 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:03:31,525\tWARNING util.py:315 -- The `process_trial_result` operation took 2.223 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_6399e6a9_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-03-18/wandb/run-20230719_120334-6399e6a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: Syncing run FSR_Trainable_6399e6a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6399e6a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)03 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb:                      mae 173.77288\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb:                     mape 57268039.80851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb:                     rmse 308.3965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb:       time_since_restore 0.78101\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb:         time_this_iter_s 0.78101\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb:             time_total_s 0.78101\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb:                timestamp 1689735809\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: 🚀 View run FSR_Trainable_6399e6a9 at: https://wandb.ai/seokjin/FSR-prediction/runs/6399e6a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=578997)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120334-6399e6a9/logs\n",
      "2023-07-19 12:03:42,003\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.410 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:03:42,005\tWARNING util.py:315 -- The `process_trial_result` operation took 2.413 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:03:42,008\tWARNING util.py:315 -- Processing trial results took 2.416 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:03:42,010\tWARNING util.py:315 -- The `process_trial_result` operation took 2.418 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_fcec382d_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-03-28/wandb/run-20230719_120344-fcec382d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Syncing run FSR_Trainable_fcec382d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fcec382d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb:                      mae █▅▄▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb:                     mape █▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb:                     rmse █▆▅▄▃▂▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb:         time_this_iter_s █▅▄▄▄▆▅▄▄▃▃▂▂▄▆▃▁▂▁▃▃▃▂▄▅▂▃▁▂▁▄▃▁▂▂▂▄▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb:                      mae 100.02991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb:                     mape 23423379.16864\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb:                     rmse 190.54484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb:       time_since_restore 58.57696\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb:         time_this_iter_s 0.5576\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb:             time_total_s 58.57696\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb:                timestamp 1689735827\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: 🚀 View run FSR_Trainable_b6fe67e4 at: https://wandb.ai/seokjin/FSR-prediction/runs/b6fe67e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=577833)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120241-b6fe67e4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:03:53,400\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.282 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:03:53,406\tWARNING util.py:315 -- The `process_trial_result` operation took 2.289 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:03:53,408\tWARNING util.py:315 -- Processing trial results took 2.291 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:03:53,411\tWARNING util.py:315 -- The `process_trial_result` operation took 2.294 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb:                      mae █▇▇▆▅▅▄▄▃▃▃▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb:                     mape ███▇▆▆▅▄▄▃▃▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb:                     rmse █▃▆▇▇██▇▇▇▆▄▃▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb:       time_since_restore ▁▂▂▃▃▄▄▅▅▅▆▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb:         time_this_iter_s █▅▃▄▅▄▄▆▃▃▁▃▁▂▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb:             time_total_s ▁▂▂▃▃▄▄▅▅▅▆▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb:                timestamp ▁▃▄▄▄▅▅▅▆▆▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120344-fcec382d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120344-fcec382d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_d41e2bd6_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-03-38/wandb/run-20230719_120356-d41e2bd6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579222)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: Syncing run FSR_Trainable_d41e2bd6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d41e2bd6\n",
      "2023-07-19 12:04:03,545\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.050 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:04:03,549\tWARNING util.py:315 -- The `process_trial_result` operation took 2.056 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:04:03,551\tWARNING util.py:315 -- Processing trial results took 2.058 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:04:03,553\tWARNING util.py:315 -- The `process_trial_result` operation took 2.060 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_826407ff_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-03-50/wandb/run-20230719_120406-826407ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: Syncing run FSR_Trainable_826407ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/826407ff\n",
      "2023-07-19 12:04:15,396\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.103 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:04:15,399\tWARNING util.py:315 -- The `process_trial_result` operation took 2.106 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:04:15,400\tWARNING util.py:315 -- Processing trial results took 2.108 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:04:15,402\tWARNING util.py:315 -- The `process_trial_result` operation took 2.109 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_741520de_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-04-00/wandb/run-20230719_120419-741520de\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: Syncing run FSR_Trainable_741520de\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/741520de\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:04:29,322\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.278 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:04:29,325\tWARNING util.py:315 -- The `process_trial_result` operation took 2.282 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:04:29,327\tWARNING util.py:315 -- Processing trial results took 2.283 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:04:29,328\tWARNING util.py:315 -- The `process_trial_result` operation took 2.284 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb:                      mae █▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▃▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb:                     mape █▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb:                     rmse █▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▂▃▃▄▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb:         time_this_iter_s ▇▆▃▄▂▂▂▃▂▂▃▂▂▁▂▅▄█▆▄▃▃▃▆▄▃▄▃▄▃▃▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb:                      mae 104.6547\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb:                     mape 32354139.8135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb:                     rmse 195.41805\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb:       time_since_restore 21.06476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb:         time_this_iter_s 0.81211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb:             time_total_s 21.06476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb:                timestamp 1689735866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: 🚀 View run FSR_Trainable_826407ff at: https://wandb.ai/seokjin/FSR-prediction/runs/826407ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579684)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120406-826407ff/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_5a6b36a8_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-04-12/wandb/run-20230719_120433-5a6b36a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: Syncing run FSR_Trainable_5a6b36a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5a6b36a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:04:43,354\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.407 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:04:43,357\tWARNING util.py:315 -- The `process_trial_result` operation took 2.412 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:04:43,358\tWARNING util.py:315 -- Processing trial results took 2.413 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:04:43,360\tWARNING util.py:315 -- The `process_trial_result` operation took 2.415 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb:                      mae █▆▄▄▃▂▂▂▁▁▁▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb:                     mape █▅▄▄▃▃▃▃▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb:                     rmse █▅▄▃▂▂▂▂▁▁▁▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb:       time_since_restore ▁▂▂▃▃▄▄▅▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb:         time_this_iter_s ██▆▄▅▄▂▆▂▂▃▅▁▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb:             time_total_s ▁▂▂▃▃▄▄▅▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb:                timestamp ▁▃▃▃▄▅▅▅▅▆▆▇▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb:                      mae 105.39848\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb:                     mape 36471527.61199\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb:                     rmse 194.68722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb:       time_since_restore 11.45058\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb:         time_this_iter_s 0.64375\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb:             time_total_s 11.45058\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb:                timestamp 1689735880\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: 🚀 View run FSR_Trainable_5a6b36a8 at: https://wandb.ai/seokjin/FSR-prediction/runs/5a6b36a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580125)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120433-5a6b36a8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_be819b00_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-04-25/wandb/run-20230719_120447-be819b00\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: Syncing run FSR_Trainable_be819b00\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/be819b00\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb:                      mae █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb:                     mape █▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb:                     rmse █▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb:         time_this_iter_s █▆▆▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb:                timestamp ▁▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb:                      mae 109.73221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb:                     mape 39666871.20422\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb:                     rmse 203.60433\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb:       time_since_restore 3.50384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb:         time_this_iter_s 0.75051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb:             time_total_s 3.50384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb:                timestamp 1689735885\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: 🚀 View run FSR_Trainable_be819b00 at: https://wandb.ai/seokjin/FSR-prediction/runs/be819b00\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580359)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120447-be819b00/logs\n",
      "2023-07-19 12:04:56,229\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.149 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:04:56,232\tWARNING util.py:315 -- The `process_trial_result` operation took 2.154 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:04:56,235\tWARNING util.py:315 -- Processing trial results took 2.157 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:04:56,237\tWARNING util.py:315 -- The `process_trial_result` operation took 2.159 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_7a6d0a42_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-04-40/wandb/run-20230719_120459-7a6d0a42\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: Syncing run FSR_Trainable_7a6d0a42\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7a6d0a42\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:05:10,415\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.802 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:05:10,420\tWARNING util.py:315 -- The `process_trial_result` operation took 1.808 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:05:10,421\tWARNING util.py:315 -- Processing trial results took 1.809 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:05:10,423\tWARNING util.py:315 -- The `process_trial_result` operation took 1.811 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb:                      mae █▄▂▁▁▁▁▁▁▁▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb:                     mape █▆▆▆▆▆▅▅▄▄▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb:                     rmse █▄▁▁▁▁▂▂▂▃▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb:       time_since_restore ▁▂▂▃▃▄▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb:         time_this_iter_s ██▅▄▄▃▂▅▃▁▆▁▂▄▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb:             time_total_s ▁▂▂▃▃▄▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb:                timestamp ▁▃▃▄▄▅▅▅▆▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb:                      mae 106.9206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb:                     mape 41580356.63294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb:                     rmse 194.72143\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb:       time_since_restore 11.28236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb:         time_this_iter_s 0.67103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb:             time_total_s 11.28236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb:                timestamp 1689735906\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: 🚀 View run FSR_Trainable_7a6d0a42 at: https://wandb.ai/seokjin/FSR-prediction/runs/7a6d0a42\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580606)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120459-7a6d0a42/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_b05cbfe8_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-04-53/wandb/run-20230719_120513-b05cbfe8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: Syncing run FSR_Trainable_b05cbfe8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b05cbfe8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb:                      mae █▅▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▁▂▃▃▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb:                     mape █▆▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb:                     rmse █▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▁▅▆▆▆▇▇▇▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb:         time_this_iter_s ▅▂▁▁▁▁▃▄▂▂▂▂▂▄▃▃▄▃▂▅▄▃▅▄▃▄▅▂▄▂▂▃▅▄▃▃▂▄█▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579450)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb:                      mae 186.80636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb:                     mape 71030969.48724\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb:                     rmse 326.7153\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb:       time_since_restore 1.96364\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb:         time_this_iter_s 1.96364\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb:             time_total_s 1.96364\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb:                timestamp 1689735908\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: 🚀 View run FSR_Trainable_b05cbfe8 at: https://wandb.ai/seokjin/FSR-prediction/runs/b05cbfe8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120513-b05cbfe8/logs\n",
      "2023-07-19 12:05:21,872\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.898 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:05:21,873\tWARNING util.py:315 -- The `process_trial_result` operation took 1.902 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:05:21,876\tWARNING util.py:315 -- Processing trial results took 1.906 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:05:21,878\tWARNING util.py:315 -- The `process_trial_result` operation took 1.907 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=580828)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_a1543e18_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-05-06/wandb/run-20230719_120524-a1543e18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: Syncing run FSR_Trainable_a1543e18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a1543e18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb:                      mae 118.88736\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb:                     mape 57459065.0124\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb:                     rmse 217.35366\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb:       time_since_restore 2.51264\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb:         time_this_iter_s 1.24948\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb:             time_total_s 2.51264\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb:                timestamp 1689735923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: 🚀 View run FSR_Trainable_a1543e18 at: https://wandb.ai/seokjin/FSR-prediction/runs/a1543e18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120524-a1543e18/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581076)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 12:05:32,732\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.188 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:05:32,736\tWARNING util.py:315 -- The `process_trial_result` operation took 2.192 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:05:32,738\tWARNING util.py:315 -- Processing trial results took 2.195 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:05:32,740\tWARNING util.py:315 -- The `process_trial_result` operation took 2.197 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_77225a5e_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-05-18/wandb/run-20230719_120535-77225a5e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Syncing run FSR_Trainable_77225a5e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/77225a5e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb:                      mae █▅▃▂▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▃▄▄▄▄▅▅▅▃▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb:                     mape █▅▅▅▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb:                     rmse ▅▄▁▁▁▁▂▂▃▂▂▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▇▇▇▇▇▇▇▄██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb:         time_this_iter_s █▆▅▆▅▅▄▇▅▅▄▅▅▅█▂▅▄▃▅▆▄▅▅▃▄▆▃▄▂▄▁▅▆▂▂▂▂▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb:                      mae 103.00365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb:                     mape 28480628.59055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb:                     rmse 202.89104\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb:       time_since_restore 64.87264\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb:         time_this_iter_s 0.69267\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb:             time_total_s 64.87264\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb:                timestamp 1689735934\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: 🚀 View run FSR_Trainable_741520de at: https://wandb.ai/seokjin/FSR-prediction/runs/741520de\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=579904)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120419-741520de/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb:                      mae █▄▃▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb:                     mape ▂▁▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb:                     rmse █▃▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb:       time_since_restore ▁▂▄▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb:         time_this_iter_s █▇▆▂▂▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb:             time_total_s ▁▂▄▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb:                timestamp ▁▄▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120535-77225a5e/logs\n",
      "2023-07-19 12:05:42,924\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.987 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:05:42,927\tWARNING util.py:315 -- The `process_trial_result` operation took 1.991 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:05:42,928\tWARNING util.py:315 -- Processing trial results took 1.993 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:05:42,930\tWARNING util.py:315 -- The `process_trial_result` operation took 1.994 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581303)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_e7a05b2f_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-05-29/wandb/run-20230719_120545-e7a05b2f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Syncing run FSR_Trainable_e7a05b2f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e7a05b2f\n",
      "2023-07-19 12:05:52,867\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.199 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:05:52,872\tWARNING util.py:315 -- The `process_trial_result` operation took 2.204 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:05:52,874\tWARNING util.py:315 -- Processing trial results took 2.206 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:05:52,875\tWARNING util.py:315 -- The `process_trial_result` operation took 2.208 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_5043e67a_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-05-39/wandb/run-20230719_120555-5043e67a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: Syncing run FSR_Trainable_5043e67a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5043e67a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:06:03,333\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.270 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:06:03,336\tWARNING util.py:315 -- The `process_trial_result` operation took 2.275 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:06:03,339\tWARNING util.py:315 -- Processing trial results took 2.277 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:06:03,341\tWARNING util.py:315 -- The `process_trial_result` operation took 2.279 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb:                      mae 151.70251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb:                     mape 82424650.97611\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb:                     rmse 258.35916\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb:       time_since_restore 0.66096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb:         time_this_iter_s 0.66096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb:             time_total_s 0.66096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb:                timestamp 1689735950\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: 🚀 View run FSR_Trainable_5043e67a at: https://wandb.ai/seokjin/FSR-prediction/runs/5043e67a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581760)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120555-5043e67a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_ae71f7ed_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-05-50/wandb/run-20230719_120606-ae71f7ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: Syncing run FSR_Trainable_ae71f7ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ae71f7ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)03 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb:                      mae 210.82966\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb:                     mape 39115671.34599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb:                     rmse 364.96291\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb:       time_since_restore 0.73286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb:         time_this_iter_s 0.73286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb:             time_total_s 0.73286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb:                timestamp 1689735961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: 🚀 View run FSR_Trainable_ae71f7ed at: https://wandb.ai/seokjin/FSR-prediction/runs/ae71f7ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581980)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120606-ae71f7ed/logs\n",
      "2023-07-19 12:06:13,920\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.160 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:06:13,923\tWARNING util.py:315 -- The `process_trial_result` operation took 2.163 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:06:13,924\tWARNING util.py:315 -- Processing trial results took 2.164 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:06:13,924\tWARNING util.py:315 -- The `process_trial_result` operation took 2.165 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_0d84e070_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-06-00/wandb/run-20230719_120617-0d84e070\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Syncing run FSR_Trainable_0d84e070\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0d84e070\n",
      "2023-07-19 12:06:26,035\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.084 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:06:26,038\tWARNING util.py:315 -- The `process_trial_result` operation took 2.088 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:06:26,039\tWARNING util.py:315 -- Processing trial results took 2.089 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:06:26,041\tWARNING util.py:315 -- The `process_trial_result` operation took 2.091 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_70fc4d25_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-06-10/wandb/run-20230719_120630-70fc4d25\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: Syncing run FSR_Trainable_70fc4d25\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/70fc4d25\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb:                      mae █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb:                     mape █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb:                     rmse █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb:         time_this_iter_s ▅▅█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb:                timestamp ▁▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb:                      mae 109.35604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb:                     mape 41981469.85398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb:                     rmse 203.84476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb:       time_since_restore 3.87543\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb:         time_this_iter_s 0.71582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb:             time_total_s 3.87543\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb:                timestamp 1689735988\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: 🚀 View run FSR_Trainable_70fc4d25 at: https://wandb.ai/seokjin/FSR-prediction/runs/70fc4d25\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582435)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120630-70fc4d25/logs\n",
      "2023-07-19 12:06:40,517\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.068 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:06:40,523\tWARNING util.py:315 -- The `process_trial_result` operation took 2.074 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:06:40,525\tWARNING util.py:315 -- Processing trial results took 2.076 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:06:40,527\tWARNING util.py:315 -- The `process_trial_result` operation took 2.078 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_d26b524c_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-06-23/wandb/run-20230719_120643-d26b524c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: Syncing run FSR_Trainable_d26b524c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d26b524c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb:                      mae 161.15153\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb:                     mape 60974881.21471\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb:                     rmse 290.55587\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb:       time_since_restore 2.57122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb:         time_this_iter_s 2.57122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb:             time_total_s 2.57122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb:                timestamp 1689735998\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: 🚀 View run FSR_Trainable_d26b524c at: https://wandb.ai/seokjin/FSR-prediction/runs/d26b524c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582668)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120643-d26b524c/logs\n",
      "2023-07-19 12:06:52,235\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.069 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:06:52,240\tWARNING util.py:315 -- The `process_trial_result` operation took 2.074 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:06:52,241\tWARNING util.py:315 -- Processing trial results took 2.075 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:06:52,242\tWARNING util.py:315 -- The `process_trial_result` operation took 2.077 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_e4b7bac0_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-06-35/wandb/run-20230719_120654-e4b7bac0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: Syncing run FSR_Trainable_e4b7bac0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e4b7bac0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb:                      mae 174.76205\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb:                     mape 53485882.52552\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb:                     rmse 304.34041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb:       time_since_restore 2.92244\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb:         time_this_iter_s 2.92244\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb:             time_total_s 2.92244\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb:                timestamp 1689736010\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: 🚀 View run FSR_Trainable_e4b7bac0 at: https://wandb.ai/seokjin/FSR-prediction/runs/e4b7bac0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120654-e4b7bac0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:07:01,701\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.808 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:07:01,703\tWARNING util.py:315 -- The `process_trial_result` operation took 1.811 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:07:01,704\tWARNING util.py:315 -- Processing trial results took 1.812 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:07:01,705\tWARNING util.py:315 -- The `process_trial_result` operation took 1.813 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb:                      mae █▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb:                     mape █▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb:                     rmse █▂▃▂▂▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb:         time_this_iter_s █▂▁▂▁▂▃▅▂▂▁▁▂▃▂▂▁▁▄▄▃▃▂▃▂▇▃▃▂▂▅▇▄▃▃▃▅▅▃▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120545-e7a05b2f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=581540)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_439eadae_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-06-47/wandb/run-20230719_120704-439eadae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: Syncing run FSR_Trainable_439eadae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/439eadae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb:                      mae █▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb:                     mape █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb:                     rmse █▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb:         time_this_iter_s █▄▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb:                      mae 108.00998\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb:                     mape 43077792.96291\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb:                     rmse 202.53518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb:       time_since_restore 4.75452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb:         time_this_iter_s 0.95228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb:             time_total_s 4.75452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb:                timestamp 1689736024\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: 🚀 View run FSR_Trainable_439eadae at: https://wandb.ai/seokjin/FSR-prediction/runs/439eadae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120704-439eadae/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583126)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 12:07:12,852\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.812 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:07:12,858\tWARNING util.py:315 -- The `process_trial_result` operation took 1.818 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:07:12,859\tWARNING util.py:315 -- Processing trial results took 1.819 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:07:12,861\tWARNING util.py:315 -- The `process_trial_result` operation took 1.821 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_f3a1c266_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-06-58/wandb/run-20230719_120716-f3a1c266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: Syncing run FSR_Trainable_f3a1c266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f3a1c266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)06 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb:                      mae 233.90019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb:                     mape 177532439.12298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb:                     rmse 377.76394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb:       time_since_restore 1.06651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb:         time_this_iter_s 1.06651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb:             time_total_s 1.06651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb:                timestamp 1689736031\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: 🚀 View run FSR_Trainable_f3a1c266 at: https://wandb.ai/seokjin/FSR-prediction/runs/f3a1c266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583367)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120716-f3a1c266/logs\n",
      "2023-07-19 12:07:23,645\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.320 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:07:23,648\tWARNING util.py:315 -- The `process_trial_result` operation took 2.323 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:07:23,649\tWARNING util.py:315 -- Processing trial results took 2.324 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:07:23,650\tWARNING util.py:315 -- The `process_trial_result` operation took 2.325 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_78b2c5a7_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-07-10/wandb/run-20230719_120726-78b2c5a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: Syncing run FSR_Trainable_78b2c5a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/78b2c5a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb:                      mae 121.44387\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb:                     mape 50792692.13785\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb:                     rmse 222.37802\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb:       time_since_restore 1.4464\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb:         time_this_iter_s 0.75959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb:             time_total_s 1.4464\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb:                timestamp 1689736044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: 🚀 View run FSR_Trainable_78b2c5a7 at: https://wandb.ai/seokjin/FSR-prediction/runs/78b2c5a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583587)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120726-78b2c5a7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb:                      mae █▄▃▂▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▂▂▃▃▃▂▃▂▁▃▃▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb:                     mape █▇▆▆▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb:                     rmse █▃▁▁▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▅▆▆▆▆▆▇▆▁▇▇▆▆▆▆▆▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb:         time_this_iter_s █▅▃▃▃▃▄▇▃▄▄▂▆▄▅▃▄▇█▆▃▃▂▇▄▅▃▃▁▄▃▃▂▂▂▅▄▁▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120617-0d84e070/logs\n",
      "2023-07-19 12:07:34,052\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.917 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:07:34,054\tWARNING util.py:315 -- The `process_trial_result` operation took 1.920 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:07:34,058\tWARNING util.py:315 -- Processing trial results took 1.924 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:07:34,059\tWARNING util.py:315 -- The `process_trial_result` operation took 1.925 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=582210)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_9416fb3d_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-07-20/wandb/run-20230719_120737-9416fb3d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: Syncing run FSR_Trainable_9416fb3d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9416fb3d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:07:44,660\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.951 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:07:44,663\tWARNING util.py:315 -- The `process_trial_result` operation took 1.956 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:07:44,664\tWARNING util.py:315 -- Processing trial results took 1.957 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:07:44,665\tWARNING util.py:315 -- The `process_trial_result` operation took 1.958 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb:                      mae █▇▆▅▄▄▃▃▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb:                     mape █▆▆▅▅▄▄▄▃▃▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb:                     rmse █▇▆▄▃▃▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb:       time_since_restore ▁▂▂▃▃▄▄▄▅▅▆▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb:         time_this_iter_s █▄▂▂▂▁▂▂▂▂▁▂▂▁▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb:             time_total_s ▁▂▂▃▃▄▄▄▅▅▆▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb:                timestamp ▁▃▃▃▄▄▅▅▆▆▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb:                      mae 103.73465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb:                     mape 38769086.46073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb:                     rmse 193.73912\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb:       time_since_restore 8.7429\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb:         time_this_iter_s 0.61484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb:             time_total_s 8.7429\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb:                timestamp 1689736061\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: 🚀 View run FSR_Trainable_9416fb3d at: https://wandb.ai/seokjin/FSR-prediction/runs/9416fb3d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=583819)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120737-9416fb3d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_69bb004d_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-07-31/wandb/run-20230719_120747-69bb004d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: Syncing run FSR_Trainable_69bb004d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/69bb004d\n",
      "2023-07-19 12:07:55,728\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.325 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:07:55,732\tWARNING util.py:315 -- The `process_trial_result` operation took 2.329 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:07:55,733\tWARNING util.py:315 -- Processing trial results took 2.331 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:07:55,734\tWARNING util.py:315 -- The `process_trial_result` operation took 2.332 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_769e78c7_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-07-41/wandb/run-20230719_120758-769e78c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: Syncing run FSR_Trainable_769e78c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/769e78c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:08:03,828\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.221 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:08:03,831\tWARNING util.py:315 -- The `process_trial_result` operation took 2.225 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:08:03,833\tWARNING util.py:315 -- Processing trial results took 2.227 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:08:03,834\tWARNING util.py:315 -- The `process_trial_result` operation took 2.228 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb:                      mae 119.46629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb:                     mape 61498685.49312\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb:                     rmse 215.95062\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb:       time_since_restore 1.91037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb:         time_this_iter_s 0.93214\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb:             time_total_s 1.91037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb:                timestamp 1689736076\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: 🚀 View run FSR_Trainable_769e78c7 at: https://wandb.ai/seokjin/FSR-prediction/runs/769e78c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584276)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120758-769e78c7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_37f4b8f9_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-07-52/wandb/run-20230719_120807-37f4b8f9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: Syncing run FSR_Trainable_37f4b8f9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/37f4b8f9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:08:15,383\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.109 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:08:15,386\tWARNING util.py:315 -- The `process_trial_result` operation took 2.113 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:08:15,390\tWARNING util.py:315 -- Processing trial results took 2.116 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:08:15,392\tWARNING util.py:315 -- The `process_trial_result` operation took 2.119 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb:                      mae █▆▄▄▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb:                     mape █▄▃▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb:                     rmse █▇▇▇▅▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb:         time_this_iter_s █▆▆▂▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb:                timestamp ▁▄▅▆▆▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb:                      mae 105.08021\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb:                     mape 46759360.39011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb:                     rmse 198.98419\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb:       time_since_restore 5.5123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb:         time_this_iter_s 0.61612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb:             time_total_s 5.5123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb:                timestamp 1689736088\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: 🚀 View run FSR_Trainable_37f4b8f9 at: https://wandb.ai/seokjin/FSR-prediction/runs/37f4b8f9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584456)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120807-37f4b8f9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_10d553c3_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_12-08-00/wandb/run-20230719_120818-10d553c3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: Syncing run FSR_Trainable_10d553c3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/10d553c3\n",
      "2023-07-19 12:08:28,532\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.869 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:08:28,534\tWARNING util.py:315 -- The `process_trial_result` operation took 1.872 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:08:28,536\tWARNING util.py:315 -- Processing trial results took 1.873 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:08:28,538\tWARNING util.py:315 -- The `process_trial_result` operation took 1.876 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_11-36-24/FSR_Trainable_ac03a684_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,inde_2023-07-19_12-08-12/wandb/run-20230719_120830-ac03a684\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: Syncing run FSR_Trainable_ac03a684\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ac03a684\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb:                      mae 180.7551\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb:                     mape 3.459147158615063e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb:                     rmse 269.33118\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb:       time_since_restore 2.23497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb:         time_this_iter_s 2.23497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb:             time_total_s 2.23497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb:                timestamp 1689736106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: 🚀 View run FSR_Trainable_ac03a684 at: https://wandb.ai/seokjin/FSR-prediction/runs/ac03a684\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584917)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120830-ac03a684/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb:                      mae █▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb:                     mape █▇▇▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb:                     rmse █▃▂▂▂▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb:         time_this_iter_s █▂▂▂▂▂▂▃▂▂▂▄▄▃▄▂▁▄▃▃▄▂▂▃▃▃▃▂▃▁▂▁▂▂▂▂▁▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb:                      mae 100.67234\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb:                     mape 31085340.97686\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb:                     rmse 193.23104\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb:       time_since_restore 54.09164\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb:         time_this_iter_s 0.50461\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb:             time_total_s 54.09164\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb:                timestamp 1689736127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: 🚀 View run FSR_Trainable_69bb004d at: https://wandb.ai/seokjin/FSR-prediction/runs/69bb004d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584046)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120747-69bb004d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb:                      mae █▃▂▂▂▂▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb:                     mape █▅▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb:                     rmse █▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb:         time_this_iter_s █▄▄▅▃▄▅▅▄▂▃▃▂▄▂▃▂▃▃▂▃▂▃▂▂▁▂▁▂▁▁▁▁▂▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb:                      mae 106.06569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb:                     mape 1.1448030193898258e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb:                     rmse 188.68839\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb:       time_since_restore 45.36968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb:         time_this_iter_s 0.36997\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb:             time_total_s 45.36968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb:                timestamp 1689736143\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: 🚀 View run FSR_Trainable_10d553c3 at: https://wandb.ai/seokjin/FSR-prediction/runs/10d553c3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=584690)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_120818-10d553c3/logs\n",
      "2023-07-19 12:09:07,807\tINFO tune.py:1111 -- Total run time: 1959.29 seconds (1955.07 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
