{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task4\n",
    "\n",
    "Index_X = FSR_for_force, FSR_for_coord\n",
    "\n",
    "Index_y = force, x_coord, y_coord\n",
    "\n",
    "Data = Splited by Subject\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-07-19_08-33-10/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-07-19_08-33-10\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "111.187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.LSTM'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    imputer = trial.suggest_categorical('imputer', ['sklearn.impute.SimpleImputer'])\n",
    "    if imputer == 'sklearn.impute.SimpleImputer':\n",
    "        trial.suggest_categorical('imputer_args/strategy', [\n",
    "            'mean',\n",
    "            'median',\n",
    "        ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': ['FSR_for_force', 'FSR_for_coord'],\n",
    "        'index_y': ['force', 'x_coord', 'y_coord'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_subject'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-19 08:33:10,671] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 08:33:12,929\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-07-19 08:33:14,296\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-19 09:23:29</td></tr>\n",
       "<tr><td>Running for: </td><td>00:50:15.07        </td></tr>\n",
       "<tr><td>Memory:      </td><td>2.8/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=100<br>Bracket: Iter 64.000: -119.15314128892258 | Iter 32.000: -121.20905056842876 | Iter 16.000: -120.30765928671032 | Iter 8.000: -129.23299230437308 | Iter 4.000: -159.12646019655867 | Iter 2.000: -187.20567489572036 | Iter 1.000: -218.7959992820755<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>imputer             </th><th>imputer_args/strateg\n",
       "y       </th><th>index_X             </th><th>index_y             </th><th>model         </th><th style=\"text-align: right;\">    model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">  model_args/num_layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">       mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_09940689</td><td>TERMINATED</td><td>172.26.215.93:479465</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_ad80</td><td>[&#x27;force&#x27;, &#x27;x_co_ab40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000144545</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      131.546   </td><td style=\"text-align: right;\">154.172</td><td style=\"text-align: right;\"> 50.4564</td><td style=\"text-align: right;\">2.15791e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_d64c9d11</td><td>TERMINATED</td><td>172.26.215.93:479529</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_3d00</td><td>[&#x27;force&#x27;, &#x27;x_co_2740</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00146727 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.97722 </td><td style=\"text-align: right;\">281.784</td><td style=\"text-align: right;\">118.581 </td><td style=\"text-align: right;\">2.41848e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_98881a9b</td><td>TERMINATED</td><td>172.26.215.93:479723</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_3ec0</td><td>[&#x27;force&#x27;, &#x27;x_co_0ec0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000116972</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       38.5264  </td><td style=\"text-align: right;\">222.635</td><td style=\"text-align: right;\"> 75.1146</td><td style=\"text-align: right;\">3.63495e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ac61c658</td><td>TERMINATED</td><td>172.26.215.93:479896</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_5200</td><td>[&#x27;force&#x27;, &#x27;x_co_7780</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.3457e-05 </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       24.3142  </td><td style=\"text-align: right;\">224.204</td><td style=\"text-align: right;\"> 81.6887</td><td style=\"text-align: right;\">2.97289e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_57412e6a</td><td>TERMINATED</td><td>172.26.215.93:480189</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_13c0</td><td>[&#x27;force&#x27;, &#x27;x_co_6040</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000281819</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        6.32065 </td><td style=\"text-align: right;\">225.423</td><td style=\"text-align: right;\"> 83.1161</td><td style=\"text-align: right;\">5.9746e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_c7074d67</td><td>TERMINATED</td><td>172.26.215.93:480462</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_7200</td><td>[&#x27;force&#x27;, &#x27;x_co_66c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000310956</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.46511 </td><td style=\"text-align: right;\">230.572</td><td style=\"text-align: right;\"> 64.8393</td><td style=\"text-align: right;\">9.92434e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_5ea282f6</td><td>TERMINATED</td><td>172.26.215.93:480665</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_86c0</td><td>[&#x27;force&#x27;, &#x27;x_co_27c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00152018 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.7843  </td><td style=\"text-align: right;\">239.912</td><td style=\"text-align: right;\"> 64.3742</td><td style=\"text-align: right;\">2.87111e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_86a93f9f</td><td>TERMINATED</td><td>172.26.215.93:480904</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_fa00</td><td>[&#x27;force&#x27;, &#x27;x_co_ff80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00391285 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">       28.3777  </td><td style=\"text-align: right;\">227.591</td><td style=\"text-align: right;\"> 70.3245</td><td style=\"text-align: right;\">1.14367e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_4a5175af</td><td>TERMINATED</td><td>172.26.215.93:481108</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_74c0</td><td>[&#x27;force&#x27;, &#x27;x_co_c2c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0509515  </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        3.44891 </td><td style=\"text-align: right;\">232.475</td><td style=\"text-align: right;\"> 69.9643</td><td style=\"text-align: right;\">3.85836e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_bb111c99</td><td>TERMINATED</td><td>172.26.215.93:481324</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_69c0</td><td>[&#x27;force&#x27;, &#x27;x_co_f540</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00154526 </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      308.592   </td><td style=\"text-align: right;\">143.842</td><td style=\"text-align: right;\"> 43.6017</td><td style=\"text-align: right;\">1.62353e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_abcd8262</td><td>TERMINATED</td><td>172.26.215.93:481541</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_aa40</td><td>[&#x27;force&#x27;, &#x27;x_co_8580</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0054976  </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       74.1069  </td><td style=\"text-align: right;\">226.38 </td><td style=\"text-align: right;\"> 82.2345</td><td style=\"text-align: right;\">3.16935e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_1be8b213</td><td>TERMINATED</td><td>172.26.215.93:481767</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_6640</td><td>[&#x27;force&#x27;, &#x27;x_co_0e80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0867523  </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        6.74552 </td><td style=\"text-align: right;\">355.071</td><td style=\"text-align: right;\">111.645 </td><td style=\"text-align: right;\">6.12039e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_fb39f723</td><td>TERMINATED</td><td>172.26.215.93:482056</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_2780</td><td>[&#x27;force&#x27;, &#x27;x_co_6180</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00219156 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">      593.032   </td><td style=\"text-align: right;\">147.814</td><td style=\"text-align: right;\"> 46.5984</td><td style=\"text-align: right;\">9.96435e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_5545c6d4</td><td>TERMINATED</td><td>172.26.215.93:482253</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_1ac0</td><td>[&#x27;force&#x27;, &#x27;x_co_7040</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000131825</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">      165.106   </td><td style=\"text-align: right;\">218.061</td><td style=\"text-align: right;\"> 65.9405</td><td style=\"text-align: right;\">3.32811e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_259a7c0c</td><td>TERMINATED</td><td>172.26.215.93:482542</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_6040</td><td>[&#x27;force&#x27;, &#x27;x_co_9900</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.24422e-05</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        7.81073 </td><td style=\"text-align: right;\">223.149</td><td style=\"text-align: right;\"> 78.5474</td><td style=\"text-align: right;\">3.02362e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_c29aa22c</td><td>TERMINATED</td><td>172.26.215.93:482752</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_8180</td><td>[&#x27;force&#x27;, &#x27;x_co_33c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        6.08563e-05</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.58522 </td><td style=\"text-align: right;\">228.428</td><td style=\"text-align: right;\"> 83.1427</td><td style=\"text-align: right;\">6.10903e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_8ad9ac30</td><td>TERMINATED</td><td>172.26.215.93:482980</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_b880</td><td>[&#x27;force&#x27;, &#x27;x_co_1540</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        8.35071e-05</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.91953 </td><td style=\"text-align: right;\">232.925</td><td style=\"text-align: right;\"> 83.9064</td><td style=\"text-align: right;\">3.74142e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_48d43b42</td><td>TERMINATED</td><td>172.26.215.93:483198</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_ae40</td><td>[&#x27;force&#x27;, &#x27;x_co_a980</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000101494</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.55829 </td><td style=\"text-align: right;\">236.837</td><td style=\"text-align: right;\"> 82.1882</td><td style=\"text-align: right;\">2.37527e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_7f2a80f4</td><td>TERMINATED</td><td>172.26.215.93:483421</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_5740</td><td>[&#x27;force&#x27;, &#x27;x_co_28c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        8.94102e-05</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.66041 </td><td style=\"text-align: right;\">233.291</td><td style=\"text-align: right;\"> 78.0503</td><td style=\"text-align: right;\">3.24735e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_42a6be99</td><td>TERMINATED</td><td>172.26.215.93:483646</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_2200</td><td>[&#x27;force&#x27;, &#x27;x_co_3440</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000316443</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      673.35    </td><td style=\"text-align: right;\">129.427</td><td style=\"text-align: right;\"> 39.9274</td><td style=\"text-align: right;\">1.71126e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_acce6591</td><td>TERMINATED</td><td>172.26.215.93:483918</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_56c0</td><td>[&#x27;force&#x27;, &#x27;x_co_9b80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        2.71078e-05</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       10.8384  </td><td style=\"text-align: right;\">256.391</td><td style=\"text-align: right;\"> 87.017 </td><td style=\"text-align: right;\">1.01134e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_fd6c7c3c</td><td>TERMINATED</td><td>172.26.215.93:484146</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_61c0</td><td>[&#x27;force&#x27;, &#x27;x_co_3100</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        2.95975e-05</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       10.5931  </td><td style=\"text-align: right;\">279.063</td><td style=\"text-align: right;\"> 82.1415</td><td style=\"text-align: right;\">5.99127e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_ba00398c</td><td>TERMINATED</td><td>172.26.215.93:484378</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_9580</td><td>[&#x27;force&#x27;, &#x27;x_co_cc40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000356691</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.48324 </td><td style=\"text-align: right;\">235.058</td><td style=\"text-align: right;\"> 64.319 </td><td style=\"text-align: right;\">1.18936e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_8dc179df</td><td>TERMINATED</td><td>172.26.215.93:484568</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_75c0</td><td>[&#x27;force&#x27;, &#x27;x_co_72c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000407984</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        4.76936 </td><td style=\"text-align: right;\">224.667</td><td style=\"text-align: right;\"> 64.9567</td><td style=\"text-align: right;\">1.12691e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_d4fa06b8</td><td>TERMINATED</td><td>172.26.215.93:484809</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_77c0</td><td>[&#x27;force&#x27;, &#x27;x_co_1780</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000486937</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        9.35918 </td><td style=\"text-align: right;\">223.066</td><td style=\"text-align: right;\"> 63.9827</td><td style=\"text-align: right;\">1.46303e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_7322ae35</td><td>TERMINATED</td><td>172.26.215.93:485005</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_c2c0</td><td>[&#x27;force&#x27;, &#x27;x_co_4580</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000639547</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      306.286   </td><td style=\"text-align: right;\">141.476</td><td style=\"text-align: right;\"> 46.0175</td><td style=\"text-align: right;\">2.33519e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ecf589ea</td><td>TERMINATED</td><td>172.26.215.93:485231</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_7200</td><td>[&#x27;force&#x27;, &#x27;x_co_6ec0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000179378</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        6.22457 </td><td style=\"text-align: right;\">224.399</td><td style=\"text-align: right;\"> 80.7052</td><td style=\"text-align: right;\">5.11766e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_9ab979db</td><td>TERMINATED</td><td>172.26.215.93:485501</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_65c0</td><td>[&#x27;force&#x27;, &#x27;x_co_81c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000821317</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">      169.96    </td><td style=\"text-align: right;\">133.998</td><td style=\"text-align: right;\"> 40.9583</td><td style=\"text-align: right;\">1.54286e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_3c823ffc</td><td>TERMINATED</td><td>172.26.215.93:485799</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_3900</td><td>[&#x27;force&#x27;, &#x27;x_co_7580</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00088071 </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">      102.18    </td><td style=\"text-align: right;\">175.678</td><td style=\"text-align: right;\"> 54.6274</td><td style=\"text-align: right;\">1.50437e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e93de5c7</td><td>TERMINATED</td><td>172.26.215.93:486045</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_0180</td><td>[&#x27;force&#x27;, &#x27;x_co_29c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000698581</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">       13.3104  </td><td style=\"text-align: right;\">202.021</td><td style=\"text-align: right;\"> 67.2259</td><td style=\"text-align: right;\">2.56076e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_62dc9803</td><td>TERMINATED</td><td>172.26.215.93:486276</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_5680</td><td>[&#x27;force&#x27;, &#x27;x_co_8780</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000816707</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.93792 </td><td style=\"text-align: right;\">225.513</td><td style=\"text-align: right;\"> 79.4044</td><td style=\"text-align: right;\">4.14485e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_6e372da9</td><td>TERMINATED</td><td>172.26.215.93:486455</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_5900</td><td>[&#x27;force&#x27;, &#x27;x_co_72c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00103383 </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       39.4812  </td><td style=\"text-align: right;\">186.514</td><td style=\"text-align: right;\"> 58.0613</td><td style=\"text-align: right;\">1.95674e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_aacfb51a</td><td>TERMINATED</td><td>172.26.215.93:486680</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_de80</td><td>[&#x27;force&#x27;, &#x27;x_co_f100</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00341342 </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      262.336   </td><td style=\"text-align: right;\">133.481</td><td style=\"text-align: right;\"> 39.4187</td><td style=\"text-align: right;\">1.2943e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_c5ab6926</td><td>TERMINATED</td><td>172.26.215.93:486900</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_46c0</td><td>[&#x27;force&#x27;, &#x27;x_co_7440</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00379425 </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      262.185   </td><td style=\"text-align: right;\">129.018</td><td style=\"text-align: right;\"> 40.5674</td><td style=\"text-align: right;\">1.6826e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_07f295fa</td><td>TERMINATED</td><td>172.26.215.93:487178</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_6380</td><td>[&#x27;force&#x27;, &#x27;x_co_5dc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0034108  </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       80.6083  </td><td style=\"text-align: right;\">161.715</td><td style=\"text-align: right;\"> 50.3421</td><td style=\"text-align: right;\">2.14025e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_630e06a5</td><td>TERMINATED</td><td>172.26.215.93:487447</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_f540</td><td>[&#x27;force&#x27;, &#x27;x_co_f180</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00246802 </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">       49.9966  </td><td style=\"text-align: right;\">198.571</td><td style=\"text-align: right;\"> 74.5025</td><td style=\"text-align: right;\">4.6722e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_f0cdf92d</td><td>TERMINATED</td><td>172.26.215.93:487688</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_2980</td><td>[&#x27;force&#x27;, &#x27;x_co_82c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00216817 </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">       57.7292  </td><td style=\"text-align: right;\">208.231</td><td style=\"text-align: right;\"> 77.0928</td><td style=\"text-align: right;\">5.41267e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_3b8ffc8c</td><td>TERMINATED</td><td>172.26.215.93:487870</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_86c0</td><td>[&#x27;force&#x27;, &#x27;x_co_8700</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000202681</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.96987 </td><td style=\"text-align: right;\">225.1  </td><td style=\"text-align: right;\"> 83.5827</td><td style=\"text-align: right;\">6.73128e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_72b7c922</td><td>TERMINATED</td><td>172.26.215.93:488118</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_2480</td><td>[&#x27;force&#x27;, &#x27;x_co_3600</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00021506 </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        7.66907 </td><td style=\"text-align: right;\">218.152</td><td style=\"text-align: right;\"> 75.5518</td><td style=\"text-align: right;\">4.58737e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_8cb3727c</td><td>TERMINATED</td><td>172.26.215.93:488371</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_2300</td><td>[&#x27;force&#x27;, &#x27;x_co_53c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000655429</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       86.0759  </td><td style=\"text-align: right;\">160.628</td><td style=\"text-align: right;\"> 48.9927</td><td style=\"text-align: right;\">2.04099e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_76612c31</td><td>TERMINATED</td><td>172.26.215.93:488548</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_1e00</td><td>[&#x27;force&#x27;, &#x27;x_co_2680</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000568837</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      110.527   </td><td style=\"text-align: right;\">129.73 </td><td style=\"text-align: right;\"> 39.5344</td><td style=\"text-align: right;\">1.54473e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_aea4e0a6</td><td>TERMINATED</td><td>172.26.215.93:488795</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_ff80</td><td>[&#x27;force&#x27;, &#x27;x_co_c7c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000621024</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        5.4496  </td><td style=\"text-align: right;\">224.748</td><td style=\"text-align: right;\"> 78.0087</td><td style=\"text-align: right;\">1.48364e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_dbfbbd1b</td><td>TERMINATED</td><td>172.26.215.93:489004</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_92c0</td><td>[&#x27;force&#x27;, &#x27;x_co_9e80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00678033 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.13532 </td><td style=\"text-align: right;\">332.48 </td><td style=\"text-align: right;\">126.624 </td><td style=\"text-align: right;\">1.76901e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_640d99b6</td><td>TERMINATED</td><td>172.26.215.93:489251</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_cfc0</td><td>[&#x27;force&#x27;, &#x27;x_co_0dc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00621693 </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.44092 </td><td style=\"text-align: right;\">229.98 </td><td style=\"text-align: right;\"> 77.7888</td><td style=\"text-align: right;\">5.08847e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_35b09d69</td><td>TERMINATED</td><td>172.26.215.93:489476</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_55c0</td><td>[&#x27;force&#x27;, &#x27;x_co_cc80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00138554 </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      151.016   </td><td style=\"text-align: right;\">136.652</td><td style=\"text-align: right;\"> 41.0292</td><td style=\"text-align: right;\">1.5236e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_fa2f16e5</td><td>TERMINATED</td><td>172.26.215.93:489687</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_2b80</td><td>[&#x27;force&#x27;, &#x27;x_co_3480</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00143561 </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.08564 </td><td style=\"text-align: right;\">226.694</td><td style=\"text-align: right;\"> 80.8332</td><td style=\"text-align: right;\">5.196e+07  </td></tr>\n",
       "<tr><td>FSR_Trainable_cd040040</td><td>TERMINATED</td><td>172.26.215.93:489936</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_ad00</td><td>[&#x27;force&#x27;, &#x27;x_co_9f80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00108384 </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        9.37257 </td><td style=\"text-align: right;\">193.325</td><td style=\"text-align: right;\"> 63.383 </td><td style=\"text-align: right;\">2.60354e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_7e662eaf</td><td>TERMINATED</td><td>172.26.215.93:490137</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_5480</td><td>[&#x27;force&#x27;, &#x27;x_co_5400</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000342817</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      111.697   </td><td style=\"text-align: right;\">124.381</td><td style=\"text-align: right;\"> 39.1489</td><td style=\"text-align: right;\">1.51666e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f69afa43</td><td>TERMINATED</td><td>172.26.215.93:490375</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_fb00</td><td>[&#x27;force&#x27;, &#x27;x_co_f900</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0102863  </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      112.002   </td><td style=\"text-align: right;\">125.223</td><td style=\"text-align: right;\"> 38.131 </td><td style=\"text-align: right;\">1.20182e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_24ff1ea4</td><td>TERMINATED</td><td>172.26.215.93:490662</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_8700</td><td>[&#x27;force&#x27;, &#x27;x_co_8f40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000264799</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      426.722   </td><td style=\"text-align: right;\">114.278</td><td style=\"text-align: right;\"> 34.4625</td><td style=\"text-align: right;\">1.09237e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_3e4d0715</td><td>TERMINATED</td><td>172.26.215.93:490942</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_0e40</td><td>[&#x27;force&#x27;, &#x27;x_co_1540</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000252935</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      433.153   </td><td style=\"text-align: right;\">120.881</td><td style=\"text-align: right;\"> 37.7526</td><td style=\"text-align: right;\">1.3721e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_a35d63f5</td><td>TERMINATED</td><td>172.26.215.93:491127</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_7b40</td><td>[&#x27;force&#x27;, &#x27;x_co_5bc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000294597</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      436.06    </td><td style=\"text-align: right;\">120.872</td><td style=\"text-align: right;\"> 37.2553</td><td style=\"text-align: right;\">1.27553e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_c486b8fd</td><td>TERMINATED</td><td>172.26.215.93:491349</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_2d00</td><td>[&#x27;force&#x27;, &#x27;x_co_3900</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00942029 </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        4.7304  </td><td style=\"text-align: right;\">225.538</td><td style=\"text-align: right;\"> 82.5653</td><td style=\"text-align: right;\">5.63303e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e94e0aaf</td><td>TERMINATED</td><td>172.26.215.93:491612</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_ee00</td><td>[&#x27;force&#x27;, &#x27;x_co_de40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0109865  </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        5.77207 </td><td style=\"text-align: right;\">225.191</td><td style=\"text-align: right;\"> 80.0048</td><td style=\"text-align: right;\">4.38146e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_1d84a1d6</td><td>TERMINATED</td><td>172.26.215.93:491822</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_ee80</td><td>[&#x27;force&#x27;, &#x27;x_co_d280</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000289588</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      217.311   </td><td style=\"text-align: right;\">111.187</td><td style=\"text-align: right;\"> 34.8554</td><td style=\"text-align: right;\">1.23459e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_c5ef8b5a</td><td>TERMINATED</td><td>172.26.215.93:492169</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_24c0</td><td>[&#x27;force&#x27;, &#x27;x_co_1e00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000292422</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       76.016   </td><td style=\"text-align: right;\">111.666</td><td style=\"text-align: right;\"> 35.9203</td><td style=\"text-align: right;\">1.96584e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_1a2896b8</td><td>TERMINATED</td><td>172.26.215.93:492427</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_1b40</td><td>[&#x27;force&#x27;, &#x27;x_co_3780</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000273912</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      218.893   </td><td style=\"text-align: right;\">117.407</td><td style=\"text-align: right;\"> 36.562 </td><td style=\"text-align: right;\">1.18541e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_724a46ef</td><td>TERMINATED</td><td>172.26.215.93:492661</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_9480</td><td>[&#x27;force&#x27;, &#x27;x_co_b500</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000289497</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      220.117   </td><td style=\"text-align: right;\">112.599</td><td style=\"text-align: right;\"> 35.8306</td><td style=\"text-align: right;\">1.18975e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_4978f3da</td><td>TERMINATED</td><td>172.26.215.93:492909</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_3880</td><td>[&#x27;force&#x27;, &#x27;x_co_2780</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000250764</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      214.192   </td><td style=\"text-align: right;\">117.2  </td><td style=\"text-align: right;\"> 36.8279</td><td style=\"text-align: right;\">1.26991e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_02392f30</td><td>TERMINATED</td><td>172.26.215.93:493086</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_9800</td><td>[&#x27;force&#x27;, &#x27;x_co_a980</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000254052</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.02488 </td><td style=\"text-align: right;\">230.717</td><td style=\"text-align: right;\"> 86.0456</td><td style=\"text-align: right;\">1.87093e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_204d60e5</td><td>TERMINATED</td><td>172.26.215.93:493335</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_1400</td><td>[&#x27;force&#x27;, &#x27;x_co_3c40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000262242</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.98505 </td><td style=\"text-align: right;\">223.923</td><td style=\"text-align: right;\"> 80.2766</td><td style=\"text-align: right;\">1.74272e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_76736994</td><td>TERMINATED</td><td>172.26.215.93:493559</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_b980</td><td>[&#x27;force&#x27;, &#x27;x_co_ba40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000152451</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      211.31    </td><td style=\"text-align: right;\">111.712</td><td style=\"text-align: right;\"> 35.6804</td><td style=\"text-align: right;\">1.23625e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_cf9095d3</td><td>TERMINATED</td><td>172.26.215.93:493868</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_53c0</td><td>[&#x27;force&#x27;, &#x27;x_co_5100</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00014716 </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      214.257   </td><td style=\"text-align: right;\">116.52 </td><td style=\"text-align: right;\"> 36.536 </td><td style=\"text-align: right;\">1.47632e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_0c74399f</td><td>TERMINATED</td><td>172.26.215.93:494103</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_0240</td><td>[&#x27;force&#x27;, &#x27;x_co_1cc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000142167</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      214.733   </td><td style=\"text-align: right;\">114.125</td><td style=\"text-align: right;\"> 36.0741</td><td style=\"text-align: right;\">1.33861e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_889e4985</td><td>TERMINATED</td><td>172.26.215.93:494356</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_4580</td><td>[&#x27;force&#x27;, &#x27;x_co_6c40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000139891</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      210.22    </td><td style=\"text-align: right;\">124.814</td><td style=\"text-align: right;\"> 38.7409</td><td style=\"text-align: right;\">1.2992e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_7edc10ce</td><td>TERMINATED</td><td>172.26.215.93:494601</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_b1c0</td><td>[&#x27;force&#x27;, &#x27;x_co_9f00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000149431</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      211.638   </td><td style=\"text-align: right;\">122.474</td><td style=\"text-align: right;\"> 37.4566</td><td style=\"text-align: right;\">1.29857e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_63087eed</td><td>TERMINATED</td><td>172.26.215.93:494876</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_9c00</td><td>[&#x27;force&#x27;, &#x27;x_co_8c40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000159241</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      204.407   </td><td style=\"text-align: right;\">113.491</td><td style=\"text-align: right;\"> 35.7893</td><td style=\"text-align: right;\">1.40121e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_79df4dfc</td><td>TERMINATED</td><td>172.26.215.93:495109</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_6a80</td><td>[&#x27;force&#x27;, &#x27;x_co_bdc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000136831</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      203.24    </td><td style=\"text-align: right;\">113.935</td><td style=\"text-align: right;\"> 36.1039</td><td style=\"text-align: right;\">1.29925e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_7b56e2bf</td><td>TERMINATED</td><td>172.26.215.93:495362</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_0780</td><td>[&#x27;force&#x27;, &#x27;x_co_9c80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000143141</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        5.34494 </td><td style=\"text-align: right;\">201.471</td><td style=\"text-align: right;\"> 72.6948</td><td style=\"text-align: right;\">3.81963e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_40319008</td><td>TERMINATED</td><td>172.26.215.93:495572</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_a1c0</td><td>[&#x27;force&#x27;, &#x27;x_co_b340</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000161439</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      201.575   </td><td style=\"text-align: right;\">113.649</td><td style=\"text-align: right;\"> 36.1722</td><td style=\"text-align: right;\">1.197e+07  </td></tr>\n",
       "<tr><td>FSR_Trainable_6a6d3f27</td><td>TERMINATED</td><td>172.26.215.93:495829</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_1780</td><td>[&#x27;force&#x27;, &#x27;x_co_19c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        7.4014e-05 </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        4.77868 </td><td style=\"text-align: right;\">218.671</td><td style=\"text-align: right;\"> 78.2085</td><td style=\"text-align: right;\">3.99991e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_b6844708</td><td>TERMINATED</td><td>172.26.215.93:496055</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_2540</td><td>[&#x27;force&#x27;, &#x27;x_co_2200</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        7.42909e-05</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.56568 </td><td style=\"text-align: right;\">233.913</td><td style=\"text-align: right;\"> 65.3596</td><td style=\"text-align: right;\">9.03102e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_38f42068</td><td>TERMINATED</td><td>172.26.215.93:496255</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_bc40</td><td>[&#x27;force&#x27;, &#x27;x_co_bcc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000414425</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      202.67    </td><td style=\"text-align: right;\">114.413</td><td style=\"text-align: right;\"> 34.9554</td><td style=\"text-align: right;\">9.70088e+06</td></tr>\n",
       "<tr><td>FSR_Trainable_9b52a0d6</td><td>TERMINATED</td><td>172.26.215.93:496536</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_8a40</td><td>[&#x27;force&#x27;, &#x27;x_co_b540</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000105615</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        4.7865  </td><td style=\"text-align: right;\">203.871</td><td style=\"text-align: right;\"> 72.6866</td><td style=\"text-align: right;\">4.52384e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_97300f2b</td><td>TERMINATED</td><td>172.26.215.93:496751</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_3880</td><td>[&#x27;force&#x27;, &#x27;x_co_1980</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000432801</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.904242</td><td style=\"text-align: right;\">246.981</td><td style=\"text-align: right;\"> 87.2661</td><td style=\"text-align: right;\">7.58539e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_2182d1af</td><td>TERMINATED</td><td>172.26.215.93:496959</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_0dc0</td><td>[&#x27;force&#x27;, &#x27;x_co_3580</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000183339</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      201.095   </td><td style=\"text-align: right;\">113.407</td><td style=\"text-align: right;\"> 35.9877</td><td style=\"text-align: right;\">1.55824e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_33bfbb89</td><td>TERMINATED</td><td>172.26.215.93:497176</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_2000</td><td>[&#x27;force&#x27;, &#x27;x_co_0340</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000104785</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        4.80742 </td><td style=\"text-align: right;\">198.814</td><td style=\"text-align: right;\"> 71.724 </td><td style=\"text-align: right;\">3.96555e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_87a052b8</td><td>TERMINATED</td><td>172.26.215.93:497445</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_3c00</td><td>[&#x27;force&#x27;, &#x27;x_co_0dc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000182444</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">      133.165   </td><td style=\"text-align: right;\">123.356</td><td style=\"text-align: right;\"> 37.9605</td><td style=\"text-align: right;\">1.36605e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_9a6ea1b4</td><td>TERMINATED</td><td>172.26.215.93:497695</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_1c00</td><td>[&#x27;force&#x27;, &#x27;x_co_28c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000181441</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      200.289   </td><td style=\"text-align: right;\">119.293</td><td style=\"text-align: right;\"> 36.2024</td><td style=\"text-align: right;\">1.07802e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_3cb4c6ba</td><td>TERMINATED</td><td>172.26.215.93:497947</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_8300</td><td>[&#x27;force&#x27;, &#x27;x_co_8dc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000167536</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.42274 </td><td style=\"text-align: right;\">195.664</td><td style=\"text-align: right;\"> 68.7116</td><td style=\"text-align: right;\">4.10018e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_018bdcbb</td><td>TERMINATED</td><td>172.26.215.93:498148</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_3940</td><td>[&#x27;force&#x27;, &#x27;x_co_0380</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000173921</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.36907 </td><td style=\"text-align: right;\">229.649</td><td style=\"text-align: right;\"> 65.8758</td><td style=\"text-align: right;\">1.07453e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_4acc8330</td><td>TERMINATED</td><td>172.26.215.93:498374</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_b440</td><td>[&#x27;force&#x27;, &#x27;x_co_9a40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00018899 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.43135 </td><td style=\"text-align: right;\">222.055</td><td style=\"text-align: right;\"> 62.5694</td><td style=\"text-align: right;\">9.17001e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_286656b1</td><td>TERMINATED</td><td>172.26.215.93:498588</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_e940</td><td>[&#x27;force&#x27;, &#x27;x_co_fe00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000116982</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.18399 </td><td style=\"text-align: right;\">224.643</td><td style=\"text-align: right;\"> 78.9939</td><td style=\"text-align: right;\">3.20101e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_aa237e74</td><td>TERMINATED</td><td>172.26.215.93:498826</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_2d80</td><td>[&#x27;force&#x27;, &#x27;x_co_c1c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        6.25818e-05</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.32669 </td><td style=\"text-align: right;\">224.436</td><td style=\"text-align: right;\"> 80.0482</td><td style=\"text-align: right;\">2.61084e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_df33489f</td><td>TERMINATED</td><td>172.26.215.93:499054</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_6bc0</td><td>[&#x27;force&#x27;, &#x27;x_co_d300</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        5.82623e-05</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        4.77262 </td><td style=\"text-align: right;\">221.555</td><td style=\"text-align: right;\"> 79.6063</td><td style=\"text-align: right;\">4.65099e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f9ed1d54</td><td>TERMINATED</td><td>172.26.215.93:499263</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_a300</td><td>[&#x27;force&#x27;, &#x27;x_co_9d40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000350022</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       33.8171  </td><td style=\"text-align: right;\">131.961</td><td style=\"text-align: right;\"> 42.3831</td><td style=\"text-align: right;\">2.29787e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_b102d1a7</td><td>TERMINATED</td><td>172.26.215.93:499488</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_7940</td><td>[&#x27;force&#x27;, &#x27;x_co_6440</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000354123</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      194.764   </td><td style=\"text-align: right;\">117.894</td><td style=\"text-align: right;\"> 36.4884</td><td style=\"text-align: right;\">1.27459e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_b863bafd</td><td>TERMINATED</td><td>172.26.215.93:499710</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_4e40</td><td>[&#x27;force&#x27;, &#x27;x_co_c180</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000126316</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        9.33643 </td><td style=\"text-align: right;\">164.665</td><td style=\"text-align: right;\"> 56.0342</td><td style=\"text-align: right;\">2.63702e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e35ff9df</td><td>TERMINATED</td><td>172.26.215.93:499984</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_81c0</td><td>[&#x27;force&#x27;, &#x27;x_co_c2c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000354447</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       32.7296  </td><td style=\"text-align: right;\">120.605</td><td style=\"text-align: right;\"> 37.8116</td><td style=\"text-align: right;\">1.63729e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_68f54235</td><td>TERMINATED</td><td>172.26.215.93:500166</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_3640</td><td>[&#x27;force&#x27;, &#x27;x_co_1e00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000490115</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.61445 </td><td style=\"text-align: right;\">202.208</td><td style=\"text-align: right;\"> 69.4221</td><td style=\"text-align: right;\">1.7903e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_d3f80ea9</td><td>TERMINATED</td><td>172.26.215.93:500405</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_4140</td><td>[&#x27;force&#x27;, &#x27;x_co_c280</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000491122</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.939163</td><td style=\"text-align: right;\">256.893</td><td style=\"text-align: right;\"> 95.7437</td><td style=\"text-align: right;\">6.2909e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_33b68655</td><td>TERMINATED</td><td>172.26.215.93:500632</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_9bc0</td><td>[&#x27;force&#x27;, &#x27;x_co_29c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        9.48821e-05</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        4.08745 </td><td style=\"text-align: right;\">206.292</td><td style=\"text-align: right;\"> 73.3928</td><td style=\"text-align: right;\">4.7253e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_fa8aeae0</td><td>TERMINATED</td><td>172.26.215.93:500850</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_9600</td><td>[&#x27;force&#x27;, &#x27;x_co_8e80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000213747</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.32697 </td><td style=\"text-align: right;\">260.763</td><td style=\"text-align: right;\"> 76.6579</td><td style=\"text-align: right;\">6.29869e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_4e34ea6f</td><td>TERMINATED</td><td>172.26.215.93:501074</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_16c0</td><td>[&#x27;force&#x27;, &#x27;x_co_3dc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000225285</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       33.3185  </td><td style=\"text-align: right;\">129.84 </td><td style=\"text-align: right;\"> 41.4465</td><td style=\"text-align: right;\">2.01169e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_58e865dc</td><td>TERMINATED</td><td>172.26.215.93:501295</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_17c0</td><td>[&#x27;force&#x27;, &#x27;x_co_1240</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000237489</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       66.9957  </td><td style=\"text-align: right;\">120.959</td><td style=\"text-align: right;\"> 37.7243</td><td style=\"text-align: right;\">1.80432e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_812f8eb1</td><td>TERMINATED</td><td>172.26.215.93:501512</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_69c0</td><td>[&#x27;force&#x27;, &#x27;x_co_5740</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000210062</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       33.9409  </td><td style=\"text-align: right;\">131.039</td><td style=\"text-align: right;\"> 42.6694</td><td style=\"text-align: right;\">2.19098e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_8e59dd34</td><td>TERMINATED</td><td>172.26.215.93:501791</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_cd00</td><td>[&#x27;force&#x27;, &#x27;x_co_4380</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000231036</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       34.3231  </td><td style=\"text-align: right;\">130.669</td><td style=\"text-align: right;\"> 42.7155</td><td style=\"text-align: right;\">2.3941e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_51044af2</td><td>TERMINATED</td><td>172.26.215.93:502017</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_3ec0</td><td>[&#x27;force&#x27;, &#x27;x_co_2940</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00030536 </td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">      186.092   </td><td style=\"text-align: right;\">120.751</td><td style=\"text-align: right;\"> 36.974 </td><td style=\"text-align: right;\">1.19457e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_86cec811</td><td>TERMINATED</td><td>172.26.215.93:502249</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_6180</td><td>[&#x27;force&#x27;, &#x27;x_co_6440</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000142267</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      142.37    </td><td style=\"text-align: right;\">116.64 </td><td style=\"text-align: right;\"> 37.0881</td><td style=\"text-align: right;\">1.33444e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_75f4fb35</td><td>TERMINATED</td><td>172.26.215.93:502430</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_46f0</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_1980</td><td>[&#x27;force&#x27;, &#x27;x_co_3440</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000325944</td><td>sklearn.preproc_44b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.70693 </td><td style=\"text-align: right;\">222.715</td><td style=\"text-align: right;\"> 81.5962</td><td style=\"text-align: right;\">5.09113e+07</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 08:33:14,333\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">       mape</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_018bdcbb</td><td>2023-07-19_09-16-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 65.8758</td><td style=\"text-align: right;\">1.07453e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">498148</td><td style=\"text-align: right;\">229.649</td><td style=\"text-align: right;\">            1.36907 </td><td style=\"text-align: right;\">          1.36907 </td><td style=\"text-align: right;\">      1.36907 </td><td style=\"text-align: right;\"> 1689725782</td><td style=\"text-align: right;\">                   1</td><td>018bdcbb  </td></tr>\n",
       "<tr><td>FSR_Trainable_02392f30</td><td>2023-07-19_09-03-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 86.0456</td><td style=\"text-align: right;\">1.87093e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">493086</td><td style=\"text-align: right;\">230.717</td><td style=\"text-align: right;\">            3.02488 </td><td style=\"text-align: right;\">          3.02488 </td><td style=\"text-align: right;\">      3.02488 </td><td style=\"text-align: right;\"> 1689724990</td><td style=\"text-align: right;\">                   1</td><td>02392f30  </td></tr>\n",
       "<tr><td>FSR_Trainable_07f295fa</td><td>2023-07-19_08-49-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 50.3421</td><td style=\"text-align: right;\">2.14025e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">487178</td><td style=\"text-align: right;\">161.715</td><td style=\"text-align: right;\">           80.6083  </td><td style=\"text-align: right;\">          2.57094 </td><td style=\"text-align: right;\">     80.6083  </td><td style=\"text-align: right;\"> 1689724143</td><td style=\"text-align: right;\">                  32</td><td>07f295fa  </td></tr>\n",
       "<tr><td>FSR_Trainable_09940689</td><td>2023-07-19_08-35-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 50.4564</td><td style=\"text-align: right;\">2.15791e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">479465</td><td style=\"text-align: right;\">154.172</td><td style=\"text-align: right;\">          131.546   </td><td style=\"text-align: right;\">          1.71718 </td><td style=\"text-align: right;\">    131.546   </td><td style=\"text-align: right;\"> 1689723351</td><td style=\"text-align: right;\">                 100</td><td>09940689  </td></tr>\n",
       "<tr><td>FSR_Trainable_0c74399f</td><td>2023-07-19_09-09-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 36.0741</td><td style=\"text-align: right;\">1.33861e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">494103</td><td style=\"text-align: right;\">114.125</td><td style=\"text-align: right;\">          214.733   </td><td style=\"text-align: right;\">          1.96194 </td><td style=\"text-align: right;\">    214.733   </td><td style=\"text-align: right;\"> 1689725379</td><td style=\"text-align: right;\">                 100</td><td>0c74399f  </td></tr>\n",
       "<tr><td>FSR_Trainable_1a2896b8</td><td>2023-07-19_09-05-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 36.562 </td><td style=\"text-align: right;\">1.18541e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">492427</td><td style=\"text-align: right;\">117.407</td><td style=\"text-align: right;\">          218.893   </td><td style=\"text-align: right;\">          2.01135 </td><td style=\"text-align: right;\">    218.893   </td><td style=\"text-align: right;\"> 1689725112</td><td style=\"text-align: right;\">                 100</td><td>1a2896b8  </td></tr>\n",
       "<tr><td>FSR_Trainable_1be8b213</td><td>2023-07-19_08-35-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">111.645 </td><td style=\"text-align: right;\">6.12039e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">481767</td><td style=\"text-align: right;\">355.071</td><td style=\"text-align: right;\">            6.74552 </td><td style=\"text-align: right;\">          6.74552 </td><td style=\"text-align: right;\">      6.74552 </td><td style=\"text-align: right;\"> 1689723334</td><td style=\"text-align: right;\">                   1</td><td>1be8b213  </td></tr>\n",
       "<tr><td>FSR_Trainable_1d84a1d6</td><td>2023-07-19_09-00-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 34.8554</td><td style=\"text-align: right;\">1.23459e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">491822</td><td style=\"text-align: right;\">111.187</td><td style=\"text-align: right;\">          217.311   </td><td style=\"text-align: right;\">          2.40588 </td><td style=\"text-align: right;\">    217.311   </td><td style=\"text-align: right;\"> 1689724801</td><td style=\"text-align: right;\">                 100</td><td>1d84a1d6  </td></tr>\n",
       "<tr><td>FSR_Trainable_204d60e5</td><td>2023-07-19_09-03-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 80.2766</td><td style=\"text-align: right;\">1.74272e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">493335</td><td style=\"text-align: right;\">223.923</td><td style=\"text-align: right;\">            2.98505 </td><td style=\"text-align: right;\">          2.98505 </td><td style=\"text-align: right;\">      2.98505 </td><td style=\"text-align: right;\"> 1689725011</td><td style=\"text-align: right;\">                   1</td><td>204d60e5  </td></tr>\n",
       "<tr><td>FSR_Trainable_2182d1af</td><td>2023-07-19_09-17-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 35.9877</td><td style=\"text-align: right;\">1.55824e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">496959</td><td style=\"text-align: right;\">113.407</td><td style=\"text-align: right;\">          201.095   </td><td style=\"text-align: right;\">          1.77541 </td><td style=\"text-align: right;\">    201.095   </td><td style=\"text-align: right;\"> 1689725841</td><td style=\"text-align: right;\">                 100</td><td>2182d1af  </td></tr>\n",
       "<tr><td>FSR_Trainable_24ff1ea4</td><td>2023-07-19_09-01-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 34.4625</td><td style=\"text-align: right;\">1.09237e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">490662</td><td style=\"text-align: right;\">114.278</td><td style=\"text-align: right;\">          426.722   </td><td style=\"text-align: right;\">          3.76362 </td><td style=\"text-align: right;\">    426.722   </td><td style=\"text-align: right;\"> 1689724869</td><td style=\"text-align: right;\">                 100</td><td>24ff1ea4  </td></tr>\n",
       "<tr><td>FSR_Trainable_259a7c0c</td><td>2023-07-19_08-37-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 78.5474</td><td style=\"text-align: right;\">3.02362e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">482542</td><td style=\"text-align: right;\">223.149</td><td style=\"text-align: right;\">            7.81073 </td><td style=\"text-align: right;\">          1.66482 </td><td style=\"text-align: right;\">      7.81073 </td><td style=\"text-align: right;\"> 1689723420</td><td style=\"text-align: right;\">                   4</td><td>259a7c0c  </td></tr>\n",
       "<tr><td>FSR_Trainable_286656b1</td><td>2023-07-19_09-16-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 78.9939</td><td style=\"text-align: right;\">3.20101e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">498588</td><td style=\"text-align: right;\">224.643</td><td style=\"text-align: right;\">            1.18399 </td><td style=\"text-align: right;\">          1.18399 </td><td style=\"text-align: right;\">      1.18399 </td><td style=\"text-align: right;\"> 1689725808</td><td style=\"text-align: right;\">                   1</td><td>286656b1  </td></tr>\n",
       "<tr><td>FSR_Trainable_33b68655</td><td>2023-07-19_09-18-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 73.3928</td><td style=\"text-align: right;\">4.7253e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">500632</td><td style=\"text-align: right;\">206.292</td><td style=\"text-align: right;\">            4.08745 </td><td style=\"text-align: right;\">          1.90347 </td><td style=\"text-align: right;\">      4.08745 </td><td style=\"text-align: right;\"> 1689725935</td><td style=\"text-align: right;\">                   2</td><td>33b68655  </td></tr>\n",
       "<tr><td>FSR_Trainable_33bfbb89</td><td>2023-07-19_09-13-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 71.724 </td><td style=\"text-align: right;\">3.96555e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">497176</td><td style=\"text-align: right;\">198.814</td><td style=\"text-align: right;\">            4.80742 </td><td style=\"text-align: right;\">          2.33403 </td><td style=\"text-align: right;\">      4.80742 </td><td style=\"text-align: right;\"> 1689725639</td><td style=\"text-align: right;\">                   2</td><td>33bfbb89  </td></tr>\n",
       "<tr><td>FSR_Trainable_35b09d69</td><td>2023-07-19_08-55-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 41.0292</td><td style=\"text-align: right;\">1.5236e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">489476</td><td style=\"text-align: right;\">136.652</td><td style=\"text-align: right;\">          151.016   </td><td style=\"text-align: right;\">          1.65925 </td><td style=\"text-align: right;\">    151.016   </td><td style=\"text-align: right;\"> 1689724505</td><td style=\"text-align: right;\">                 100</td><td>35b09d69  </td></tr>\n",
       "<tr><td>FSR_Trainable_38f42068</td><td>2023-07-19_09-15-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 34.9554</td><td style=\"text-align: right;\">9.70088e+06</td><td>172.26.215.93</td><td style=\"text-align: right;\">496255</td><td style=\"text-align: right;\">114.413</td><td style=\"text-align: right;\">          202.67    </td><td style=\"text-align: right;\">          2.30775 </td><td style=\"text-align: right;\">    202.67    </td><td style=\"text-align: right;\"> 1689725750</td><td style=\"text-align: right;\">                 100</td><td>38f42068  </td></tr>\n",
       "<tr><td>FSR_Trainable_3b8ffc8c</td><td>2023-07-19_08-50-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 83.5827</td><td style=\"text-align: right;\">6.73128e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">487870</td><td style=\"text-align: right;\">225.1  </td><td style=\"text-align: right;\">            2.96987 </td><td style=\"text-align: right;\">          2.96987 </td><td style=\"text-align: right;\">      2.96987 </td><td style=\"text-align: right;\"> 1689724232</td><td style=\"text-align: right;\">                   1</td><td>3b8ffc8c  </td></tr>\n",
       "<tr><td>FSR_Trainable_3c823ffc</td><td>2023-07-19_08-46-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 54.6274</td><td style=\"text-align: right;\">1.50437e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">485799</td><td style=\"text-align: right;\">175.678</td><td style=\"text-align: right;\">          102.18    </td><td style=\"text-align: right;\">          2.86497 </td><td style=\"text-align: right;\">    102.18    </td><td style=\"text-align: right;\"> 1689724000</td><td style=\"text-align: right;\">                  32</td><td>3c823ffc  </td></tr>\n",
       "<tr><td>FSR_Trainable_3cb4c6ba</td><td>2023-07-19_09-16-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 68.7116</td><td style=\"text-align: right;\">4.10018e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">497947</td><td style=\"text-align: right;\">195.664</td><td style=\"text-align: right;\">            2.42274 </td><td style=\"text-align: right;\">          1.0397  </td><td style=\"text-align: right;\">      2.42274 </td><td style=\"text-align: right;\"> 1689725767</td><td style=\"text-align: right;\">                   2</td><td>3cb4c6ba  </td></tr>\n",
       "<tr><td>FSR_Trainable_3e4d0715</td><td>2023-07-19_09-02-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 37.7526</td><td style=\"text-align: right;\">1.3721e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">490942</td><td style=\"text-align: right;\">120.881</td><td style=\"text-align: right;\">          433.153   </td><td style=\"text-align: right;\">          4.33141 </td><td style=\"text-align: right;\">    433.153   </td><td style=\"text-align: right;\"> 1689724960</td><td style=\"text-align: right;\">                 100</td><td>3e4d0715  </td></tr>\n",
       "<tr><td>FSR_Trainable_40319008</td><td>2023-07-19_09-14-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 36.1722</td><td style=\"text-align: right;\">1.197e+07  </td><td>172.26.215.93</td><td style=\"text-align: right;\">495572</td><td style=\"text-align: right;\">113.649</td><td style=\"text-align: right;\">          201.575   </td><td style=\"text-align: right;\">          2.00361 </td><td style=\"text-align: right;\">    201.575   </td><td style=\"text-align: right;\"> 1689725688</td><td style=\"text-align: right;\">                 100</td><td>40319008  </td></tr>\n",
       "<tr><td>FSR_Trainable_42a6be99</td><td>2023-07-19_08-50-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 39.9274</td><td style=\"text-align: right;\">1.71126e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">483646</td><td style=\"text-align: right;\">129.427</td><td style=\"text-align: right;\">          673.35    </td><td style=\"text-align: right;\">          7.27552 </td><td style=\"text-align: right;\">    673.35    </td><td style=\"text-align: right;\"> 1689724202</td><td style=\"text-align: right;\">                 100</td><td>42a6be99  </td></tr>\n",
       "<tr><td>FSR_Trainable_48d43b42</td><td>2023-07-19_08-37-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 82.1882</td><td style=\"text-align: right;\">2.37527e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">483198</td><td style=\"text-align: right;\">236.837</td><td style=\"text-align: right;\">            1.55829 </td><td style=\"text-align: right;\">          1.55829 </td><td style=\"text-align: right;\">      1.55829 </td><td style=\"text-align: right;\"> 1689723476</td><td style=\"text-align: right;\">                   1</td><td>48d43b42  </td></tr>\n",
       "<tr><td>FSR_Trainable_4978f3da</td><td>2023-07-19_09-06-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 36.8279</td><td style=\"text-align: right;\">1.26991e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">492909</td><td style=\"text-align: right;\">117.2  </td><td style=\"text-align: right;\">          214.192   </td><td style=\"text-align: right;\">          2.02651 </td><td style=\"text-align: right;\">    214.192   </td><td style=\"text-align: right;\"> 1689725202</td><td style=\"text-align: right;\">                 100</td><td>4978f3da  </td></tr>\n",
       "<tr><td>FSR_Trainable_4a5175af</td><td>2023-07-19_08-34-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 69.9643</td><td style=\"text-align: right;\">3.85836e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">481108</td><td style=\"text-align: right;\">232.475</td><td style=\"text-align: right;\">            3.44891 </td><td style=\"text-align: right;\">          1.12707 </td><td style=\"text-align: right;\">      3.44891 </td><td style=\"text-align: right;\"> 1689723293</td><td style=\"text-align: right;\">                   2</td><td>4a5175af  </td></tr>\n",
       "<tr><td>FSR_Trainable_4acc8330</td><td>2023-07-19_09-16-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 62.5694</td><td style=\"text-align: right;\">9.17001e+14</td><td>172.26.215.93</td><td style=\"text-align: right;\">498374</td><td style=\"text-align: right;\">222.055</td><td style=\"text-align: right;\">            2.43135 </td><td style=\"text-align: right;\">          2.43135 </td><td style=\"text-align: right;\">      2.43135 </td><td style=\"text-align: right;\"> 1689725799</td><td style=\"text-align: right;\">                   1</td><td>4acc8330  </td></tr>\n",
       "<tr><td>FSR_Trainable_4e34ea6f</td><td>2023-07-19_09-19-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 41.4465</td><td style=\"text-align: right;\">2.01169e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">501074</td><td style=\"text-align: right;\">129.84 </td><td style=\"text-align: right;\">           33.3185  </td><td style=\"text-align: right;\">          4.02594 </td><td style=\"text-align: right;\">     33.3185  </td><td style=\"text-align: right;\"> 1689725985</td><td style=\"text-align: right;\">                   8</td><td>4e34ea6f  </td></tr>\n",
       "<tr><td>FSR_Trainable_51044af2</td><td>2023-07-19_09-23-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 36.974 </td><td style=\"text-align: right;\">1.19457e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">502017</td><td style=\"text-align: right;\">120.751</td><td style=\"text-align: right;\">          186.092   </td><td style=\"text-align: right;\">          1.98791 </td><td style=\"text-align: right;\">    186.092   </td><td style=\"text-align: right;\"> 1689726209</td><td style=\"text-align: right;\">                  64</td><td>51044af2  </td></tr>\n",
       "<tr><td>FSR_Trainable_5545c6d4</td><td>2023-07-19_08-38-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 65.9405</td><td style=\"text-align: right;\">3.32811e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">482253</td><td style=\"text-align: right;\">218.061</td><td style=\"text-align: right;\">          165.106   </td><td style=\"text-align: right;\">          4.96267 </td><td style=\"text-align: right;\">    165.106   </td><td style=\"text-align: right;\"> 1689723538</td><td style=\"text-align: right;\">                  32</td><td>5545c6d4  </td></tr>\n",
       "<tr><td>FSR_Trainable_57412e6a</td><td>2023-07-19_08-34-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 83.1161</td><td style=\"text-align: right;\">5.9746e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">480189</td><td style=\"text-align: right;\">225.423</td><td style=\"text-align: right;\">            6.32065 </td><td style=\"text-align: right;\">          3.2005  </td><td style=\"text-align: right;\">      6.32065 </td><td style=\"text-align: right;\"> 1689723245</td><td style=\"text-align: right;\">                   2</td><td>57412e6a  </td></tr>\n",
       "<tr><td>FSR_Trainable_58e865dc</td><td>2023-07-19_09-20-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 37.7243</td><td style=\"text-align: right;\">1.80432e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">501295</td><td style=\"text-align: right;\">120.959</td><td style=\"text-align: right;\">           66.9957  </td><td style=\"text-align: right;\">          4.19947 </td><td style=\"text-align: right;\">     66.9957  </td><td style=\"text-align: right;\"> 1689726030</td><td style=\"text-align: right;\">                  16</td><td>58e865dc  </td></tr>\n",
       "<tr><td>FSR_Trainable_5ea282f6</td><td>2023-07-19_08-34-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 64.3742</td><td style=\"text-align: right;\">2.87111e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">480665</td><td style=\"text-align: right;\">239.912</td><td style=\"text-align: right;\">            0.7843  </td><td style=\"text-align: right;\">          0.7843  </td><td style=\"text-align: right;\">      0.7843  </td><td style=\"text-align: right;\"> 1689723268</td><td style=\"text-align: right;\">                   1</td><td>5ea282f6  </td></tr>\n",
       "<tr><td>FSR_Trainable_62dc9803</td><td>2023-07-19_08-46-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 79.4044</td><td style=\"text-align: right;\">4.14485e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">486276</td><td style=\"text-align: right;\">225.513</td><td style=\"text-align: right;\">            3.93792 </td><td style=\"text-align: right;\">          3.93792 </td><td style=\"text-align: right;\">      3.93792 </td><td style=\"text-align: right;\"> 1689723994</td><td style=\"text-align: right;\">                   1</td><td>62dc9803  </td></tr>\n",
       "<tr><td>FSR_Trainable_63087eed</td><td>2023-07-19_09-12-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 35.7893</td><td style=\"text-align: right;\">1.40121e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">494876</td><td style=\"text-align: right;\">113.491</td><td style=\"text-align: right;\">          204.407   </td><td style=\"text-align: right;\">          1.94866 </td><td style=\"text-align: right;\">    204.407   </td><td style=\"text-align: right;\"> 1689725577</td><td style=\"text-align: right;\">                 100</td><td>63087eed  </td></tr>\n",
       "<tr><td>FSR_Trainable_630e06a5</td><td>2023-07-19_08-50-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 74.5025</td><td style=\"text-align: right;\">4.6722e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">487447</td><td style=\"text-align: right;\">198.571</td><td style=\"text-align: right;\">           49.9966  </td><td style=\"text-align: right;\">         11.5022  </td><td style=\"text-align: right;\">     49.9966  </td><td style=\"text-align: right;\"> 1689724210</td><td style=\"text-align: right;\">                   4</td><td>630e06a5  </td></tr>\n",
       "<tr><td>FSR_Trainable_640d99b6</td><td>2023-07-19_08-52-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 77.7888</td><td style=\"text-align: right;\">5.08847e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">489251</td><td style=\"text-align: right;\">229.98 </td><td style=\"text-align: right;\">            1.44092 </td><td style=\"text-align: right;\">          1.44092 </td><td style=\"text-align: right;\">      1.44092 </td><td style=\"text-align: right;\"> 1689724331</td><td style=\"text-align: right;\">                   1</td><td>640d99b6  </td></tr>\n",
       "<tr><td>FSR_Trainable_68f54235</td><td>2023-07-19_09-18-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 69.4221</td><td style=\"text-align: right;\">1.7903e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">500166</td><td style=\"text-align: right;\">202.208</td><td style=\"text-align: right;\">            1.61445 </td><td style=\"text-align: right;\">          0.702483</td><td style=\"text-align: right;\">      1.61445 </td><td style=\"text-align: right;\"> 1689725903</td><td style=\"text-align: right;\">                   2</td><td>68f54235  </td></tr>\n",
       "<tr><td>FSR_Trainable_6a6d3f27</td><td>2023-07-19_09-11-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 78.2085</td><td style=\"text-align: right;\">3.99991e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">495829</td><td style=\"text-align: right;\">218.671</td><td style=\"text-align: right;\">            4.77868 </td><td style=\"text-align: right;\">          2.25336 </td><td style=\"text-align: right;\">      4.77868 </td><td style=\"text-align: right;\"> 1689725503</td><td style=\"text-align: right;\">                   2</td><td>6a6d3f27  </td></tr>\n",
       "<tr><td>FSR_Trainable_6e372da9</td><td>2023-07-19_08-47-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 58.0613</td><td style=\"text-align: right;\">1.95674e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">486455</td><td style=\"text-align: right;\">186.514</td><td style=\"text-align: right;\">           39.4812  </td><td style=\"text-align: right;\">          2.43446 </td><td style=\"text-align: right;\">     39.4812  </td><td style=\"text-align: right;\"> 1689724047</td><td style=\"text-align: right;\">                  16</td><td>6e372da9  </td></tr>\n",
       "<tr><td>FSR_Trainable_724a46ef</td><td>2023-07-19_09-05-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 35.8306</td><td style=\"text-align: right;\">1.18975e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">492661</td><td style=\"text-align: right;\">112.599</td><td style=\"text-align: right;\">          220.117   </td><td style=\"text-align: right;\">          1.9901  </td><td style=\"text-align: right;\">    220.117   </td><td style=\"text-align: right;\"> 1689725144</td><td style=\"text-align: right;\">                 100</td><td>724a46ef  </td></tr>\n",
       "<tr><td>FSR_Trainable_72b7c922</td><td>2023-07-19_08-51-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 75.5518</td><td style=\"text-align: right;\">4.58737e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">488118</td><td style=\"text-align: right;\">218.152</td><td style=\"text-align: right;\">            7.66907 </td><td style=\"text-align: right;\">          3.60475 </td><td style=\"text-align: right;\">      7.66907 </td><td style=\"text-align: right;\"> 1689724260</td><td style=\"text-align: right;\">                   2</td><td>72b7c922  </td></tr>\n",
       "<tr><td>FSR_Trainable_7322ae35</td><td>2023-07-19_08-46-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 46.0175</td><td style=\"text-align: right;\">2.33519e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">485005</td><td style=\"text-align: right;\">141.476</td><td style=\"text-align: right;\">          306.286   </td><td style=\"text-align: right;\">          2.55884 </td><td style=\"text-align: right;\">    306.286   </td><td style=\"text-align: right;\"> 1689723979</td><td style=\"text-align: right;\">                 100</td><td>7322ae35  </td></tr>\n",
       "<tr><td>FSR_Trainable_75f4fb35</td><td>2023-07-19_09-20-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 81.5962</td><td style=\"text-align: right;\">5.09113e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">502430</td><td style=\"text-align: right;\">222.715</td><td style=\"text-align: right;\">            2.70693 </td><td style=\"text-align: right;\">          2.70693 </td><td style=\"text-align: right;\">      2.70693 </td><td style=\"text-align: right;\"> 1689726056</td><td style=\"text-align: right;\">                   1</td><td>75f4fb35  </td></tr>\n",
       "<tr><td>FSR_Trainable_76612c31</td><td>2023-07-19_08-53-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 39.5344</td><td style=\"text-align: right;\">1.54473e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">488548</td><td style=\"text-align: right;\">129.73 </td><td style=\"text-align: right;\">          110.527   </td><td style=\"text-align: right;\">          1.01068 </td><td style=\"text-align: right;\">    110.527   </td><td style=\"text-align: right;\"> 1689724419</td><td style=\"text-align: right;\">                 100</td><td>76612c31  </td></tr>\n",
       "<tr><td>FSR_Trainable_76736994</td><td>2023-07-19_09-07-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 35.6804</td><td style=\"text-align: right;\">1.23625e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">493559</td><td style=\"text-align: right;\">111.712</td><td style=\"text-align: right;\">          211.31    </td><td style=\"text-align: right;\">          2.02933 </td><td style=\"text-align: right;\">    211.31    </td><td style=\"text-align: right;\"> 1689725250</td><td style=\"text-align: right;\">                 100</td><td>76736994  </td></tr>\n",
       "<tr><td>FSR_Trainable_79df4dfc</td><td>2023-07-19_09-13-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 36.1039</td><td style=\"text-align: right;\">1.29925e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">495109</td><td style=\"text-align: right;\">113.935</td><td style=\"text-align: right;\">          203.24    </td><td style=\"text-align: right;\">          1.94482 </td><td style=\"text-align: right;\">    203.24    </td><td style=\"text-align: right;\"> 1689725608</td><td style=\"text-align: right;\">                 100</td><td>79df4dfc  </td></tr>\n",
       "<tr><td>FSR_Trainable_7b56e2bf</td><td>2023-07-19_09-10-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 72.6948</td><td style=\"text-align: right;\">3.81963e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">495362</td><td style=\"text-align: right;\">201.471</td><td style=\"text-align: right;\">            5.34494 </td><td style=\"text-align: right;\">          2.30623 </td><td style=\"text-align: right;\">      5.34494 </td><td style=\"text-align: right;\"> 1689725456</td><td style=\"text-align: right;\">                   2</td><td>7b56e2bf  </td></tr>\n",
       "<tr><td>FSR_Trainable_7e662eaf</td><td>2023-07-19_08-55-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 39.1489</td><td style=\"text-align: right;\">1.51666e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">490137</td><td style=\"text-align: right;\">124.381</td><td style=\"text-align: right;\">          111.697   </td><td style=\"text-align: right;\">          1.1231  </td><td style=\"text-align: right;\">    111.697   </td><td style=\"text-align: right;\"> 1689724501</td><td style=\"text-align: right;\">                 100</td><td>7e662eaf  </td></tr>\n",
       "<tr><td>FSR_Trainable_7edc10ce</td><td>2023-07-19_09-11-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 37.4566</td><td style=\"text-align: right;\">1.29857e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">494601</td><td style=\"text-align: right;\">122.474</td><td style=\"text-align: right;\">          211.638   </td><td style=\"text-align: right;\">          1.82628 </td><td style=\"text-align: right;\">    211.638   </td><td style=\"text-align: right;\"> 1689725484</td><td style=\"text-align: right;\">                 100</td><td>7edc10ce  </td></tr>\n",
       "<tr><td>FSR_Trainable_7f2a80f4</td><td>2023-07-19_08-38-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 78.0503</td><td style=\"text-align: right;\">3.24735e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">483421</td><td style=\"text-align: right;\">233.291</td><td style=\"text-align: right;\">            1.66041 </td><td style=\"text-align: right;\">          1.66041 </td><td style=\"text-align: right;\">      1.66041 </td><td style=\"text-align: right;\"> 1689723494</td><td style=\"text-align: right;\">                   1</td><td>7f2a80f4  </td></tr>\n",
       "<tr><td>FSR_Trainable_812f8eb1</td><td>2023-07-19_09-20-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 42.6694</td><td style=\"text-align: right;\">2.19098e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">501512</td><td style=\"text-align: right;\">131.039</td><td style=\"text-align: right;\">           33.9409  </td><td style=\"text-align: right;\">          4.13165 </td><td style=\"text-align: right;\">     33.9409  </td><td style=\"text-align: right;\"> 1689726006</td><td style=\"text-align: right;\">                   8</td><td>812f8eb1  </td></tr>\n",
       "<tr><td>FSR_Trainable_86a93f9f</td><td>2023-07-19_08-35-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 70.3245</td><td style=\"text-align: right;\">1.14367e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">480904</td><td style=\"text-align: right;\">227.591</td><td style=\"text-align: right;\">           28.3777  </td><td style=\"text-align: right;\">         15.5652  </td><td style=\"text-align: right;\">     28.3777  </td><td style=\"text-align: right;\"> 1689723310</td><td style=\"text-align: right;\">                   2</td><td>86a93f9f  </td></tr>\n",
       "<tr><td>FSR_Trainable_86cec811</td><td>2023-07-19_09-23-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 37.0881</td><td style=\"text-align: right;\">1.33444e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">502249</td><td style=\"text-align: right;\">116.64 </td><td style=\"text-align: right;\">          142.37    </td><td style=\"text-align: right;\">          1.28607 </td><td style=\"text-align: right;\">    142.37    </td><td style=\"text-align: right;\"> 1689726190</td><td style=\"text-align: right;\">                 100</td><td>86cec811  </td></tr>\n",
       "<tr><td>FSR_Trainable_87a052b8</td><td>2023-07-19_09-16-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 37.9605</td><td style=\"text-align: right;\">1.36605e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">497445</td><td style=\"text-align: right;\">123.356</td><td style=\"text-align: right;\">          133.165   </td><td style=\"text-align: right;\">          2.07623 </td><td style=\"text-align: right;\">    133.165   </td><td style=\"text-align: right;\"> 1689725791</td><td style=\"text-align: right;\">                  64</td><td>87a052b8  </td></tr>\n",
       "<tr><td>FSR_Trainable_889e4985</td><td>2023-07-19_09-10-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 38.7409</td><td style=\"text-align: right;\">1.2992e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">494356</td><td style=\"text-align: right;\">124.814</td><td style=\"text-align: right;\">          210.22    </td><td style=\"text-align: right;\">          2.07719 </td><td style=\"text-align: right;\">    210.22    </td><td style=\"text-align: right;\"> 1689725436</td><td style=\"text-align: right;\">                 100</td><td>889e4985  </td></tr>\n",
       "<tr><td>FSR_Trainable_8ad9ac30</td><td>2023-07-19_08-37-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 83.9064</td><td style=\"text-align: right;\">3.74142e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">482980</td><td style=\"text-align: right;\">232.925</td><td style=\"text-align: right;\">            3.91953 </td><td style=\"text-align: right;\">          3.91953 </td><td style=\"text-align: right;\">      3.91953 </td><td style=\"text-align: right;\"> 1689723458</td><td style=\"text-align: right;\">                   1</td><td>8ad9ac30  </td></tr>\n",
       "<tr><td>FSR_Trainable_8cb3727c</td><td>2023-07-19_08-52-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 48.9927</td><td style=\"text-align: right;\">2.04099e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">488371</td><td style=\"text-align: right;\">160.628</td><td style=\"text-align: right;\">           86.0759  </td><td style=\"text-align: right;\">          2.47038 </td><td style=\"text-align: right;\">     86.0759  </td><td style=\"text-align: right;\"> 1689724369</td><td style=\"text-align: right;\">                  32</td><td>8cb3727c  </td></tr>\n",
       "<tr><td>FSR_Trainable_8dc179df</td><td>2023-07-19_08-40-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 64.9567</td><td style=\"text-align: right;\">1.12691e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">484568</td><td style=\"text-align: right;\">224.667</td><td style=\"text-align: right;\">            4.76936 </td><td style=\"text-align: right;\">          2.34442 </td><td style=\"text-align: right;\">      4.76936 </td><td style=\"text-align: right;\"> 1689723635</td><td style=\"text-align: right;\">                   2</td><td>8dc179df  </td></tr>\n",
       "<tr><td>FSR_Trainable_8e59dd34</td><td>2023-07-19_09-20-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 42.7155</td><td style=\"text-align: right;\">2.3941e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">501791</td><td style=\"text-align: right;\">130.669</td><td style=\"text-align: right;\">           34.3231  </td><td style=\"text-align: right;\">          4.21457 </td><td style=\"text-align: right;\">     34.3231  </td><td style=\"text-align: right;\"> 1689726035</td><td style=\"text-align: right;\">                   8</td><td>8e59dd34  </td></tr>\n",
       "<tr><td>FSR_Trainable_97300f2b</td><td>2023-07-19_09-13-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 87.2661</td><td style=\"text-align: right;\">7.58539e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">496751</td><td style=\"text-align: right;\">246.981</td><td style=\"text-align: right;\">            0.904242</td><td style=\"text-align: right;\">          0.904242</td><td style=\"text-align: right;\">      0.904242</td><td style=\"text-align: right;\"> 1689725609</td><td style=\"text-align: right;\">                   1</td><td>97300f2b  </td></tr>\n",
       "<tr><td>FSR_Trainable_98881a9b</td><td>2023-07-19_08-34-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 75.1146</td><td style=\"text-align: right;\">3.63495e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">479723</td><td style=\"text-align: right;\">222.635</td><td style=\"text-align: right;\">           38.5264  </td><td style=\"text-align: right;\">          2.54251 </td><td style=\"text-align: right;\">     38.5264  </td><td style=\"text-align: right;\"> 1689723258</td><td style=\"text-align: right;\">                  16</td><td>98881a9b  </td></tr>\n",
       "<tr><td>FSR_Trainable_9a6ea1b4</td><td>2023-07-19_09-18-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 36.2024</td><td style=\"text-align: right;\">1.07802e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">497695</td><td style=\"text-align: right;\">119.293</td><td style=\"text-align: right;\">          200.289   </td><td style=\"text-align: right;\">          2.23097 </td><td style=\"text-align: right;\">    200.289   </td><td style=\"text-align: right;\"> 1689725920</td><td style=\"text-align: right;\">                 100</td><td>9a6ea1b4  </td></tr>\n",
       "<tr><td>FSR_Trainable_9ab979db</td><td>2023-07-19_08-44-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 40.9583</td><td style=\"text-align: right;\">1.54286e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">485501</td><td style=\"text-align: right;\">133.998</td><td style=\"text-align: right;\">          169.96    </td><td style=\"text-align: right;\">          2.36575 </td><td style=\"text-align: right;\">    169.96    </td><td style=\"text-align: right;\"> 1689723875</td><td style=\"text-align: right;\">                  64</td><td>9ab979db  </td></tr>\n",
       "<tr><td>FSR_Trainable_9b52a0d6</td><td>2023-07-19_09-13-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 72.6866</td><td style=\"text-align: right;\">4.52384e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">496536</td><td style=\"text-align: right;\">203.871</td><td style=\"text-align: right;\">            4.7865  </td><td style=\"text-align: right;\">          2.32001 </td><td style=\"text-align: right;\">      4.7865  </td><td style=\"text-align: right;\"> 1689725595</td><td style=\"text-align: right;\">                   2</td><td>9b52a0d6  </td></tr>\n",
       "<tr><td>FSR_Trainable_a35d63f5</td><td>2023-07-19_09-02-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 37.2553</td><td style=\"text-align: right;\">1.27553e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">491127</td><td style=\"text-align: right;\">120.872</td><td style=\"text-align: right;\">          436.06    </td><td style=\"text-align: right;\">          4.88611 </td><td style=\"text-align: right;\">    436.06    </td><td style=\"text-align: right;\"> 1689724973</td><td style=\"text-align: right;\">                 100</td><td>a35d63f5  </td></tr>\n",
       "<tr><td>FSR_Trainable_aa237e74</td><td>2023-07-19_09-17-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 80.0482</td><td style=\"text-align: right;\">2.61084e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">498826</td><td style=\"text-align: right;\">224.436</td><td style=\"text-align: right;\">            1.32669 </td><td style=\"text-align: right;\">          1.32669 </td><td style=\"text-align: right;\">      1.32669 </td><td style=\"text-align: right;\"> 1689725820</td><td style=\"text-align: right;\">                   1</td><td>aa237e74  </td></tr>\n",
       "<tr><td>FSR_Trainable_aacfb51a</td><td>2023-07-19_08-51-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 39.4187</td><td style=\"text-align: right;\">1.2943e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">486680</td><td style=\"text-align: right;\">133.481</td><td style=\"text-align: right;\">          262.336   </td><td style=\"text-align: right;\">          2.03271 </td><td style=\"text-align: right;\">    262.336   </td><td style=\"text-align: right;\"> 1689724289</td><td style=\"text-align: right;\">                 100</td><td>aacfb51a  </td></tr>\n",
       "<tr><td>FSR_Trainable_abcd8262</td><td>2023-07-19_08-36-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 82.2345</td><td style=\"text-align: right;\">3.16935e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">481541</td><td style=\"text-align: right;\">226.38 </td><td style=\"text-align: right;\">           74.1069  </td><td style=\"text-align: right;\">          4.36725 </td><td style=\"text-align: right;\">     74.1069  </td><td style=\"text-align: right;\"> 1689723395</td><td style=\"text-align: right;\">                  16</td><td>abcd8262  </td></tr>\n",
       "<tr><td>FSR_Trainable_ac61c658</td><td>2023-07-19_08-34-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 81.6887</td><td style=\"text-align: right;\">2.97289e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">479896</td><td style=\"text-align: right;\">224.204</td><td style=\"text-align: right;\">           24.3142  </td><td style=\"text-align: right;\">          1.27737 </td><td style=\"text-align: right;\">     24.3142  </td><td style=\"text-align: right;\"> 1689723252</td><td style=\"text-align: right;\">                  16</td><td>ac61c658  </td></tr>\n",
       "<tr><td>FSR_Trainable_acce6591</td><td>2023-07-19_08-39-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 87.017 </td><td style=\"text-align: right;\">1.01134e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">483918</td><td style=\"text-align: right;\">256.391</td><td style=\"text-align: right;\">           10.8384  </td><td style=\"text-align: right;\">         10.8384  </td><td style=\"text-align: right;\">     10.8384  </td><td style=\"text-align: right;\"> 1689723564</td><td style=\"text-align: right;\">                   1</td><td>acce6591  </td></tr>\n",
       "<tr><td>FSR_Trainable_aea4e0a6</td><td>2023-07-19_08-51-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 78.0087</td><td style=\"text-align: right;\">1.48364e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">488795</td><td style=\"text-align: right;\">224.748</td><td style=\"text-align: right;\">            5.4496  </td><td style=\"text-align: right;\">          2.43136 </td><td style=\"text-align: right;\">      5.4496  </td><td style=\"text-align: right;\"> 1689724312</td><td style=\"text-align: right;\">                   2</td><td>aea4e0a6  </td></tr>\n",
       "<tr><td>FSR_Trainable_b102d1a7</td><td>2023-07-19_09-21-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 36.4884</td><td style=\"text-align: right;\">1.27459e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">499488</td><td style=\"text-align: right;\">117.894</td><td style=\"text-align: right;\">          194.764   </td><td style=\"text-align: right;\">          1.64055 </td><td style=\"text-align: right;\">    194.764   </td><td style=\"text-align: right;\"> 1689726070</td><td style=\"text-align: right;\">                 100</td><td>b102d1a7  </td></tr>\n",
       "<tr><td>FSR_Trainable_b6844708</td><td>2023-07-19_09-11-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 65.3596</td><td style=\"text-align: right;\">9.03102e+14</td><td>172.26.215.93</td><td style=\"text-align: right;\">496055</td><td style=\"text-align: right;\">233.913</td><td style=\"text-align: right;\">            2.56568 </td><td style=\"text-align: right;\">          2.56568 </td><td style=\"text-align: right;\">      2.56568 </td><td style=\"text-align: right;\"> 1689725518</td><td style=\"text-align: right;\">                   1</td><td>b6844708  </td></tr>\n",
       "<tr><td>FSR_Trainable_b863bafd</td><td>2023-07-19_09-17-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 56.0342</td><td style=\"text-align: right;\">2.63702e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">499710</td><td style=\"text-align: right;\">164.665</td><td style=\"text-align: right;\">            9.33643 </td><td style=\"text-align: right;\">          2.14614 </td><td style=\"text-align: right;\">      9.33643 </td><td style=\"text-align: right;\"> 1689725876</td><td style=\"text-align: right;\">                   4</td><td>b863bafd  </td></tr>\n",
       "<tr><td>FSR_Trainable_ba00398c</td><td>2023-07-19_08-40-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 64.319 </td><td style=\"text-align: right;\">1.18936e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">484378</td><td style=\"text-align: right;\">235.058</td><td style=\"text-align: right;\">            2.48324 </td><td style=\"text-align: right;\">          2.48324 </td><td style=\"text-align: right;\">      2.48324 </td><td style=\"text-align: right;\"> 1689723610</td><td style=\"text-align: right;\">                   1</td><td>ba00398c  </td></tr>\n",
       "<tr><td>FSR_Trainable_bb111c99</td><td>2023-07-19_08-40-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 43.6017</td><td style=\"text-align: right;\">1.62353e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">481324</td><td style=\"text-align: right;\">143.842</td><td style=\"text-align: right;\">          308.592   </td><td style=\"text-align: right;\">          3.4267  </td><td style=\"text-align: right;\">    308.592   </td><td style=\"text-align: right;\"> 1689723636</td><td style=\"text-align: right;\">                 100</td><td>bb111c99  </td></tr>\n",
       "<tr><td>FSR_Trainable_c29aa22c</td><td>2023-07-19_08-37-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 83.1427</td><td style=\"text-align: right;\">6.10903e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">482752</td><td style=\"text-align: right;\">228.428</td><td style=\"text-align: right;\">            3.58522 </td><td style=\"text-align: right;\">          3.58522 </td><td style=\"text-align: right;\">      3.58522 </td><td style=\"text-align: right;\"> 1689723437</td><td style=\"text-align: right;\">                   1</td><td>c29aa22c  </td></tr>\n",
       "<tr><td>FSR_Trainable_c486b8fd</td><td>2023-07-19_08-55-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 82.5653</td><td style=\"text-align: right;\">5.63303e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">491349</td><td style=\"text-align: right;\">225.538</td><td style=\"text-align: right;\">            4.7304  </td><td style=\"text-align: right;\">          4.7304  </td><td style=\"text-align: right;\">      4.7304  </td><td style=\"text-align: right;\"> 1689724541</td><td style=\"text-align: right;\">                   1</td><td>c486b8fd  </td></tr>\n",
       "<tr><td>FSR_Trainable_c5ab6926</td><td>2023-07-19_08-51-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 40.5674</td><td style=\"text-align: right;\">1.6826e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">486900</td><td style=\"text-align: right;\">129.018</td><td style=\"text-align: right;\">          262.185   </td><td style=\"text-align: right;\">          2.26321 </td><td style=\"text-align: right;\">    262.185   </td><td style=\"text-align: right;\"> 1689724303</td><td style=\"text-align: right;\">                 100</td><td>c5ab6926  </td></tr>\n",
       "<tr><td>FSR_Trainable_c5ef8b5a</td><td>2023-07-19_09-01-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 35.9203</td><td style=\"text-align: right;\">1.96584e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">492169</td><td style=\"text-align: right;\">111.666</td><td style=\"text-align: right;\">           76.016   </td><td style=\"text-align: right;\">          0.674191</td><td style=\"text-align: right;\">     76.016   </td><td style=\"text-align: right;\"> 1689724898</td><td style=\"text-align: right;\">                 100</td><td>c5ef8b5a  </td></tr>\n",
       "<tr><td>FSR_Trainable_c7074d67</td><td>2023-07-19_08-34-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 64.8393</td><td style=\"text-align: right;\">9.92434e+14</td><td>172.26.215.93</td><td style=\"text-align: right;\">480462</td><td style=\"text-align: right;\">230.572</td><td style=\"text-align: right;\">            1.46511 </td><td style=\"text-align: right;\">          1.46511 </td><td style=\"text-align: right;\">      1.46511 </td><td style=\"text-align: right;\"> 1689723257</td><td style=\"text-align: right;\">                   1</td><td>c7074d67  </td></tr>\n",
       "<tr><td>FSR_Trainable_cd040040</td><td>2023-07-19_08-53-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 63.383 </td><td style=\"text-align: right;\">2.60354e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">489936</td><td style=\"text-align: right;\">193.325</td><td style=\"text-align: right;\">            9.37257 </td><td style=\"text-align: right;\">          2.22784 </td><td style=\"text-align: right;\">      9.37257 </td><td style=\"text-align: right;\"> 1689724382</td><td style=\"text-align: right;\">                   4</td><td>cd040040  </td></tr>\n",
       "<tr><td>FSR_Trainable_cf9095d3</td><td>2023-07-19_09-09-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 36.536 </td><td style=\"text-align: right;\">1.47632e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">493868</td><td style=\"text-align: right;\">116.52 </td><td style=\"text-align: right;\">          214.257   </td><td style=\"text-align: right;\">          2.2843  </td><td style=\"text-align: right;\">    214.257   </td><td style=\"text-align: right;\"> 1689725349</td><td style=\"text-align: right;\">                 100</td><td>cf9095d3  </td></tr>\n",
       "<tr><td>FSR_Trainable_d3f80ea9</td><td>2023-07-19_09-18-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 95.7437</td><td style=\"text-align: right;\">6.2909e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">500405</td><td style=\"text-align: right;\">256.893</td><td style=\"text-align: right;\">            0.939163</td><td style=\"text-align: right;\">          0.939163</td><td style=\"text-align: right;\">      0.939163</td><td style=\"text-align: right;\"> 1689725916</td><td style=\"text-align: right;\">                   1</td><td>d3f80ea9  </td></tr>\n",
       "<tr><td>FSR_Trainable_d4fa06b8</td><td>2023-07-19_08-40-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 63.9827</td><td style=\"text-align: right;\">1.46303e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">484809</td><td style=\"text-align: right;\">223.066</td><td style=\"text-align: right;\">            9.35918 </td><td style=\"text-align: right;\">          2.48584 </td><td style=\"text-align: right;\">      9.35918 </td><td style=\"text-align: right;\"> 1689723659</td><td style=\"text-align: right;\">                   4</td><td>d4fa06b8  </td></tr>\n",
       "<tr><td>FSR_Trainable_d64c9d11</td><td>2023-07-19_08-33-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">118.581 </td><td style=\"text-align: right;\">2.41848e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">479529</td><td style=\"text-align: right;\">281.784</td><td style=\"text-align: right;\">            2.97722 </td><td style=\"text-align: right;\">          2.97722 </td><td style=\"text-align: right;\">      2.97722 </td><td style=\"text-align: right;\"> 1689723209</td><td style=\"text-align: right;\">                   1</td><td>d64c9d11  </td></tr>\n",
       "<tr><td>FSR_Trainable_dbfbbd1b</td><td>2023-07-19_08-51-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">126.624 </td><td style=\"text-align: right;\">1.76901e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">489004</td><td style=\"text-align: right;\">332.48 </td><td style=\"text-align: right;\">            2.13532 </td><td style=\"text-align: right;\">          2.13532 </td><td style=\"text-align: right;\">      2.13532 </td><td style=\"text-align: right;\"> 1689724319</td><td style=\"text-align: right;\">                   1</td><td>dbfbbd1b  </td></tr>\n",
       "<tr><td>FSR_Trainable_df33489f</td><td>2023-07-19_09-17-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 79.6063</td><td style=\"text-align: right;\">4.65099e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">499054</td><td style=\"text-align: right;\">221.555</td><td style=\"text-align: right;\">            4.77262 </td><td style=\"text-align: right;\">          4.77262 </td><td style=\"text-align: right;\">      4.77262 </td><td style=\"text-align: right;\"> 1689725836</td><td style=\"text-align: right;\">                   1</td><td>df33489f  </td></tr>\n",
       "<tr><td>FSR_Trainable_e35ff9df</td><td>2023-07-19_09-18-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 37.8116</td><td style=\"text-align: right;\">1.63729e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">499984</td><td style=\"text-align: right;\">120.605</td><td style=\"text-align: right;\">           32.7296  </td><td style=\"text-align: right;\">          1.67875 </td><td style=\"text-align: right;\">     32.7296  </td><td style=\"text-align: right;\"> 1689725926</td><td style=\"text-align: right;\">                  16</td><td>e35ff9df  </td></tr>\n",
       "<tr><td>FSR_Trainable_e93de5c7</td><td>2023-07-19_08-46-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 67.2259</td><td style=\"text-align: right;\">2.56076e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">486045</td><td style=\"text-align: right;\">202.021</td><td style=\"text-align: right;\">           13.3104  </td><td style=\"text-align: right;\">          2.96892 </td><td style=\"text-align: right;\">     13.3104  </td><td style=\"text-align: right;\"> 1689723982</td><td style=\"text-align: right;\">                   4</td><td>e93de5c7  </td></tr>\n",
       "<tr><td>FSR_Trainable_e94e0aaf</td><td>2023-07-19_08-56-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 80.0048</td><td style=\"text-align: right;\">4.38146e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">491612</td><td style=\"text-align: right;\">225.191</td><td style=\"text-align: right;\">            5.77207 </td><td style=\"text-align: right;\">          5.77207 </td><td style=\"text-align: right;\">      5.77207 </td><td style=\"text-align: right;\"> 1689724562</td><td style=\"text-align: right;\">                   1</td><td>e94e0aaf  </td></tr>\n",
       "<tr><td>FSR_Trainable_ecf589ea</td><td>2023-07-19_08-41-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 80.7052</td><td style=\"text-align: right;\">5.11766e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">485231</td><td style=\"text-align: right;\">224.399</td><td style=\"text-align: right;\">            6.22457 </td><td style=\"text-align: right;\">          3.14646 </td><td style=\"text-align: right;\">      6.22457 </td><td style=\"text-align: right;\"> 1689723685</td><td style=\"text-align: right;\">                   2</td><td>ecf589ea  </td></tr>\n",
       "<tr><td>FSR_Trainable_f0cdf92d</td><td>2023-07-19_08-51-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 77.0928</td><td style=\"text-align: right;\">5.41267e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">487688</td><td style=\"text-align: right;\">208.231</td><td style=\"text-align: right;\">           57.7292  </td><td style=\"text-align: right;\">         13.9287  </td><td style=\"text-align: right;\">     57.7292  </td><td style=\"text-align: right;\"> 1689724275</td><td style=\"text-align: right;\">                   4</td><td>f0cdf92d  </td></tr>\n",
       "<tr><td>FSR_Trainable_f69afa43</td><td>2023-07-19_08-55-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 38.131 </td><td style=\"text-align: right;\">1.20182e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">490375</td><td style=\"text-align: right;\">125.223</td><td style=\"text-align: right;\">          112.002   </td><td style=\"text-align: right;\">          1.11452 </td><td style=\"text-align: right;\">    112.002   </td><td style=\"text-align: right;\"> 1689724514</td><td style=\"text-align: right;\">                 100</td><td>f69afa43  </td></tr>\n",
       "<tr><td>FSR_Trainable_f9ed1d54</td><td>2023-07-19_09-17-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 42.3831</td><td style=\"text-align: right;\">2.29787e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">499263</td><td style=\"text-align: right;\">131.961</td><td style=\"text-align: right;\">           33.8171  </td><td style=\"text-align: right;\">          4.15433 </td><td style=\"text-align: right;\">     33.8171  </td><td style=\"text-align: right;\"> 1689725879</td><td style=\"text-align: right;\">                   8</td><td>f9ed1d54  </td></tr>\n",
       "<tr><td>FSR_Trainable_fa2f16e5</td><td>2023-07-19_08-52-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 80.8332</td><td style=\"text-align: right;\">5.196e+07  </td><td>172.26.215.93</td><td style=\"text-align: right;\">489687</td><td style=\"text-align: right;\">226.694</td><td style=\"text-align: right;\">            3.08564 </td><td style=\"text-align: right;\">          3.08564 </td><td style=\"text-align: right;\">      3.08564 </td><td style=\"text-align: right;\"> 1689724356</td><td style=\"text-align: right;\">                   1</td><td>fa2f16e5  </td></tr>\n",
       "<tr><td>FSR_Trainable_fa8aeae0</td><td>2023-07-19_09-19-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 76.6579</td><td style=\"text-align: right;\">6.29869e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">500850</td><td style=\"text-align: right;\">260.763</td><td style=\"text-align: right;\">            2.32697 </td><td style=\"text-align: right;\">          2.32697 </td><td style=\"text-align: right;\">      2.32697 </td><td style=\"text-align: right;\"> 1689725941</td><td style=\"text-align: right;\">                   1</td><td>fa8aeae0  </td></tr>\n",
       "<tr><td>FSR_Trainable_fb39f723</td><td>2023-07-19_08-45-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 46.5984</td><td style=\"text-align: right;\">9.96435e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">482056</td><td style=\"text-align: right;\">147.814</td><td style=\"text-align: right;\">          593.032   </td><td style=\"text-align: right;\">         10.776   </td><td style=\"text-align: right;\">    593.032   </td><td style=\"text-align: right;\"> 1689723954</td><td style=\"text-align: right;\">                  64</td><td>fb39f723  </td></tr>\n",
       "<tr><td>FSR_Trainable_fd6c7c3c</td><td>2023-07-19_08-39-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 82.1415</td><td style=\"text-align: right;\">5.99127e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">484146</td><td style=\"text-align: right;\">279.063</td><td style=\"text-align: right;\">           10.5931  </td><td style=\"text-align: right;\">         10.5931  </td><td style=\"text-align: right;\">     10.5931  </td><td style=\"text-align: right;\"> 1689723591</td><td style=\"text-align: right;\">                   1</td><td>fd6c7c3c  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_09940689_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_08-33-14/wandb/run-20230719_083324-09940689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: Syncing run FSR_Trainable_09940689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/09940689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_d64c9d11_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_08-33-19/wandb/run-20230719_083333-d64c9d11\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: Syncing run FSR_Trainable_d64c9d11\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d64c9d11\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb:                      mae 118.58105\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb:                     mape 2.418479672129364e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb:                     rmse 281.78443\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb:       time_since_restore 2.97722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb:         time_this_iter_s 2.97722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb:             time_total_s 2.97722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb:                timestamp 1689723209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: 🚀 View run FSR_Trainable_d64c9d11 at: https://wandb.ai/seokjin/FSR-prediction/runs/d64c9d11\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479722)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083333-d64c9d11/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_98881a9b_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_08-33-26/wandb/run-20230719_083341-98881a9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: Syncing run FSR_Trainable_98881a9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/98881a9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_ac61c658_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_08-33-34/wandb/run-20230719_083351-ac61c658\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Syncing run FSR_Trainable_ac61c658\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ac61c658\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_57412e6a_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_08-33-43/wandb/run-20230719_083405-57412e6a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: Syncing run FSR_Trainable_57412e6a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/57412e6a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb:                      mae 83.11606\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb:                     mape 59745988.08757\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb:                     rmse 225.42285\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb:       time_since_restore 6.32065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb:         time_this_iter_s 3.2005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb:             time_total_s 6.32065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb:                timestamp 1689723245\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: 🚀 View run FSR_Trainable_57412e6a at: https://wandb.ai/seokjin/FSR-prediction/runs/57412e6a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480289)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083405-57412e6a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb:                      mae █▇▇▆▆▆▅▅▄▄▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb:                     mape ▁▂▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb:                     rmse █▇▇▆▆▅▅▄▄▄▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▆▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb:         time_this_iter_s ▆▃▃▂▅▅▅▇▄█▃▄▂▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▆▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb:                timestamp ▁▂▂▃▃▄▄▅▅▆▆▇▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083351-ac61c658/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083351-ac61c658/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083351-ac61c658/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083351-ac61c658/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083351-ac61c658/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083351-ac61c658/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083351-ac61c658/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083351-ac61c658/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083351-ac61c658/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083351-ac61c658/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083351-ac61c658/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083351-ac61c658/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083351-ac61c658/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480077)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083351-ac61c658/logs\n",
      "2023-07-19 08:34:19,692\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.989 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:34:19,697\tWARNING util.py:315 -- The `process_trial_result` operation took 1.995 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:34:19,700\tWARNING util.py:315 -- Processing trial results took 1.998 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:34:19,702\tWARNING util.py:315 -- The `process_trial_result` operation took 2.000 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_c7074d67_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_08-33-56/wandb/run-20230719_083422-c7074d67\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Syncing run FSR_Trainable_c7074d67\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c7074d67\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb:                      mae ▁▂▂▃▄▄▅▆▇▇███▇▅▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb:                     mape ▁▂▂▃▃▄▄▅▆▆▇█████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb:                     rmse ███▇▇▇▇▇▇▇▇▇▇▆▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▆▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb:         time_this_iter_s ▃▅▂▅▄▄▆▇▄█▄▃▂▂▁▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▆▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb:                timestamp ▁▂▂▃▃▄▄▅▅▆▆▇▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb:                      mae 75.11459\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb:                     mape 36349546.56723\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb:                     rmse 222.63478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb:       time_since_restore 38.52639\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb:         time_this_iter_s 2.54251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb:             time_total_s 38.52639\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb:                timestamp 1689723258\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: 🚀 View run FSR_Trainable_98881a9b at: https://wandb.ai/seokjin/FSR-prediction/runs/98881a9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479895)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083341-98881a9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083422-c7074d67/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083422-c7074d67/logs\n",
      "2023-07-19 08:34:31,310\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.946 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:34:31,315\tWARNING util.py:315 -- The `process_trial_result` operation took 2.952 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:34:31,317\tWARNING util.py:315 -- Processing trial results took 2.954 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:34:31,319\tWARNING util.py:315 -- The `process_trial_result` operation took 2.956 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480531)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_5ea282f6_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_08-34-16/wandb/run-20230719_083434-5ea282f6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: Syncing run FSR_Trainable_5ea282f6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5ea282f6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb:                      mae 64.37417\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb:                     mape 2871113167803302.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb:                     rmse 239.91213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb:       time_since_restore 0.7843\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb:         time_this_iter_s 0.7843\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb:             time_total_s 0.7843\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb:                timestamp 1689723268\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: 🚀 View run FSR_Trainable_5ea282f6 at: https://wandb.ai/seokjin/FSR-prediction/runs/5ea282f6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083434-5ea282f6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480768)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_86a93f9f_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_08-34-27/wandb/run-20230719_083445-86a93f9f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: Syncing run FSR_Trainable_86a93f9f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/86a93f9f\n",
      "2023-07-19 08:34:52,797\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.143 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:34:52,800\tWARNING util.py:315 -- The `process_trial_result` operation took 2.147 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:34:52,801\tWARNING util.py:315 -- Processing trial results took 2.148 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:34:52,803\tWARNING util.py:315 -- The `process_trial_result` operation took 2.150 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:34:54,876\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.031 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:34:54,878\tWARNING util.py:315 -- The `process_trial_result` operation took 2.034 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:34:54,880\tWARNING util.py:315 -- Processing trial results took 2.035 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:34:54,881\tWARNING util.py:315 -- The `process_trial_result` operation took 2.037 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_4a5175af_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_08-34-38/wandb/run-20230719_083455-4a5175af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: Syncing run FSR_Trainable_4a5175af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4a5175af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb:                      mae 69.96432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb:                     mape 3.858361716146085e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb:                     rmse 232.47483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb:       time_since_restore 3.44891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb:         time_this_iter_s 1.12707\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb:             time_total_s 3.44891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb:                timestamp 1689723293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: 🚀 View run FSR_Trainable_4a5175af at: https://wandb.ai/seokjin/FSR-prediction/runs/4a5175af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481195)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083455-4a5175af/logs\n",
      "2023-07-19 08:35:10,068\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.823 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:35:10,073\tWARNING util.py:315 -- The `process_trial_result` operation took 2.829 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:35:10,075\tWARNING util.py:315 -- Processing trial results took 2.831 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:35:10,078\tWARNING util.py:315 -- The `process_trial_result` operation took 2.834 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_bb111c99_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-34-48/wandb/run-20230719_083512-bb111c99\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: Syncing run FSR_Trainable_bb111c99\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/bb111c99\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 08:35:23,061\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.970 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:35:23,064\tWARNING util.py:315 -- The `process_trial_result` operation took 1.973 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:35:23,066\tWARNING util.py:315 -- Processing trial results took 1.976 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:35:23,068\tWARNING util.py:315 -- The `process_trial_result` operation took 1.978 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_abcd8262_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-35-03/wandb/run-20230719_083523-abcd8262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: Syncing run FSR_Trainable_abcd8262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/abcd8262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 08:35:36,427\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.831 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:35:36,432\tWARNING util.py:315 -- The `process_trial_result` operation took 1.837 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:35:36,434\tWARNING util.py:315 -- Processing trial results took 1.839 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:35:36,436\tWARNING util.py:315 -- The `process_trial_result` operation took 1.841 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_1be8b213_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-35-16/wandb/run-20230719_083535-1be8b213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: Syncing run FSR_Trainable_1be8b213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1be8b213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb:                      mae 111.64536\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb:                     mape 6.12038904488874e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb:                     rmse 355.07079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb:       time_since_restore 6.74552\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb:         time_this_iter_s 6.74552\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb:             time_total_s 6.74552\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb:                timestamp 1689723334\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: 🚀 View run FSR_Trainable_1be8b213 at: https://wandb.ai/seokjin/FSR-prediction/runs/1be8b213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481873)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083535-1be8b213/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb:                      mae ██▇▇▇▆▅▅▅▅▅▅▅▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb:                     mape ███▇▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb:                     rmse ██▇▇▆▆▅▅▅▅▅▅▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb:         time_this_iter_s ▄▂▅▃▁▄▄▃▄▆▇█▄▄▃▆▄▂▂▄▁▃▃▃▂▄▆▄▇▆▄▄▃▄▅▄▃▄▅▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb:                      mae 50.45636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb:                     mape 21579093.35101\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb:                     rmse 154.17184\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb:       time_since_restore 131.54565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb:         time_this_iter_s 1.71718\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb:             time_total_s 131.54565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb:                timestamp 1689723351\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: 🚀 View run FSR_Trainable_09940689 at: https://wandb.ai/seokjin/FSR-prediction/runs/09940689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=479528)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083324-09940689/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb:       training_iteration ▁█\n",
      "2023-07-19 08:36:01,338\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.458 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:36:01,344\tWARNING util.py:315 -- The `process_trial_result` operation took 2.465 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:36:01,346\tWARNING util.py:315 -- Processing trial results took 2.467 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:36:01,349\tWARNING util.py:315 -- The `process_trial_result` operation took 2.470 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=480990)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_fb39f723_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-35-27/wandb/run-20230719_083602-fb39f723\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: Syncing run FSR_Trainable_fb39f723\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fb39f723\n",
      "2023-07-19 08:36:17,291\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.724 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:36:17,295\tWARNING util.py:315 -- The `process_trial_result` operation took 2.728 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:36:17,299\tWARNING util.py:315 -- Processing trial results took 2.733 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:36:17,301\tWARNING util.py:315 -- The `process_trial_result` operation took 2.734 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_5545c6d4_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-35-49/wandb/run-20230719_083617-5545c6d4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: Syncing run FSR_Trainable_5545c6d4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5545c6d4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb:                      mae ▃▁▂▂▄▄█▄▅▅▅▅▅▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb:                     mape ▂▁▂▂▄▆█▄▄▄▄▄▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb:                     rmse ▄▂▁▁▂▄█▄▅▅▅▅▅▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▃▄▄▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb:         time_this_iter_s ▂▁▃▁▁▂▂▃▃▂█▆▄▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▃▄▄▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb:                timestamp ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb:                      mae 82.23451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb:                     mape 3.169346446246542e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb:                     rmse 226.37987\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb:       time_since_restore 74.10685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb:         time_this_iter_s 4.36725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb:             time_total_s 74.10685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb:                timestamp 1689723395\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: 🚀 View run FSR_Trainable_abcd8262 at: https://wandb.ai/seokjin/FSR-prediction/runs/abcd8262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481650)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083523-abcd8262/logs\n",
      "2023-07-19 08:36:54,533\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.241 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:36:54,537\tWARNING util.py:315 -- The `process_trial_result` operation took 2.245 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:36:54,539\tWARNING util.py:315 -- Processing trial results took 2.247 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:36:54,540\tWARNING util.py:315 -- The `process_trial_result` operation took 2.248 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_259a7c0c_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-36-05/wandb/run-20230719_083658-259a7c0c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: Syncing run FSR_Trainable_259a7c0c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/259a7c0c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb:                      mae █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb:                     mape ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb:                     rmse █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb:         time_this_iter_s █▆▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb:                      mae 78.5474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb:                     mape 30236185.68695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb:                     rmse 223.14865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb:       time_since_restore 7.81073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb:         time_this_iter_s 1.66482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb:             time_total_s 7.81073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb:                timestamp 1689723420\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: 🚀 View run FSR_Trainable_259a7c0c at: https://wandb.ai/seokjin/FSR-prediction/runs/259a7c0c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482595)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083658-259a7c0c/logs\n",
      "2023-07-19 08:37:19,810\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.515 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:37:19,816\tWARNING util.py:315 -- The `process_trial_result` operation took 2.522 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:37:19,818\tWARNING util.py:315 -- Processing trial results took 2.524 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:37:19,819\tWARNING util.py:315 -- The `process_trial_result` operation took 2.526 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_c29aa22c_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-36-50/wandb/run-20230719_083722-c29aa22c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: Syncing run FSR_Trainable_c29aa22c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c29aa22c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb:                      mae 83.14274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb:                     mape 61090275.20991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb:                     rmse 228.42828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb:       time_since_restore 3.58522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb:         time_this_iter_s 3.58522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb:             time_total_s 3.58522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb:                timestamp 1689723437\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: 🚀 View run FSR_Trainable_c29aa22c at: https://wandb.ai/seokjin/FSR-prediction/runs/c29aa22c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083722-c29aa22c/logs\n",
      "2023-07-19 08:37:40,555\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.417 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:37:40,562\tWARNING util.py:315 -- The `process_trial_result` operation took 2.425 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:37:40,564\tWARNING util.py:315 -- Processing trial results took 2.427 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:37:40,566\tWARNING util.py:315 -- The `process_trial_result` operation took 2.428 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_8ad9ac30_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-37-13/wandb/run-20230719_083743-8ad9ac30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: Syncing run FSR_Trainable_8ad9ac30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8ad9ac30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb:                      mae 83.90639\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb:                     mape 37414192.45166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb:                     rmse 232.9247\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb:       time_since_restore 3.91953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb:         time_this_iter_s 3.91953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb:             time_total_s 3.91953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb:                timestamp 1689723458\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: 🚀 View run FSR_Trainable_8ad9ac30 at: https://wandb.ai/seokjin/FSR-prediction/runs/8ad9ac30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483057)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083743-8ad9ac30/logs\n",
      "2023-07-19 08:37:58,965\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.887 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:37:58,971\tWARNING util.py:315 -- The `process_trial_result` operation took 2.894 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:37:58,974\tWARNING util.py:315 -- Processing trial results took 2.897 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:37:58,976\tWARNING util.py:315 -- The `process_trial_result` operation took 2.899 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_48d43b42_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-37-34/wandb/run-20230719_083803-48d43b42\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: Syncing run FSR_Trainable_48d43b42\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/48d43b42\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb:                      mae 82.18818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb:                     mape 23752684.91276\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb:                     rmse 236.83722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb:       time_since_restore 1.55829\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb:         time_this_iter_s 1.55829\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb:             time_total_s 1.55829\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb:                timestamp 1689723476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: 🚀 View run FSR_Trainable_48d43b42 at: https://wandb.ai/seokjin/FSR-prediction/runs/48d43b42\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483283)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083803-48d43b42/logs\n",
      "2023-07-19 08:38:17,486\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.700 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:38:17,491\tWARNING util.py:315 -- The `process_trial_result` operation took 2.705 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:38:17,492\tWARNING util.py:315 -- Processing trial results took 2.707 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:38:17,494\tWARNING util.py:315 -- The `process_trial_result` operation took 2.709 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_7f2a80f4_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-37-54/wandb/run-20230719_083821-7f2a80f4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: Syncing run FSR_Trainable_7f2a80f4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7f2a80f4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb:                      mae 78.05031\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb:                     mape 32473495.82071\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb:                     rmse 233.29086\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb:       time_since_restore 1.66041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb:         time_this_iter_s 1.66041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb:             time_total_s 1.66041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb:                timestamp 1689723494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: 🚀 View run FSR_Trainable_7f2a80f4 at: https://wandb.ai/seokjin/FSR-prediction/runs/7f2a80f4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483507)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083821-7f2a80f4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_42a6be99_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-38-13/wandb/run-20230719_083840-42a6be99\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: Syncing run FSR_Trainable_42a6be99\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/42a6be99\n",
      "2023-07-19 08:38:41,773\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.640 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:38:41,779\tWARNING util.py:315 -- The `process_trial_result` operation took 2.646 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:38:41,781\tWARNING util.py:315 -- Processing trial results took 2.649 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:38:41,783\tWARNING util.py:315 -- The `process_trial_result` operation took 2.651 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb:                      mae ▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb:                     mape ▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▄▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb:                     rmse █████▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▄▄▃▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb:         time_this_iter_s █▃▁▂▁▂▁▂▂▁▂▁▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▃▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb:                      mae 65.94046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb:                     mape 3328110811588327.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb:                     rmse 218.06125\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb:       time_since_restore 165.10552\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb:         time_this_iter_s 4.96267\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb:             time_total_s 165.10552\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb:                timestamp 1689723538\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: 🚀 View run FSR_Trainable_5545c6d4 at: https://wandb.ai/seokjin/FSR-prediction/runs/5545c6d4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482362)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083617-5545c6d4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_acce6591_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-38-31/wandb/run-20230719_083922-acce6591\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: Syncing run FSR_Trainable_acce6591\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/acce6591\n",
      "2023-07-19 08:39:26,422\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.214 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:39:26,425\tWARNING util.py:315 -- The `process_trial_result` operation took 2.218 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:39:26,427\tWARNING util.py:315 -- Processing trial results took 2.220 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:39:26,428\tWARNING util.py:315 -- The `process_trial_result` operation took 2.222 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb:                      mae 87.01703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb:                     mape 1.0113393097874584e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb:                     rmse 256.39118\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb:       time_since_restore 10.83841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb:         time_this_iter_s 10.83841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb:             time_total_s 10.83841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb:                timestamp 1689723564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: 🚀 View run FSR_Trainable_acce6591 at: https://wandb.ai/seokjin/FSR-prediction/runs/acce6591\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483973)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083922-acce6591/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_fd6c7c3c_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-39-13/wandb/run-20230719_083950-fd6c7c3c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: Syncing run FSR_Trainable_fd6c7c3c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd6c7c3c\n",
      "2023-07-19 08:39:54,314\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.560 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:39:54,330\tWARNING util.py:315 -- The `process_trial_result` operation took 2.577 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:39:54,334\tWARNING util.py:315 -- Processing trial results took 2.580 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:39:54,339\tWARNING util.py:315 -- The `process_trial_result` operation took 2.586 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb:                      mae 82.14146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb:                     mape 5.991265231311563e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb:                     rmse 279.06305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb:       time_since_restore 10.59312\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb:         time_this_iter_s 10.59312\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb:             time_total_s 10.59312\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb:                timestamp 1689723591\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: 🚀 View run FSR_Trainable_fd6c7c3c at: https://wandb.ai/seokjin/FSR-prediction/runs/fd6c7c3c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484201)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083950-fd6c7c3c/logs\n",
      "2023-07-19 08:40:13,517\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.756 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:40:13,523\tWARNING util.py:315 -- The `process_trial_result` operation took 2.763 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:40:13,524\tWARNING util.py:315 -- Processing trial results took 2.765 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:40:13,527\tWARNING util.py:315 -- The `process_trial_result` operation took 2.768 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_ba00398c_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-39-41/wandb/run-20230719_084017-ba00398c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: Syncing run FSR_Trainable_ba00398c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ba00398c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb:                      mae 64.31897\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb:                     mape 1189359690159301.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb:                     rmse 235.05781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb:       time_since_restore 2.48324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb:         time_this_iter_s 2.48324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb:             time_total_s 2.48324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb:                timestamp 1689723610\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: 🚀 View run FSR_Trainable_ba00398c at: https://wandb.ai/seokjin/FSR-prediction/runs/ba00398c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484430)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084017-ba00398c/logs\n",
      "2023-07-19 08:40:32,794\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.567 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:40:32,798\tWARNING util.py:315 -- The `process_trial_result` operation took 2.572 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:40:32,800\tWARNING util.py:315 -- Processing trial results took 2.574 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:40:32,803\tWARNING util.py:315 -- The `process_trial_result` operation took 2.577 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_8dc179df_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-40-08/wandb/run-20230719_084037-8dc179df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Syncing run FSR_Trainable_8dc179df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8dc179df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb:                      mae █▆▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb:                     mape █▃▃▃▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb:                     rmse █▆▅▄▅▅▅▄▄▃▃▃▃▂▂▂▂▃▂▁▁▁▁▁▁▁▁▁▂▂▂▂▃▁▂▂▁▂▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb:         time_this_iter_s ▆▁▁▃▂▄▄▂█▄▃▃▃▄▂▃▄▃▄▄▃▃▂▄▂▄▃▄▃▃▄▃▃▄▄▃▃▅▃▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb:                      mae 43.60173\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb:                     mape 16235262.18438\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb:                     rmse 143.84212\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb:       time_since_restore 308.59204\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb:         time_this_iter_s 3.4267\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb:             time_total_s 308.59204\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb:                timestamp 1689723636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: 🚀 View run FSR_Trainable_bb111c99 at: https://wandb.ai/seokjin/FSR-prediction/runs/bb111c99\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=481430)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083512-bb111c99/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb:       training_iteration ▁█\n",
      "2023-07-19 08:40:52,694\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.939 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:40:52,698\tWARNING util.py:315 -- The `process_trial_result` operation took 2.944 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:40:52,701\tWARNING util.py:315 -- Processing trial results took 2.947 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:40:52,703\tWARNING util.py:315 -- The `process_trial_result` operation took 2.949 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_d4fa06b8_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-40-27/wandb/run-20230719_084056-d4fa06b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: Syncing run FSR_Trainable_d4fa06b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d4fa06b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb:                      mae ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb:                     mape █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb:                     rmse █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb:         time_this_iter_s ▅▅▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb:                      mae 63.98272\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb:                     mape 1463034530182177.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb:                     rmse 223.06621\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb:       time_since_restore 9.35918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb:         time_this_iter_s 2.48584\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb:             time_total_s 9.35918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb:                timestamp 1689723659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: 🚀 View run FSR_Trainable_d4fa06b8 at: https://wandb.ai/seokjin/FSR-prediction/runs/d4fa06b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=484891)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084056-d4fa06b8/logs\n",
      "2023-07-19 08:41:09,226\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.963 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:41:09,239\tWARNING util.py:315 -- The `process_trial_result` operation took 2.976 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:41:09,242\tWARNING util.py:315 -- Processing trial results took 2.979 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:41:09,250\tWARNING util.py:315 -- The `process_trial_result` operation took 2.987 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_7322ae35_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-40-47/wandb/run-20230719_084111-7322ae35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: Syncing run FSR_Trainable_7322ae35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7322ae35\n",
      "2023-07-19 08:41:22,827\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.009 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:41:22,834\tWARNING util.py:315 -- The `process_trial_result` operation took 3.017 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:41:22,836\tWARNING util.py:315 -- Processing trial results took 3.019 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:41:22,844\tWARNING util.py:315 -- The `process_trial_result` operation took 3.026 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_ecf589ea_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-41-02/wandb/run-20230719_084125-ecf589ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: Syncing run FSR_Trainable_ecf589ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ecf589ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb:                      mae 80.70516\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb:                     mape 51176553.32225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb:                     rmse 224.39918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb:       time_since_restore 6.22457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb:         time_this_iter_s 3.14646\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb:             time_total_s 6.22457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb:                timestamp 1689723685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: 🚀 View run FSR_Trainable_ecf589ea at: https://wandb.ai/seokjin/FSR-prediction/runs/ecf589ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485327)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084125-ecf589ea/logs\n",
      "2023-07-19 08:41:46,058\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.281 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:41:46,062\tWARNING util.py:315 -- The `process_trial_result` operation took 2.285 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:41:46,063\tWARNING util.py:315 -- Processing trial results took 2.287 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:41:46,065\tWARNING util.py:315 -- The `process_trial_result` operation took 2.289 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_9ab979db_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-41-16/wandb/run-20230719_084148-9ab979db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: Syncing run FSR_Trainable_9ab979db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9ab979db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb:                      mae ▇█▇▇▆▅▅▄▃▃▃▃▂▂▂▂▁▁▁▂▂▂▂▂▁▁▁▁▁▁▂▁▂▁▂▁▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb:                     mape █▆▆▅▄▃▃▃▃▃▄▄▄▄▃▃▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb:                     rmse ▇█▇▆▆▆▆▅▄▄▃▃▂▂▂▂▂▁▁▂▂▂▂▂▁▁▁▁▂▁▂▂▂▂▃▁▂▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb:         time_this_iter_s ▄▂▂▂▂▂▂▂▂▂▂▁▂▁▂▃▄▂▁▂▂▁▂▁▁▁▁▃▂█▄▅▃▂▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb:                      mae 40.95831\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb:                     mape 15428631.83523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb:                     rmse 133.99837\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb:       time_since_restore 169.95965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb:         time_this_iter_s 2.36575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb:             time_total_s 169.95965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb:                timestamp 1689723875\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: 🚀 View run FSR_Trainable_9ab979db at: https://wandb.ai/seokjin/FSR-prediction/runs/9ab979db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485557)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084148-9ab979db/logs\n",
      "2023-07-19 08:44:57,951\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.822 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:44:57,979\tWARNING util.py:315 -- The `process_trial_result` operation took 2.851 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:44:57,982\tWARNING util.py:315 -- Processing trial results took 2.854 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:44:57,984\tWARNING util.py:315 -- The `process_trial_result` operation took 2.857 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_3c823ffc_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-41-40/wandb/run-20230719_084500-3c823ffc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: Syncing run FSR_Trainable_3c823ffc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3c823ffc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb:                      mae ▅▄▄▅▇▅▆▅▇▃▃▃▂▄▃▅▃▅▆▇▄█▆▂▂▅▂▆▃▅▅▆▃▇▂▃▃▄▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb:                     mape ▁▂▄▆▆▇▆▄█▂▃▃▅▃▃▂▂▅▅█▇▅▅▂▂▂▃▂▃▃▂▁▃▁▃▁▃▄▅▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb:                     rmse ▆▅▄▄▆▄▄▅▅▄▃▃▁▄▂▆▃▄▄▅▂▇▄▂▂▅▁▇▂▄▅▆▂█▁▃▃▃▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb:         time_this_iter_s ▄█▂▂▂▃▃▃▃▂▃▃▃▄▂▄▃▄▃▁▄▃▃▃▃▃▃▄▃▃▂▆▄▂▃▃▁▁▂▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb:                      mae 46.59837\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb:                     mape 9964354036295460.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb:                     rmse 147.81376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb:       time_since_restore 593.03203\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb:         time_this_iter_s 10.77603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb:             time_total_s 593.03203\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb:                timestamp 1689723954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: 🚀 View run FSR_Trainable_fb39f723 at: https://wandb.ai/seokjin/FSR-prediction/runs/fb39f723\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=482123)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083602-fb39f723/logs\n",
      "2023-07-19 08:46:13,350\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.316 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:46:13,353\tWARNING util.py:315 -- The `process_trial_result` operation took 2.320 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:46:13,354\tWARNING util.py:315 -- Processing trial results took 2.321 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:46:13,359\tWARNING util.py:315 -- The `process_trial_result` operation took 2.325 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_e93de5c7_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-44-51/wandb/run-20230719_084615-e93de5c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Syncing run FSR_Trainable_e93de5c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e93de5c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb:                      mae █▅▄▄▃▄▃▄▄▃▃▂▂▃▃▃▃▂▂▃▃▂▂▂▂▁▂▂▂▂▂▁▁▂▂▂▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb:                     mape █▄▃▂▂▁▁▁▂▁▁▁▁▃▁▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb:                     rmse █▆▅▄▄▅▅▅▆▅▄▄▃▄▃▃▄▂▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▂▂▂▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb:         time_this_iter_s ▆▄▄▃▄▃▃▄▃▃▃▃▃▄▃▃▃▃▃▄█▄▂▃▂▃▃▃▃▂▁▁▂▂▂▂▄▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb:                      mae 46.01748\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb:                     mape 23351897.34436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb:                     rmse 141.47598\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb:       time_since_restore 306.28578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb:         time_this_iter_s 2.55884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb:             time_total_s 306.28578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb:                timestamp 1689723979\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: 🚀 View run FSR_Trainable_7322ae35 at: https://wandb.ai/seokjin/FSR-prediction/runs/7322ae35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485112)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084111-7322ae35/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb:                      mae █▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb:                     mape █▇▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb:                     rmse █▁▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb:         time_this_iter_s █▅▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb:                timestamp ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084615-e93de5c7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084615-e93de5c7/logs\n",
      "2023-07-19 08:46:37,273\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.371 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:46:37,277\tWARNING util.py:315 -- The `process_trial_result` operation took 2.376 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:46:37,288\tWARNING util.py:315 -- Processing trial results took 2.387 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:46:37,291\tWARNING util.py:315 -- The `process_trial_result` operation took 2.390 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_62dc9803_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-46-07/wandb/run-20230719_084638-62dc9803\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Syncing run FSR_Trainable_62dc9803\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/62dc9803\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb:                      mae █▃▅▅▄▃▂▂▁▁▁▁▂▂▂▂▃▃▂▂▁▂▂▂▂▂▂▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb:                     mape █▃▆▆▆▅▃▃▃▃▃▄▅▄▃▃▇▄▄▄▂▃▃▄▅▂▃▃▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb:                     rmse █▄▅▅▄▂▂▂▁▁▁▁▂▃▃▄▅▄▄▄▄▄▄▄▄▄▄▄▃▃▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb:         time_this_iter_s ▇▇▂▂▂▁▂▂▂▃▂▃▃▄▄▂▇█▅▄▃▃▄▄▅▃▃▂▁▂▅▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb:                      mae 54.62743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb:                     mape 15043718.904\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb:                     rmse 175.67845\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb:       time_since_restore 102.18021\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb:         time_this_iter_s 2.86497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb:             time_total_s 102.18021\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb:                timestamp 1689724000\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: 🚀 View run FSR_Trainable_3c823ffc at: https://wandb.ai/seokjin/FSR-prediction/runs/3c823ffc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084500-3c823ffc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb:       training_iteration ▁\n",
      "2023-07-19 08:46:47,591\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.355 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:46:47,596\tWARNING util.py:315 -- The `process_trial_result` operation took 2.361 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:46:47,600\tWARNING util.py:315 -- Processing trial results took 2.364 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:46:47,604\tWARNING util.py:315 -- The `process_trial_result` operation took 2.368 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=485851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486341)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_6e372da9_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-46-31/wandb/run-20230719_084650-6e372da9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: Syncing run FSR_Trainable_6e372da9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6e372da9\n",
      "2023-07-19 08:46:59,730\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.197 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:46:59,757\tWARNING util.py:315 -- The `process_trial_result` operation took 2.226 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:46:59,759\tWARNING util.py:315 -- Processing trial results took 2.227 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:46:59,761\tWARNING util.py:315 -- The `process_trial_result` operation took 2.229 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_aacfb51a_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-46-42/wandb/run-20230719_084702-aacfb51a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: Syncing run FSR_Trainable_aacfb51a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/aacfb51a\n",
      "2023-07-19 08:47:12,914\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.757 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:47:12,917\tWARNING util.py:315 -- The `process_trial_result` operation took 2.762 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:47:12,952\tWARNING util.py:315 -- Processing trial results took 2.797 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:47:12,954\tWARNING util.py:315 -- The `process_trial_result` operation took 2.799 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_c5ab6926_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-46-54/wandb/run-20230719_084715-c5ab6926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: Syncing run FSR_Trainable_c5ab6926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c5ab6926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb:                      mae █▇█▇▇▅▃▂▃▁▁▂▂▂▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb:                     mape █▄▄▄▅▄▂▁▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb:                     rmse ▆▇█▇▆▄▃▂▂▁▁▂▃▄▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb:         time_this_iter_s ▇▄▃▁█▄▃▄▅▅▆▃▂▂▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb:                timestamp ▁▂▂▃▃▄▄▅▅▅▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb:                      mae 58.06131\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb:                     mape 19567449.38486\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb:                     rmse 186.51428\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb:       time_since_restore 39.4812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb:         time_this_iter_s 2.43446\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb:             time_total_s 39.4812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb:                timestamp 1689724047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: 🚀 View run FSR_Trainable_6e372da9 at: https://wandb.ai/seokjin/FSR-prediction/runs/6e372da9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486561)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084650-6e372da9/logs\n",
      "2023-07-19 08:47:45,303\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.287 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:47:45,306\tWARNING util.py:315 -- The `process_trial_result` operation took 2.291 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:47:45,308\tWARNING util.py:315 -- Processing trial results took 2.294 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:47:45,310\tWARNING util.py:315 -- The `process_trial_result` operation took 2.296 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_07f295fa_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-47-07/wandb/run-20230719_084748-07f295fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: Syncing run FSR_Trainable_07f295fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/07f295fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb:                      mae ▇██▅▅▄▃▄▄▄▃▄▃▂▁▂▂▁▂▂▂▂▃▃▂▃▃▃▂▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb:                     mape █▆█▅▂▃▁▄▅█▆▄▆▇▄▄▃▁▄▄▃▃▄▄▃▄▂▃▃▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb:                     rmse ███▅▅▃▄▄▄▄▃▅▃▂▁▃▃▁▃▂▂▂▃▃▂▃▂▂▂▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb:         time_this_iter_s ▆█▅▂▂▂▃▂▂▂▁▂▂▂▂▃▂▄▄▅▄▆▄▃▄▂▃▁▂▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb:                      mae 50.34206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb:                     mape 21402461.08048\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb:                     rmse 161.71522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb:       time_since_restore 80.60828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb:         time_this_iter_s 2.57094\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb:             time_total_s 80.60828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb:                timestamp 1689724143\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: 🚀 View run FSR_Trainable_07f295fa at: https://wandb.ai/seokjin/FSR-prediction/runs/07f295fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487233)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084748-07f295fa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_630e06a5_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-47-39/wandb/run-20230719_084928-630e06a5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Syncing run FSR_Trainable_630e06a5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/630e06a5\n",
      "2023-07-19 08:49:33,752\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.250 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:49:33,756\tWARNING util.py:315 -- The `process_trial_result` operation took 2.255 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:49:33,757\tWARNING util.py:315 -- Processing trial results took 2.256 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:49:33,759\tWARNING util.py:315 -- The `process_trial_result` operation took 2.258 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb:                      mae █▅▄▃▃▂▁▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▂▁▁▁▁▁▂▁▁▁▁▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb:                     mape █▅▄▃▃▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▂▁▂▁▂▅▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb:                     rmse █▅▅▄▃▃▁▁▁▂▂▂▂▁▂▂▂▂▂▂▂▂▁▁▂▁▂▂▂▂▂▁▂▁▁▂▁▁▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb:         time_this_iter_s ▅▃▄▄▃▃▄▁▄▄▄▃▃▃▅▃▂█▅▃▄▃▁▁▂▂▁▃▂▂▂▁▂▂▄▃▂▃▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb:                      mae 39.92739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb:                     mape 17112605.29575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb:                     rmse 129.4271\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb:       time_since_restore 673.34995\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb:         time_this_iter_s 7.27552\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb:             time_total_s 673.34995\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb:                timestamp 1689724202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: 🚀 View run FSR_Trainable_42a6be99 at: https://wandb.ai/seokjin/FSR-prediction/runs/42a6be99\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=483734)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_083840-42a6be99/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb:                      mae ▁▁█▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb:                     mape ▂▁█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb:                     rmse ▃▁█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb:         time_this_iter_s █▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb:                timestamp ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084928-630e06a5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084928-630e06a5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084928-630e06a5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084928-630e06a5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084928-630e06a5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084928-630e06a5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084928-630e06a5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084928-630e06a5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084928-630e06a5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084928-630e06a5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084928-630e06a5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084928-630e06a5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084928-630e06a5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487501)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084928-630e06a5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_f0cdf92d_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-49-17/wandb/run-20230719_085024-f0cdf92d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: Syncing run FSR_Trainable_f0cdf92d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0cdf92d\n",
      "2023-07-19 08:50:32,421\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.406 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:50:32,437\tWARNING util.py:315 -- The `process_trial_result` operation took 2.423 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:50:32,439\tWARNING util.py:315 -- Processing trial results took 2.425 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:50:32,441\tWARNING util.py:315 -- The `process_trial_result` operation took 2.427 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:50:35,776\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.115 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:50:35,781\tWARNING util.py:315 -- The `process_trial_result` operation took 3.121 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:50:35,785\tWARNING util.py:315 -- Processing trial results took 3.125 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:50:35,802\tWARNING util.py:315 -- The `process_trial_result` operation took 3.142 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_3b8ffc8c_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-50-15/wandb/run-20230719_085037-3b8ffc8c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: Syncing run FSR_Trainable_3b8ffc8c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3b8ffc8c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb:                      mae 83.58275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb:                     mape 67312784.80712\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb:                     rmse 225.10042\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb:       time_since_restore 2.96987\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb:         time_this_iter_s 2.96987\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb:             time_total_s 2.96987\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb:                timestamp 1689724232\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: 🚀 View run FSR_Trainable_3b8ffc8c at: https://wandb.ai/seokjin/FSR-prediction/runs/3b8ffc8c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487959)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085037-3b8ffc8c/logs\n",
      "2023-07-19 08:50:56,708\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.127 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:50:56,712\tWARNING util.py:315 -- The `process_trial_result` operation took 3.133 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:50:56,716\tWARNING util.py:315 -- Processing trial results took 3.137 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:50:56,718\tWARNING util.py:315 -- The `process_trial_result` operation took 3.139 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_72b7c922_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-50-29/wandb/run-20230719_085059-72b7c922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: Syncing run FSR_Trainable_72b7c922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/72b7c922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb:                      mae 75.55184\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb:                     mape 45873732.5854\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb:                     rmse 218.15244\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb:       time_since_restore 7.66907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb:         time_this_iter_s 3.60475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb:             time_total_s 7.66907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb:                timestamp 1689724260\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: 🚀 View run FSR_Trainable_72b7c922 at: https://wandb.ai/seokjin/FSR-prediction/runs/72b7c922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488197)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085059-72b7c922/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb:                      mae ▇▁█▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb:                     mape ▃▁▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb:                     rmse █▁▇▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb:         time_this_iter_s ▄▁█▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb:                timestamp ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb:                      mae 77.09278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb:                     mape 54126666.9403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb:                     rmse 208.23059\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb:       time_since_restore 57.72922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb:         time_this_iter_s 13.92867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb:             time_total_s 57.72922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb:                timestamp 1689724275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: 🚀 View run FSR_Trainable_f0cdf92d at: https://wandb.ai/seokjin/FSR-prediction/runs/f0cdf92d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=487757)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085024-f0cdf92d/logs\n",
      "2023-07-19 08:51:22,623\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.786 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:51:22,627\tWARNING util.py:315 -- The `process_trial_result` operation took 2.790 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:51:22,628\tWARNING util.py:315 -- Processing trial results took 2.792 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:51:22,629\tWARNING util.py:315 -- The `process_trial_result` operation took 2.793 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_8cb3727c_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-50-49/wandb/run-20230719_085124-8cb3727c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: Syncing run FSR_Trainable_8cb3727c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8cb3727c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 08:51:33,350\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.255 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:51:33,353\tWARNING util.py:315 -- The `process_trial_result` operation took 2.259 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:51:33,355\tWARNING util.py:315 -- Processing trial results took 2.260 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:51:33,357\tWARNING util.py:315 -- The `process_trial_result` operation took 2.262 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb:                      mae █▅▅▆▃▃▃▁▃▂▃▂▃▂▃▃▃▃▃▂▂▃▂▂▂▁▁▁▁▂▁▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb:                     mape ▆▅▆█▅▄▃▁▃▃▃▃▄▂▃▂▃▃▃▃▂▃▂▃▁▂▁▁▁▂▁▂▂▂▁▂▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb:                     rmse █▄▄▅▃▃▃▂▂▁▂▂▃▂▃▄▃▂▂▂▂▃▂▂▂▁▂▁▂▁▁▂▂▁▁▁▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb:         time_this_iter_s ▆▂▃▂▂▂▂▄▅▃▂▂▂▃▅▅▄▃▂▂▃▄▇▄▄▄▄▅▄▃▄▃▅▄▄▆▄▁█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb:                      mae 39.41867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb:                     mape 12942950.68347\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb:                     rmse 133.48113\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb:       time_since_restore 262.33618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb:         time_this_iter_s 2.03271\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb:             time_total_s 262.33618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb:                timestamp 1689724289\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: 🚀 View run FSR_Trainable_aacfb51a at: https://wandb.ai/seokjin/FSR-prediction/runs/aacfb51a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486786)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084702-aacfb51a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_76612c31_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-51-14/wandb/run-20230719_085137-76612c31\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: Syncing run FSR_Trainable_76612c31\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/76612c31\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb:                      mae █▄▅▃▅▃▃▄▃▄▂▆▄▂▂▂▁▂▄▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▃▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb:                     mape ▇▃▄▁▃▄▁▆▇▇▄█▇▄▃▃▅▄▆▄▆▄▃▅▅▃▃▃▃▂▃▂▂▂▁▂▆▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb:                     rmse █▃▅▄▆▃▄▅▄▅▃▇▄▁▁▂▁▂▄▂▂▁▁▁▂▂▂▂▂▂▂▃▂▁▁▁▃▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb:         time_this_iter_s ▆▂▂▂▂▄▄▃▂▂▂▂▅▅▅▃▁▃▃▃█▃▄▄▄▅▄▂▄▃▅▄▄▆▄▁█▁▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb:                      mae 40.56739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb:                     mape 16825990.35311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb:                     rmse 129.01782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb:       time_since_restore 262.18452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb:         time_this_iter_s 2.26321\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb:             time_total_s 262.18452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb:                timestamp 1689724303\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: 🚀 View run FSR_Trainable_c5ab6926 at: https://wandb.ai/seokjin/FSR-prediction/runs/c5ab6926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=486998)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_084715-c5ab6926/logs\n",
      "2023-07-19 08:51:50,304\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.883 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:51:50,307\tWARNING util.py:315 -- The `process_trial_result` operation took 2.887 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:51:50,316\tWARNING util.py:315 -- Processing trial results took 2.896 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:51:50,326\tWARNING util.py:315 -- The `process_trial_result` operation took 2.907 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_aea4e0a6_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-51-29/wandb/run-20230719_085153-aea4e0a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: Syncing run FSR_Trainable_aea4e0a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/aea4e0a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb:                      mae 78.00874\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb:                     mape 1.4836393301457597e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb:                     rmse 224.74797\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb:       time_since_restore 5.4496\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb:         time_this_iter_s 2.43136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb:             time_total_s 5.4496\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb:                timestamp 1689724312\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: 🚀 View run FSR_Trainable_aea4e0a6 at: https://wandb.ai/seokjin/FSR-prediction/runs/aea4e0a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488885)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085153-aea4e0a6/logs\n",
      "2023-07-19 08:52:01,812\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.252 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:52:01,815\tWARNING util.py:315 -- The `process_trial_result` operation took 2.256 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:52:01,817\tWARNING util.py:315 -- Processing trial results took 2.258 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:52:01,819\tWARNING util.py:315 -- The `process_trial_result` operation took 2.260 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_dbfbbd1b_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-51-44/wandb/run-20230719_085204-dbfbbd1b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: Syncing run FSR_Trainable_dbfbbd1b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/dbfbbd1b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb:                      mae 126.62383\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb:                     mape 1.7690057718103155e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb:                     rmse 332.47956\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb:       time_since_restore 2.13532\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb:         time_this_iter_s 2.13532\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb:             time_total_s 2.13532\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb:                timestamp 1689724319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: 🚀 View run FSR_Trainable_dbfbbd1b at: https://wandb.ai/seokjin/FSR-prediction/runs/dbfbbd1b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489113)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085204-dbfbbd1b/logs\n",
      "2023-07-19 08:52:13,568\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.808 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:52:13,572\tWARNING util.py:315 -- The `process_trial_result` operation took 1.813 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:52:13,574\tWARNING util.py:315 -- Processing trial results took 1.816 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:52:13,589\tWARNING util.py:315 -- The `process_trial_result` operation took 1.831 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_640d99b6_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-51-57/wandb/run-20230719_085216-640d99b6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: Syncing run FSR_Trainable_640d99b6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/640d99b6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)04 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb:                      mae 77.78882\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb:                     mape 50884719.1313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb:                     rmse 229.97976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb:       time_since_restore 1.44092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb:         time_this_iter_s 1.44092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb:             time_total_s 1.44092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb:                timestamp 1689724331\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: 🚀 View run FSR_Trainable_640d99b6 at: https://wandb.ai/seokjin/FSR-prediction/runs/640d99b6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489342)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085216-640d99b6/logs\n",
      "2023-07-19 08:52:26,367\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.065 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:52:26,371\tWARNING util.py:315 -- The `process_trial_result` operation took 2.072 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:52:26,377\tWARNING util.py:315 -- Processing trial results took 2.077 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:52:26,379\tWARNING util.py:315 -- The `process_trial_result` operation took 2.080 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_35b09d69_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-52-10/wandb/run-20230719_085229-35b09d69\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Syncing run FSR_Trainable_35b09d69\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/35b09d69\n",
      "2023-07-19 08:52:38,959\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.051 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:52:38,963\tWARNING util.py:315 -- The `process_trial_result` operation took 2.055 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:52:38,964\tWARNING util.py:315 -- Processing trial results took 2.056 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:52:38,972\tWARNING util.py:315 -- The `process_trial_result` operation took 2.064 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_fa2f16e5_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-52-22/wandb/run-20230719_085241-fa2f16e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: Syncing run FSR_Trainable_fa2f16e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fa2f16e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb:                      mae 80.83325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb:                     mape 51959966.41639\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb:                     rmse 226.69415\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb:       time_since_restore 3.08564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb:         time_this_iter_s 3.08564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb:             time_total_s 3.08564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb:                timestamp 1689724356\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: 🚀 View run FSR_Trainable_fa2f16e5 at: https://wandb.ai/seokjin/FSR-prediction/runs/fa2f16e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489786)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085241-fa2f16e5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb:                      mae █▄▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▂▃▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb:                     mape █▄▄▅▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▂▂▁▂▁▁▁▂▄▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb:                     rmse █▄▅▄▃▃▂▂▃▃▃▃▃▃▃▃▂▂▂▂▂▃▂▂▂▂▃▄▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb:         time_this_iter_s █▃▂▂▂▄▂▂▄▄▃▂▃▂▂▁▁▁▁▂▂▁▁▂▂▂▃▃▃▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb:                      mae 48.9927\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb:                     mape 20409909.40476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb:                     rmse 160.6281\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb:       time_since_restore 86.07594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb:         time_this_iter_s 2.47038\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb:             time_total_s 86.07594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb:                timestamp 1689724369\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: 🚀 View run FSR_Trainable_8cb3727c at: https://wandb.ai/seokjin/FSR-prediction/runs/8cb3727c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488426)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085124-8cb3727c/logs\n",
      "2023-07-19 08:52:55,989\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.949 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:52:55,992\tWARNING util.py:315 -- The `process_trial_result` operation took 1.953 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:52:55,994\tWARNING util.py:315 -- Processing trial results took 1.955 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:52:55,995\tWARNING util.py:315 -- The `process_trial_result` operation took 1.956 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_cd040040_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-52-33/wandb/run-20230719_085258-cd040040\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: Syncing run FSR_Trainable_cd040040\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cd040040\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 08:53:05,480\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.906 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:53:05,482\tWARNING util.py:315 -- The `process_trial_result` operation took 1.908 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:53:05,484\tWARNING util.py:315 -- Processing trial results took 1.911 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:53:05,485\tWARNING util.py:315 -- The `process_trial_result` operation took 1.912 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb:                      mae █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb:                     mape █▅▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb:                     rmse █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb:         time_this_iter_s █▆▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb:                      mae 63.38304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb:                     mape 26035410.84537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb:                     rmse 193.3251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb:       time_since_restore 9.37257\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb:         time_this_iter_s 2.22784\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb:             time_total_s 9.37257\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb:                timestamp 1689724382\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: 🚀 View run FSR_Trainable_cd040040 at: https://wandb.ai/seokjin/FSR-prediction/runs/cd040040\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490021)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085258-cd040040/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_7e662eaf_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-52-51/wandb/run-20230719_085308-7e662eaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: Syncing run FSR_Trainable_7e662eaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7e662eaf\n",
      "2023-07-19 08:53:19,436\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.808 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:53:19,438\tWARNING util.py:315 -- The `process_trial_result` operation took 1.812 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:53:19,440\tWARNING util.py:315 -- Processing trial results took 1.814 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:53:19,440\tWARNING util.py:315 -- The `process_trial_result` operation took 1.814 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_f69afa43_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-53-02/wandb/run-20230719_085323-f69afa43\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: Syncing run FSR_Trainable_f69afa43\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f69afa43\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb:                      mae █▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb:                     mape █▄▄▃▄▄▄▄▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb:                     rmse █▅▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb:         time_this_iter_s ▅▄▃▂█▆▄▂▃▁▂▁▂▄▁▁▃▃▂▆▄▂▂▂▂▃▂▃▄▂▄▃▂▄▂▃▃▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb:                      mae 39.5344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb:                     mape 15447343.40695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb:                     rmse 129.73026\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb:       time_since_restore 110.52663\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb:         time_this_iter_s 1.01068\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb:             time_total_s 110.52663\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb:                timestamp 1689724419\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: 🚀 View run FSR_Trainable_76612c31 at: https://wandb.ai/seokjin/FSR-prediction/runs/76612c31\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=488657)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085137-76612c31/logs\n",
      "2023-07-19 08:53:58,553\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.917 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:53:58,556\tWARNING util.py:315 -- The `process_trial_result` operation took 1.922 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:53:58,557\tWARNING util.py:315 -- Processing trial results took 1.923 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:53:58,559\tWARNING util.py:315 -- The `process_trial_result` operation took 1.924 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_24ff1ea4_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-53-16/wandb/run-20230719_085359-24ff1ea4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: Syncing run FSR_Trainable_24ff1ea4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/24ff1ea4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb:                      mae █▅▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb:                     mape █▃▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb:                     rmse █▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb:         time_this_iter_s ▇▄▄█▆▇▂▂▂▃▁▁▂▂▃▄▇▄▄▄▃▃▂▄▃▂▄▃▄▂▄▃▂▃▅▄▂▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb:                      mae 39.14895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb:                     mape 15166587.94274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb:                     rmse 124.38076\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb:       time_since_restore 111.69698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb:         time_this_iter_s 1.1231\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb:             time_total_s 111.69698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb:                timestamp 1689724501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: 🚀 View run FSR_Trainable_7e662eaf at: https://wandb.ai/seokjin/FSR-prediction/runs/7e662eaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490240)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085308-7e662eaf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb:                      mae █▇▄▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▁▂▁▁▂▁▂▂▃▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb:                     mape ▅▅▇█▆▆▅▄▄▄▁▁▂▂▂▁▂▄▃▃▃▂▂▂▁▂▁▁▁▁▁▁▁▁▂▂▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb:                     rmse █▆▃▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▂▂▂▂▂▁▁▂▂▂▂▃▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb:         time_this_iter_s █▄▂▄▂▁▂▄▂▄▂▆▄▃▂▄▂▂▃▂▃▆▃▄▂▃▃▂▅▃▄▃▄▂▃▄▆▃▂▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085229-35b09d69/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=489568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb:                      mae ▅▃▅▃▅▃▃▄▂▂▂▄▅▄▄▄▆▃▂▃▂▂▄▂▂▂▂▁▄▂▂▅█▆▄▂▃▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb:                     mape ▇▂▄▇▆▅▇▅▃▃▄▃▄▄▂▅█▃▂▃▃▄▅▄▃▃▃▂▄▃▂▄▆▃▂▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb:                     rmse ▆▅█▄▆▃▂▃▁▁▁▄▅▄▄▄▆▃▂▃▂▁▄▂▃▂▂▁▄▂▃▄█▇▄▃▄▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb:         time_this_iter_s █▆▂▃▃▃▂▂▃▃▃▃█▄▆▅▃▃▃▂▂▃▃▃▃▄▂▁▁▆▃▅▃▂▄▄▅▆▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb:                      mae 38.13096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb:                     mape 12018165.89425\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb:                     rmse 125.22348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb:       time_since_restore 112.00173\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb:         time_this_iter_s 1.11452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb:             time_total_s 112.00173\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb:                timestamp 1689724514\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: 🚀 View run FSR_Trainable_f69afa43 at: https://wandb.ai/seokjin/FSR-prediction/runs/f69afa43\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085323-f69afa43/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490469)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 08:55:20,532\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.210 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:55:20,535\tWARNING util.py:315 -- The `process_trial_result` operation took 2.213 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:55:20,537\tWARNING util.py:315 -- Processing trial results took 2.215 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:55:20,539\tWARNING util.py:315 -- The `process_trial_result` operation took 2.217 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_3e4d0715_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-53-51/wandb/run-20230719_085521-3e4d0715\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: Syncing run FSR_Trainable_3e4d0715\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3e4d0715\n",
      "2023-07-19 08:55:31,876\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.117 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:55:31,879\tWARNING util.py:315 -- The `process_trial_result` operation took 2.121 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:55:31,881\tWARNING util.py:315 -- Processing trial results took 2.123 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:55:31,882\tWARNING util.py:315 -- The `process_trial_result` operation took 2.124 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_a35d63f5_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-55-13/wandb/run-20230719_085532-a35d63f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: Syncing run FSR_Trainable_a35d63f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a35d63f5\n",
      "2023-07-19 08:55:43,931\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.584 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:55:43,937\tWARNING util.py:315 -- The `process_trial_result` operation took 2.590 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:55:43,939\tWARNING util.py:315 -- Processing trial results took 2.592 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:55:43,941\tWARNING util.py:315 -- The `process_trial_result` operation took 2.594 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_c486b8fd_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-55-24/wandb/run-20230719_085544-c486b8fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: Syncing run FSR_Trainable_c486b8fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c486b8fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb:                      mae 82.56525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb:                     mape 56330263.66371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb:                     rmse 225.53801\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb:       time_since_restore 4.7304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb:         time_this_iter_s 4.7304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb:             time_total_s 4.7304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb:                timestamp 1689724541\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: 🚀 View run FSR_Trainable_c486b8fd at: https://wandb.ai/seokjin/FSR-prediction/runs/c486b8fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491445)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085544-c486b8fd/logs\n",
      "2023-07-19 08:56:04,973\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.131 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:56:04,976\tWARNING util.py:315 -- The `process_trial_result` operation took 2.135 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:56:04,978\tWARNING util.py:315 -- Processing trial results took 2.137 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:56:04,993\tWARNING util.py:315 -- The `process_trial_result` operation took 2.151 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_e94e0aaf_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-55-36/wandb/run-20230719_085606-e94e0aaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: Syncing run FSR_Trainable_e94e0aaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e94e0aaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb:                      mae 80.00482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb:                     mape 43814574.40731\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb:                     rmse 225.19123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb:       time_since_restore 5.77207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb:         time_this_iter_s 5.77207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb:             time_total_s 5.77207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb:                timestamp 1689724562\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: 🚀 View run FSR_Trainable_e94e0aaf at: https://wandb.ai/seokjin/FSR-prediction/runs/e94e0aaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491669)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085606-e94e0aaf/logs\n",
      "2023-07-19 08:56:23,093\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.151 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:56:23,096\tWARNING util.py:315 -- The `process_trial_result` operation took 2.155 s, which may be a performance bottleneck.\n",
      "2023-07-19 08:56:23,097\tWARNING util.py:315 -- Processing trial results took 2.156 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 08:56:23,098\tWARNING util.py:315 -- The `process_trial_result` operation took 2.157 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_1d84a1d6_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-55-57/wandb/run-20230719_085626-1d84a1d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: Syncing run FSR_Trainable_1d84a1d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1d84a1d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb:                      mae █▅▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb:                     mape █▅▄▃▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb:                     rmse █▅▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▂▁▁▂▂▂▂▁▂▁▂▁▁▁▁▁▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb:         time_this_iter_s ▇▅▃▄▃▂▂▃▄▅▄▃▇█▃▅▆█▂▄▂▃▅▃▄▂▂▁▃▃▄▁▂▁▃▃▄▂▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb:                      mae 34.85542\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb:                     mape 12345919.64239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb:                     rmse 111.18659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb:       time_since_restore 217.31054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb:         time_this_iter_s 2.40588\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb:             time_total_s 217.31054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb:                timestamp 1689724801\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: 🚀 View run FSR_Trainable_1d84a1d6 at: https://wandb.ai/seokjin/FSR-prediction/runs/1d84a1d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491901)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085626-1d84a1d6/logs\n",
      "2023-07-19 09:00:18,481\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.570 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:00:18,482\tWARNING util.py:315 -- The `process_trial_result` operation took 2.572 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:00:18,487\tWARNING util.py:315 -- Processing trial results took 2.577 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:00:18,488\tWARNING util.py:315 -- The `process_trial_result` operation took 2.578 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_c5ef8b5a_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_08-56-18/wandb/run-20230719_090022-c5ef8b5a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: Syncing run FSR_Trainable_c5ef8b5a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c5ef8b5a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb:                      mae █▅▃▃▂▂▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb:                     mape █▅▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb:                     rmse █▆▃▃▂▂▂▁▁▁▂▂▂▂▃▂▃▃▂▂▃▂▂▃▂▂▂▂▁▁▂▁▁▁▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb:         time_this_iter_s ▅▂▂▂▂▃▁▂▄▁▄▃▇▃▆▅▂▄▅▅▇▆█▄▃▃▄▂▄▂▃▃▄▄▃▅▃▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb:                      mae 34.46247\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb:                     mape 10923731.26805\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb:                     rmse 114.27786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb:       time_since_restore 426.72227\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb:         time_this_iter_s 3.76362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb:             time_total_s 426.72227\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb:                timestamp 1689724869\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: 🚀 View run FSR_Trainable_24ff1ea4 at: https://wandb.ai/seokjin/FSR-prediction/runs/24ff1ea4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=490718)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085359-24ff1ea4/logs\n",
      "2023-07-19 09:01:25,758\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.812 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:01:25,761\tWARNING util.py:315 -- The `process_trial_result` operation took 1.815 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:01:25,762\tWARNING util.py:315 -- Processing trial results took 1.816 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:01:25,762\tWARNING util.py:315 -- The `process_trial_result` operation took 1.817 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_1a2896b8_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-00-14/wandb/run-20230719_090128-1a2896b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: Syncing run FSR_Trainable_1a2896b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1a2896b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb:                      mae █▆▄▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb:                     mape █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▂▂▃▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb:                     rmse █▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb:         time_this_iter_s ██▄▅▃▃▃▄▄▅█▆▂▃▄▂▃▄▃▄▃▃▃▄▂▂▄▃▃▂▁▃▆▄▃▄▃▄▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb:                      mae 35.9203\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb:                     mape 19658364.51563\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb:                     rmse 111.66563\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb:       time_since_restore 76.01602\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb:         time_this_iter_s 0.67419\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb:             time_total_s 76.01602\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb:                timestamp 1689724898\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: 🚀 View run FSR_Trainable_c5ef8b5a at: https://wandb.ai/seokjin/FSR-prediction/runs/c5ef8b5a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492223)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_090022-c5ef8b5a/logs\n",
      "2023-07-19 09:01:55,954\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.166 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:01:55,956\tWARNING util.py:315 -- The `process_trial_result` operation took 2.168 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:01:55,957\tWARNING util.py:315 -- Processing trial results took 2.169 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:01:55,958\tWARNING util.py:315 -- The `process_trial_result` operation took 2.171 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_724a46ef_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-01-21/wandb/run-20230719_090158-724a46ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: Syncing run FSR_Trainable_724a46ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/724a46ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb:                      mae █▅▃▂▂▂▂▁▁▁▁▁▁▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▂▂▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb:                     mape █▅▄▄▃▃▂▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb:                     rmse █▅▃▃▂▂▂▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb:         time_this_iter_s ▅▅▅▃▆▃▅▃▃▃▅▃▇▅█▄▃▃▂▃▃▂▂▃▂▃▂▅▃▃▁▂▁▂▁▂▅▆▆▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb:                      mae 37.75264\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb:                     mape 13720986.18186\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb:                     rmse 120.88146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb:       time_since_restore 433.15311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb:         time_this_iter_s 4.33141\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb:             time_total_s 433.15311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb:                timestamp 1689724960\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: 🚀 View run FSR_Trainable_3e4d0715 at: https://wandb.ai/seokjin/FSR-prediction/runs/3e4d0715\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491009)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085521-3e4d0715/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb:                      mae █▄▃▂▂▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb:                     mape █▇▅▅▄▃▂▁▁▁▂▂▁▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb:                     rmse █▄▃▂▁▁▁▁▁▂▂▂▂▂▂▂▃▂▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb:         time_this_iter_s ▇▅▃█▃▆▅▂▃▅▅▇▆▅▄▂▃▄▃▃▁▃▃▂▃▃▄▃▂▂▂▁▃▂▂▅▆▇▃▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb:                      mae 37.25533\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb:                     mape 12755279.72653\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb:                     rmse 120.87162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb:       time_since_restore 436.05973\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb:         time_this_iter_s 4.88611\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb:             time_total_s 436.05973\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb:                timestamp 1689724973\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: 🚀 View run FSR_Trainable_a35d63f5 at: https://wandb.ai/seokjin/FSR-prediction/runs/a35d63f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=491233)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_085532-a35d63f5/logs\n",
      "2023-07-19 09:03:00,026\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.146 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:03:00,029\tWARNING util.py:315 -- The `process_trial_result` operation took 2.150 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:03:00,031\tWARNING util.py:315 -- Processing trial results took 2.152 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:03:00,033\tWARNING util.py:315 -- The `process_trial_result` operation took 2.154 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_4978f3da_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-01-51/wandb/run-20230719_090303-4978f3da\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: Syncing run FSR_Trainable_4978f3da\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4978f3da\n",
      "2023-07-19 09:03:12,718\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.888 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:03:12,723\tWARNING util.py:315 -- The `process_trial_result` operation took 1.893 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:03:12,724\tWARNING util.py:315 -- Processing trial results took 1.895 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:03:12,728\tWARNING util.py:315 -- The `process_trial_result` operation took 1.898 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_02392f30_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-02-54/wandb/run-20230719_090316-02392f30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: Syncing run FSR_Trainable_02392f30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/02392f30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb:                      mae 86.04559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb:                     mape 1.870932649719355e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb:                     rmse 230.71724\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb:       time_since_restore 3.02488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb:         time_this_iter_s 3.02488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb:             time_total_s 3.02488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb:                timestamp 1689724990\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: 🚀 View run FSR_Trainable_02392f30 at: https://wandb.ai/seokjin/FSR-prediction/runs/02392f30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493189)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_090316-02392f30/logs\n",
      "2023-07-19 09:03:33,985\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.949 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:03:33,990\tWARNING util.py:315 -- The `process_trial_result` operation took 2.955 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:03:33,991\tWARNING util.py:315 -- Processing trial results took 2.956 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:03:33,993\tWARNING util.py:315 -- The `process_trial_result` operation took 2.958 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_204d60e5_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-03-07/wandb/run-20230719_090337-204d60e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: Syncing run FSR_Trainable_204d60e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/204d60e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb:                      mae 80.27659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb:                     mape 1.742720945179556e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb:                     rmse 223.92291\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb:       time_since_restore 2.98505\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb:         time_this_iter_s 2.98505\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb:             time_total_s 2.98505\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb:                timestamp 1689725011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: 🚀 View run FSR_Trainable_204d60e5 at: https://wandb.ai/seokjin/FSR-prediction/runs/204d60e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493421)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_090337-204d60e5/logs\n",
      "2023-07-19 09:03:52,593\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.073 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:03:52,595\tWARNING util.py:315 -- The `process_trial_result` operation took 2.076 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:03:52,599\tWARNING util.py:315 -- Processing trial results took 2.079 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:03:52,600\tWARNING util.py:315 -- The `process_trial_result` operation took 2.080 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_76736994_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-03-28/wandb/run-20230719_090355-76736994\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: Syncing run FSR_Trainable_76736994\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/76736994\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb:                      mae █▄▂▁▁▁▁▁▁▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb:                     mape █▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb:                     rmse █▄▂▁▁▁▁▁▁▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb:         time_this_iter_s ▆▂▂▁▂▁▄▁▃▄▇█▃▂▁▄▄▂▅▄▄█▆▂▅▄▂▂▁▂▁▂▂▃▂▃▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb:                      mae 36.56205\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb:                     mape 11854076.96465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb:                     rmse 117.40709\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb:       time_since_restore 218.89348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb:         time_this_iter_s 2.01135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb:             time_total_s 218.89348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb:                timestamp 1689725112\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: 🚀 View run FSR_Trainable_1a2896b8 at: https://wandb.ai/seokjin/FSR-prediction/runs/1a2896b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492482)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_090128-1a2896b8/logs\n",
      "2023-07-19 09:05:29,377\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.893 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:05:29,379\tWARNING util.py:315 -- The `process_trial_result` operation took 1.895 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:05:29,381\tWARNING util.py:315 -- Processing trial results took 1.897 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:05:29,382\tWARNING util.py:315 -- The `process_trial_result` operation took 1.899 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_cf9095d3_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-03-47/wandb/run-20230719_090532-cf9095d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: Syncing run FSR_Trainable_cf9095d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cf9095d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)06 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb:                      mae █▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb:                     mape █▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▂▂▁▁▁▁▁▁▁▂▁▂▂▂▁▂▂▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb:                     rmse █▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb:         time_this_iter_s ▇▄█▅▅▃█▃▂▄▅▄▅▄▃▆▅▂▂▄▂▁▁▁▂▁▁▃▂▃▂▂▁▁▃▁▄▄▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb:                      mae 35.83056\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb:                     mape 11897537.05527\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb:                     rmse 112.59902\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb:       time_since_restore 220.11712\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb:         time_this_iter_s 1.9901\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb:             time_total_s 220.11712\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb:                timestamp 1689725144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: 🚀 View run FSR_Trainable_724a46ef at: https://wandb.ai/seokjin/FSR-prediction/runs/724a46ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492717)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_090158-724a46ef/logs\n",
      "2023-07-19 09:06:00,453\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.775 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:06:00,454\tWARNING util.py:315 -- The `process_trial_result` operation took 1.777 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:06:00,457\tWARNING util.py:315 -- Processing trial results took 1.780 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:06:00,458\tWARNING util.py:315 -- The `process_trial_result` operation took 1.781 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_0c74399f_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-05-24/wandb/run-20230719_090603-0c74399f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: Syncing run FSR_Trainable_0c74399f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0c74399f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb:                      mae █▅▂▂▂▁▁▁▁▁▁▁▂▂▂▂▁▁▁▁▁▁▁▂▁▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb:                     mape █▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb:                     rmse █▅▂▁▁▁▁▁▁▁▁▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb:         time_this_iter_s █▄▄▄▄▅▆▃▅▄▃▂▂▃▂▃▃▄▃▃▂▂▂▂▃▁▄▂▂▃▂▃▃▄▃▂▂▂▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb:                      mae 36.82788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb:                     mape 12699069.44413\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb:                     rmse 117.2004\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb:       time_since_restore 214.19213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb:         time_this_iter_s 2.02651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb:             time_total_s 214.19213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb:                timestamp 1689725202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: 🚀 View run FSR_Trainable_4978f3da at: https://wandb.ai/seokjin/FSR-prediction/runs/4978f3da\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=492969)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_090303-4978f3da/logs\n",
      "2023-07-19 09:07:01,722\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.606 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:07:01,726\tWARNING util.py:315 -- The `process_trial_result` operation took 2.615 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:07:01,727\tWARNING util.py:315 -- Processing trial results took 2.616 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:07:01,730\tWARNING util.py:315 -- The `process_trial_result` operation took 2.619 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...413)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_889e4985_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-05-56/wandb/run-20230719_090705-889e4985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: Syncing run FSR_Trainable_889e4985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/889e4985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb:                      mae █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb:                     mape █▄▄▄▃▃▂▂▂▂▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb:                     rmse █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb:         time_this_iter_s ▇▅▃▂▂▂▂▂▂▄▃▃▁▂▂▁▂▁▃▃▃▄▂▄▅▂▂▂▂▂▄▂▄▃█▅▂▂▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb:                      mae 35.68043\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb:                     mape 12362529.59042\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb:                     rmse 111.71162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb:       time_since_restore 211.31039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb:         time_this_iter_s 2.02933\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb:             time_total_s 211.31039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb:                timestamp 1689725250\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: 🚀 View run FSR_Trainable_76736994 at: https://wandb.ai/seokjin/FSR-prediction/runs/76736994\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493649)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_090355-76736994/logs\n",
      "2023-07-19 09:07:46,646\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.157 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:07:46,649\tWARNING util.py:315 -- The `process_trial_result` operation took 2.161 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:07:46,652\tWARNING util.py:315 -- Processing trial results took 2.164 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:07:46,653\tWARNING util.py:315 -- The `process_trial_result` operation took 2.165 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_7edc10ce_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-06-55/wandb/run-20230719_090749-7edc10ce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: Syncing run FSR_Trainable_7edc10ce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7edc10ce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb:                      mae █▅▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb:                     mape █▅▄▃▂▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb:                     rmse █▅▂▂▂▁▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb:         time_this_iter_s █▃▄▂▂▁▅▄▂▃▃▃▃▃▂▃█▅▂▂▂▃▂▂▁▅▂▃▂▂▆▄▃▄▃▅▅▆█▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb:                      mae 36.53603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb:                     mape 14763162.22447\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb:                     rmse 116.52035\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb:       time_since_restore 214.257\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb:         time_this_iter_s 2.2843\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb:             time_total_s 214.257\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb:                timestamp 1689725349\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: 🚀 View run FSR_Trainable_cf9095d3 at: https://wandb.ai/seokjin/FSR-prediction/runs/cf9095d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=493924)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_090532-cf9095d3/logs\n",
      "2023-07-19 09:09:26,927\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.682 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:09:26,929\tWARNING util.py:315 -- The `process_trial_result` operation took 1.685 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:09:26,931\tWARNING util.py:315 -- Processing trial results took 1.688 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:09:26,933\tWARNING util.py:315 -- The `process_trial_result` operation took 1.689 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_63087eed_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-07-41/wandb/run-20230719_090929-63087eed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: Syncing run FSR_Trainable_63087eed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/63087eed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb:                      mae █▅▃▂▂▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb:                     mape █▅▃▃▃▃▂▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb:                     rmse █▅▃▂▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb:         time_this_iter_s ▆▃▁▃▃▁▁▃▂▄▄█▅▂▁▂▂▃▁▅▅▂▂▂▁▅▁▂▂▂▂▄▃▅▄▄▁▆▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb:                      mae 36.07411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb:                     mape 13386053.55567\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb:                     rmse 114.12493\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb:       time_since_restore 214.73289\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb:         time_this_iter_s 1.96194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb:             time_total_s 214.73289\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb:                timestamp 1689725379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: 🚀 View run FSR_Trainable_0c74399f at: https://wandb.ai/seokjin/FSR-prediction/runs/0c74399f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494159)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_090603-0c74399f/logs\n",
      "2023-07-19 09:09:56,442\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.109 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:09:56,444\tWARNING util.py:315 -- The `process_trial_result` operation took 2.112 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:09:56,447\tWARNING util.py:315 -- Processing trial results took 2.115 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:09:56,448\tWARNING util.py:315 -- The `process_trial_result` operation took 2.116 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_79df4dfc_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-09-22/wandb/run-20230719_090959-79df4dfc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: Syncing run FSR_Trainable_79df4dfc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/79df4dfc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb:                      mae █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb:                     mape █▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb:                     rmse █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb:         time_this_iter_s █▃▂▁▂▂▃▁▃▃▂▂▂▁▄▂▂▃▂▃▃▄▄▃▂▂▄▂▂▂▁▁▃▂▆▁▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb:                      mae 38.74088\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb:                     mape 12991999.18357\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb:                     rmse 124.81425\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb:       time_since_restore 210.21954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb:         time_this_iter_s 2.07719\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb:             time_total_s 210.21954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb:                timestamp 1689725436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: 🚀 View run FSR_Trainable_889e4985 at: https://wandb.ai/seokjin/FSR-prediction/runs/889e4985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494413)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_090705-889e4985/logs\n",
      "2023-07-19 09:10:53,984\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.042 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:10:53,987\tWARNING util.py:315 -- The `process_trial_result` operation took 2.046 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:10:53,989\tWARNING util.py:315 -- Processing trial results took 2.047 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:10:53,989\tWARNING util.py:315 -- The `process_trial_result` operation took 2.048 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_7b56e2bf_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-09-51/wandb/run-20230719_091056-7b56e2bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: Syncing run FSR_Trainable_7b56e2bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7b56e2bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb:                      mae 72.69482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb:                     mape 38196259.43991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb:                     rmse 201.47073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb:       time_since_restore 5.34494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb:         time_this_iter_s 2.30623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb:             time_total_s 5.34494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb:                timestamp 1689725456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: 🚀 View run FSR_Trainable_7b56e2bf at: https://wandb.ai/seokjin/FSR-prediction/runs/7b56e2bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091056-7b56e2bf/logs\n",
      "2023-07-19 09:11:13,749\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.204 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:11:13,751\tWARNING util.py:315 -- The `process_trial_result` operation took 2.207 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:11:13,752\tWARNING util.py:315 -- Processing trial results took 2.207 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:11:13,753\tWARNING util.py:315 -- The `process_trial_result` operation took 2.208 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_40319008_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-10-49/wandb/run-20230719_091116-40319008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: Syncing run FSR_Trainable_40319008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/40319008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb:                      mae █▅▃▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb:                     mape █▆▃▂▂▂▃▃▃▃▃▃▃▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb:                     rmse █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb:         time_this_iter_s ▆▃▃▂▃▂█▃▂▃▃▄▄▆▅▃▄▂▂▂▃▁▂▅▄▃▂▃▂▂▂▂▁▂▅▃▂▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb:                      mae 37.45658\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb:                     mape 12985675.49726\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb:                     rmse 122.47408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb:       time_since_restore 211.63784\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb:         time_this_iter_s 1.82628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb:             time_total_s 211.63784\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb:                timestamp 1689725484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: 🚀 View run FSR_Trainable_7edc10ce at: https://wandb.ai/seokjin/FSR-prediction/runs/7edc10ce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_090749-7edc10ce/logs\n",
      "2023-07-19 09:11:41,322\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.883 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:11:41,325\tWARNING util.py:315 -- The `process_trial_result` operation took 1.886 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:11:41,326\tWARNING util.py:315 -- Processing trial results took 1.887 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:11:41,327\tWARNING util.py:315 -- The `process_trial_result` operation took 1.888 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_6a6d3f27_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-11-09/wandb/run-20230719_091144-6a6d3f27\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: Syncing run FSR_Trainable_6a6d3f27\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6a6d3f27\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb:                      mae 78.20846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb:                     mape 39999117.24679\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb:                     rmse 218.67103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb:       time_since_restore 4.77868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb:         time_this_iter_s 2.25336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb:             time_total_s 4.77868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb:                timestamp 1689725503\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: 🚀 View run FSR_Trainable_6a6d3f27 at: https://wandb.ai/seokjin/FSR-prediction/runs/6a6d3f27\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495884)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091144-6a6d3f27/logs\n",
      "2023-07-19 09:12:01,148\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.217 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:12:01,153\tWARNING util.py:315 -- The `process_trial_result` operation took 2.223 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:12:01,155\tWARNING util.py:315 -- Processing trial results took 2.225 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:12:01,158\tWARNING util.py:315 -- The `process_trial_result` operation took 2.228 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_b6844708_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-11-37/wandb/run-20230719_091203-b6844708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: Syncing run FSR_Trainable_b6844708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b6844708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb:                      mae 65.35961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb:                     mape 903102436501147.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb:                     rmse 233.9135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb:       time_since_restore 2.56568\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb:         time_this_iter_s 2.56568\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb:             time_total_s 2.56568\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb:                timestamp 1689725518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: 🚀 View run FSR_Trainable_b6844708 at: https://wandb.ai/seokjin/FSR-prediction/runs/b6844708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496115)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091203-b6844708/logs\n",
      "2023-07-19 09:12:17,974\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.012 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:12:17,976\tWARNING util.py:315 -- The `process_trial_result` operation took 2.016 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:12:17,978\tWARNING util.py:315 -- Processing trial results took 2.017 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:12:17,978\tWARNING util.py:315 -- The `process_trial_result` operation took 2.018 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_38f42068_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-11-56/wandb/run-20230719_091220-38f42068\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: Syncing run FSR_Trainable_38f42068\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/38f42068\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb:                      mae █▅▃▃▂▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb:                     mape █▅▃▃▂▂▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▂▁▂▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb:                     rmse █▅▃▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb:         time_this_iter_s █▃▃▃▂▂▄▃█▂▃▃▃▃▃▂█▁▃▂▄▃▂▃▂▃▇▃▆▄▁▃▄▃▃▃▃▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb:                      mae 35.78928\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb:                     mape 14012130.59195\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb:                     rmse 113.49136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb:       time_since_restore 204.40713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb:         time_this_iter_s 1.94866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb:             time_total_s 204.40713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb:                timestamp 1689725577\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: 🚀 View run FSR_Trainable_63087eed at: https://wandb.ai/seokjin/FSR-prediction/runs/63087eed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=494931)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_090929-63087eed/logs\n",
      "2023-07-19 09:13:13,601\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.214 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:13:13,603\tWARNING util.py:315 -- The `process_trial_result` operation took 2.217 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:13:13,605\tWARNING util.py:315 -- Processing trial results took 2.218 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:13:13,606\tWARNING util.py:315 -- The `process_trial_result` operation took 2.219 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_9b52a0d6_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-12-13/wandb/run-20230719_091316-9b52a0d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: Syncing run FSR_Trainable_9b52a0d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9b52a0d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: | 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb:                      mae 72.68655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb:                     mape 45238384.95744\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb:                     rmse 203.87145\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb:       time_since_restore 4.7865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb:         time_this_iter_s 2.32001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb:             time_total_s 4.7865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb:                timestamp 1689725595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: 🚀 View run FSR_Trainable_9b52a0d6 at: https://wandb.ai/seokjin/FSR-prediction/runs/9b52a0d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496594)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091316-9b52a0d6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 09:13:32,069\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.718 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:13:32,074\tWARNING util.py:315 -- The `process_trial_result` operation took 2.723 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:13:32,076\tWARNING util.py:315 -- Processing trial results took 2.725 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:13:32,078\tWARNING util.py:315 -- The `process_trial_result` operation took 2.727 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb:                      mae █▅▃▂▂▂▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb:                     mape █▆▄▃▃▃▃▃▃▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb:                     rmse █▅▃▂▂▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb:         time_this_iter_s █▅▄▄▃▃▂▃▁▂▃▄▃▂▄▄▃▃▂▆▆▄▁▂▃▂▅▃▃▃▃▃▅▄▄▂▆▆▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb:                      mae 36.10388\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb:                     mape 12992542.10971\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb:                     rmse 113.93502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb:       time_since_restore 203.23986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb:         time_this_iter_s 1.94482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb:             time_total_s 203.23986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb:                timestamp 1689725608\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: 🚀 View run FSR_Trainable_79df4dfc at: https://wandb.ai/seokjin/FSR-prediction/runs/79df4dfc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495165)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_090959-79df4dfc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_97300f2b_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-13-09/wandb/run-20230719_091335-97300f2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: Syncing run FSR_Trainable_97300f2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/97300f2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb:                      mae 87.26608\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb:                     mape 75853876.68965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb:                     rmse 246.98099\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb:       time_since_restore 0.90424\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb:         time_this_iter_s 0.90424\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb:             time_total_s 0.90424\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb:                timestamp 1689725609\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: 🚀 View run FSR_Trainable_97300f2b at: https://wandb.ai/seokjin/FSR-prediction/runs/97300f2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496822)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091335-97300f2b/logs\n",
      "2023-07-19 09:13:45,160\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.728 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:13:45,163\tWARNING util.py:315 -- The `process_trial_result` operation took 1.732 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:13:45,165\tWARNING util.py:315 -- Processing trial results took 1.734 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:13:45,166\tWARNING util.py:315 -- The `process_trial_result` operation took 1.735 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_2182d1af_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-13-28/wandb/run-20230719_091348-2182d1af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: Syncing run FSR_Trainable_2182d1af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2182d1af\n",
      "2023-07-19 09:13:56,801\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.812 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:13:56,804\tWARNING util.py:315 -- The `process_trial_result` operation took 1.815 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:13:56,805\tWARNING util.py:315 -- Processing trial results took 1.817 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:13:56,806\tWARNING util.py:315 -- The `process_trial_result` operation took 1.818 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_33bfbb89_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-13-41/wandb/run-20230719_091359-33bfbb89\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: Syncing run FSR_Trainable_33bfbb89\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/33bfbb89\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb:                      mae 71.72403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb:                     mape 39655548.38702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb:                     rmse 198.8141\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb:       time_since_restore 4.80742\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb:         time_this_iter_s 2.33403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb:             time_total_s 4.80742\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb:                timestamp 1689725639\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: 🚀 View run FSR_Trainable_33bfbb89 at: https://wandb.ai/seokjin/FSR-prediction/runs/33bfbb89\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497276)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091359-33bfbb89/logs\n",
      "2023-07-19 09:14:15,203\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.460 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:14:15,205\tWARNING util.py:315 -- The `process_trial_result` operation took 1.463 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:14:15,208\tWARNING util.py:315 -- Processing trial results took 1.465 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:14:15,208\tWARNING util.py:315 -- The `process_trial_result` operation took 1.466 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_87a052b8_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-13-52/wandb/run-20230719_091418-87a052b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Syncing run FSR_Trainable_87a052b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/87a052b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb:                      mae █▆▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb:                     mape █▆▄▃▃▃▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb:                     rmse █▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb:         time_this_iter_s █▅▂▃▂▄▂▃▂▂▃▁▂▄▄▃▃▃▃▄▄▃▃▃▄▄▁▁▅▄▇▅▂▆▃▂▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb:                      mae 36.17217\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb:                     mape 11969992.41045\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb:                     rmse 113.64892\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb:       time_since_restore 201.57454\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb:         time_this_iter_s 2.00361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb:             time_total_s 201.57454\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb:                timestamp 1689725688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: 🚀 View run FSR_Trainable_40319008 at: https://wandb.ai/seokjin/FSR-prediction/runs/40319008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=495651)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091116-40319008/logs\n",
      "2023-07-19 09:15:05,002\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.261 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:15:05,005\tWARNING util.py:315 -- The `process_trial_result` operation took 2.265 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:15:05,005\tWARNING util.py:315 -- Processing trial results took 2.265 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:15:05,006\tWARNING util.py:315 -- The `process_trial_result` operation took 2.267 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_9a6ea1b4_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-14-11/wandb/run-20230719_091507-9a6ea1b4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: Syncing run FSR_Trainable_9a6ea1b4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9a6ea1b4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb:                      mae █▄▃▂▂▂▁▁▁▂▂▂▂▁▁▁▁▂▂▁▂▂▂▁▁▁▂▂▁▂▁▁▁▁▁▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb:                     mape █▅▅▅▄▄▃▃▃▃▄▃▃▃▂▂▂▂▂▂▂▁▂▁▂▂▂▁▁▂▁▁▁▁▁▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb:                     rmse █▄▂▂▁▂▁▁▁▂▂▂▂▂▁▂▁▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▂▁▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb:         time_this_iter_s ▇▄▃▃▃▃▃▃▄▃▃▁█▂▆▁▁▃▅▂▃▄▆▄▃▃▃▄▃▄▄▅▃▆▄▄▅▃▃▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb:                      mae 34.95542\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb:                     mape 9700880.45098\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb:                     rmse 114.41252\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb:       time_since_restore 202.67008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb:         time_this_iter_s 2.30775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb:             time_total_s 202.67008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb:                timestamp 1689725750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: 🚀 View run FSR_Trainable_38f42068 at: https://wandb.ai/seokjin/FSR-prediction/runs/38f42068\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=496339)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091220-38f42068/logs\n",
      "2023-07-19 09:16:06,597\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.588 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:16:06,598\tWARNING util.py:315 -- The `process_trial_result` operation took 2.590 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:16:06,601\tWARNING util.py:315 -- Processing trial results took 2.593 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:16:06,604\tWARNING util.py:315 -- The `process_trial_result` operation took 2.596 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_3cb4c6ba_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-15-00/wandb/run-20230719_091610-3cb4c6ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: Syncing run FSR_Trainable_3cb4c6ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3cb4c6ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb:                      mae 68.71159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb:                     mape 41001808.05225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb:                     rmse 195.66395\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb:       time_since_restore 2.42274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb:         time_this_iter_s 1.0397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb:             time_total_s 2.42274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb:                timestamp 1689725767\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: 🚀 View run FSR_Trainable_3cb4c6ba at: https://wandb.ai/seokjin/FSR-prediction/runs/3cb4c6ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498004)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091610-3cb4c6ba/logs\n",
      "2023-07-19 09:16:24,990\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.370 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:16:24,993\tWARNING util.py:315 -- The `process_trial_result` operation took 2.373 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:16:24,995\tWARNING util.py:315 -- Processing trial results took 2.375 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:16:24,997\tWARNING util.py:315 -- The `process_trial_result` operation took 2.377 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_018bdcbb_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-16-02/wandb/run-20230719_091628-018bdcbb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: Syncing run FSR_Trainable_018bdcbb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/018bdcbb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb:                      mae 65.87583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb:                     mape 1074532031571064.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb:                     rmse 229.6495\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb:       time_since_restore 1.36907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb:         time_this_iter_s 1.36907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb:             time_total_s 1.36907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb:                timestamp 1689725782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: 🚀 View run FSR_Trainable_018bdcbb at: https://wandb.ai/seokjin/FSR-prediction/runs/018bdcbb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498236)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091628-018bdcbb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb:                      mae █▆▄▃▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb:                     mape █▆▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb:                     rmse █▆▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb:         time_this_iter_s █▅▂▅▂▁▁▁▂▃▃▃▄▂▃▃▅▃▅▄▂▄▃▄▂▄▃▄▇▄▂▂▃▆▇▅▄▃▅▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "2023-07-19 09:16:41,505\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.129 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:16:41,509\tWARNING util.py:315 -- The `process_trial_result` operation took 2.134 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:16:41,510\tWARNING util.py:315 -- Processing trial results took 2.135 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:16:41,511\tWARNING util.py:315 -- The `process_trial_result` operation took 2.136 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_4acc8330_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-16-21/wandb/run-20230719_091643-4acc8330\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: Syncing run FSR_Trainable_4acc8330\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4acc8330\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb:                      mae 62.56942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb:                     mape 917001340927319.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb:                     rmse 222.05534\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb:       time_since_restore 2.43135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb:         time_this_iter_s 2.43135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb:             time_total_s 2.43135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb:                timestamp 1689725799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: 🚀 View run FSR_Trainable_4acc8330 at: https://wandb.ai/seokjin/FSR-prediction/runs/4acc8330\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091643-4acc8330/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498474)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 09:16:50,766\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.980 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:16:50,769\tWARNING util.py:315 -- The `process_trial_result` operation took 1.984 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:16:50,771\tWARNING util.py:315 -- Processing trial results took 1.986 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:16:50,772\tWARNING util.py:315 -- The `process_trial_result` operation took 1.987 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_286656b1_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-16-37/wandb/run-20230719_091653-286656b1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: Syncing run FSR_Trainable_286656b1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/286656b1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb:                      mae 78.99392\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb:                     mape 32010068.94988\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb:                     rmse 224.64253\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb:       time_since_restore 1.18399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb:         time_this_iter_s 1.18399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb:             time_total_s 1.18399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb:                timestamp 1689725808\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: 🚀 View run FSR_Trainable_286656b1 at: https://wandb.ai/seokjin/FSR-prediction/runs/286656b1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498690)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091653-286656b1/logs\n",
      "2023-07-19 09:17:02,823\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.897 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:17:02,826\tWARNING util.py:315 -- The `process_trial_result` operation took 1.903 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:17:02,828\tWARNING util.py:315 -- Processing trial results took 1.906 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:17:02,832\tWARNING util.py:315 -- The `process_trial_result` operation took 1.909 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_aa237e74_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-16-47/wandb/run-20230719_091705-aa237e74\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: Syncing run FSR_Trainable_aa237e74\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/aa237e74\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb:                      mae 80.04821\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb:                     mape 26108405.90941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb:                     rmse 224.43649\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb:       time_since_restore 1.32669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb:         time_this_iter_s 1.32669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb:             time_total_s 1.32669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb:                timestamp 1689725820\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: 🚀 View run FSR_Trainable_aa237e74 at: https://wandb.ai/seokjin/FSR-prediction/runs/aa237e74\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=498918)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091705-aa237e74/logs\n",
      "2023-07-19 09:17:18,039\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.853 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:17:18,043\tWARNING util.py:315 -- The `process_trial_result` operation took 1.858 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:17:18,044\tWARNING util.py:315 -- Processing trial results took 1.860 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:17:18,046\tWARNING util.py:315 -- The `process_trial_result` operation took 1.861 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_df33489f_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-16-59/wandb/run-20230719_091718-df33489f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: Syncing run FSR_Trainable_df33489f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/df33489f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb:                      mae 79.60631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb:                     mape 46509917.45298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb:                     rmse 221.55534\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb:       time_since_restore 4.77262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb:         time_this_iter_s 4.77262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb:             time_total_s 4.77262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb:                timestamp 1689725836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: 🚀 View run FSR_Trainable_df33489f at: https://wandb.ai/seokjin/FSR-prediction/runs/df33489f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499144)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091718-df33489f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091718-df33489f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb:                      mae █▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb:                     mape █▅▃▂▂▂▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb:                     rmse █▅▂▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb:         time_this_iter_s █▃▅▃▄▄▅▄▄▄▄▄▄▃▄▇▃▅▄▄▆▅▄█▇▄▇▆▅▆▅▂▃▂▁▁▄▁▇▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "2023-07-19 09:17:28,799\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.864 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:17:28,801\tWARNING util.py:315 -- The `process_trial_result` operation took 1.868 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:17:28,802\tWARNING util.py:315 -- Processing trial results took 1.869 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:17:28,804\tWARNING util.py:315 -- The `process_trial_result` operation took 1.870 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...369)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_f9ed1d54_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-17-11/wandb/run-20230719_091729-f9ed1d54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Syncing run FSR_Trainable_f9ed1d54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f9ed1d54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f9ed1d54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f9ed1d54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f9ed1d54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f9ed1d54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f9ed1d54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f9ed1d54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f9ed1d54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f9ed1d54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f9ed1d54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f9ed1d54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f9ed1d54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f9ed1d54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f9ed1d54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497057)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f9ed1d54\n",
      "2023-07-19 09:17:37,426\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.109 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:17:37,430\tWARNING util.py:315 -- The `process_trial_result` operation took 2.114 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:17:37,432\tWARNING util.py:315 -- Processing trial results took 2.115 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:17:37,433\tWARNING util.py:315 -- The `process_trial_result` operation took 2.117 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_b102d1a7_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-17-22/wandb/run-20230719_091740-b102d1a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: Syncing run FSR_Trainable_b102d1a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b102d1a7\n",
      "2023-07-19 09:17:49,423\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.241 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:17:49,427\tWARNING util.py:315 -- The `process_trial_result` operation took 2.246 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:17:49,429\tWARNING util.py:315 -- Processing trial results took 2.248 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:17:49,431\tWARNING util.py:315 -- The `process_trial_result` operation took 2.250 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_b863bafd_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-17-32/wandb/run-20230719_091753-b863bafd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: Syncing run FSR_Trainable_b863bafd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b863bafd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb:                      mae █▅▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb:                     mape █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb:                     rmse █▅▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb:         time_this_iter_s ▆█▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb:                      mae 56.0342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb:                     mape 26370242.75556\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb:                     rmse 164.66471\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb:       time_since_restore 9.33643\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb:         time_this_iter_s 2.14614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb:             time_total_s 9.33643\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb:                timestamp 1689725876\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: 🚀 View run FSR_Trainable_b863bafd at: https://wandb.ai/seokjin/FSR-prediction/runs/b863bafd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499809)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091753-b863bafd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb:                      mae █▅▄▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb:                     mape █▇▅▄▃▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb:                     rmse █▅▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb:         time_this_iter_s █▁▇▇▇▆▇▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091729-f9ed1d54/logs\n",
      "2023-07-19 09:18:12,291\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.218 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:18:12,294\tWARNING util.py:315 -- The `process_trial_result` operation took 2.222 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:18:12,296\tWARNING util.py:315 -- Processing trial results took 2.223 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:18:12,297\tWARNING util.py:315 -- The `process_trial_result` operation took 2.224 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499369)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_e35ff9df_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-17-44/wandb/run-20230719_091814-e35ff9df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: Syncing run FSR_Trainable_e35ff9df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e35ff9df\n",
      "2023-07-19 09:18:22,312\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.280 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:18:22,314\tWARNING util.py:315 -- The `process_trial_result` operation took 2.284 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:18:22,315\tWARNING util.py:315 -- Processing trial results took 2.285 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:18:22,316\tWARNING util.py:315 -- The `process_trial_result` operation took 2.286 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_68f54235_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-18-07/wandb/run-20230719_091825-68f54235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: Syncing run FSR_Trainable_68f54235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/68f54235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb:                      mae 69.4221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb:                     mape 17903044.471\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb:                     rmse 202.20751\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb:       time_since_restore 1.61445\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb:         time_this_iter_s 0.70248\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb:             time_total_s 1.61445\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb:                timestamp 1689725903\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: 🚀 View run FSR_Trainable_68f54235 at: https://wandb.ai/seokjin/FSR-prediction/runs/68f54235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500263)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091825-68f54235/logs\n",
      "2023-07-19 09:18:38,591\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.228 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:18:38,596\tWARNING util.py:315 -- The `process_trial_result` operation took 2.234 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:18:38,598\tWARNING util.py:315 -- Processing trial results took 2.236 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:18:38,599\tWARNING util.py:315 -- The `process_trial_result` operation took 2.237 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_d3f80ea9_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-18-19/wandb/run-20230719_091842-d3f80ea9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb: Syncing run FSR_Trainable_d3f80ea9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d3f80ea9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb:                      mae █▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb:                     mape █▅▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb:                     rmse █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb:         time_this_iter_s █▆▇▇▆▅▄▃▄▆▅▅█▄▄▅▃▇▃▅▂▁▄▃▇▂▄▂▅▄▆▄▄▂▅▅▆▅▅▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb:                      mae 36.20236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb:                     mape 10780165.55966\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb:                     rmse 119.29332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb:       time_since_restore 200.28863\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb:         time_this_iter_s 2.23097\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb:             time_total_s 200.28863\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb:                timestamp 1689725920\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: 🚀 View run FSR_Trainable_9a6ea1b4 at: https://wandb.ai/seokjin/FSR-prediction/runs/9a6ea1b4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=497751)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091507-9a6ea1b4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500491)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb:                      mae █▆▄▃▂▂▁▁▁▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb:                     mape █▆▄▃▃▃▃▃▃▃▄▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb:                     rmse █▆▄▃▂▁▁▁▁▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb:         time_this_iter_s █▅▄▃▆▅▅▅▄▃▂▅▅▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb:                timestamp ▁▂▂▃▃▄▄▅▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "2023-07-19 09:18:53,594\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.976 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:18:53,597\tWARNING util.py:315 -- The `process_trial_result` operation took 1.980 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:18:53,599\tWARNING util.py:315 -- Processing trial results took 1.983 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:18:53,601\tWARNING util.py:315 -- The `process_trial_result` operation took 1.984 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: iterations_since_restore 16\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb:                      mae 37.81158\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb:                     mape 16372891.63396\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb:                     rmse 120.60543\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb:       time_since_restore 32.72957\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb:         time_this_iter_s 1.67875\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb:             time_total_s 32.72957\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb:                timestamp 1689725926\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb:       training_iteration 16\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: 🚀 View run FSR_Trainable_e35ff9df at: https://wandb.ai/seokjin/FSR-prediction/runs/e35ff9df\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500052)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091814-e35ff9df/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_33b68655_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-18-35/wandb/run-20230719_091856-33b68655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Syncing run FSR_Trainable_33b68655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/33b68655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091856-33b68655/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091856-33b68655/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091856-33b68655/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091856-33b68655/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091856-33b68655/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091856-33b68655/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091856-33b68655/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091856-33b68655/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091856-33b68655/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091856-33b68655/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091856-33b68655/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091856-33b68655/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091856-33b68655/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500736)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091856-33b68655/logs\n",
      "2023-07-19 09:19:03,437\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.910 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:19:03,441\tWARNING util.py:315 -- The `process_trial_result` operation took 1.914 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:19:03,443\tWARNING util.py:315 -- Processing trial results took 1.916 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:19:03,444\tWARNING util.py:315 -- The `process_trial_result` operation took 1.918 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_fa8aeae0_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-18-49/wandb/run-20230719_091905-fa8aeae0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: Syncing run FSR_Trainable_fa8aeae0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fa8aeae0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb:                      mae 76.65786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb:                     mape 6.2986916480997464e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb:                     rmse 260.76262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb:       time_since_restore 2.32697\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb:         time_this_iter_s 2.32697\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb:             time_total_s 2.32697\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb:                timestamp 1689725941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: 🚀 View run FSR_Trainable_fa8aeae0 at: https://wandb.ai/seokjin/FSR-prediction/runs/fa8aeae0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=500953)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091905-fa8aeae0/logs\n",
      "2023-07-19 09:19:14,421\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.869 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:19:14,424\tWARNING util.py:315 -- The `process_trial_result` operation took 1.873 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:19:14,426\tWARNING util.py:315 -- Processing trial results took 1.875 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:19:14,427\tWARNING util.py:315 -- The `process_trial_result` operation took 1.877 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_4e34ea6f_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-18-59/wandb/run-20230719_091914-4e34ea6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: Syncing run FSR_Trainable_4e34ea6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4e34ea6f\n",
      "2023-07-19 09:19:25,246\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.193 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:19:25,250\tWARNING util.py:315 -- The `process_trial_result` operation took 2.198 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:19:25,251\tWARNING util.py:315 -- Processing trial results took 2.199 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:19:25,252\tWARNING util.py:315 -- The `process_trial_result` operation took 2.201 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_58e865dc_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-19-08/wandb/run-20230719_091925-58e865dc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: Syncing run FSR_Trainable_58e865dc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/58e865dc\n",
      "2023-07-19 09:19:37,022\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.937 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:19:37,027\tWARNING util.py:315 -- The `process_trial_result` operation took 1.943 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:19:37,029\tWARNING util.py:315 -- Processing trial results took 1.945 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:19:37,031\tWARNING util.py:315 -- The `process_trial_result` operation took 1.947 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_812f8eb1_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-19-18/wandb/run-20230719_091937-812f8eb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: Syncing run FSR_Trainable_812f8eb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/812f8eb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb:                      mae █▆▅▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb:                     mape █▅▄▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb:                     rmse █▆▄▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb:         time_this_iter_s ▅▁▆▃▄█▆▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb:                      mae 41.44652\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb:                     mape 20116913.67439\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb:                     rmse 129.84029\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb:       time_since_restore 33.31846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb:         time_this_iter_s 4.02594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb:             time_total_s 33.31846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb:                timestamp 1689725985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: 🚀 View run FSR_Trainable_4e34ea6f at: https://wandb.ai/seokjin/FSR-prediction/runs/4e34ea6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501177)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091914-4e34ea6f/logs\n",
      "2023-07-19 09:20:05,673\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.346 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:20:05,677\tWARNING util.py:315 -- The `process_trial_result` operation took 2.350 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:20:05,679\tWARNING util.py:315 -- Processing trial results took 2.353 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:20:05,681\tWARNING util.py:315 -- The `process_trial_result` operation took 2.355 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_8e59dd34_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-19-30/wandb/run-20230719_092005-8e59dd34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Syncing run FSR_Trainable_8e59dd34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8e59dd34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb:                      mae █▇▆▅▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb:                     mape █▇▇▅▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb:                     rmse ██▆▅▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb:         time_this_iter_s █▄▃▃▂▁▅▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb:                      mae 42.66941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb:                     mape 21909823.48545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb:                     rmse 131.03942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb:       time_since_restore 33.94091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb:         time_this_iter_s 4.13165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb:             time_total_s 33.94091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb:                timestamp 1689726006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: 🚀 View run FSR_Trainable_812f8eb1 at: https://wandb.ai/seokjin/FSR-prediction/runs/812f8eb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501611)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091937-812f8eb1/logs\n",
      "2023-07-19 09:20:25,826\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.144 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:20:25,829\tWARNING util.py:315 -- The `process_trial_result` operation took 2.147 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:20:25,832\tWARNING util.py:315 -- Processing trial results took 2.150 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:20:25,833\tWARNING util.py:315 -- The `process_trial_result` operation took 2.152 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_51044af2_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-19-58/wandb/run-20230719_092026-51044af2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: Syncing run FSR_Trainable_51044af2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/51044af2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb:                      mae █▇▅▄▃▃▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb:                     mape ██▇▅▃▃▃▃▃▃▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb:                     rmse █▇▅▄▃▃▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb:         time_this_iter_s █▃█▁▅▄▄▂▁▇▂▄▂▄█▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb:                timestamp ▁▂▂▂▃▃▄▄▅▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb:                      mae 37.72433\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb:                     mape 18043233.73771\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb:                     rmse 120.95902\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb:       time_since_restore 66.99574\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb:         time_this_iter_s 4.19947\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb:             time_total_s 66.99574\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb:                timestamp 1689726030\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: 🚀 View run FSR_Trainable_58e865dc at: https://wandb.ai/seokjin/FSR-prediction/runs/58e865dc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501397)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091925-58e865dc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb:                      mae █▇▅▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb:                     mape █▆▆▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb:                     rmse █▇▅▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb:         time_this_iter_s █▄▃▂▇▁▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb:                timestamp ▁▃▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "2023-07-19 09:20:46,476\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.511 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:20:46,480\tWARNING util.py:315 -- The `process_trial_result` operation took 2.516 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:20:46,482\tWARNING util.py:315 -- Processing trial results took 2.518 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:20:46,484\tWARNING util.py:315 -- The `process_trial_result` operation took 2.520 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=501845)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_86cec811_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_09-20-18/wandb/run-20230719_092049-86cec811\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: Syncing run FSR_Trainable_86cec811\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/86cec811\n",
      "2023-07-19 09:20:59,065\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.442 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:20:59,067\tWARNING util.py:315 -- The `process_trial_result` operation took 2.445 s, which may be a performance bottleneck.\n",
      "2023-07-19 09:20:59,069\tWARNING util.py:315 -- Processing trial results took 2.447 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 09:20:59,071\tWARNING util.py:315 -- The `process_trial_result` operation took 2.449 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_08-33-10/FSR_Trainable_75f4fb35_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Sim_2023-07-19_09-20-41/wandb/run-20230719_092101-75f4fb35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: Syncing run FSR_Trainable_75f4fb35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75f4fb35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb:                      mae 81.59622\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb:                     mape 50911255.43317\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb:                     rmse 222.71454\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb:       time_since_restore 2.70693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb:         time_this_iter_s 2.70693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb:             time_total_s 2.70693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb:                timestamp 1689726056\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: 🚀 View run FSR_Trainable_75f4fb35 at: https://wandb.ai/seokjin/FSR-prediction/runs/75f4fb35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502527)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_092101-75f4fb35/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb:                      mae █▄▂▂▁▁▁▁▁▁▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▂▂▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb:                     mape █▇▅▅▄▃▂▂▂▂▄▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▂▅▂▁▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb:                     rmse █▄▂▂▁▁▁▁▁▁▁▂▂▂▂▃▁▂▂▂▂▂▂▂▂▁▂▁▁▂▂▁▁▁▂▂▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb:         time_this_iter_s █▄▅▄▄▃▄▅▆▅▄▆▃▁▄▂▄▁▅▂▅▄▆▄▅▄▄▅▅▄█▄▄▄▃▅▅▆▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb:                      mae 36.48836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb:                     mape 12745904.35826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb:                     rmse 117.89426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb:       time_since_restore 194.76411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb:         time_this_iter_s 1.64055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb:             time_total_s 194.76411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb:                timestamp 1689726070\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: 🚀 View run FSR_Trainable_b102d1a7 at: https://wandb.ai/seokjin/FSR-prediction/runs/b102d1a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=499596)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_091740-b102d1a7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb:                      mae █▅▃▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb:                     mape █▄▃▃▃▃▃▃▃▃▃▃▃▂▁▁▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb:                     rmse █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb:         time_this_iter_s █▆▇▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▁▁▁▂▁▂▂▂▂▂▂▂▂▂▂▂▄▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb:                      mae 37.08809\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb:                     mape 13344427.10689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb:                     rmse 116.64016\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb:       time_since_restore 142.37037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb:         time_this_iter_s 1.28607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb:             time_total_s 142.37037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb:                timestamp 1689726190\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: 🚀 View run FSR_Trainable_86cec811 at: https://wandb.ai/seokjin/FSR-prediction/runs/86cec811\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502316)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_092049-86cec811/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb:                      mae █▆▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb:                     mape █▇▅▅▅▅▅▄▄▃▃▃▂▂▂▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb:                     rmse █▆▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▂▂▁▂▂▂▂▂▂▂▃▁▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb:         time_this_iter_s █▆▄▄▆▅▄▄▃▂▂▃▂▂▃▃▃▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▄▄▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb:                      mae 36.97403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb:                     mape 11945712.11369\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb:                     rmse 120.75129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb:       time_since_restore 186.09158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb:         time_this_iter_s 1.98791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb:             time_total_s 186.09158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb:                timestamp 1689726209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: 🚀 View run FSR_Trainable_51044af2 at: https://wandb.ai/seokjin/FSR-prediction/runs/51044af2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=502073)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_092026-51044af2/logs\n",
      "2023-07-19 09:23:33,488\tINFO tune.py:1111 -- Total run time: 3019.19 seconds (3015.05 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
