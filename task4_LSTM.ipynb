{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task4\n",
    "\n",
    "Index_X = FSR_for_force, FSR_for_coord\n",
    "\n",
    "Index_y = force, x_coord, y_coord\n",
    "\n",
    "Data = Splited by Subject\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-08-11_00-39-35/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-08-11_00-39-35\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "179.106\n",
    "\n",
    "1.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.LSTM'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    imputer = trial.suggest_categorical('imputer', ['sklearn.impute.SimpleImputer'])\n",
    "    if imputer == 'sklearn.impute.SimpleImputer':\n",
    "        trial.suggest_categorical('imputer_args/strategy', [\n",
    "            'mean',\n",
    "            'median',\n",
    "        ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': ['FSR_for_force', 'FSR_for_coord'],\n",
    "        'index_y': ['force', 'x_coord', 'y_coord'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_subject'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-11 03:31:19,197] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='metric',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='metric',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='metric',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 00:39:38,322\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "2023-08-11 00:39:40,542\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-08-11 01:12:33</td></tr>\n",
       "<tr><td>Running for: </td><td>00:32:53.05        </td></tr>\n",
       "<tr><td>Memory:      </td><td>7.3/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=83<br>Bracket: Iter 64.000: -0.6796549768647238 | Iter 32.000: -0.7055017016535721 | Iter 16.000: -0.7285606432971392 | Iter 8.000: -0.7668891739783584 | Iter 4.000: -0.8199686857249004 | Iter 2.000: -0.8429314828773979 | Iter 1.000: -1.0182954817032774<br>Logical resource usage: 8.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  : ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
       "  ... 68 more trials not shown (68 TERMINATED)\n",
       "  \n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>imputer             </th><th>imputer_args/strateg\n",
       "y       </th><th>index_X             </th><th>index_y             </th><th>model         </th><th style=\"text-align: right;\">    model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">  model_args/num_layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  tmae_force</th><th style=\"text-align: right;\">  trmse_force</th><th style=\"text-align: right;\">  tmape_force</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_21fdf89c</td><td>RUNNING   </td><td>172.26.215.93:201683</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>mean  </td><td>[&#x27;FSR_for_force_e9c0</td><td>[&#x27;force&#x27;, &#x27;x_co_cfc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00154315 </td><td>sklearn.preproc_8cf0</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">        61.9934 </td><td style=\"text-align: right;\">    0.284117</td><td style=\"text-align: right;\">     0.167455</td><td style=\"text-align: right;\">  5.17233e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_3838379f</td><td>RUNNING   </td><td>172.26.215.93:201909</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>mean  </td><td>[&#x27;FSR_for_force_77c0</td><td>[&#x27;force&#x27;, &#x27;x_co_58c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00172314 </td><td>sklearn.preproc_8cf0</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">        46.3924 </td><td style=\"text-align: right;\">    0.317156</td><td style=\"text-align: right;\">     0.174848</td><td style=\"text-align: right;\">  6.47185e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_815ec5f7</td><td>RUNNING   </td><td>172.26.215.93:201433</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>mean  </td><td>[&#x27;FSR_for_force_48c0</td><td>[&#x27;force&#x27;, &#x27;x_co_5f00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00166242 </td><td>sklearn.preproc_8cf0</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">        78.3732 </td><td style=\"text-align: right;\">    0.278604</td><td style=\"text-align: right;\">     0.168532</td><td style=\"text-align: right;\">  5.023e+14  </td></tr>\n",
       "<tr><td>FSR_Trainable_9a152f5f</td><td>RUNNING   </td><td>172.26.215.93:202290</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>mean  </td><td>[&#x27;FSR_for_force_3700</td><td>[&#x27;force&#x27;, &#x27;x_co_2480</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000856839</td><td>sklearn.preproc_8cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.65613</td><td style=\"text-align: right;\">    0.61006 </td><td style=\"text-align: right;\">     0.328768</td><td style=\"text-align: right;\">  1.46976e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_48eada07</td><td>PENDING   </td><td>                    </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>mean  </td><td>[&#x27;FSR_for_force_64c0</td><td>[&#x27;force&#x27;, &#x27;x_co_d6c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00163449 </td><td>sklearn.preproc_8cf0</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">            </td><td style=\"text-align: right;\">             </td><td style=\"text-align: right;\">             </td></tr>\n",
       "<tr><td>FSR_Trainable_0263d11d</td><td>TERMINATED</td><td>172.26.215.93:188629</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>mean  </td><td>[&#x27;FSR_for_force_cb80</td><td>[&#x27;force&#x27;, &#x27;x_co_da40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00295366 </td><td>sklearn.preproc_8e10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.17325</td><td style=\"text-align: right;\">    2.9087  </td><td style=\"text-align: right;\">     1.72364 </td><td style=\"text-align: right;\"> 32.867      </td></tr>\n",
       "<tr><td>FSR_Trainable_043a5c07</td><td>TERMINATED</td><td>172.26.215.93:181673</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>mean  </td><td>[&#x27;FSR_for_force_1780</td><td>[&#x27;force&#x27;, &#x27;x_co_1480</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00485025 </td><td>sklearn.preproc_8cf0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       400.072  </td><td style=\"text-align: right;\">    0.29724 </td><td style=\"text-align: right;\">     0.188638</td><td style=\"text-align: right;\">  4.43445e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_08dbc938</td><td>TERMINATED</td><td>172.26.215.93:194170</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>mean  </td><td>[&#x27;FSR_for_force_2700</td><td>[&#x27;force&#x27;, &#x27;x_co_0400</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00275751 </td><td>sklearn.preproc_89f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        16.8932 </td><td style=\"text-align: right;\">   10.2377  </td><td style=\"text-align: right;\">     7.1166  </td><td style=\"text-align: right;\">  2.46074e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_08ece1c7</td><td>TERMINATED</td><td>172.26.215.93:191403</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>mean  </td><td>[&#x27;FSR_for_force_3300</td><td>[&#x27;force&#x27;, &#x27;x_co_3500</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000310297</td><td>sklearn.preproc_8cf0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        55.0824 </td><td style=\"text-align: right;\">    0.513415</td><td style=\"text-align: right;\">     0.303107</td><td style=\"text-align: right;\">  1.07223e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_0daad9a2</td><td>TERMINATED</td><td>172.26.215.93:189324</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>mean  </td><td>[&#x27;FSR_for_force_fbc0</td><td>[&#x27;force&#x27;, &#x27;x_co_f680</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000758025</td><td>sklearn.preproc_8cf0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       152.569  </td><td style=\"text-align: right;\">    0.390066</td><td style=\"text-align: right;\">     0.243769</td><td style=\"text-align: right;\">  7.04061e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_12d3f88c</td><td>TERMINATED</td><td>172.26.215.93:199460</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>mean  </td><td>[&#x27;FSR_for_force_60c0</td><td>[&#x27;force&#x27;, &#x27;x_co_42c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000959015</td><td>sklearn.preproc_8cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       116.262  </td><td style=\"text-align: right;\">    0.269049</td><td style=\"text-align: right;\">     0.170632</td><td style=\"text-align: right;\">  4.0038e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_14a7a158</td><td>TERMINATED</td><td>172.26.215.93:196148</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>mean  </td><td>[&#x27;FSR_for_force_4940</td><td>[&#x27;force&#x27;, &#x27;x_co_7a40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00192089 </td><td>sklearn.preproc_8cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       101.278  </td><td style=\"text-align: right;\">    0.265228</td><td style=\"text-align: right;\">     0.173557</td><td style=\"text-align: right;\">  3.85182e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_170adff7</td><td>TERMINATED</td><td>172.26.215.93:196857</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>mean  </td><td>[&#x27;FSR_for_force_ef80</td><td>[&#x27;force&#x27;, &#x27;x_co_e180</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00113796 </td><td>sklearn.preproc_8cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       101.114  </td><td style=\"text-align: right;\">    0.250942</td><td style=\"text-align: right;\">     0.166022</td><td style=\"text-align: right;\">  3.52458e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_19e6058b</td><td>TERMINATED</td><td>172.26.215.93:183244</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>median</td><td>[&#x27;FSR_for_force_e0c0</td><td>[&#x27;force&#x27;, &#x27;x_co_1280</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        4.825e-05  </td><td>sklearn.preproc_8cf0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        12.1478 </td><td style=\"text-align: right;\">    0.932246</td><td style=\"text-align: right;\">     0.521179</td><td style=\"text-align: right;\">  1.37614e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_1a98df0c</td><td>TERMINATED</td><td>172.26.215.93:191593</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>mean  </td><td>[&#x27;FSR_for_force_2600</td><td>[&#x27;force&#x27;, &#x27;x_co_1800</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00123903 </td><td>sklearn.preproc_8cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       230.736  </td><td style=\"text-align: right;\">    0.25361 </td><td style=\"text-align: right;\">     0.174099</td><td style=\"text-align: right;\">  3.30162e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_1c686623</td><td>TERMINATED</td><td>172.26.215.93:198147</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>mean  </td><td>[&#x27;FSR_for_force_c180</td><td>[&#x27;force&#x27;, &#x27;x_co_be80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00602797 </td><td>sklearn.preproc_8e10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.38901</td><td style=\"text-align: right;\">    2.45558 </td><td style=\"text-align: right;\">     1.52909 </td><td style=\"text-align: right;\">103.261      </td></tr>\n",
       "<tr><td>FSR_Trainable_22929497</td><td>TERMINATED</td><td>172.26.215.93:189779</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>mean  </td><td>[&#x27;FSR_for_force_1780</td><td>[&#x27;force&#x27;, &#x27;x_co_ac00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000953432</td><td>sklearn.preproc_8cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       463.487  </td><td style=\"text-align: right;\">    0.292846</td><td style=\"text-align: right;\">     0.191543</td><td style=\"text-align: right;\">  4.32979e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_23bfb9b7</td><td>TERMINATED</td><td>172.26.215.93:200699</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>mean  </td><td>[&#x27;FSR_for_force_53c0</td><td>[&#x27;force&#x27;, &#x27;x_co_5b00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0048073  </td><td>sklearn.preproc_89f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.17463</td><td style=\"text-align: right;\">    5.83398 </td><td style=\"text-align: right;\">     6.39416 </td><td style=\"text-align: right;\">  3.69002e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_3409809b</td><td>TERMINATED</td><td>172.26.215.93:183943</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>median</td><td>[&#x27;FSR_for_force_3cc0</td><td>[&#x27;force&#x27;, &#x27;x_co_d880</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000137474</td><td>sklearn.preproc_8cf0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        66.6879 </td><td style=\"text-align: right;\">    0.482375</td><td style=\"text-align: right;\">     0.32815 </td><td style=\"text-align: right;\">  7.30487e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_3433d7fa</td><td>TERMINATED</td><td>172.26.215.93:192188</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8b10</td><td>sklearn.impute._7e60</td><td>mean  </td><td>[&#x27;FSR_for_force_b440</td><td>[&#x27;force&#x27;, &#x27;x_co_9100</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00135528 </td><td>sklearn.preproc_8cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       243.691  </td><td style=\"text-align: right;\">    0.263792</td><td style=\"text-align: right;\">     0.169348</td><td style=\"text-align: right;\">  4.06841e+14</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 00:39:40,613\tINFO wandb.py:320 -- Already logged into W&B.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_e5ebe0c9_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-11_00-39-40/wandb/run-20230811_003952-e5ebe0c9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Syncing run FSR_Trainable_e5ebe0c9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e5ebe0c9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">  mae_coord</th><th style=\"text-align: right;\">  mae_force</th><th style=\"text-align: right;\">  mape_coord</th><th style=\"text-align: right;\">  mape_force</th><th style=\"text-align: right;\">   metric</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">  rmse_coord</th><th style=\"text-align: right;\">  rmse_force</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  tmae_coord</th><th style=\"text-align: right;\">  tmae_force</th><th style=\"text-align: right;\">  tmape_coord</th><th style=\"text-align: right;\">  tmape_force</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th><th style=\"text-align: right;\">  trmse_coord</th><th style=\"text-align: right;\">  trmse_force</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_0263d11d</td><td>2023-08-11_00-46-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    3.83395</td><td style=\"text-align: right;\">   1243.99 </td><td style=\"text-align: right;\">    1.10314 </td><td style=\"text-align: right;\"> 4.96748e+08</td><td style=\"text-align: right;\"> 6.20889 </td><td>172.26.215.93</td><td style=\"text-align: right;\">188629</td><td style=\"text-align: right;\">     2.35646</td><td style=\"text-align: right;\">     829.21 </td><td style=\"text-align: right;\">            3.17325 </td><td style=\"text-align: right;\">          3.17325 </td><td style=\"text-align: right;\">      3.17325 </td><td style=\"text-align: right;\"> 1691682413</td><td style=\"text-align: right;\">     7.28164</td><td style=\"text-align: right;\">    2.9087  </td><td style=\"text-align: right;\">  5.22389e+14</td><td style=\"text-align: right;\"> 32.867      </td><td style=\"text-align: right;\">                   1</td><td>0263d11d  </td><td style=\"text-align: right;\">     4.48525 </td><td style=\"text-align: right;\">     1.72364 </td></tr>\n",
       "<tr><td>FSR_Trainable_043a5c07</td><td>2023-08-11_00-47-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">    5.56496</td><td style=\"text-align: right;\">    718.046</td><td style=\"text-align: right;\">    1.27169 </td><td style=\"text-align: right;\"> 9.01965e+17</td><td style=\"text-align: right;\"> 0.715225</td><td>172.26.215.93</td><td style=\"text-align: right;\">181673</td><td style=\"text-align: right;\">     2.65691</td><td style=\"text-align: right;\">     480.161</td><td style=\"text-align: right;\">          400.072   </td><td style=\"text-align: right;\">          6.67965 </td><td style=\"text-align: right;\">    400.072   </td><td style=\"text-align: right;\"> 1691682420</td><td style=\"text-align: right;\">     1.18188</td><td style=\"text-align: right;\">    0.29724 </td><td style=\"text-align: right;\">  7.81235e+13</td><td style=\"text-align: right;\">  4.43445e+14</td><td style=\"text-align: right;\">                  64</td><td>043a5c07  </td><td style=\"text-align: right;\">     0.526588</td><td style=\"text-align: right;\">     0.188638</td></tr>\n",
       "<tr><td>FSR_Trainable_08dbc938</td><td>2023-08-11_01-00-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    5.49974</td><td style=\"text-align: right;\">   1272.08 </td><td style=\"text-align: right;\">    1.54442 </td><td style=\"text-align: right;\"> 7.11904e+17</td><td style=\"text-align: right;\">42.3578  </td><td>172.26.215.93</td><td style=\"text-align: right;\">194170</td><td style=\"text-align: right;\">     2.56798</td><td style=\"text-align: right;\">     941.895</td><td style=\"text-align: right;\">           16.8932  </td><td style=\"text-align: right;\">         16.8932  </td><td style=\"text-align: right;\">     16.8932  </td><td style=\"text-align: right;\"> 1691683241</td><td style=\"text-align: right;\">    34.9217 </td><td style=\"text-align: right;\">   10.2377  </td><td style=\"text-align: right;\">  2.23357e+16</td><td style=\"text-align: right;\">  2.46074e+16</td><td style=\"text-align: right;\">                   1</td><td>08dbc938  </td><td style=\"text-align: right;\">    35.2412  </td><td style=\"text-align: right;\">     7.1166  </td></tr>\n",
       "<tr><td>FSR_Trainable_08ece1c7</td><td>2023-08-11_00-54-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    4.95041</td><td style=\"text-align: right;\">   1422.36 </td><td style=\"text-align: right;\">    1.23723 </td><td style=\"text-align: right;\"> 2.77166e+18</td><td style=\"text-align: right;\"> 0.831566</td><td>172.26.215.93</td><td style=\"text-align: right;\">191403</td><td style=\"text-align: right;\">     2.56307</td><td style=\"text-align: right;\">     943.166</td><td style=\"text-align: right;\">           55.0824  </td><td style=\"text-align: right;\">          6.38127 </td><td style=\"text-align: right;\">     55.0824  </td><td style=\"text-align: right;\"> 1691682855</td><td style=\"text-align: right;\">     1.08289</td><td style=\"text-align: right;\">    0.513415</td><td style=\"text-align: right;\">  8.25355e+13</td><td style=\"text-align: right;\">  1.07223e+15</td><td style=\"text-align: right;\">                   8</td><td>08ece1c7  </td><td style=\"text-align: right;\">     0.528459</td><td style=\"text-align: right;\">     0.303107</td></tr>\n",
       "<tr><td>FSR_Trainable_0daad9a2</td><td>2023-08-11_00-50-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">    4.79424</td><td style=\"text-align: right;\">   1045.58 </td><td style=\"text-align: right;\">    1.27489 </td><td style=\"text-align: right;\"> 1.75673e+18</td><td style=\"text-align: right;\"> 0.749347</td><td>172.26.215.93</td><td style=\"text-align: right;\">189324</td><td style=\"text-align: right;\">     2.49671</td><td style=\"text-align: right;\">     726.741</td><td style=\"text-align: right;\">          152.569   </td><td style=\"text-align: right;\">          4.51588 </td><td style=\"text-align: right;\">    152.569   </td><td style=\"text-align: right;\"> 1691682609</td><td style=\"text-align: right;\">     1.03638</td><td style=\"text-align: right;\">    0.390066</td><td style=\"text-align: right;\">  8.08905e+13</td><td style=\"text-align: right;\">  7.04061e+14</td><td style=\"text-align: right;\">                  32</td><td>0daad9a2  </td><td style=\"text-align: right;\">     0.505578</td><td style=\"text-align: right;\">     0.243769</td></tr>\n",
       "<tr><td>FSR_Trainable_12d3f88c</td><td>2023-08-11_01-10-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    5.10608</td><td style=\"text-align: right;\">    660.407</td><td style=\"text-align: right;\">    1.19581 </td><td style=\"text-align: right;\"> 7.7794e+17 </td><td style=\"text-align: right;\"> 0.66628 </td><td>172.26.215.93</td><td style=\"text-align: right;\">199460</td><td style=\"text-align: right;\">     2.53949</td><td style=\"text-align: right;\">     463.226</td><td style=\"text-align: right;\">          116.262   </td><td style=\"text-align: right;\">          1.56638 </td><td style=\"text-align: right;\">    116.262   </td><td style=\"text-align: right;\"> 1691683833</td><td style=\"text-align: right;\">     1.07379</td><td style=\"text-align: right;\">    0.269049</td><td style=\"text-align: right;\">  7.55217e+13</td><td style=\"text-align: right;\">  4.0038e+14 </td><td style=\"text-align: right;\">                 100</td><td>12d3f88c  </td><td style=\"text-align: right;\">     0.495648</td><td style=\"text-align: right;\">     0.170632</td></tr>\n",
       "<tr><td>FSR_Trainable_14a7a158</td><td>2023-08-11_01-05-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    5.34303</td><td style=\"text-align: right;\">    674.391</td><td style=\"text-align: right;\">    1.18652 </td><td style=\"text-align: right;\"> 8.19509e+17</td><td style=\"text-align: right;\"> 0.679719</td><td>172.26.215.93</td><td style=\"text-align: right;\">196148</td><td style=\"text-align: right;\">     2.58393</td><td style=\"text-align: right;\">     485.066</td><td style=\"text-align: right;\">          101.278   </td><td style=\"text-align: right;\">          0.914241</td><td style=\"text-align: right;\">    101.278   </td><td style=\"text-align: right;\"> 1691683516</td><td style=\"text-align: right;\">     1.12622</td><td style=\"text-align: right;\">    0.265228</td><td style=\"text-align: right;\">  7.34211e+13</td><td style=\"text-align: right;\">  3.85182e+14</td><td style=\"text-align: right;\">                 100</td><td>14a7a158  </td><td style=\"text-align: right;\">     0.506162</td><td style=\"text-align: right;\">     0.173557</td></tr>\n",
       "<tr><td>FSR_Trainable_170adff7</td><td>2023-08-11_01-06-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    5.31857</td><td style=\"text-align: right;\">    632.744</td><td style=\"text-align: right;\">    1.17394 </td><td style=\"text-align: right;\"> 7.06356e+17</td><td style=\"text-align: right;\"> 0.672783</td><td>172.26.215.93</td><td style=\"text-align: right;\">196857</td><td style=\"text-align: right;\">     2.58986</td><td style=\"text-align: right;\">     467.646</td><td style=\"text-align: right;\">          101.114   </td><td style=\"text-align: right;\">          0.877878</td><td style=\"text-align: right;\">    101.114   </td><td style=\"text-align: right;\"> 1691683586</td><td style=\"text-align: right;\">     1.12069</td><td style=\"text-align: right;\">    0.250942</td><td style=\"text-align: right;\">  7.45695e+13</td><td style=\"text-align: right;\">  3.52458e+14</td><td style=\"text-align: right;\">                 100</td><td>170adff7  </td><td style=\"text-align: right;\">     0.506761</td><td style=\"text-align: right;\">     0.166022</td></tr>\n",
       "<tr><td>FSR_Trainable_19e6058b</td><td>2023-08-11_00-41-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">   27.0655 </td><td style=\"text-align: right;\">   2613.88 </td><td style=\"text-align: right;\">    4.03607 </td><td style=\"text-align: right;\"> 3.16548e+18</td><td style=\"text-align: right;\"> 2.39461 </td><td>172.26.215.93</td><td style=\"text-align: right;\">183244</td><td style=\"text-align: right;\">     8.96946</td><td style=\"text-align: right;\">    1692.64 </td><td style=\"text-align: right;\">           12.1478  </td><td style=\"text-align: right;\">          3.24274 </td><td style=\"text-align: right;\">     12.1478  </td><td style=\"text-align: right;\"> 1691682087</td><td style=\"text-align: right;\">     5.8739 </td><td style=\"text-align: right;\">    0.932246</td><td style=\"text-align: right;\">  2.41018e+13</td><td style=\"text-align: right;\">  1.37614e+15</td><td style=\"text-align: right;\">                   4</td><td>19e6058b  </td><td style=\"text-align: right;\">     1.87343 </td><td style=\"text-align: right;\">     0.521179</td></tr>\n",
       "<tr><td>FSR_Trainable_1a98df0c</td><td>2023-08-11_00-57-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    5.2624 </td><td style=\"text-align: right;\">    659.165</td><td style=\"text-align: right;\">    1.17718 </td><td style=\"text-align: right;\"> 7.52539e+17</td><td style=\"text-align: right;\"> 0.683731</td><td>172.26.215.93</td><td style=\"text-align: right;\">191593</td><td style=\"text-align: right;\">     2.6214 </td><td style=\"text-align: right;\">     493.077</td><td style=\"text-align: right;\">          230.736   </td><td style=\"text-align: right;\">          2.39619 </td><td style=\"text-align: right;\">    230.736   </td><td style=\"text-align: right;\"> 1691683055</td><td style=\"text-align: right;\">     1.10912</td><td style=\"text-align: right;\">    0.25361 </td><td style=\"text-align: right;\">  7.38813e+13</td><td style=\"text-align: right;\">  3.30162e+14</td><td style=\"text-align: right;\">                 100</td><td>1a98df0c  </td><td style=\"text-align: right;\">     0.509632</td><td style=\"text-align: right;\">     0.174099</td></tr>\n",
       "<tr><td>FSR_Trainable_1c686623</td><td>2023-08-11_01-06-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    4.26798</td><td style=\"text-align: right;\">    933.212</td><td style=\"text-align: right;\">    1.23191 </td><td style=\"text-align: right;\"> 4.64659e+08</td><td style=\"text-align: right;\"> 5.97432 </td><td>172.26.215.93</td><td style=\"text-align: right;\">198147</td><td style=\"text-align: right;\">     2.37793</td><td style=\"text-align: right;\">     640.663</td><td style=\"text-align: right;\">            1.38901 </td><td style=\"text-align: right;\">          1.38901 </td><td style=\"text-align: right;\">      1.38901 </td><td style=\"text-align: right;\"> 1691683614</td><td style=\"text-align: right;\">     7.85804</td><td style=\"text-align: right;\">    2.45558 </td><td style=\"text-align: right;\">  1.56213e+15</td><td style=\"text-align: right;\">103.261      </td><td style=\"text-align: right;\">                   1</td><td>1c686623  </td><td style=\"text-align: right;\">     4.44522 </td><td style=\"text-align: right;\">     1.52909 </td></tr>\n",
       "<tr><td>FSR_Trainable_21fdf89c</td><td>2023-08-11_01-12-35</td><td>False </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        55</td><td style=\"text-align: right;\">    5.50878</td><td style=\"text-align: right;\">    669.959</td><td style=\"text-align: right;\">    1.21319 </td><td style=\"text-align: right;\"> 1.00302e+18</td><td style=\"text-align: right;\"> 0.679117</td><td>172.26.215.93</td><td style=\"text-align: right;\">201683</td><td style=\"text-align: right;\">     2.60229</td><td style=\"text-align: right;\">     433.544</td><td style=\"text-align: right;\">           65.3405  </td><td style=\"text-align: right;\">          1.241   </td><td style=\"text-align: right;\">     65.3405  </td><td style=\"text-align: right;\"> 1691683955</td><td style=\"text-align: right;\">     1.15914</td><td style=\"text-align: right;\">    0.282987</td><td style=\"text-align: right;\">  7.44771e+13</td><td style=\"text-align: right;\">  5.10267e+14</td><td style=\"text-align: right;\">                  55</td><td>21fdf89c  </td><td style=\"text-align: right;\">     0.511446</td><td style=\"text-align: right;\">     0.167671</td></tr>\n",
       "<tr><td>FSR_Trainable_22929497</td><td>2023-08-11_00-56-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.77914</td><td style=\"text-align: right;\">    759.216</td><td style=\"text-align: right;\">    1.11783 </td><td style=\"text-align: right;\"> 9.88918e+17</td><td style=\"text-align: right;\"> 0.688665</td><td>172.26.215.93</td><td style=\"text-align: right;\">189779</td><td style=\"text-align: right;\">     2.46949</td><td style=\"text-align: right;\">     551.223</td><td style=\"text-align: right;\">          463.487   </td><td style=\"text-align: right;\">          4.54836 </td><td style=\"text-align: right;\">    463.487   </td><td style=\"text-align: right;\"> 1691682961</td><td style=\"text-align: right;\">     1.02513</td><td style=\"text-align: right;\">    0.292846</td><td style=\"text-align: right;\">  7.53817e+13</td><td style=\"text-align: right;\">  4.32979e+14</td><td style=\"text-align: right;\">                 100</td><td>22929497  </td><td style=\"text-align: right;\">     0.497122</td><td style=\"text-align: right;\">     0.191543</td></tr>\n",
       "<tr><td>FSR_Trainable_23bfb9b7</td><td>2023-08-11_01-09-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    3.92869</td><td style=\"text-align: right;\">   1240.35 </td><td style=\"text-align: right;\">    1.14618 </td><td style=\"text-align: right;\"> 9.77763e+16</td><td style=\"text-align: right;\">39.5156  </td><td>172.26.215.93</td><td style=\"text-align: right;\">200699</td><td style=\"text-align: right;\">     2.37467</td><td style=\"text-align: right;\">    1049.67 </td><td style=\"text-align: right;\">            2.17463 </td><td style=\"text-align: right;\">          2.17463 </td><td style=\"text-align: right;\">      2.17463 </td><td style=\"text-align: right;\"> 1691683798</td><td style=\"text-align: right;\">    28.1012 </td><td style=\"text-align: right;\">    5.83398 </td><td style=\"text-align: right;\">  3.01863e+15</td><td style=\"text-align: right;\">  3.69002e+15</td><td style=\"text-align: right;\">                   1</td><td>23bfb9b7  </td><td style=\"text-align: right;\">    33.1214  </td><td style=\"text-align: right;\">     6.39416 </td></tr>\n",
       "<tr><td>FSR_Trainable_3409809b</td><td>2023-08-11_00-43-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">   28.1765 </td><td style=\"text-align: right;\">   1300.83 </td><td style=\"text-align: right;\">    4.67383 </td><td style=\"text-align: right;\"> 1.48141e+18</td><td style=\"text-align: right;\"> 2.29339 </td><td>172.26.215.93</td><td style=\"text-align: right;\">183943</td><td style=\"text-align: right;\">     9.08836</td><td style=\"text-align: right;\">    1035.85 </td><td style=\"text-align: right;\">           66.6879  </td><td style=\"text-align: right;\">         17.5633  </td><td style=\"text-align: right;\">     66.6879  </td><td style=\"text-align: right;\"> 1691682181</td><td style=\"text-align: right;\">     6.23833</td><td style=\"text-align: right;\">    0.482375</td><td style=\"text-align: right;\">  1.02716e+13</td><td style=\"text-align: right;\">  7.30487e+14</td><td style=\"text-align: right;\">                   4</td><td>3409809b  </td><td style=\"text-align: right;\">     1.96524 </td><td style=\"text-align: right;\">     0.32815 </td></tr>\n",
       "<tr><td>FSR_Trainable_3433d7fa</td><td>2023-08-11_01-00-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.71034</td><td style=\"text-align: right;\">    683.525</td><td style=\"text-align: right;\">    1.1519  </td><td style=\"text-align: right;\"> 9.46102e+17</td><td style=\"text-align: right;\"> 0.660066</td><td>172.26.215.93</td><td style=\"text-align: right;\">192188</td><td style=\"text-align: right;\">     2.44939</td><td style=\"text-align: right;\">     482.889</td><td style=\"text-align: right;\">          243.691   </td><td style=\"text-align: right;\">          2.49823 </td><td style=\"text-align: right;\">    243.691   </td><td style=\"text-align: right;\"> 1691683206</td><td style=\"text-align: right;\">     1.00609</td><td style=\"text-align: right;\">    0.263792</td><td style=\"text-align: right;\">  7.87793e+13</td><td style=\"text-align: right;\">  4.06841e+14</td><td style=\"text-align: right;\">                 100</td><td>3433d7fa  </td><td style=\"text-align: right;\">     0.490719</td><td style=\"text-align: right;\">     0.169348</td></tr>\n",
       "<tr><td>FSR_Trainable_36b10ce7</td><td>2023-08-11_01-01-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   23.9793 </td><td style=\"text-align: right;\">   1888.52 </td><td style=\"text-align: right;\">    4.12449 </td><td style=\"text-align: right;\"> 2.50076e+18</td><td style=\"text-align: right;\"> 2.0306  </td><td>172.26.215.93</td><td style=\"text-align: right;\">194875</td><td style=\"text-align: right;\">     8.09344</td><td style=\"text-align: right;\">    1379.67 </td><td style=\"text-align: right;\">            1.40836 </td><td style=\"text-align: right;\">          1.40836 </td><td style=\"text-align: right;\">      1.40836 </td><td style=\"text-align: right;\"> 1691683285</td><td style=\"text-align: right;\">     5.1599 </td><td style=\"text-align: right;\">    0.661676</td><td style=\"text-align: right;\">  3.1396e+13 </td><td style=\"text-align: right;\">  1.14325e+15</td><td style=\"text-align: right;\">                   1</td><td>36b10ce7  </td><td style=\"text-align: right;\">     1.63728 </td><td style=\"text-align: right;\">     0.393313</td></tr>\n",
       "<tr><td>FSR_Trainable_3838379f</td><td>2023-08-11_01-12-31</td><td>False </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        42</td><td style=\"text-align: right;\">    5.69851</td><td style=\"text-align: right;\">    723.417</td><td style=\"text-align: right;\">    1.18844 </td><td style=\"text-align: right;\"> 1.15923e+18</td><td style=\"text-align: right;\"> 0.68969 </td><td>172.26.215.93</td><td style=\"text-align: right;\">201909</td><td style=\"text-align: right;\">     2.6394 </td><td style=\"text-align: right;\">     451.329</td><td style=\"text-align: right;\">           47.7667  </td><td style=\"text-align: right;\">          1.37436 </td><td style=\"text-align: right;\">     47.7667  </td><td style=\"text-align: right;\"> 1691683951</td><td style=\"text-align: right;\">     1.19314</td><td style=\"text-align: right;\">    0.314822</td><td style=\"text-align: right;\">  7.5466e+13 </td><td style=\"text-align: right;\">  6.39579e+14</td><td style=\"text-align: right;\">                  42</td><td>3838379f  </td><td style=\"text-align: right;\">     0.515517</td><td style=\"text-align: right;\">     0.174173</td></tr>\n",
       "<tr><td>FSR_Trainable_3a9efe0a</td><td>2023-08-11_00-50-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    3.84098</td><td style=\"text-align: right;\">   1054.14 </td><td style=\"text-align: right;\">    1.15528 </td><td style=\"text-align: right;\"> 6.01463e+16</td><td style=\"text-align: right;\">39.6931  </td><td>172.26.215.93</td><td style=\"text-align: right;\">190623</td><td style=\"text-align: right;\">     2.37358</td><td style=\"text-align: right;\">     879.347</td><td style=\"text-align: right;\">            5.55498 </td><td style=\"text-align: right;\">          5.55498 </td><td style=\"text-align: right;\">      5.55498 </td><td style=\"text-align: right;\"> 1691682652</td><td style=\"text-align: right;\">    27.88   </td><td style=\"text-align: right;\">    5.66274 </td><td style=\"text-align: right;\">  2.08844e+15</td><td style=\"text-align: right;\">  2.2517e+15 </td><td style=\"text-align: right;\">                   1</td><td>3a9efe0a  </td><td style=\"text-align: right;\">    33.1617  </td><td style=\"text-align: right;\">     6.53139 </td></tr>\n",
       "<tr><td>FSR_Trainable_3c320acd</td><td>2023-08-11_00-40-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    4.16052</td><td style=\"text-align: right;\">   1208.9  </td><td style=\"text-align: right;\">    1.01715 </td><td style=\"text-align: right;\"> 5.36695e+16</td><td style=\"text-align: right;\">55.3019  </td><td>172.26.215.93</td><td style=\"text-align: right;\">182021</td><td style=\"text-align: right;\">     2.53921</td><td style=\"text-align: right;\">    1071.63 </td><td style=\"text-align: right;\">            3.25225 </td><td style=\"text-align: right;\">          3.25225 </td><td style=\"text-align: right;\">      3.25225 </td><td style=\"text-align: right;\"> 1691682020</td><td style=\"text-align: right;\">    44.2738 </td><td style=\"text-align: right;\">    6.24538 </td><td style=\"text-align: right;\">  2.26432e+15</td><td style=\"text-align: right;\">  1.35384e+15</td><td style=\"text-align: right;\">                   1</td><td>3c320acd  </td><td style=\"text-align: right;\">    48.0936  </td><td style=\"text-align: right;\">     7.20836 </td></tr>\n",
       "<tr><td>FSR_Trainable_3ea710a0</td><td>2023-08-11_00-41-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    4.0561 </td><td style=\"text-align: right;\">   1472.76 </td><td style=\"text-align: right;\">    1.14597 </td><td style=\"text-align: right;\"> 6.098e+08  </td><td style=\"text-align: right;\"> 6.59686 </td><td>172.26.215.93</td><td style=\"text-align: right;\">183021</td><td style=\"text-align: right;\">     2.39998</td><td style=\"text-align: right;\">     959.636</td><td style=\"text-align: right;\">            5.08803 </td><td style=\"text-align: right;\">          2.42417 </td><td style=\"text-align: right;\">      5.08803 </td><td style=\"text-align: right;\"> 1691682068</td><td style=\"text-align: right;\">     7.66565</td><td style=\"text-align: right;\">    3.49865 </td><td style=\"text-align: right;\">  7.13642e+14</td><td style=\"text-align: right;\"> 74.0941     </td><td style=\"text-align: right;\">                   2</td><td>3ea710a0  </td><td style=\"text-align: right;\">     4.54199 </td><td style=\"text-align: right;\">     2.05487 </td></tr>\n",
       "<tr><td>FSR_Trainable_3fbd5160</td><td>2023-08-11_00-55-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    5.3052 </td><td style=\"text-align: right;\">    747.403</td><td style=\"text-align: right;\">    1.20196 </td><td style=\"text-align: right;\"> 8.28954e+17</td><td style=\"text-align: right;\"> 0.693551</td><td>172.26.215.93</td><td style=\"text-align: right;\">189105</td><td style=\"text-align: right;\">     2.6316 </td><td style=\"text-align: right;\">     546.419</td><td style=\"text-align: right;\">          474.508   </td><td style=\"text-align: right;\">          4.78671 </td><td style=\"text-align: right;\">    474.508   </td><td style=\"text-align: right;\"> 1691682928</td><td style=\"text-align: right;\">     1.12348</td><td style=\"text-align: right;\">    0.27521 </td><td style=\"text-align: right;\">  7.51492e+13</td><td style=\"text-align: right;\">  3.53797e+14</td><td style=\"text-align: right;\">                 100</td><td>3fbd5160  </td><td style=\"text-align: right;\">     0.511896</td><td style=\"text-align: right;\">     0.181654</td></tr>\n",
       "<tr><td>FSR_Trainable_48c46c0b</td><td>2023-08-11_00-58-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    5.72483</td><td style=\"text-align: right;\">    702.413</td><td style=\"text-align: right;\">    1.22078 </td><td style=\"text-align: right;\"> 8.49978e+17</td><td style=\"text-align: right;\"> 0.727917</td><td>172.26.215.93</td><td style=\"text-align: right;\">191902</td><td style=\"text-align: right;\">     2.79728</td><td style=\"text-align: right;\">     503.362</td><td style=\"text-align: right;\">          248.665   </td><td style=\"text-align: right;\">          2.96304 </td><td style=\"text-align: right;\">    248.665   </td><td style=\"text-align: right;\"> 1691683134</td><td style=\"text-align: right;\">     1.19137</td><td style=\"text-align: right;\">    0.283781</td><td style=\"text-align: right;\">  6.97924e+13</td><td style=\"text-align: right;\">  4.0364e+14 </td><td style=\"text-align: right;\">                 100</td><td>48c46c0b  </td><td style=\"text-align: right;\">     0.537545</td><td style=\"text-align: right;\">     0.190372</td></tr>\n",
       "<tr><td>FSR_Trainable_4be3dabe</td><td>2023-08-11_00-40-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    4.47826</td><td style=\"text-align: right;\">   1507.26 </td><td style=\"text-align: right;\">    1.08617 </td><td style=\"text-align: right;\"> 8.79761e+08</td><td style=\"text-align: right;\"> 6.64384 </td><td>172.26.215.93</td><td style=\"text-align: right;\">182789</td><td style=\"text-align: right;\">     2.52933</td><td style=\"text-align: right;\">    1013.24 </td><td style=\"text-align: right;\">            4.02472 </td><td style=\"text-align: right;\">          1.82069 </td><td style=\"text-align: right;\">      4.02472 </td><td style=\"text-align: right;\"> 1691682056</td><td style=\"text-align: right;\">     8.25227</td><td style=\"text-align: right;\">    3.33216 </td><td style=\"text-align: right;\">464.969      </td><td style=\"text-align: right;\"> 84.5943     </td><td style=\"text-align: right;\">                   2</td><td>4be3dabe  </td><td style=\"text-align: right;\">     4.56369 </td><td style=\"text-align: right;\">     2.08015 </td></tr>\n",
       "<tr><td>FSR_Trainable_4cf41fd6</td><td>2023-08-11_00-41-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    7.0977 </td><td style=\"text-align: right;\">   1549.34 </td><td style=\"text-align: right;\">    1.59299 </td><td style=\"text-align: right;\"> 3.11374e+18</td><td style=\"text-align: right;\"> 0.916595</td><td>172.26.215.93</td><td style=\"text-align: right;\">183484</td><td style=\"text-align: right;\">     2.84428</td><td style=\"text-align: right;\">     981.382</td><td style=\"text-align: right;\">           18.5167  </td><td style=\"text-align: right;\">          2.00465 </td><td style=\"text-align: right;\">     18.5167  </td><td style=\"text-align: right;\"> 1691682109</td><td style=\"text-align: right;\">     1.50118</td><td style=\"text-align: right;\">    0.604986</td><td style=\"text-align: right;\">  8.15482e+13</td><td style=\"text-align: right;\">  1.38589e+15</td><td style=\"text-align: right;\">                   8</td><td>4cf41fd6  </td><td style=\"text-align: right;\">     0.580025</td><td style=\"text-align: right;\">     0.33657 </td></tr>\n",
       "<tr><td>FSR_Trainable_4d46109e</td><td>2023-08-11_01-07-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   24.8518 </td><td style=\"text-align: right;\">   1414.88 </td><td style=\"text-align: right;\">    4.22355 </td><td style=\"text-align: right;\"> 2.30089e+18</td><td style=\"text-align: right;\"> 2.05078 </td><td>172.26.215.93</td><td style=\"text-align: right;\">199151</td><td style=\"text-align: right;\">     8.32346</td><td style=\"text-align: right;\">    1021.35 </td><td style=\"text-align: right;\">            1.16815 </td><td style=\"text-align: right;\">          1.16815 </td><td style=\"text-align: right;\">      1.16815 </td><td style=\"text-align: right;\"> 1691683676</td><td style=\"text-align: right;\">     5.37808</td><td style=\"text-align: right;\">    0.49429 </td><td style=\"text-align: right;\">  2.57576e+13</td><td style=\"text-align: right;\">  7.41904e+14</td><td style=\"text-align: right;\">                   1</td><td>4d46109e  </td><td style=\"text-align: right;\">     1.71609 </td><td style=\"text-align: right;\">     0.33469 </td></tr>\n",
       "<tr><td>FSR_Trainable_4d8df89d</td><td>2023-08-11_00-40-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">   29.5961 </td><td style=\"text-align: right;\">   1555.56 </td><td style=\"text-align: right;\">    4.58694 </td><td style=\"text-align: right;\"> 9.31989e+17</td><td style=\"text-align: right;\"> 2.44041 </td><td>172.26.215.93</td><td style=\"text-align: right;\">181837</td><td style=\"text-align: right;\">     9.29733</td><td style=\"text-align: right;\">    1332.95 </td><td style=\"text-align: right;\">            7.6936  </td><td style=\"text-align: right;\">          3.52207 </td><td style=\"text-align: right;\">      7.6936  </td><td style=\"text-align: right;\"> 1691682015</td><td style=\"text-align: right;\">     6.6344 </td><td style=\"text-align: right;\">    0.546602</td><td style=\"text-align: right;\">  1.32655e+13</td><td style=\"text-align: right;\">  5.36619e+14</td><td style=\"text-align: right;\">                   2</td><td>4d8df89d  </td><td style=\"text-align: right;\">     2.04815 </td><td style=\"text-align: right;\">     0.392256</td></tr>\n",
       "<tr><td>FSR_Trainable_4f5ed776</td><td>2023-08-11_01-10-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   27.6653 </td><td style=\"text-align: right;\">   2399.13 </td><td style=\"text-align: right;\">    4.28174 </td><td style=\"text-align: right;\"> 2.76088e+18</td><td style=\"text-align: right;\"> 2.49734 </td><td>172.26.215.93</td><td style=\"text-align: right;\">201182</td><td style=\"text-align: right;\">     8.7146 </td><td style=\"text-align: right;\">    1760.15 </td><td style=\"text-align: right;\">            5.65688 </td><td style=\"text-align: right;\">          5.65688 </td><td style=\"text-align: right;\">      5.65688 </td><td style=\"text-align: right;\"> 1691683842</td><td style=\"text-align: right;\">     6.40817</td><td style=\"text-align: right;\">    0.788755</td><td style=\"text-align: right;\">  2.55813e+13</td><td style=\"text-align: right;\">  1.12709e+15</td><td style=\"text-align: right;\">                   1</td><td>4f5ed776  </td><td style=\"text-align: right;\">     2.01264 </td><td style=\"text-align: right;\">     0.484698</td></tr>\n",
       "<tr><td>FSR_Trainable_573038ac</td><td>2023-08-11_00-52-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">    4.68979</td><td style=\"text-align: right;\">   1037.16 </td><td style=\"text-align: right;\">    1.24008 </td><td style=\"text-align: right;\"> 1.78677e+18</td><td style=\"text-align: right;\"> 0.745481</td><td>172.26.215.93</td><td style=\"text-align: right;\">190101</td><td style=\"text-align: right;\">     2.46909</td><td style=\"text-align: right;\">     718.452</td><td style=\"text-align: right;\">          151.593   </td><td style=\"text-align: right;\">          4.32423 </td><td style=\"text-align: right;\">    151.593   </td><td style=\"text-align: right;\"> 1691682720</td><td style=\"text-align: right;\">     1.02677</td><td style=\"text-align: right;\">    0.387594</td><td style=\"text-align: right;\">  8.10199e+13</td><td style=\"text-align: right;\">  7.09594e+14</td><td style=\"text-align: right;\">                  32</td><td>573038ac  </td><td style=\"text-align: right;\">     0.503986</td><td style=\"text-align: right;\">     0.241495</td></tr>\n",
       "<tr><td>FSR_Trainable_580494a3</td><td>2023-08-11_00-49-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">    4.67138</td><td style=\"text-align: right;\">   1361.65 </td><td style=\"text-align: right;\">    1.21916 </td><td style=\"text-align: right;\"> 2.68363e+18</td><td style=\"text-align: right;\"> 0.800991</td><td>172.26.215.93</td><td style=\"text-align: right;\">189559</td><td style=\"text-align: right;\">     2.49582</td><td style=\"text-align: right;\">     898.278</td><td style=\"text-align: right;\">           74.5158  </td><td style=\"text-align: right;\">          4.46208 </td><td style=\"text-align: right;\">     74.5158  </td><td style=\"text-align: right;\"> 1691682542</td><td style=\"text-align: right;\">     1.01725</td><td style=\"text-align: right;\">    0.493219</td><td style=\"text-align: right;\">  8.34627e+13</td><td style=\"text-align: right;\">  1.03986e+15</td><td style=\"text-align: right;\">                  16</td><td>580494a3  </td><td style=\"text-align: right;\">     0.511661</td><td style=\"text-align: right;\">     0.28933 </td></tr>\n",
       "<tr><td>FSR_Trainable_5a8ffac3</td><td>2023-08-11_01-07-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">    6.04044</td><td style=\"text-align: right;\">    810.799</td><td style=\"text-align: right;\">    1.3371  </td><td style=\"text-align: right;\"> 1.28777e+18</td><td style=\"text-align: right;\"> 0.753029</td><td>172.26.215.93</td><td style=\"text-align: right;\">197906</td><td style=\"text-align: right;\">     2.78897</td><td style=\"text-align: right;\">     518.76 </td><td style=\"text-align: right;\">           29.8279  </td><td style=\"text-align: right;\">          1.24428 </td><td style=\"text-align: right;\">     29.8279  </td><td style=\"text-align: right;\"> 1691683636</td><td style=\"text-align: right;\">     1.26682</td><td style=\"text-align: right;\">    0.359596</td><td style=\"text-align: right;\">  7.63769e+13</td><td style=\"text-align: right;\">  7.00708e+14</td><td style=\"text-align: right;\">                  32</td><td>5a8ffac3  </td><td style=\"text-align: right;\">     0.54448 </td><td style=\"text-align: right;\">     0.208549</td></tr>\n",
       "<tr><td>FSR_Trainable_5aadf73d</td><td>2023-08-11_01-06-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   17.7585 </td><td style=\"text-align: right;\">   1738.3  </td><td style=\"text-align: right;\">    2.75363 </td><td style=\"text-align: right;\"> 2.73615e+18</td><td style=\"text-align: right;\"> 1.68468 </td><td>172.26.215.93</td><td style=\"text-align: right;\">197697</td><td style=\"text-align: right;\">     6.10234</td><td style=\"text-align: right;\">    1167.93 </td><td style=\"text-align: right;\">            1.66906 </td><td style=\"text-align: right;\">          1.66906 </td><td style=\"text-align: right;\">      1.66906 </td><td style=\"text-align: right;\"> 1691683582</td><td style=\"text-align: right;\">     3.86006</td><td style=\"text-align: right;\">    0.710857</td><td style=\"text-align: right;\">  2.98195e+13</td><td style=\"text-align: right;\">  1.4639e+15 </td><td style=\"text-align: right;\">                   1</td><td>5aadf73d  </td><td style=\"text-align: right;\">     1.26664 </td><td style=\"text-align: right;\">     0.418041</td></tr>\n",
       "<tr><td>FSR_Trainable_5ce6d566</td><td>2023-08-11_01-07-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    5.15352</td><td style=\"text-align: right;\">    647.313</td><td style=\"text-align: right;\">    1.19093 </td><td style=\"text-align: right;\"> 7.31065e+17</td><td style=\"text-align: right;\"> 0.663459</td><td>172.26.215.93</td><td style=\"text-align: right;\">197190</td><td style=\"text-align: right;\">     2.55623</td><td style=\"text-align: right;\">     473.545</td><td style=\"text-align: right;\">           91.7337  </td><td style=\"text-align: right;\">          0.885232</td><td style=\"text-align: right;\">     91.7337  </td><td style=\"text-align: right;\"> 1691683641</td><td style=\"text-align: right;\">     1.07891</td><td style=\"text-align: right;\">    0.248612</td><td style=\"text-align: right;\">  7.55674e+13</td><td style=\"text-align: right;\">  3.35444e+14</td><td style=\"text-align: right;\">                 100</td><td>5ce6d566  </td><td style=\"text-align: right;\">     0.497569</td><td style=\"text-align: right;\">     0.16589 </td></tr>\n",
       "<tr><td>FSR_Trainable_611b154e</td><td>2023-08-11_00-42-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">    6.12881</td><td style=\"text-align: right;\">   1026.64 </td><td style=\"text-align: right;\">    1.34483 </td><td style=\"text-align: right;\"> 1.31681e+18</td><td style=\"text-align: right;\"> 0.820329</td><td>172.26.215.93</td><td style=\"text-align: right;\">184387</td><td style=\"text-align: right;\">     2.72447</td><td style=\"text-align: right;\">     696.089</td><td style=\"text-align: right;\">           21.6874  </td><td style=\"text-align: right;\">          1.14909 </td><td style=\"text-align: right;\">     21.6874  </td><td style=\"text-align: right;\"> 1691682167</td><td style=\"text-align: right;\">     1.3152 </td><td style=\"text-align: right;\">    0.442225</td><td style=\"text-align: right;\">  7.61004e+13</td><td style=\"text-align: right;\">  7.29168e+14</td><td style=\"text-align: right;\">                  16</td><td>611b154e  </td><td style=\"text-align: right;\">     0.549668</td><td style=\"text-align: right;\">     0.270661</td></tr>\n",
       "<tr><td>FSR_Trainable_65119616</td><td>2023-08-11_01-12-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    5.45673</td><td style=\"text-align: right;\">    648.159</td><td style=\"text-align: right;\">    1.24063 </td><td style=\"text-align: right;\"> 7.01457e+17</td><td style=\"text-align: right;\"> 0.704512</td><td>172.26.215.93</td><td style=\"text-align: right;\">199929</td><td style=\"text-align: right;\">     2.73209</td><td style=\"text-align: right;\">     484.052</td><td style=\"text-align: right;\">          171.944   </td><td style=\"text-align: right;\">          1.97425 </td><td style=\"text-align: right;\">    171.944   </td><td style=\"text-align: right;\"> 1691683925</td><td style=\"text-align: right;\">     1.13576</td><td style=\"text-align: right;\">    0.253109</td><td style=\"text-align: right;\">  7.51836e+13</td><td style=\"text-align: right;\">  3.22586e+14</td><td style=\"text-align: right;\">                 100</td><td>65119616  </td><td style=\"text-align: right;\">     0.526939</td><td style=\"text-align: right;\">     0.177573</td></tr>\n",
       "<tr><td>FSR_Trainable_683aa5cb</td><td>2023-08-11_01-05-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.87937</td><td style=\"text-align: right;\">    688.209</td><td style=\"text-align: right;\">    1.12172 </td><td style=\"text-align: right;\"> 9.41745e+17</td><td style=\"text-align: right;\"> 0.658378</td><td>172.26.215.93</td><td style=\"text-align: right;\">196393</td><td style=\"text-align: right;\">     2.47244</td><td style=\"text-align: right;\">     462.14 </td><td style=\"text-align: right;\">          101.473   </td><td style=\"text-align: right;\">          0.95716 </td><td style=\"text-align: right;\">    101.473   </td><td style=\"text-align: right;\"> 1691683537</td><td style=\"text-align: right;\">     1.0346 </td><td style=\"text-align: right;\">    0.273943</td><td style=\"text-align: right;\">  7.5565e+13 </td><td style=\"text-align: right;\">  4.37289e+14</td><td style=\"text-align: right;\">                 100</td><td>683aa5cb  </td><td style=\"text-align: right;\">     0.488776</td><td style=\"text-align: right;\">     0.169602</td></tr>\n",
       "<tr><td>FSR_Trainable_739c4bd5</td><td>2023-08-11_01-08-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">    5.8043 </td><td style=\"text-align: right;\">    704.02 </td><td style=\"text-align: right;\">    1.24093 </td><td style=\"text-align: right;\"> 9.42846e+17</td><td style=\"text-align: right;\"> 0.704296</td><td>172.26.215.93</td><td style=\"text-align: right;\">198882</td><td style=\"text-align: right;\">     2.73028</td><td style=\"text-align: right;\">     486.672</td><td style=\"text-align: right;\">           59.8378  </td><td style=\"text-align: right;\">          1.54562 </td><td style=\"text-align: right;\">     59.8378  </td><td style=\"text-align: right;\"> 1691683732</td><td style=\"text-align: right;\">     1.20357</td><td style=\"text-align: right;\">    0.281742</td><td style=\"text-align: right;\">  7.61052e+13</td><td style=\"text-align: right;\">  4.43164e+14</td><td style=\"text-align: right;\">                  64</td><td>739c4bd5  </td><td style=\"text-align: right;\">     0.52612 </td><td style=\"text-align: right;\">     0.178176</td></tr>\n",
       "<tr><td>FSR_Trainable_75852e34</td><td>2023-08-11_00-59-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   23.9341 </td><td style=\"text-align: right;\">   4668.96 </td><td style=\"text-align: right;\">    3.82597 </td><td style=\"text-align: right;\"> 8.3017e+18 </td><td style=\"text-align: right;\"> 2.5763  </td><td>172.26.215.93</td><td style=\"text-align: right;\">193706</td><td style=\"text-align: right;\">     7.6704 </td><td style=\"text-align: right;\">    2571.29 </td><td style=\"text-align: right;\">            1.29663 </td><td style=\"text-align: right;\">          1.29663 </td><td style=\"text-align: right;\">      1.29663 </td><td style=\"text-align: right;\"> 1691683183</td><td style=\"text-align: right;\">     5.39151</td><td style=\"text-align: right;\">    1.83708 </td><td style=\"text-align: right;\">  3.48663e+13</td><td style=\"text-align: right;\">  3.96962e+15</td><td style=\"text-align: right;\">                   1</td><td>75852e34  </td><td style=\"text-align: right;\">     1.71047 </td><td style=\"text-align: right;\">     0.865826</td></tr>\n",
       "<tr><td>FSR_Trainable_75e926e4</td><td>2023-08-11_00-44-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    4.44215</td><td style=\"text-align: right;\">   1313.93 </td><td style=\"text-align: right;\">    1.08549 </td><td style=\"text-align: right;\"> 2.33832e+17</td><td style=\"text-align: right;\">54.7292  </td><td>172.26.215.93</td><td style=\"text-align: right;\">186482</td><td style=\"text-align: right;\">     2.50879</td><td style=\"text-align: right;\">     942.503</td><td style=\"text-align: right;\">            2.27228 </td><td style=\"text-align: right;\">          2.27228 </td><td style=\"text-align: right;\">      2.27228 </td><td style=\"text-align: right;\"> 1691682276</td><td style=\"text-align: right;\">    44.4576 </td><td style=\"text-align: right;\">    7.35641 </td><td style=\"text-align: right;\">  4.95265e+15</td><td style=\"text-align: right;\">  7.87585e+15</td><td style=\"text-align: right;\">                   1</td><td>75e926e4  </td><td style=\"text-align: right;\">    47.9345  </td><td style=\"text-align: right;\">     6.79468 </td></tr>\n",
       "<tr><td>FSR_Trainable_770d6969</td><td>2023-08-11_01-01-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    5.51642</td><td style=\"text-align: right;\">   1304.52 </td><td style=\"text-align: right;\">    1.56192 </td><td style=\"text-align: right;\"> 2.27886e+18</td><td style=\"text-align: right;\"> 0.836866</td><td>172.26.215.93</td><td style=\"text-align: right;\">193944</td><td style=\"text-align: right;\">     2.57792</td><td style=\"text-align: right;\">     964.857</td><td style=\"text-align: right;\">          106.305   </td><td style=\"text-align: right;\">         10.7448  </td><td style=\"text-align: right;\">    106.305   </td><td style=\"text-align: right;\"> 1691683319</td><td style=\"text-align: right;\">     1.18219</td><td style=\"text-align: right;\">    0.449443</td><td style=\"text-align: right;\">  9.96096e+13</td><td style=\"text-align: right;\">  7.6398e+14 </td><td style=\"text-align: right;\">                   8</td><td>770d6969  </td><td style=\"text-align: right;\">     0.526353</td><td style=\"text-align: right;\">     0.310513</td></tr>\n",
       "<tr><td>FSR_Trainable_7d5d0f23</td><td>2023-08-11_01-00-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    5.51941</td><td style=\"text-align: right;\">   1292.95 </td><td style=\"text-align: right;\">    1.57133 </td><td style=\"text-align: right;\"> 2.27592e+18</td><td style=\"text-align: right;\"> 0.837084</td><td>172.26.215.93</td><td style=\"text-align: right;\">192745</td><td style=\"text-align: right;\">     2.57452</td><td style=\"text-align: right;\">     968.003</td><td style=\"text-align: right;\">          148.677   </td><td style=\"text-align: right;\">         18.7124  </td><td style=\"text-align: right;\">    148.677   </td><td style=\"text-align: right;\"> 1691683224</td><td style=\"text-align: right;\">     1.17993</td><td style=\"text-align: right;\">    0.443113</td><td style=\"text-align: right;\">  9.55485e+13</td><td style=\"text-align: right;\">  7.5306e+14 </td><td style=\"text-align: right;\">                   8</td><td>7d5d0f23  </td><td style=\"text-align: right;\">     0.528004</td><td style=\"text-align: right;\">     0.30908 </td></tr>\n",
       "<tr><td>FSR_Trainable_80318608</td><td>2023-08-11_00-47-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">    6.70781</td><td style=\"text-align: right;\">   1159.68 </td><td style=\"text-align: right;\">    1.40529 </td><td style=\"text-align: right;\"> 1.42254e+18</td><td style=\"text-align: right;\"> 0.89507 </td><td>172.26.215.93</td><td style=\"text-align: right;\">188109</td><td style=\"text-align: right;\">     2.87786</td><td style=\"text-align: right;\">     840.926</td><td style=\"text-align: right;\">           56.5303  </td><td style=\"text-align: right;\">          3.24482 </td><td style=\"text-align: right;\">     56.5303  </td><td style=\"text-align: right;\"> 1691682445</td><td style=\"text-align: right;\">     1.44017</td><td style=\"text-align: right;\">    0.479076</td><td style=\"text-align: right;\">  7.84726e+13</td><td style=\"text-align: right;\">  7.4758e+14 </td><td style=\"text-align: right;\">                  16</td><td>80318608  </td><td style=\"text-align: right;\">     0.581537</td><td style=\"text-align: right;\">     0.313532</td></tr>\n",
       "<tr><td>FSR_Trainable_815ec5f7</td><td>2023-08-11_01-12-34</td><td>False </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        70</td><td style=\"text-align: right;\">    5.3548 </td><td style=\"text-align: right;\">    660.871</td><td style=\"text-align: right;\">    1.19525 </td><td style=\"text-align: right;\"> 9.45309e+17</td><td style=\"text-align: right;\"> 0.671363</td><td>172.26.215.93</td><td style=\"text-align: right;\">201433</td><td style=\"text-align: right;\">     2.58539</td><td style=\"text-align: right;\">     447.079</td><td style=\"text-align: right;\">           80.5191  </td><td style=\"text-align: right;\">          1.24249 </td><td style=\"text-align: right;\">     80.5191  </td><td style=\"text-align: right;\"> 1691683954</td><td style=\"text-align: right;\">     1.11851</td><td style=\"text-align: right;\">    0.276595</td><td style=\"text-align: right;\">  7.47044e+13</td><td style=\"text-align: right;\">  4.90827e+14</td><td style=\"text-align: right;\">                  70</td><td>815ec5f7  </td><td style=\"text-align: right;\">     0.50278 </td><td style=\"text-align: right;\">     0.168583</td></tr>\n",
       "<tr><td>FSR_Trainable_8198c6d1</td><td>2023-08-11_01-01-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    5.65497</td><td style=\"text-align: right;\">   1067.53 </td><td style=\"text-align: right;\">    1.50747 </td><td style=\"text-align: right;\"> 4.96847e+08</td><td style=\"text-align: right;\"> 7.27306 </td><td>172.26.215.93</td><td style=\"text-align: right;\">194402</td><td style=\"text-align: right;\">     2.8025 </td><td style=\"text-align: right;\">     796.867</td><td style=\"text-align: right;\">           16.6283  </td><td style=\"text-align: right;\">         16.6283  </td><td style=\"text-align: right;\">     16.6283  </td><td style=\"text-align: right;\"> 1691683262</td><td style=\"text-align: right;\">    10.0441 </td><td style=\"text-align: right;\">    3.06129 </td><td style=\"text-align: right;\">  3.95225e+15</td><td style=\"text-align: right;\">240.3        </td><td style=\"text-align: right;\">                   1</td><td>8198c6d1  </td><td style=\"text-align: right;\">     5.07031 </td><td style=\"text-align: right;\">     2.20275 </td></tr>\n",
       "<tr><td>FSR_Trainable_839c76a6</td><td>2023-08-11_01-01-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    5.18261</td><td style=\"text-align: right;\">    641.094</td><td style=\"text-align: right;\">    1.17282 </td><td style=\"text-align: right;\"> 6.57276e+17</td><td style=\"text-align: right;\"> 0.668627</td><td>172.26.215.93</td><td style=\"text-align: right;\">193278</td><td style=\"text-align: right;\">     2.57804</td><td style=\"text-align: right;\">     491.49 </td><td style=\"text-align: right;\">          116.86    </td><td style=\"text-align: right;\">          1.09081 </td><td style=\"text-align: right;\">    116.86    </td><td style=\"text-align: right;\"> 1691683290</td><td style=\"text-align: right;\">     1.08474</td><td style=\"text-align: right;\">    0.245539</td><td style=\"text-align: right;\">  7.35831e+13</td><td style=\"text-align: right;\">  3.10088e+14</td><td style=\"text-align: right;\">                 100</td><td>839c76a6  </td><td style=\"text-align: right;\">     0.499586</td><td style=\"text-align: right;\">     0.169041</td></tr>\n",
       "<tr><td>FSR_Trainable_863ef801</td><td>2023-08-11_00-41-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">   23.6067 </td><td style=\"text-align: right;\">   2976.88 </td><td style=\"text-align: right;\">    3.43213 </td><td style=\"text-align: right;\"> 4.32633e+18</td><td style=\"text-align: right;\"> 2.30881 </td><td>172.26.215.93</td><td style=\"text-align: right;\">183703</td><td style=\"text-align: right;\">     7.6966 </td><td style=\"text-align: right;\">    1915.66 </td><td style=\"text-align: right;\">            3.11362 </td><td style=\"text-align: right;\">          0.672511</td><td style=\"text-align: right;\">      3.11362 </td><td style=\"text-align: right;\"> 1691682105</td><td style=\"text-align: right;\">     5.33934</td><td style=\"text-align: right;\">    1.06633 </td><td style=\"text-align: right;\">  3.2615e+13 </td><td style=\"text-align: right;\">  1.94872e+15</td><td style=\"text-align: right;\">                   4</td><td>863ef801  </td><td style=\"text-align: right;\">     1.74002 </td><td style=\"text-align: right;\">     0.568791</td></tr>\n",
       "<tr><td>FSR_Trainable_86fbc03e</td><td>2023-08-11_01-03-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    5.2328 </td><td style=\"text-align: right;\">    649.04 </td><td style=\"text-align: right;\">    1.20949 </td><td style=\"text-align: right;\"> 7.65052e+17</td><td style=\"text-align: right;\"> 0.670563</td><td>172.26.215.93</td><td style=\"text-align: right;\">195592</td><td style=\"text-align: right;\">     2.55412</td><td style=\"text-align: right;\">     469.219</td><td style=\"text-align: right;\">           90.6322  </td><td style=\"text-align: right;\">          0.975868</td><td style=\"text-align: right;\">     90.6322  </td><td style=\"text-align: right;\"> 1691683435</td><td style=\"text-align: right;\">     1.10206</td><td style=\"text-align: right;\">    0.26526 </td><td style=\"text-align: right;\">  7.68585e+13</td><td style=\"text-align: right;\">  3.94009e+14</td><td style=\"text-align: right;\">                 100</td><td>86fbc03e  </td><td style=\"text-align: right;\">     0.499155</td><td style=\"text-align: right;\">     0.171408</td></tr>\n",
       "<tr><td>FSR_Trainable_8c57c14c</td><td>2023-08-11_00-46-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    5.42776</td><td style=\"text-align: right;\">    789.541</td><td style=\"text-align: right;\">    1.1884  </td><td style=\"text-align: right;\"> 9.51678e+17</td><td style=\"text-align: right;\"> 0.716643</td><td>172.26.215.93</td><td style=\"text-align: right;\">185813</td><td style=\"text-align: right;\">     2.68192</td><td style=\"text-align: right;\">     562.329</td><td style=\"text-align: right;\">          106.805   </td><td style=\"text-align: right;\">          1.35465 </td><td style=\"text-align: right;\">    106.805   </td><td style=\"text-align: right;\"> 1691682361</td><td style=\"text-align: right;\">     1.14774</td><td style=\"text-align: right;\">    0.312781</td><td style=\"text-align: right;\">  7.1437e+13 </td><td style=\"text-align: right;\">  4.62638e+14</td><td style=\"text-align: right;\">                 100</td><td>8c57c14c  </td><td style=\"text-align: right;\">     0.52035 </td><td style=\"text-align: right;\">     0.196293</td></tr>\n",
       "<tr><td>FSR_Trainable_8d1c2f35</td><td>2023-08-11_00-50-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    3.85052</td><td style=\"text-align: right;\">   1030.46 </td><td style=\"text-align: right;\">    1.16629 </td><td style=\"text-align: right;\"> 6.33308e+16</td><td style=\"text-align: right;\">39.724   </td><td>172.26.215.93</td><td style=\"text-align: right;\">190394</td><td style=\"text-align: right;\">     2.36374</td><td style=\"text-align: right;\">     905.38 </td><td style=\"text-align: right;\">            5.52543 </td><td style=\"text-align: right;\">          5.52543 </td><td style=\"text-align: right;\">      5.52543 </td><td style=\"text-align: right;\"> 1691682629</td><td style=\"text-align: right;\">    27.8917 </td><td style=\"text-align: right;\">    5.5582  </td><td style=\"text-align: right;\">  1.8406e+15 </td><td style=\"text-align: right;\">  2.32134e+15</td><td style=\"text-align: right;\">                   1</td><td>8d1c2f35  </td><td style=\"text-align: right;\">    33.254   </td><td style=\"text-align: right;\">     6.47002 </td></tr>\n",
       "<tr><td>FSR_Trainable_90abffb2</td><td>2023-08-11_00-45-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">    6.09887</td><td style=\"text-align: right;\">   1027.33 </td><td style=\"text-align: right;\">    1.29335 </td><td style=\"text-align: right;\"> 1.56634e+18</td><td style=\"text-align: right;\"> 0.812884</td><td>172.26.215.93</td><td style=\"text-align: right;\">187175</td><td style=\"text-align: right;\">     2.72564</td><td style=\"text-align: right;\">     671.528</td><td style=\"text-align: right;\">           19.5351  </td><td style=\"text-align: right;\">          1.19951 </td><td style=\"text-align: right;\">     19.5351  </td><td style=\"text-align: right;\"> 1691682339</td><td style=\"text-align: right;\">     1.32776</td><td style=\"text-align: right;\">    0.436289</td><td style=\"text-align: right;\">  6.71191e+13</td><td style=\"text-align: right;\">  7.75968e+14</td><td style=\"text-align: right;\">                  16</td><td>90abffb2  </td><td style=\"text-align: right;\">     0.546151</td><td style=\"text-align: right;\">     0.266733</td></tr>\n",
       "<tr><td>FSR_Trainable_9a152f5f</td><td>2023-08-11_01-12-31</td><td>False </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    7.32607</td><td style=\"text-align: right;\">   1586.36 </td><td style=\"text-align: right;\">    1.64273 </td><td style=\"text-align: right;\"> 3.4416e+18 </td><td style=\"text-align: right;\"> 0.999315</td><td>172.26.215.93</td><td style=\"text-align: right;\">202290</td><td style=\"text-align: right;\">     3.01592</td><td style=\"text-align: right;\">     986.152</td><td style=\"text-align: right;\">            2.65613 </td><td style=\"text-align: right;\">          2.65613 </td><td style=\"text-align: right;\">      2.65613 </td><td style=\"text-align: right;\"> 1691683951</td><td style=\"text-align: right;\">     1.68902</td><td style=\"text-align: right;\">    0.61006 </td><td style=\"text-align: right;\">  9.78014e+13</td><td style=\"text-align: right;\">  1.46976e+15</td><td style=\"text-align: right;\">                   1</td><td>9a152f5f  </td><td style=\"text-align: right;\">     0.670548</td><td style=\"text-align: right;\">     0.328768</td></tr>\n",
       "<tr><td>FSR_Trainable_a152f03f</td><td>2023-08-11_01-08-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">    5.50745</td><td style=\"text-align: right;\">    782.713</td><td style=\"text-align: right;\">    1.23523 </td><td style=\"text-align: right;\"> 1.30637e+18</td><td style=\"text-align: right;\"> 0.717212</td><td>172.26.215.93</td><td style=\"text-align: right;\">198965</td><td style=\"text-align: right;\">     2.61629</td><td style=\"text-align: right;\">     477.63 </td><td style=\"text-align: right;\">           29.3188  </td><td style=\"text-align: right;\">          0.832298</td><td style=\"text-align: right;\">     29.3188  </td><td style=\"text-align: right;\"> 1691683703</td><td style=\"text-align: right;\">     1.16595</td><td style=\"text-align: right;\">    0.357425</td><td style=\"text-align: right;\">  8.01481e+13</td><td style=\"text-align: right;\">  7.3443e+14 </td><td style=\"text-align: right;\">                  32</td><td>a152f03f  </td><td style=\"text-align: right;\">     0.515521</td><td style=\"text-align: right;\">     0.201691</td></tr>\n",
       "<tr><td>FSR_Trainable_a9e43461</td><td>2023-08-11_00-43-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">    6.67276</td><td style=\"text-align: right;\">   1052.24 </td><td style=\"text-align: right;\">    1.56047 </td><td style=\"text-align: right;\"> 1.42675e+18</td><td style=\"text-align: right;\"> 0.84314 </td><td>172.26.215.93</td><td style=\"text-align: right;\">184630</td><td style=\"text-align: right;\">     2.82578</td><td style=\"text-align: right;\">     688.471</td><td style=\"text-align: right;\">           22.3069  </td><td style=\"text-align: right;\">          1.26604 </td><td style=\"text-align: right;\">     22.3069  </td><td style=\"text-align: right;\"> 1691682181</td><td style=\"text-align: right;\">     1.41006</td><td style=\"text-align: right;\">    0.457903</td><td style=\"text-align: right;\">  7.66688e+13</td><td style=\"text-align: right;\">  7.68211e+14</td><td style=\"text-align: right;\">                  16</td><td>a9e43461  </td><td style=\"text-align: right;\">     0.565528</td><td style=\"text-align: right;\">     0.277612</td></tr>\n",
       "<tr><td>FSR_Trainable_ad00c085</td><td>2023-08-11_01-07-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    4.10781</td><td style=\"text-align: right;\">    814.642</td><td style=\"text-align: right;\">    1.1379  </td><td style=\"text-align: right;\"> 3.10034e+08</td><td style=\"text-align: right;\"> 5.7562  </td><td>172.26.215.93</td><td style=\"text-align: right;\">198383</td><td style=\"text-align: right;\">     2.30694</td><td style=\"text-align: right;\">     582.754</td><td style=\"text-align: right;\">            1.19177 </td><td style=\"text-align: right;\">          1.19177 </td><td style=\"text-align: right;\">      1.19177 </td><td style=\"text-align: right;\"> 1691683632</td><td style=\"text-align: right;\">     7.66433</td><td style=\"text-align: right;\">    2.0798  </td><td style=\"text-align: right;\">  1.46936e+15</td><td style=\"text-align: right;\">114.954      </td><td style=\"text-align: right;\">                   1</td><td>ad00c085  </td><td style=\"text-align: right;\">     4.34961 </td><td style=\"text-align: right;\">     1.40659 </td></tr>\n",
       "<tr><td>FSR_Trainable_ad7a4599</td><td>2023-08-11_00-46-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    9.42992</td><td style=\"text-align: right;\">   1348.52 </td><td style=\"text-align: right;\">    1.7224  </td><td style=\"text-align: right;\"> 2.40924e+18</td><td style=\"text-align: right;\"> 1.0975  </td><td>172.26.215.93</td><td style=\"text-align: right;\">188323</td><td style=\"text-align: right;\">     3.83838</td><td style=\"text-align: right;\">     882.042</td><td style=\"text-align: right;\">           16.2668  </td><td style=\"text-align: right;\">          4.17775 </td><td style=\"text-align: right;\">     16.2668  </td><td style=\"text-align: right;\"> 1691682416</td><td style=\"text-align: right;\">     2.0284 </td><td style=\"text-align: right;\">    0.529038</td><td style=\"text-align: right;\">  7.18666e+13</td><td style=\"text-align: right;\">  1.12949e+15</td><td style=\"text-align: right;\">                   4</td><td>ad7a4599  </td><td style=\"text-align: right;\">     0.796525</td><td style=\"text-align: right;\">     0.300977</td></tr>\n",
       "<tr><td>FSR_Trainable_af4ab174</td><td>2023-08-11_00-45-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   25.0304 </td><td style=\"text-align: right;\">   2995.78 </td><td style=\"text-align: right;\">    3.90709 </td><td style=\"text-align: right;\"> 4.48397e+18</td><td style=\"text-align: right;\"> 2.37176 </td><td>172.26.215.93</td><td style=\"text-align: right;\">186930</td><td style=\"text-align: right;\">     8.846  </td><td style=\"text-align: right;\">    1836.81 </td><td style=\"text-align: right;\">            1.55204 </td><td style=\"text-align: right;\">          1.55204 </td><td style=\"text-align: right;\">      1.55204 </td><td style=\"text-align: right;\"> 1691682302</td><td style=\"text-align: right;\">     5.36967</td><td style=\"text-align: right;\">    1.11243 </td><td style=\"text-align: right;\">  7.87725e+13</td><td style=\"text-align: right;\">  1.89249e+15</td><td style=\"text-align: right;\">                   1</td><td>af4ab174  </td><td style=\"text-align: right;\">     1.75389 </td><td style=\"text-align: right;\">     0.617872</td></tr>\n",
       "<tr><td>FSR_Trainable_b1203ce7</td><td>2023-08-11_00-43-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    9.11387</td><td style=\"text-align: right;\">   1285.15 </td><td style=\"text-align: right;\">    2.32794 </td><td style=\"text-align: right;\"> 2.77977e+18</td><td style=\"text-align: right;\"> 0.996561</td><td>172.26.215.93</td><td style=\"text-align: right;\">185120</td><td style=\"text-align: right;\">     3.56882</td><td style=\"text-align: right;\">     849.342</td><td style=\"text-align: right;\">            5.36449 </td><td style=\"text-align: right;\">          1.38057 </td><td style=\"text-align: right;\">      5.36449 </td><td style=\"text-align: right;\"> 1691682204</td><td style=\"text-align: right;\">     1.8746 </td><td style=\"text-align: right;\">    0.505987</td><td style=\"text-align: right;\">  7.73751e+13</td><td style=\"text-align: right;\">  1.18024e+15</td><td style=\"text-align: right;\">                   4</td><td>b1203ce7  </td><td style=\"text-align: right;\">     0.70344 </td><td style=\"text-align: right;\">     0.293121</td></tr>\n",
       "<tr><td>FSR_Trainable_b1d49451</td><td>2023-08-11_00-58-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    5.21524</td><td style=\"text-align: right;\">   1312.36 </td><td style=\"text-align: right;\">    1.39514 </td><td style=\"text-align: right;\"> 2.29721e+18</td><td style=\"text-align: right;\"> 0.828825</td><td>172.26.215.93</td><td style=\"text-align: right;\">192951</td><td style=\"text-align: right;\">     2.51204</td><td style=\"text-align: right;\">     974.261</td><td style=\"text-align: right;\">           33.086   </td><td style=\"text-align: right;\">          3.94714 </td><td style=\"text-align: right;\">     33.086   </td><td style=\"text-align: right;\"> 1691683126</td><td style=\"text-align: right;\">     1.13646</td><td style=\"text-align: right;\">    0.465332</td><td style=\"text-align: right;\">  9.23396e+13</td><td style=\"text-align: right;\">  8.37049e+14</td><td style=\"text-align: right;\">                   8</td><td>b1d49451  </td><td style=\"text-align: right;\">     0.51613 </td><td style=\"text-align: right;\">     0.312695</td></tr>\n",
       "<tr><td>FSR_Trainable_b47983cf</td><td>2023-08-11_00-40-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">   28.7994 </td><td style=\"text-align: right;\">   1624.19 </td><td style=\"text-align: right;\">    4.4975  </td><td style=\"text-align: right;\"> 2.61416e+18</td><td style=\"text-align: right;\"> 2.37971 </td><td>172.26.215.93</td><td style=\"text-align: right;\">182329</td><td style=\"text-align: right;\">     9.12522</td><td style=\"text-align: right;\">    1099.26 </td><td style=\"text-align: right;\">            8.89746 </td><td style=\"text-align: right;\">          2.07106 </td><td style=\"text-align: right;\">      8.89746 </td><td style=\"text-align: right;\"> 1691682039</td><td style=\"text-align: right;\">     6.51122</td><td style=\"text-align: right;\">    0.622707</td><td style=\"text-align: right;\">  1.51647e+13</td><td style=\"text-align: right;\">  1.24973e+15</td><td style=\"text-align: right;\">                   4</td><td>b47983cf  </td><td style=\"text-align: right;\">     2.03318 </td><td style=\"text-align: right;\">     0.346531</td></tr>\n",
       "<tr><td>FSR_Trainable_bb462b5a</td><td>2023-08-11_00-43-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    4.20241</td><td style=\"text-align: right;\">   1114.61 </td><td style=\"text-align: right;\">    0.996784</td><td style=\"text-align: right;\"> 5.77923e+08</td><td style=\"text-align: right;\"> 6.17791 </td><td>172.26.215.93</td><td style=\"text-align: right;\">185581</td><td style=\"text-align: right;\">     2.49016</td><td style=\"text-align: right;\">     727.895</td><td style=\"text-align: right;\">            0.971077</td><td style=\"text-align: right;\">          0.971077</td><td style=\"text-align: right;\">      0.971077</td><td style=\"text-align: right;\"> 1691682221</td><td style=\"text-align: right;\">     7.81838</td><td style=\"text-align: right;\">    2.76831 </td><td style=\"text-align: right;\">202.084      </td><td style=\"text-align: right;\"> 55.3187     </td><td style=\"text-align: right;\">                   1</td><td>bb462b5a  </td><td style=\"text-align: right;\">     4.48385 </td><td style=\"text-align: right;\">     1.69407 </td></tr>\n",
       "<tr><td>FSR_Trainable_bbb73642</td><td>2023-08-11_00-53-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    5.01754</td><td style=\"text-align: right;\">   1426.73 </td><td style=\"text-align: right;\">    1.2501  </td><td style=\"text-align: right;\"> 2.82847e+18</td><td style=\"text-align: right;\"> 0.830535</td><td>172.26.215.93</td><td style=\"text-align: right;\">191137</td><td style=\"text-align: right;\">     2.55998</td><td style=\"text-align: right;\">     936.803</td><td style=\"text-align: right;\">           54.8615  </td><td style=\"text-align: right;\">          6.27369 </td><td style=\"text-align: right;\">     54.8615  </td><td style=\"text-align: right;\"> 1691682793</td><td style=\"text-align: right;\">     1.09739</td><td style=\"text-align: right;\">    0.517691</td><td style=\"text-align: right;\">  8.26005e+13</td><td style=\"text-align: right;\">  1.09971e+15</td><td style=\"text-align: right;\">                   8</td><td>bbb73642  </td><td style=\"text-align: right;\">     0.527526</td><td style=\"text-align: right;\">     0.303009</td></tr>\n",
       "<tr><td>FSR_Trainable_c4772750</td><td>2023-08-11_00-43-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    6.55975</td><td style=\"text-align: right;\">   1305.87 </td><td style=\"text-align: right;\">    1.4701  </td><td style=\"text-align: right;\"> 2.20747e+18</td><td style=\"text-align: right;\"> 0.86286 </td><td>172.26.215.93</td><td style=\"text-align: right;\">184905</td><td style=\"text-align: right;\">     2.80112</td><td style=\"text-align: right;\">     809.938</td><td style=\"text-align: right;\">            9.87519 </td><td style=\"text-align: right;\">          1.11771 </td><td style=\"text-align: right;\">      9.87519 </td><td style=\"text-align: right;\"> 1691682194</td><td style=\"text-align: right;\">     1.41136</td><td style=\"text-align: right;\">    0.529779</td><td style=\"text-align: right;\">  7.75855e+13</td><td style=\"text-align: right;\">  1.01099e+15</td><td style=\"text-align: right;\">                   8</td><td>c4772750  </td><td style=\"text-align: right;\">     0.56783 </td><td style=\"text-align: right;\">     0.295031</td></tr>\n",
       "<tr><td>FSR_Trainable_ca89b0d0</td><td>2023-08-11_01-09-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    4.0089 </td><td style=\"text-align: right;\">   1154.85 </td><td style=\"text-align: right;\">    1.14775 </td><td style=\"text-align: right;\"> 2.61336e+16</td><td style=\"text-align: right;\">40.2224  </td><td>172.26.215.93</td><td style=\"text-align: right;\">200436</td><td style=\"text-align: right;\">     2.38092</td><td style=\"text-align: right;\">     994.542</td><td style=\"text-align: right;\">            1.87592 </td><td style=\"text-align: right;\">          1.87592 </td><td style=\"text-align: right;\">      1.87592 </td><td style=\"text-align: right;\"> 1691683772</td><td style=\"text-align: right;\">    28.0657 </td><td style=\"text-align: right;\">    5.99127 </td><td style=\"text-align: right;\">  3.14715e+15</td><td style=\"text-align: right;\">  1.05435e+15</td><td style=\"text-align: right;\">                   1</td><td>ca89b0d0  </td><td style=\"text-align: right;\">    33.1527  </td><td style=\"text-align: right;\">     7.0697  </td></tr>\n",
       "<tr><td>FSR_Trainable_cb5a0b86</td><td>2023-08-11_01-09-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">    5.67946</td><td style=\"text-align: right;\">    747.563</td><td style=\"text-align: right;\">    1.23693 </td><td style=\"text-align: right;\"> 1.24805e+18</td><td style=\"text-align: right;\"> 0.715552</td><td>172.26.215.93</td><td style=\"text-align: right;\">199693</td><td style=\"text-align: right;\">     2.65419</td><td style=\"text-align: right;\">     455.565</td><td style=\"text-align: right;\">           34.1208  </td><td style=\"text-align: right;\">          1.08131 </td><td style=\"text-align: right;\">     34.1208  </td><td style=\"text-align: right;\"> 1691683747</td><td style=\"text-align: right;\">     1.20395</td><td style=\"text-align: right;\">    0.335201</td><td style=\"text-align: right;\">  7.71068e+13</td><td style=\"text-align: right;\">  6.78171e+14</td><td style=\"text-align: right;\">                  32</td><td>cb5a0b86  </td><td style=\"text-align: right;\">     0.525042</td><td style=\"text-align: right;\">     0.19051 </td></tr>\n",
       "<tr><td>FSR_Trainable_ce96e157</td><td>2023-08-11_00-44-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    6.41898</td><td style=\"text-align: right;\">   1350.92 </td><td style=\"text-align: right;\">    1.34113 </td><td style=\"text-align: right;\"> 2.77078e+18</td><td style=\"text-align: right;\"> 0.851871</td><td>172.26.215.93</td><td style=\"text-align: right;\">186034</td><td style=\"text-align: right;\">     2.85685</td><td style=\"text-align: right;\">     818.592</td><td style=\"text-align: right;\">            8.42941 </td><td style=\"text-align: right;\">          0.942358</td><td style=\"text-align: right;\">      8.42941 </td><td style=\"text-align: right;\"> 1691682256</td><td style=\"text-align: right;\">     1.36795</td><td style=\"text-align: right;\">    0.53255 </td><td style=\"text-align: right;\">  7.82906e+13</td><td style=\"text-align: right;\">  1.21929e+15</td><td style=\"text-align: right;\">                   8</td><td>ce96e157  </td><td style=\"text-align: right;\">     0.564006</td><td style=\"text-align: right;\">     0.287865</td></tr>\n",
       "<tr><td>FSR_Trainable_cf85d82f</td><td>2023-08-11_00-59-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   26.4617 </td><td style=\"text-align: right;\">   3859.29 </td><td style=\"text-align: right;\">    4.47407 </td><td style=\"text-align: right;\"> 5.41566e+18</td><td style=\"text-align: right;\"> 2.48227 </td><td>172.26.215.93</td><td style=\"text-align: right;\">193475</td><td style=\"text-align: right;\">     9.45393</td><td style=\"text-align: right;\">    2304.2  </td><td style=\"text-align: right;\">            1.12285 </td><td style=\"text-align: right;\">          1.12285 </td><td style=\"text-align: right;\">      1.12285 </td><td style=\"text-align: right;\"> 1691683162</td><td style=\"text-align: right;\">     5.42994</td><td style=\"text-align: right;\">    1.37647 </td><td style=\"text-align: right;\">  4.50152e+13</td><td style=\"text-align: right;\">  2.31329e+15</td><td style=\"text-align: right;\">                   1</td><td>cf85d82f  </td><td style=\"text-align: right;\">     1.77018 </td><td style=\"text-align: right;\">     0.712087</td></tr>\n",
       "<tr><td>FSR_Trainable_cf94ab7d</td><td>2023-08-11_01-03-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">    5.75114</td><td style=\"text-align: right;\">    691.08 </td><td style=\"text-align: right;\">    1.22888 </td><td style=\"text-align: right;\"> 9.60051e+17</td><td style=\"text-align: right;\"> 0.701167</td><td>172.26.215.93</td><td style=\"text-align: right;\">195371</td><td style=\"text-align: right;\">     2.71969</td><td style=\"text-align: right;\">     472.004</td><td style=\"text-align: right;\">           59.9459  </td><td style=\"text-align: right;\">          0.756234</td><td style=\"text-align: right;\">     59.9459  </td><td style=\"text-align: right;\"> 1691683385</td><td style=\"text-align: right;\">     1.19417</td><td style=\"text-align: right;\">    0.274265</td><td style=\"text-align: right;\">  7.30225e+13</td><td style=\"text-align: right;\">  4.3084e+14 </td><td style=\"text-align: right;\">                  64</td><td>cf94ab7d  </td><td style=\"text-align: right;\">     0.526336</td><td style=\"text-align: right;\">     0.174831</td></tr>\n",
       "<tr><td>FSR_Trainable_d220b1c8</td><td>2023-08-11_00-45-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    6.80064</td><td style=\"text-align: right;\">   1270.09 </td><td style=\"text-align: right;\">    1.43705 </td><td style=\"text-align: right;\"> 2.39503e+18</td><td style=\"text-align: right;\"> 0.868029</td><td>172.26.215.93</td><td style=\"text-align: right;\">187407</td><td style=\"text-align: right;\">     2.90345</td><td style=\"text-align: right;\">     744.252</td><td style=\"text-align: right;\">           10.5259  </td><td style=\"text-align: right;\">          1.25714 </td><td style=\"text-align: right;\">     10.5259  </td><td style=\"text-align: right;\"> 1691682344</td><td style=\"text-align: right;\">     1.45116</td><td style=\"text-align: right;\">    0.5315  </td><td style=\"text-align: right;\">  7.15201e+13</td><td style=\"text-align: right;\">  1.12683e+15</td><td style=\"text-align: right;\">                   8</td><td>d220b1c8  </td><td style=\"text-align: right;\">     0.579685</td><td style=\"text-align: right;\">     0.288344</td></tr>\n",
       "<tr><td>FSR_Trainable_d3008e43</td><td>2023-08-11_00-44-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    4.34236</td><td style=\"text-align: right;\">   1305.39 </td><td style=\"text-align: right;\">    1.1705  </td><td style=\"text-align: right;\"> 1.56069e+17</td><td style=\"text-align: right;\">55.1045  </td><td>172.26.215.93</td><td style=\"text-align: right;\">186699</td><td style=\"text-align: right;\">     2.48189</td><td style=\"text-align: right;\">     978.524</td><td style=\"text-align: right;\">            2.69474 </td><td style=\"text-align: right;\">          2.69474 </td><td style=\"text-align: right;\">      2.69474 </td><td style=\"text-align: right;\"> 1691682290</td><td style=\"text-align: right;\">    44.6437 </td><td style=\"text-align: right;\">    6.95313 </td><td style=\"text-align: right;\">  4.44924e+15</td><td style=\"text-align: right;\">  5.13527e+15</td><td style=\"text-align: right;\">                   1</td><td>d3008e43  </td><td style=\"text-align: right;\">    48.1902  </td><td style=\"text-align: right;\">     6.91425 </td></tr>\n",
       "<tr><td>FSR_Trainable_d3e59f10</td><td>2023-08-11_01-01-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    8.36734</td><td style=\"text-align: right;\">   1397.34 </td><td style=\"text-align: right;\">    1.60743 </td><td style=\"text-align: right;\"> 2.76216e+18</td><td style=\"text-align: right;\"> 0.952299</td><td>172.26.215.93</td><td style=\"text-align: right;\">194640</td><td style=\"text-align: right;\">     3.2369 </td><td style=\"text-align: right;\">     878.568</td><td style=\"text-align: right;\">            7.73455 </td><td style=\"text-align: right;\">          4.03737 </td><td style=\"text-align: right;\">      7.73455 </td><td style=\"text-align: right;\"> 1691683276</td><td style=\"text-align: right;\">     1.78955</td><td style=\"text-align: right;\">    0.540537</td><td style=\"text-align: right;\">  7.80316e+13</td><td style=\"text-align: right;\">  1.23101e+15</td><td style=\"text-align: right;\">                   2</td><td>d3e59f10  </td><td style=\"text-align: right;\">     0.659072</td><td style=\"text-align: right;\">     0.293226</td></tr>\n",
       "<tr><td>FSR_Trainable_d50fa8ea</td><td>2023-08-11_00-53-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">    4.66451</td><td style=\"text-align: right;\">   1363.56 </td><td style=\"text-align: right;\">    1.21272 </td><td style=\"text-align: right;\"> 2.65188e+18</td><td style=\"text-align: right;\"> 0.805603</td><td>172.26.215.93</td><td style=\"text-align: right;\">190863</td><td style=\"text-align: right;\">     2.49393</td><td style=\"text-align: right;\">     911.986</td><td style=\"text-align: right;\">          109.747   </td><td style=\"text-align: right;\">          6.62783 </td><td style=\"text-align: right;\">    109.747   </td><td style=\"text-align: right;\"> 1691682781</td><td style=\"text-align: right;\">     1.01892</td><td style=\"text-align: right;\">    0.489389</td><td style=\"text-align: right;\">  8.39714e+13</td><td style=\"text-align: right;\">  1.00097e+15</td><td style=\"text-align: right;\">                  16</td><td>d50fa8ea  </td><td style=\"text-align: right;\">     0.510801</td><td style=\"text-align: right;\">     0.294802</td></tr>\n",
       "<tr><td>FSR_Trainable_d54e09c5</td><td>2023-08-11_01-07-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">    5.64031</td><td style=\"text-align: right;\">    718.214</td><td style=\"text-align: right;\">    1.23197 </td><td style=\"text-align: right;\"> 1.05316e+18</td><td style=\"text-align: right;\"> 0.686396</td><td>172.26.215.93</td><td style=\"text-align: right;\">197433</td><td style=\"text-align: right;\">     2.65553</td><td style=\"text-align: right;\">     476.705</td><td style=\"text-align: right;\">           59.249   </td><td style=\"text-align: right;\">          0.845902</td><td style=\"text-align: right;\">     59.249   </td><td style=\"text-align: right;\"> 1691683625</td><td style=\"text-align: right;\">     1.16694</td><td style=\"text-align: right;\">    0.290744</td><td style=\"text-align: right;\">  7.43267e+13</td><td style=\"text-align: right;\">  5.0318e+14 </td><td style=\"text-align: right;\">                  64</td><td>d54e09c5  </td><td style=\"text-align: right;\">     0.511977</td><td style=\"text-align: right;\">     0.174419</td></tr>\n",
       "<tr><td>FSR_Trainable_da121a16</td><td>2023-08-11_00-40-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    3.89164</td><td style=\"text-align: right;\">   1148.38 </td><td style=\"text-align: right;\">    1.12231 </td><td style=\"text-align: right;\"> 2.03298e+16</td><td style=\"text-align: right;\">40.3533  </td><td>172.26.215.93</td><td style=\"text-align: right;\">182561</td><td style=\"text-align: right;\">     2.38759</td><td style=\"text-align: right;\">     959.616</td><td style=\"text-align: right;\">            5.24813 </td><td style=\"text-align: right;\">          5.24813 </td><td style=\"text-align: right;\">      5.24813 </td><td style=\"text-align: right;\"> 1691682044</td><td style=\"text-align: right;\">    27.9747 </td><td style=\"text-align: right;\">    5.91984 </td><td style=\"text-align: right;\">  2.07975e+15</td><td style=\"text-align: right;\">  5.82121e+14</td><td style=\"text-align: right;\">                   1</td><td>da121a16  </td><td style=\"text-align: right;\">    33.2002  </td><td style=\"text-align: right;\">     7.15308 </td></tr>\n",
       "<tr><td>FSR_Trainable_dcd3bac3</td><td>2023-08-11_01-04-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    5.47585</td><td style=\"text-align: right;\">    642.533</td><td style=\"text-align: right;\">    1.20156 </td><td style=\"text-align: right;\"> 7.70046e+17</td><td style=\"text-align: right;\"> 0.677254</td><td>172.26.215.93</td><td style=\"text-align: right;\">195825</td><td style=\"text-align: right;\">     2.63934</td><td style=\"text-align: right;\">     461.765</td><td style=\"text-align: right;\">           89.419   </td><td style=\"text-align: right;\">          0.803214</td><td style=\"text-align: right;\">     89.419   </td><td style=\"text-align: right;\"> 1691683449</td><td style=\"text-align: right;\">     1.14019</td><td style=\"text-align: right;\">    0.259522</td><td style=\"text-align: right;\">  7.59235e+13</td><td style=\"text-align: right;\">  3.8895e+14 </td><td style=\"text-align: right;\">                 100</td><td>dcd3bac3  </td><td style=\"text-align: right;\">     0.508423</td><td style=\"text-align: right;\">     0.168831</td></tr>\n",
       "<tr><td>FSR_Trainable_dcea2390</td><td>2023-08-11_00-44-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    6.62935</td><td style=\"text-align: right;\">   1394.46 </td><td style=\"text-align: right;\">    1.25294 </td><td style=\"text-align: right;\"> 2.59864e+18</td><td style=\"text-align: right;\"> 0.87911 </td><td>172.26.215.93</td><td style=\"text-align: right;\">186252</td><td style=\"text-align: right;\">     2.87324</td><td style=\"text-align: right;\">     873.702</td><td style=\"text-align: right;\">            8.75897 </td><td style=\"text-align: right;\">          0.871741</td><td style=\"text-align: right;\">      8.75897 </td><td style=\"text-align: right;\"> 1691682270</td><td style=\"text-align: right;\">     1.4223 </td><td style=\"text-align: right;\">    0.580445</td><td style=\"text-align: right;\">  7.65511e+13</td><td style=\"text-align: right;\">  1.35308e+15</td><td style=\"text-align: right;\">                   8</td><td>dcea2390  </td><td style=\"text-align: right;\">     0.571216</td><td style=\"text-align: right;\">     0.307894</td></tr>\n",
       "<tr><td>FSR_Trainable_dfe69d9f</td><td>2023-08-11_00-43-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    6.35359</td><td style=\"text-align: right;\">   1198.5  </td><td style=\"text-align: right;\">    1.44282 </td><td style=\"text-align: right;\"> 1.02327e+09</td><td style=\"text-align: right;\"> 7.50896 </td><td>172.26.215.93</td><td style=\"text-align: right;\">185352</td><td style=\"text-align: right;\">     2.83721</td><td style=\"text-align: right;\">     751.673</td><td style=\"text-align: right;\">            0.911505</td><td style=\"text-align: right;\">          0.911505</td><td style=\"text-align: right;\">      0.911505</td><td style=\"text-align: right;\"> 1691682210</td><td style=\"text-align: right;\">    11.1622 </td><td style=\"text-align: right;\">    3.69523 </td><td style=\"text-align: right;\">877.708      </td><td style=\"text-align: right;\">256.141      </td><td style=\"text-align: right;\">                   1</td><td>dfe69d9f  </td><td style=\"text-align: right;\">     5.21052 </td><td style=\"text-align: right;\">     2.29844 </td></tr>\n",
       "<tr><td>FSR_Trainable_e0ff0bca</td><td>2023-08-11_01-10-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   25.024  </td><td style=\"text-align: right;\">   1756.92 </td><td style=\"text-align: right;\">    3.89332 </td><td style=\"text-align: right;\"> 2.94688e+18</td><td style=\"text-align: right;\"> 2.20657 </td><td>172.26.215.93</td><td style=\"text-align: right;\">200949</td><td style=\"text-align: right;\">     8.12372</td><td style=\"text-align: right;\">    1065.14 </td><td style=\"text-align: right;\">            3.88747 </td><td style=\"text-align: right;\">          3.88747 </td><td style=\"text-align: right;\">      3.88747 </td><td style=\"text-align: right;\"> 1691683821</td><td style=\"text-align: right;\">     5.6373 </td><td style=\"text-align: right;\">    0.724879</td><td style=\"text-align: right;\">  2.35171e+13</td><td style=\"text-align: right;\">  1.49764e+15</td><td style=\"text-align: right;\">                   1</td><td>e0ff0bca  </td><td style=\"text-align: right;\">     1.82036 </td><td style=\"text-align: right;\">     0.386201</td></tr>\n",
       "<tr><td>FSR_Trainable_e5ebe0c9</td><td>2023-08-11_00-40-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    3.71661</td><td style=\"text-align: right;\">   1144.81 </td><td style=\"text-align: right;\">    1.10492 </td><td style=\"text-align: right;\"> 2.21582e+16</td><td style=\"text-align: right;\">40.3111  </td><td>172.26.215.93</td><td style=\"text-align: right;\">181599</td><td style=\"text-align: right;\">     2.36866</td><td style=\"text-align: right;\">     982.865</td><td style=\"text-align: right;\">           27.5601  </td><td style=\"text-align: right;\">         14.8443  </td><td style=\"text-align: right;\">     27.5601  </td><td style=\"text-align: right;\"> 1691682016</td><td style=\"text-align: right;\">    27.6083 </td><td style=\"text-align: right;\">    5.87529 </td><td style=\"text-align: right;\">  7.99322e+14</td><td style=\"text-align: right;\">  6.26039e+14</td><td style=\"text-align: right;\">                   2</td><td>e5ebe0c9  </td><td style=\"text-align: right;\">    33.1842  </td><td style=\"text-align: right;\">     7.1269  </td></tr>\n",
       "<tr><td>FSR_Trainable_e8d19252</td><td>2023-08-11_00-46-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    7.94244</td><td style=\"text-align: right;\">   1311.88 </td><td style=\"text-align: right;\">    1.5382  </td><td style=\"text-align: right;\"> 2.34704e+18</td><td style=\"text-align: right;\"> 0.976071</td><td>172.26.215.93</td><td style=\"text-align: right;\">187882</td><td style=\"text-align: right;\">     3.25277</td><td style=\"text-align: right;\">     797.541</td><td style=\"text-align: right;\">           15.9565  </td><td style=\"text-align: right;\">          5.10097 </td><td style=\"text-align: right;\">     15.9565  </td><td style=\"text-align: right;\"> 1691682387</td><td style=\"text-align: right;\">     1.71657</td><td style=\"text-align: right;\">    0.541914</td><td style=\"text-align: right;\">  6.24919e+13</td><td style=\"text-align: right;\">  1.06556e+15</td><td style=\"text-align: right;\">                   4</td><td>e8d19252  </td><td style=\"text-align: right;\">     0.669754</td><td style=\"text-align: right;\">     0.306317</td></tr>\n",
       "<tr><td>FSR_Trainable_ea590b92</td><td>2023-08-11_01-07-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   27.5737 </td><td style=\"text-align: right;\">   2027.96 </td><td style=\"text-align: right;\">    4.65913 </td><td style=\"text-align: right;\"> 2.30319e+18</td><td style=\"text-align: right;\"> 2.26603 </td><td>172.26.215.93</td><td style=\"text-align: right;\">198622</td><td style=\"text-align: right;\">     9.09135</td><td style=\"text-align: right;\">    1588.62 </td><td style=\"text-align: right;\">            1.72794 </td><td style=\"text-align: right;\">          1.72794 </td><td style=\"text-align: right;\">      1.72794 </td><td style=\"text-align: right;\"> 1691683645</td><td style=\"text-align: right;\">     5.89055</td><td style=\"text-align: right;\">    0.707441</td><td style=\"text-align: right;\">  2.85668e+13</td><td style=\"text-align: right;\">  1.15417e+15</td><td style=\"text-align: right;\">                   1</td><td>ea590b92  </td><td style=\"text-align: right;\">     1.82812 </td><td style=\"text-align: right;\">     0.437915</td></tr>\n",
       "<tr><td>FSR_Trainable_ebb1cdcf</td><td>2023-08-11_00-42-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    6.73784</td><td style=\"text-align: right;\">   1548.65 </td><td style=\"text-align: right;\">    1.49378 </td><td style=\"text-align: right;\"> 3.21516e+18</td><td style=\"text-align: right;\"> 0.889989</td><td>172.26.215.93</td><td style=\"text-align: right;\">184166</td><td style=\"text-align: right;\">     2.76245</td><td style=\"text-align: right;\">     971.7  </td><td style=\"text-align: right;\">           10.3289  </td><td style=\"text-align: right;\">          1.13066 </td><td style=\"text-align: right;\">     10.3289  </td><td style=\"text-align: right;\"> 1691682138</td><td style=\"text-align: right;\">     1.41756</td><td style=\"text-align: right;\">    0.594703</td><td style=\"text-align: right;\">  8.39562e+13</td><td style=\"text-align: right;\">  1.38154e+15</td><td style=\"text-align: right;\">                   8</td><td>ebb1cdcf  </td><td style=\"text-align: right;\">     0.559647</td><td style=\"text-align: right;\">     0.330343</td></tr>\n",
       "<tr><td>FSR_Trainable_f4c384a8</td><td>2023-08-11_01-03-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.83178</td><td style=\"text-align: right;\">    646.037</td><td style=\"text-align: right;\">    1.18762 </td><td style=\"text-align: right;\"> 8.56196e+17</td><td style=\"text-align: right;\"> 0.651841</td><td>172.26.215.93</td><td style=\"text-align: right;\">195127</td><td style=\"text-align: right;\">     2.4626 </td><td style=\"text-align: right;\">     438.719</td><td style=\"text-align: right;\">           87.4017  </td><td style=\"text-align: right;\">          0.913461</td><td style=\"text-align: right;\">     87.4017  </td><td style=\"text-align: right;\"> 1691683404</td><td style=\"text-align: right;\">     1.0334 </td><td style=\"text-align: right;\">    0.26308 </td><td style=\"text-align: right;\">  7.89453e+13</td><td style=\"text-align: right;\">  4.29663e+14</td><td style=\"text-align: right;\">                 100</td><td>f4c384a8  </td><td style=\"text-align: right;\">     0.490006</td><td style=\"text-align: right;\">     0.161834</td></tr>\n",
       "<tr><td>FSR_Trainable_f58d2a52</td><td>2023-08-11_00-47-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    3.89689</td><td style=\"text-align: right;\">   1159.56 </td><td style=\"text-align: right;\">    1.09283 </td><td style=\"text-align: right;\"> 4.96697e+08</td><td style=\"text-align: right;\"> 6.01643 </td><td>172.26.215.93</td><td style=\"text-align: right;\">188892</td><td style=\"text-align: right;\">     2.32999</td><td style=\"text-align: right;\">     767.172</td><td style=\"text-align: right;\">            2.29969 </td><td style=\"text-align: right;\">          2.29969 </td><td style=\"text-align: right;\">      2.29969 </td><td style=\"text-align: right;\"> 1691682430</td><td style=\"text-align: right;\">     7.33573</td><td style=\"text-align: right;\">    2.78256 </td><td style=\"text-align: right;\">  8.48233e+14</td><td style=\"text-align: right;\"> 44.8819     </td><td style=\"text-align: right;\">                   1</td><td>f58d2a52  </td><td style=\"text-align: right;\">     4.41342 </td><td style=\"text-align: right;\">     1.60301 </td></tr>\n",
       "<tr><td>FSR_Trainable_f67d3b44</td><td>2023-08-11_00-46-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    7.15594</td><td style=\"text-align: right;\">   1193.93 </td><td style=\"text-align: right;\">    1.35867 </td><td style=\"text-align: right;\"> 1.94967e+18</td><td style=\"text-align: right;\"> 0.857755</td><td>172.26.215.93</td><td style=\"text-align: right;\">187679</td><td style=\"text-align: right;\">     2.99137</td><td style=\"text-align: right;\">     763.166</td><td style=\"text-align: right;\">            9.27454 </td><td style=\"text-align: right;\">          0.884586</td><td style=\"text-align: right;\">      9.27454 </td><td style=\"text-align: right;\"> 1691682365</td><td style=\"text-align: right;\">     1.51441</td><td style=\"text-align: right;\">    0.463531</td><td style=\"text-align: right;\">  7.02214e+13</td><td style=\"text-align: right;\">  8.4529e+14 </td><td style=\"text-align: right;\">                   8</td><td>f67d3b44  </td><td style=\"text-align: right;\">     0.588967</td><td style=\"text-align: right;\">     0.268788</td></tr>\n",
       "<tr><td>FSR_Trainable_f90c4aea</td><td>2023-08-11_01-06-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.83217</td><td style=\"text-align: right;\">    639.887</td><td style=\"text-align: right;\">    1.21167 </td><td style=\"text-align: right;\"> 8.60552e+17</td><td style=\"text-align: right;\"> 0.654993</td><td>172.26.215.93</td><td style=\"text-align: right;\">196649</td><td style=\"text-align: right;\">     2.46451</td><td style=\"text-align: right;\">     428.977</td><td style=\"text-align: right;\">           96.6735  </td><td style=\"text-align: right;\">          0.903866</td><td style=\"text-align: right;\">     96.6735  </td><td style=\"text-align: right;\"> 1691683565</td><td style=\"text-align: right;\">     1.02878</td><td style=\"text-align: right;\">    0.265384</td><td style=\"text-align: right;\">  7.87081e+13</td><td style=\"text-align: right;\">  4.39775e+14</td><td style=\"text-align: right;\">                 100</td><td>f90c4aea  </td><td style=\"text-align: right;\">     0.49223 </td><td style=\"text-align: right;\">     0.162764</td></tr>\n",
       "<tr><td>FSR_Trainable_f9bd9eb4</td><td>2023-08-11_01-10-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">    5.72732</td><td style=\"text-align: right;\">    750.433</td><td style=\"text-align: right;\">    1.2655  </td><td style=\"text-align: right;\"> 1.17023e+18</td><td style=\"text-align: right;\"> 0.711731</td><td>172.26.215.93</td><td style=\"text-align: right;\">200214</td><td style=\"text-align: right;\">     2.65271</td><td style=\"text-align: right;\">     480.205</td><td style=\"text-align: right;\">           54.9887  </td><td style=\"text-align: right;\">          1.45285 </td><td style=\"text-align: right;\">     54.9887  </td><td style=\"text-align: right;\"> 1691683816</td><td style=\"text-align: right;\">     1.21189</td><td style=\"text-align: right;\">    0.335242</td><td style=\"text-align: right;\">  7.9562e+13 </td><td style=\"text-align: right;\">  6.77515e+14</td><td style=\"text-align: right;\">                  32</td><td>f9bd9eb4  </td><td style=\"text-align: right;\">     0.523421</td><td style=\"text-align: right;\">     0.188311</td></tr>\n",
       "<tr><td>FSR_Trainable_fb304c25</td><td>2023-08-11_00-57-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">    5.88433</td><td style=\"text-align: right;\">    737.747</td><td style=\"text-align: right;\">    1.26192 </td><td style=\"text-align: right;\"> 1.23025e+18</td><td style=\"text-align: right;\"> 0.725312</td><td>172.26.215.93</td><td style=\"text-align: right;\">192441</td><td style=\"text-align: right;\">     2.72662</td><td style=\"text-align: right;\">     455.371</td><td style=\"text-align: right;\">           70.5804  </td><td style=\"text-align: right;\">          2.18232 </td><td style=\"text-align: right;\">     70.5804  </td><td style=\"text-align: right;\"> 1691683051</td><td style=\"text-align: right;\">     1.24054</td><td style=\"text-align: right;\">    0.328354</td><td style=\"text-align: right;\">  7.59149e+13</td><td style=\"text-align: right;\">  6.6467e+14 </td><td style=\"text-align: right;\">                  32</td><td>fb304c25  </td><td style=\"text-align: right;\">     0.53671 </td><td style=\"text-align: right;\">     0.188602</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_043a5c07_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-11_00-39-46/wandb/run-20230811_004002-043a5c07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb: Syncing run FSR_Trainable_043a5c07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/043a5c07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_4d8df89d_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-11_00-39-54/wandb/run-20230811_004013-4d8df89d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: Syncing run FSR_Trainable_4d8df89d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4d8df89d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:                mae_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:               mape_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:               rmse_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:               tmae_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:              tmape_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:              trmse_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:                mae_coord 29.59613\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:                mae_force 1555.55714\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:               mape_coord 4.58694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:               mape_force 9.319891908075972e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:                   metric 2.44041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:               rmse_coord 9.29733\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:               rmse_force 1332.94906\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:       time_since_restore 7.6936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:         time_this_iter_s 3.52207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:             time_total_s 7.6936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:                timestamp 1691682015\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:               tmae_coord 6.6344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:               tmae_force 0.5466\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:              tmape_coord 13265509299749.87\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:              tmape_force 536619241274424.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:              trmse_coord 2.04815\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb:              trmse_force 0.39226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: 🚀 View run FSR_Trainable_4d8df89d at: https://wandb.ai/seokjin/FSR-prediction/runs/4d8df89d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182020)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004013-4d8df89d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb:               mape_force ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb:               rmse_coord ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb:              tmape_force ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181672)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_3c320acd_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-11_00-40-05/wandb/run-20230811_004024-3c320acd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: Syncing run FSR_Trainable_3c320acd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3c320acd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:                mae_coord 4.16052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:                mae_force 1208.90121\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:               mape_coord 1.01715\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:               mape_force 5.366952537262153e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:                   metric 55.30192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:               rmse_coord 2.53921\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:               rmse_force 1071.62618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:       time_since_restore 3.25225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:         time_this_iter_s 3.25225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:             time_total_s 3.25225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:                timestamp 1691682020\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:               tmae_coord 44.27381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:               tmae_force 6.24538\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:              tmape_coord 2264323352507076.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:              tmape_force 1353841322196577.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:              trmse_coord 48.09356\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb:              trmse_force 7.20836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: 🚀 View run FSR_Trainable_3c320acd at: https://wandb.ai/seokjin/FSR-prediction/runs/3c320acd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182209)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004024-3c320acd/logs\n",
      "2023-08-11 00:40:33,084\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.563 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:40:33,090\tWARNING util.py:315 -- The `process_trial_result` operation took 2.570 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:40:33,092\tWARNING util.py:315 -- Processing trial results took 2.572 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:40:33,096\tWARNING util.py:315 -- The `process_trial_result` operation took 2.576 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_b47983cf_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-11_00-40-17/wandb/run-20230811_004035-b47983cf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: Syncing run FSR_Trainable_b47983cf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b47983cf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:                mae_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:                mae_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:               mape_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:               mape_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:                   metric █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:               rmse_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:               rmse_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:         time_this_iter_s █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:               tmae_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:               tmae_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:              tmape_coord ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:              tmape_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:              trmse_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:              trmse_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:                mae_coord 28.79936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:                mae_force 1624.18534\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:               mape_coord 4.4975\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:               mape_force 2.614155906158957e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:                   metric 2.37971\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:               rmse_coord 9.12522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:               rmse_force 1099.26319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:       time_since_restore 8.89746\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:         time_this_iter_s 2.07106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:             time_total_s 8.89746\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:                timestamp 1691682039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:               tmae_coord 6.51122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:               tmae_force 0.62271\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:              tmape_coord 15164744207838.45\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:              tmape_force 1249729245158698.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:              trmse_coord 2.03318\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb:              trmse_force 0.34653\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: 🚀 View run FSR_Trainable_b47983cf at: https://wandb.ai/seokjin/FSR-prediction/runs/b47983cf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182445)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004035-b47983cf/logs\n",
      "2023-08-11 00:40:46,703\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.455 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-08-11 00:40:46,711\tWARNING util.py:315 -- The `process_trial_result` operation took 2.464 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:40:46,714\tWARNING util.py:315 -- Processing trial results took 2.466 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:40:46,716\tWARNING util.py:315 -- The `process_trial_result` operation took 2.469 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_da121a16_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-11_00-40-27/wandb/run-20230811_004046-da121a16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: Syncing run FSR_Trainable_da121a16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/da121a16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:                mae_coord 3.89164\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:                mae_force 1148.38441\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:               mape_coord 1.12231\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:               mape_force 2.032983241095099e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:                   metric 40.35329\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:               rmse_coord 2.38759\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:               rmse_force 959.61641\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:       time_since_restore 5.24813\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:         time_this_iter_s 5.24813\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:             time_total_s 5.24813\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:                timestamp 1691682044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:               tmae_coord 27.97474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:               tmae_force 5.91984\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:              tmape_coord 2079747933852322.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:              tmape_force 582120885041333.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:              trmse_coord 33.2002\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb:              trmse_force 7.15308\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: 🚀 View run FSR_Trainable_da121a16 at: https://wandb.ai/seokjin/FSR-prediction/runs/da121a16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182670)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004046-da121a16/logs\n",
      "2023-08-11 00:40:54,429\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.197 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:40:54,432\tWARNING util.py:315 -- The `process_trial_result` operation took 2.201 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:40:54,436\tWARNING util.py:315 -- Processing trial results took 2.205 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:40:54,438\tWARNING util.py:315 -- The `process_trial_result` operation took 2.207 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_4be3dabe_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-11_00-40-39/wandb/run-20230811_004057-4be3dabe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: Syncing run FSR_Trainable_4be3dabe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4be3dabe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:                mae_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:               mape_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:               rmse_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:               tmae_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:              tmape_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:              trmse_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:                mae_coord 4.47826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:                mae_force 1507.26361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:               mape_coord 1.08617\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:               mape_force 879761148.15239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:                   metric 6.64384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:               rmse_coord 2.52933\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:               rmse_force 1013.23573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:       time_since_restore 4.02472\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:         time_this_iter_s 1.82069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:             time_total_s 4.02472\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:                timestamp 1691682056\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:               tmae_coord 8.25227\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:               tmae_force 3.33216\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:              tmape_coord 464.96883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:              tmape_force 84.59431\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:              trmse_coord 4.56369\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb:              trmse_force 2.08015\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: 🚀 View run FSR_Trainable_4be3dabe at: https://wandb.ai/seokjin/FSR-prediction/runs/4be3dabe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=182902)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004057-4be3dabe/logs\n",
      "2023-08-11 00:41:06,389\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.751 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:41:06,393\tWARNING util.py:315 -- The `process_trial_result` operation took 2.757 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:41:06,399\tWARNING util.py:315 -- Processing trial results took 2.763 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:41:06,403\tWARNING util.py:315 -- The `process_trial_result` operation took 2.767 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_3ea710a0_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-11_00-40-50/wandb/run-20230811_004108-3ea710a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: Syncing run FSR_Trainable_3ea710a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3ea710a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:                mae_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:               mape_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:               mape_force ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:               rmse_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:               tmae_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:              tmape_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:              trmse_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:                mae_coord 4.0561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:                mae_force 1472.75928\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:               mape_coord 1.14597\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:               mape_force 609800325.68138\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:                   metric 6.59686\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:               rmse_coord 2.39998\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:               rmse_force 959.63567\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:       time_since_restore 5.08803\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:         time_this_iter_s 2.42417\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:             time_total_s 5.08803\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:                timestamp 1691682068\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:               tmae_coord 7.66565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:               tmae_force 3.49865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:              tmape_coord 713641749909832.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:              tmape_force 74.09409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:              trmse_coord 4.54199\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb:              trmse_force 2.05487\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: 🚀 View run FSR_Trainable_3ea710a0 at: https://wandb.ai/seokjin/FSR-prediction/runs/3ea710a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183129)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004108-3ea710a0/logs\n",
      "2023-08-11 00:41:19,287\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.800 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:41:19,299\tWARNING util.py:315 -- The `process_trial_result` operation took 2.812 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:41:19,303\tWARNING util.py:315 -- Processing trial results took 2.817 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:41:19,306\tWARNING util.py:315 -- The `process_trial_result` operation took 2.820 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_19e6058b_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-11_00-41-00/wandb/run-20230811_004121-19e6058b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: Syncing run FSR_Trainable_19e6058b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/19e6058b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: - 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:                mae_coord █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:                mae_force █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:               mape_coord █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:               mape_force █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:                   metric █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:               rmse_coord █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:               rmse_force █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:       time_since_restore ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:         time_this_iter_s █▃▁▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:             time_total_s ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:               tmae_coord █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:               tmae_force █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:              tmape_coord ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:              tmape_force █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:              trmse_coord █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:              trmse_force █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:                mae_coord 27.0655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:                mae_force 2613.87921\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:               mape_coord 4.03607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:               mape_force 3.1654780408004567e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:                   metric 2.39461\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:               rmse_coord 8.96946\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:               rmse_force 1692.63983\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:       time_since_restore 12.14782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:         time_this_iter_s 3.24274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:             time_total_s 12.14782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:                timestamp 1691682087\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:               tmae_coord 5.8739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:               tmae_force 0.93225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:              tmape_coord 24101798101347.074\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:              tmape_force 1376144846501931.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:              trmse_coord 1.87343\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb:              trmse_force 0.52118\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: 🚀 View run FSR_Trainable_19e6058b at: https://wandb.ai/seokjin/FSR-prediction/runs/19e6058b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183363)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004121-19e6058b/logs\n",
      "2023-08-11 00:41:32,791\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.066 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:41:32,796\tWARNING util.py:315 -- The `process_trial_result` operation took 3.072 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:41:32,801\tWARNING util.py:315 -- Processing trial results took 3.077 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:41:32,805\tWARNING util.py:315 -- The `process_trial_result` operation took 3.081 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_4cf41fd6_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-41-12/wandb/run-20230811_004135-4cf41fd6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Syncing run FSR_Trainable_4cf41fd6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4cf41fd6\n",
      "2023-08-11 00:41:42,988\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.865 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:41:42,992\tWARNING util.py:315 -- The `process_trial_result` operation took 2.870 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:41:42,995\tWARNING util.py:315 -- Processing trial results took 2.873 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:41:42,997\tWARNING util.py:315 -- The `process_trial_result` operation took 2.876 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_863ef801_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-41-26/wandb/run-20230811_004146-863ef801\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: Syncing run FSR_Trainable_863ef801\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/863ef801\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:                mae_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:                mae_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:               mape_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:               mape_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:                   metric █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:               rmse_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:               rmse_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:         time_this_iter_s █▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:               tmae_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:               tmae_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:              tmape_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:              tmape_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:              trmse_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:              trmse_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:                mae_coord 23.6067\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:                mae_force 2976.87667\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:               mape_coord 3.43213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:               mape_force 4.32633047260512e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:                   metric 2.30881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:               rmse_coord 7.6966\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:               rmse_force 1915.65513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:       time_since_restore 3.11362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:         time_this_iter_s 0.67251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:             time_total_s 3.11362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:                timestamp 1691682105\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:               tmae_coord 5.33934\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:               tmae_force 1.06633\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:              tmape_coord 32614954464517.836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:              tmape_force 1948720671643795.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:              trmse_coord 1.74002\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb:              trmse_force 0.56879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: 🚀 View run FSR_Trainable_863ef801 at: https://wandb.ai/seokjin/FSR-prediction/runs/863ef801\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183813)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004146-863ef801/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:                mae_coord ▆█▆▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:                mae_force █▁▄▄▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:               mape_coord █▁▁▃▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:               mape_force █▁▄▄▄▄▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:                   metric ▇█▆▃▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:               rmse_coord ▆█▆▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:               rmse_force █▁▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:         time_this_iter_s █▃▁▁▃▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:                timestamp ▁▃▄▅▅▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:               tmae_coord ▅█▆▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:               tmae_force █▁▅▆▅▆▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:              tmape_coord ▆▁▂▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:              tmape_force █▁▆▇▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:              trmse_coord ▆█▆▃▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb:              trmse_force █▁▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004135-4cf41fd6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=183584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_3409809b_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-41-39/wandb/run-20230811_004202-3409809b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: Syncing run FSR_Trainable_3409809b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3409809b\n",
      "2023-08-11 00:42:07,922\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.809 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:42:07,925\tWARNING util.py:315 -- The `process_trial_result` operation took 2.813 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:42:07,928\tWARNING util.py:315 -- Processing trial results took 2.816 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:42:07,931\tWARNING util.py:315 -- The `process_trial_result` operation took 2.819 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:42:10,705\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.684 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:42:10,707\tWARNING util.py:315 -- The `process_trial_result` operation took 2.687 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:42:10,709\tWARNING util.py:315 -- Processing trial results took 2.689 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:42:10,712\tWARNING util.py:315 -- The `process_trial_result` operation took 2.692 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_ebb1cdcf_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-41-52/wandb/run-20230811_004211-ebb1cdcf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: Syncing run FSR_Trainable_ebb1cdcf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ebb1cdcf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: | 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: / 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: - 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 00:42:24,251\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.756 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:42:24,257\tWARNING util.py:315 -- The `process_trial_result` operation took 2.766 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:42:24,259\tWARNING util.py:315 -- Processing trial results took 2.769 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:42:24,266\tWARNING util.py:315 -- The `process_trial_result` operation took 2.776 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:                mae_coord █▅▅▅▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:                mae_force ▅▁█▇▆▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:               mape_coord █▅▄▄▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:               mape_force ▆▁▅██▆▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:                   metric █▅▅▅▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:               rmse_coord █▅▅▄▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:               rmse_force █▁▇▆▅▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:         time_this_iter_s █▂▃▃▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:                timestamp ▁▃▅▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:               tmae_coord █▆▅▆▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:               tmae_force ▆▁█▅▅▄▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:              tmape_coord ▆▁▅▃▆▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:              tmape_force █▁█▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:              trmse_coord █▆▅▅▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:              trmse_force █▁█▆▄▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:                mae_coord 6.73784\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:                mae_force 1548.65021\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:               mape_coord 1.49378\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:               mape_force 3.215158577475147e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:                   metric 0.88999\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:               rmse_coord 2.76245\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:               rmse_force 971.7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:       time_since_restore 10.32887\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:         time_this_iter_s 1.13066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:             time_total_s 10.32887\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:                timestamp 1691682138\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:               tmae_coord 1.41756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:               tmae_force 0.5947\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:              tmape_coord 83956212393595.77\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:              tmape_force 1381544291950626.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:              trmse_coord 0.55965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb:              trmse_force 0.33034\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: 🚀 View run FSR_Trainable_ebb1cdcf at: https://wandb.ai/seokjin/FSR-prediction/runs/ebb1cdcf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184241)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004211-ebb1cdcf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_611b154e_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-42-03/wandb/run-20230811_004228-611b154e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: Syncing run FSR_Trainable_611b154e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/611b154e\n",
      "2023-08-11 00:42:40,176\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.792 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:42:40,181\tWARNING util.py:315 -- The `process_trial_result` operation took 2.798 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:42:40,187\tWARNING util.py:315 -- Processing trial results took 2.804 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:42:40,190\tWARNING util.py:315 -- The `process_trial_result` operation took 2.807 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_a9e43461_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-42-19/wandb/run-20230811_004244-a9e43461\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: Syncing run FSR_Trainable_a9e43461\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9e43461\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:                mae_coord ██▅▃▂▂▂▂▂▂▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:                mae_force █▅▆▄▄▄▄▃▂▂▄▄▃▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:               mape_coord ▄█▅▃▂▂▃▃▃▃▄▄▄▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:               mape_force █▆▅▅▅▅▅▄▃▃▃▂▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:                   metric █▆▆▂▂▁▂▁▁▂▃▄▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:               rmse_coord █▆▄▂▁▁▁▁▁▁▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:               rmse_force █▅▆▃▂▂▂▂▁▂▄▄▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▄▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:         time_this_iter_s ▆▂▃▃▁▁▁▁▂▃█▄▄▅▆▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▄▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:                timestamp ▁▂▂▃▃▃▄▄▅▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:               tmae_coord ██▅▂▂▂▂▂▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:               tmae_force █▅▆▅▄▄▄▃▃▃▄▄▃▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:              tmape_coord ▃▃▁▆▆▆▇▇▇▇▇▆██▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:              tmape_force █▅▄▅▅▅▄▄▃▃▃▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:              trmse_coord ██▅▂▁▁▁▁▁▁▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:              trmse_force █▄▇▃▂▂▃▂▂▃▅▆▃▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:                mae_coord 6.12881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:                mae_force 1026.63845\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:               mape_coord 1.34483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:               mape_force 1.3168076814977613e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:                   metric 0.82033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:               rmse_coord 2.72447\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:               rmse_force 696.08947\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:       time_since_restore 21.68739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:         time_this_iter_s 1.14909\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:             time_total_s 21.68739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:                timestamp 1691682167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:               tmae_coord 1.3152\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:               tmae_force 0.44223\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:              tmape_coord 76100437884237.27\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:              tmape_force 729167864895012.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:              trmse_coord 0.54967\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb:              trmse_force 0.27066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: 🚀 View run FSR_Trainable_611b154e at: https://wandb.ai/seokjin/FSR-prediction/runs/611b154e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184494)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004228-611b154e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:                mae_coord █▅▄▅▅▃▁▁▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:                mae_force █▆▆▄▄▂▁▂▃▃▂▃▂▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:               mape_coord █▃▃▅▅▂▁▂▆▄▄▄▃▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:               mape_force █▆▆▄▄▃▁▁▂▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:                   metric █▆▅▄▄▂▁▂▄▃▃▃▂▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:               rmse_coord █▅▄▅▅▂▁▂▄▃▃▃▂▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:               rmse_force █▇▆▃▂▂▁▂▃▃▂▄▂▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:         time_this_iter_s █▃▃▄▄▁▃▄▁▃▄▂▄▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:                timestamp ▁▂▂▃▄▄▄▅▅▅▆▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:               tmae_coord █▅▅▅▅▃▂▁▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:               tmae_force █▆▆▄▅▃▁▂▄▃▄▄▃▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:              tmape_coord ▃▃▄▃▁▅▅▅▆▇▇█▇▇█▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:              tmape_force █▆▆▅▅▄▁▁▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:              trmse_coord █▅▅▆▅▃▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:              trmse_force █▆▆▄▄▂▁▃▅▅▄▅▃▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:                mae_coord 6.67276\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:                mae_force 1052.24233\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:               mape_coord 1.56047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:               mape_force 1.4267472724031268e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:                   metric 0.84314\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:               rmse_coord 2.82578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:               rmse_force 688.47066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:       time_since_restore 22.30687\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:         time_this_iter_s 1.26604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:             time_total_s 22.30687\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:                timestamp 1691682181\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:               tmae_coord 1.41006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:               tmae_force 0.4579\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:              tmape_coord 76668826349689.56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:              tmape_force 768211379338780.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:              trmse_coord 0.56553\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb:              trmse_force 0.27761\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: 🚀 View run FSR_Trainable_a9e43461 at: https://wandb.ai/seokjin/FSR-prediction/runs/a9e43461\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184722)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004244-a9e43461/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:                mae_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:                mae_force ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:               mape_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:               mape_force ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:                   metric █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:               rmse_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:               rmse_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:         time_this_iter_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:                timestamp ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:               tmae_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:               tmae_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:              tmape_coord ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:              tmape_force ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:              trmse_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb:              trmse_force █▆▃▁\n",
      "2023-08-11 00:43:06,482\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.494 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:43:06,491\tWARNING util.py:315 -- The `process_trial_result` operation took 2.504 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:43:06,494\tWARNING util.py:315 -- Processing trial results took 2.507 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:43:06,497\tWARNING util.py:315 -- The `process_trial_result` operation took 2.510 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_c4772750_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-42-35/wandb/run-20230811_004310-c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: Syncing run FSR_Trainable_c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184043)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: \\ 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:                mae_coord █▇▇▆▄▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:                mae_force ▆███▂▁▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:               mape_coord █▇▆▆▇▃▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:               mape_force ▆▇██▂▁▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:                   metric ███▇▃▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:               rmse_coord █▇▆▅▃▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:               rmse_force ███▇▅▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:         time_this_iter_s █▄▃▂▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:                timestamp ▁▄▅▅▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:               tmae_coord ██▇▆▄▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:               tmae_force ▅███▃▁▃▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:              tmape_coord ▆▁▂▅█▆▇▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:              tmape_force ▅███▃▁▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:              trmse_coord ███▆▃▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:              trmse_force ▇██▇▄▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:                mae_coord 6.55975\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:                mae_force 1305.86907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:               mape_coord 1.4701\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:               mape_force 2.2074749826764198e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:                   metric 0.86286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:               rmse_coord 2.80112\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:               rmse_force 809.93804\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:       time_since_restore 9.87519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:         time_this_iter_s 1.11771\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:             time_total_s 9.87519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:                timestamp 1691682194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:               tmae_coord 1.41136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:               tmae_force 0.52978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:              tmape_coord 77585467561509.34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:              tmape_force 1010991469807019.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:              trmse_coord 0.56783\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb:              trmse_force 0.29503\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: 🚀 View run FSR_Trainable_c4772750 at: https://wandb.ai/seokjin/FSR-prediction/runs/c4772750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=184972)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004310-c4772750/logs\n",
      "2023-08-11 00:43:20,455\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.613 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:43:20,457\tWARNING util.py:315 -- The `process_trial_result` operation took 2.618 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:43:20,458\tWARNING util.py:315 -- Processing trial results took 2.618 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:43:20,460\tWARNING util.py:315 -- The `process_trial_result` operation took 2.620 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_b1203ce7_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-43-02/wandb/run-20230811_004323-b1203ce7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: Syncing run FSR_Trainable_b1203ce7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b1203ce7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:                mae_coord ▁▁▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:                mae_force █▆▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:               mape_coord ▁▂▂█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:               mape_force █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:                   metric █▇▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:               rmse_coord ▁▁▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:               rmse_force █▇▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:       time_since_restore ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:         time_this_iter_s █▁▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:             time_total_s ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:               tmae_coord ▁▂▂█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:               tmae_force █▆▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:              tmape_coord █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:              tmape_force █▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:              trmse_coord ▁▂▂█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:              trmse_force █▇▆▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:                mae_coord 9.11387\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:                mae_force 1285.15202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:               mape_coord 2.32794\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:               mape_force 2.7797687795970857e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:                   metric 0.99656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:               rmse_coord 3.56882\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:               rmse_force 849.3422\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:       time_since_restore 5.36449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:         time_this_iter_s 1.38057\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:             time_total_s 5.36449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:                timestamp 1691682204\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:               tmae_coord 1.8746\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:               tmae_force 0.50599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:              tmape_coord 77375112247979.52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:              tmape_force 1180237593065515.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:              trmse_coord 0.70344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb:              trmse_force 0.29312\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: 🚀 View run FSR_Trainable_b1203ce7 at: https://wandb.ai/seokjin/FSR-prediction/runs/b1203ce7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185213)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004323-b1203ce7/logs\n",
      "2023-08-11 00:43:33,078\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.619 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:43:33,083\tWARNING util.py:315 -- The `process_trial_result` operation took 2.626 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:43:33,088\tWARNING util.py:315 -- Processing trial results took 2.631 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:43:33,091\tWARNING util.py:315 -- The `process_trial_result` operation took 2.633 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_dfe69d9f_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-43-16/wandb/run-20230811_004336-dfe69d9f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: Syncing run FSR_Trainable_dfe69d9f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/dfe69d9f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:                mae_coord 6.35359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:                mae_force 1198.49608\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:               mape_coord 1.44282\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:               mape_force 1023271489.13878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:                   metric 7.50896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:               rmse_coord 2.83721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:               rmse_force 751.6728\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:       time_since_restore 0.9115\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:         time_this_iter_s 0.9115\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:             time_total_s 0.9115\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:                timestamp 1691682210\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:               tmae_coord 11.16215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:               tmae_force 3.69523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:              tmape_coord 877.708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:              tmape_force 256.14109\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:              trmse_coord 5.21052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb:              trmse_force 2.29844\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: 🚀 View run FSR_Trainable_dfe69d9f at: https://wandb.ai/seokjin/FSR-prediction/runs/dfe69d9f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185446)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004336-dfe69d9f/logs\n",
      "2023-08-11 00:43:44,156\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.349 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:43:44,163\tWARNING util.py:315 -- The `process_trial_result` operation took 2.357 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:43:44,164\tWARNING util.py:315 -- Processing trial results took 2.358 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:43:44,165\tWARNING util.py:315 -- The `process_trial_result` operation took 2.359 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_bb462b5a_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-43-29/wandb/run-20230811_004347-bb462b5a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: Syncing run FSR_Trainable_bb462b5a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/bb462b5a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:                mae_coord 4.20241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:                mae_force 1114.60595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:               mape_coord 0.99678\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:               mape_force 577922896.79127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:                   metric 6.17791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:               rmse_coord 2.49016\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:               rmse_force 727.89478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:       time_since_restore 0.97108\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:         time_this_iter_s 0.97108\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:             time_total_s 0.97108\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:                timestamp 1691682221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:               tmae_coord 7.81838\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:               tmae_force 2.76831\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:              tmape_coord 202.08366\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:              tmape_force 55.31866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:              trmse_coord 4.48385\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb:              trmse_force 1.69407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: 🚀 View run FSR_Trainable_bb462b5a at: https://wandb.ai/seokjin/FSR-prediction/runs/bb462b5a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185671)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004347-bb462b5a/logs\n",
      "2023-08-11 00:43:56,299\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.193 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:43:56,303\tWARNING util.py:315 -- The `process_trial_result` operation took 2.197 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:43:56,305\tWARNING util.py:315 -- Processing trial results took 2.199 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:43:56,307\tWARNING util.py:315 -- The `process_trial_result` operation took 2.201 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_8c57c14c_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-43-40/wandb/run-20230811_004402-8c57c14c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: Syncing run FSR_Trainable_8c57c14c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8c57c14c\n",
      "2023-08-11 00:44:08,941\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.247 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:44:08,947\tWARNING util.py:315 -- The `process_trial_result` operation took 2.254 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:44:08,951\tWARNING util.py:315 -- Processing trial results took 2.257 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:44:08,955\tWARNING util.py:315 -- The `process_trial_result` operation took 2.261 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_ce96e157_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-43-52/wandb/run-20230811_004412-ce96e157\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: Syncing run FSR_Trainable_ce96e157\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ce96e157\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:                mae_coord █▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:                mae_force █▃▂▃▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:               mape_coord █▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:               mape_force █▃▂▃▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:                   metric █▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:               rmse_coord █▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:               rmse_force █▃▃▃▃▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:         time_this_iter_s █▆▃▃▁▆▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:                timestamp ▁▄▅▅▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:               tmae_coord █▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:               tmae_force █▂▂▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:              tmape_coord ▁███████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:              tmape_force █▂▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:              trmse_coord █▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:              trmse_force █▃▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:                mae_coord 6.41898\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:                mae_force 1350.91693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:               mape_coord 1.34113\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:               mape_force 2.770777113336992e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:                   metric 0.85187\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:               rmse_coord 2.85685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:               rmse_force 818.59233\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:       time_since_restore 8.42941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:         time_this_iter_s 0.94236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:             time_total_s 8.42941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:                timestamp 1691682256\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:               tmae_coord 1.36795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:               tmae_force 0.53255\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:              tmape_coord 78290574579194.14\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:              tmape_force 1219291034614978.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:              trmse_coord 0.56401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb:              trmse_force 0.28786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: 🚀 View run FSR_Trainable_ce96e157 at: https://wandb.ai/seokjin/FSR-prediction/runs/ce96e157\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186121)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004412-ce96e157/logs\n",
      "2023-08-11 00:44:23,196\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.128 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:44:23,208\tWARNING util.py:315 -- The `process_trial_result` operation took 2.142 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:44:23,212\tWARNING util.py:315 -- Processing trial results took 2.145 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:44:23,214\tWARNING util.py:315 -- The `process_trial_result` operation took 2.148 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_dcea2390_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-44-05/wandb/run-20230811_004427-dcea2390\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: Syncing run FSR_Trainable_dcea2390\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/dcea2390\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:                mae_coord █▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:                mae_force █▁▂▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:               mape_coord █▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:               mape_force █▁▃▃▃▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:                   metric █▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:               rmse_coord █▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:               rmse_force █▁▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:         time_this_iter_s █▅▃▃▁▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:                timestamp ▁▃▄▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:               tmae_coord █▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:               tmae_force █▁▂▂▂▂▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:              tmape_coord ▁█████▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:              tmape_force █▁▃▃▃▃▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:              trmse_coord █▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:              trmse_force █▁▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:                mae_coord 6.62935\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:                mae_force 1394.45752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:               mape_coord 1.25294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:               mape_force 2.598641083102461e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:                   metric 0.87911\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:               rmse_coord 2.87324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:               rmse_force 873.7019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:       time_since_restore 8.75897\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:         time_this_iter_s 0.87174\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:             time_total_s 8.75897\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:                timestamp 1691682270\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:               tmae_coord 1.4223\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:               tmae_force 0.58044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:              tmape_coord 76551059532753.12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:              tmape_force 1353084086613990.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:              trmse_coord 0.57122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb:              trmse_force 0.30789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: 🚀 View run FSR_Trainable_dcea2390 at: https://wandb.ai/seokjin/FSR-prediction/runs/dcea2390\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186347)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004427-dcea2390/logs\n",
      "2023-08-11 00:44:38,828\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.696 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:44:38,840\tWARNING util.py:315 -- The `process_trial_result` operation took 2.709 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:44:38,843\tWARNING util.py:315 -- Processing trial results took 2.712 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:44:38,847\tWARNING util.py:315 -- The `process_trial_result` operation took 2.716 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_75e926e4_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-44-19/wandb/run-20230811_004442-75e926e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: Syncing run FSR_Trainable_75e926e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75e926e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:                mae_coord 4.44215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:                mae_force 1313.92501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:               mape_coord 1.08549\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:               mape_force 2.3383203275829834e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:                   metric 54.72917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:               rmse_coord 2.50879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:               rmse_force 942.50306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:       time_since_restore 2.27228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:         time_this_iter_s 2.27228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:             time_total_s 2.27228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:                timestamp 1691682276\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:               tmae_coord 44.4576\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:               tmae_force 7.35641\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:              tmape_coord 4952650475166811.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:              tmape_force 7875845003443533.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:              trmse_coord 47.93448\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb:              trmse_force 6.79468\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: 🚀 View run FSR_Trainable_75e926e4 at: https://wandb.ai/seokjin/FSR-prediction/runs/75e926e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186579)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004442-75e926e4/logs\n",
      "2023-08-11 00:44:53,109\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.805 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:44:53,114\tWARNING util.py:315 -- The `process_trial_result` operation took 2.812 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:44:53,116\tWARNING util.py:315 -- Processing trial results took 2.814 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:44:53,118\tWARNING util.py:315 -- The `process_trial_result` operation took 2.816 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_d3008e43_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-44-33/wandb/run-20230811_004456-d3008e43\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: Syncing run FSR_Trainable_d3008e43\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d3008e43\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:                mae_coord 4.34236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:                mae_force 1305.39232\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:               mape_coord 1.1705\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:               mape_force 1.5606881076027405e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:                   metric 55.10449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:               rmse_coord 2.48189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:               rmse_force 978.52426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:       time_since_restore 2.69474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:         time_this_iter_s 2.69474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:             time_total_s 2.69474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:                timestamp 1691682290\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:               tmae_coord 44.6437\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:               tmae_force 6.95313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:              tmape_coord 4449238173389543.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:              tmape_force 5135269220167697.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:              trmse_coord 48.19024\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb:              trmse_force 6.91425\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: 🚀 View run FSR_Trainable_d3008e43 at: https://wandb.ai/seokjin/FSR-prediction/runs/d3008e43\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=186809)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004456-d3008e43/logs\n",
      "2023-08-11 00:45:04,551\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.321 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:45:04,556\tWARNING util.py:315 -- The `process_trial_result` operation took 2.327 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:45:04,558\tWARNING util.py:315 -- Processing trial results took 2.329 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:45:04,560\tWARNING util.py:315 -- The `process_trial_result` operation took 2.331 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_af4ab174_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-44-47/wandb/run-20230811_004507-af4ab174\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: Syncing run FSR_Trainable_af4ab174\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/af4ab174\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:                mae_coord 25.03041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:                mae_force 2995.77797\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:               mape_coord 3.90709\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:               mape_force 4.483969372630291e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:                   metric 2.37176\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:               rmse_coord 8.846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:               rmse_force 1836.81386\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:       time_since_restore 1.55204\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:         time_this_iter_s 1.55204\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:             time_total_s 1.55204\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:                timestamp 1691682302\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:               tmae_coord 5.36967\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:               tmae_force 1.11243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:              tmape_coord 78772549119320.34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:              tmape_force 1892487353747274.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:              trmse_coord 1.75389\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb:              trmse_force 0.61787\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: 🚀 View run FSR_Trainable_af4ab174 at: https://wandb.ai/seokjin/FSR-prediction/runs/af4ab174\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187037)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004507-af4ab174/logs\n",
      "2023-08-11 00:45:19,482\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.900 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:45:19,489\tWARNING util.py:315 -- The `process_trial_result` operation took 2.907 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:45:19,492\tWARNING util.py:315 -- Processing trial results took 2.910 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:45:19,493\tWARNING util.py:315 -- The `process_trial_result` operation took 2.912 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_90abffb2_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-45-00/wandb/run-20230811_004526-90abffb2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: Syncing run FSR_Trainable_90abffb2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/90abffb2\n",
      "2023-08-11 00:45:35,572\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.401 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:45:35,576\tWARNING util.py:315 -- The `process_trial_result` operation took 2.405 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:45:35,577\tWARNING util.py:315 -- Processing trial results took 2.407 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:45:35,581\tWARNING util.py:315 -- The `process_trial_result` operation took 2.411 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_d220b1c8_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-45-14/wandb/run-20230811_004539-d220b1c8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Syncing run FSR_Trainable_d220b1c8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d220b1c8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:                mae_coord █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:                mae_force █▅▅▃▂▂▂▂▂▂▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:               mape_coord █▂▂▃▂▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:               mape_force █▅▅▄▂▃▃▃▃▃▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:                   metric █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:               rmse_coord █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:               rmse_force █▆▅▄▂▂▂▂▂▂▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:       time_since_restore ▁▂▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:         time_this_iter_s █▃▂▁▁▁▂▁▂▁▁▂▂▃▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:             time_total_s ▁▂▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:                timestamp ▁▂▃▃▃▄▄▄▅▅▆▆▆▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:               tmae_coord █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:               tmae_force █▄▄▃▁▁▂▂▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:              tmape_coord ▁▇██▇▇▆▆▆▅▅▅▅▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:              tmape_force █▄▄▄▂▂▂▂▂▂▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:              trmse_coord █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:              trmse_force █▄▄▃▁▁▁▁▁▂▂▂▂▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:                mae_coord 6.09887\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:                mae_force 1027.32534\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:               mape_coord 1.29335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:               mape_force 1.5663427774954427e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:                   metric 0.81288\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:               rmse_coord 2.72564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:               rmse_force 671.52783\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:       time_since_restore 19.53507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:         time_this_iter_s 1.19951\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:             time_total_s 19.53507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:                timestamp 1691682339\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:               tmae_coord 1.32776\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:               tmae_force 0.43629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:              tmape_coord 67119079477507.78\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:              tmape_force 775968026694928.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:              trmse_coord 0.54615\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb:              trmse_force 0.26673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: 🚀 View run FSR_Trainable_90abffb2 at: https://wandb.ai/seokjin/FSR-prediction/runs/90abffb2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187275)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004526-90abffb2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:                mae_coord █▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:                mae_force █▅▄▂▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:               mape_coord █▁▁▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:               mape_force ▆█▆▂▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:                   metric █▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:               rmse_coord █▁▁▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:               rmse_force █▅▄▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:         time_this_iter_s ▅▂▁▂▁█▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:                timestamp ▁▃▄▅▅▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:               tmae_coord █▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:               tmae_force █▄▂▁▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:              tmape_coord ▁██▇▇▇▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:              tmape_force █▅▄▂▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:              trmse_coord █▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb:              trmse_force █▃▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187496)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004539-d220b1c8/logs\n",
      "2023-08-11 00:45:57,205\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.591 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:45:57,208\tWARNING util.py:315 -- The `process_trial_result` operation took 2.595 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:45:57,214\tWARNING util.py:315 -- Processing trial results took 2.600 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:45:57,216\tWARNING util.py:315 -- The `process_trial_result` operation took 2.602 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_f67d3b44_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-45-31/wandb/run-20230811_004601-f67d3b44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Syncing run FSR_Trainable_f67d3b44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f67d3b44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:                mae_coord █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:                mae_force █▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:               mape_coord █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:               mape_force █▆▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:                   metric █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:               rmse_coord █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:               rmse_force █▄▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:         time_this_iter_s ▆▂▁▁▁▄▂▃▁▂▃▃▂▁▄▂▂▃█▃▂▃▄▂▃▁▆▄▃▃▃▅▃▃▄▂▂▂▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:               tmae_coord █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:               tmae_force █▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:              tmape_coord ▁▆████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:              tmape_force █▅▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:              trmse_coord █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:              trmse_force █▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:                mae_coord 5.42776\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:                mae_force 789.54074\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:               mape_coord 1.1884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:               mape_force 9.516780489597234e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:                   metric 0.71664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:               rmse_coord 2.68192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:               rmse_force 562.32934\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:       time_since_restore 106.80491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:         time_this_iter_s 1.35465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:             time_total_s 106.80491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:                timestamp 1691682361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:               tmae_coord 1.14774\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:               tmae_force 0.31278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:              tmape_coord 71436983625607.48\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:              tmape_force 462637600587568.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:              trmse_coord 0.52035\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb:              trmse_force 0.19629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: 🚀 View run FSR_Trainable_8c57c14c at: https://wandb.ai/seokjin/FSR-prediction/runs/8c57c14c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=185898)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004402-8c57c14c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:                mae_coord █▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:                mae_force █▃▃▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:               mape_coord █▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:               mape_force █▃▄▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:                   metric █▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:               rmse_coord █▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:               rmse_force █▃▃▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:         time_this_iter_s █▄▄▅▅▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:                timestamp ▁▄▄▅▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:               tmae_coord █▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:               tmae_force █▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:              tmape_coord █▆▆▆▅▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:              tmape_force █▃▃▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:              trmse_coord █▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb:              trmse_force █▃▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004601-f67d3b44/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004601-f67d3b44/logs\n",
      "2023-08-11 00:46:14,499\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.747 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:46:14,506\tWARNING util.py:315 -- The `process_trial_result` operation took 2.755 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:46:14,512\tWARNING util.py:315 -- Processing trial results took 2.761 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:46:14,514\tWARNING util.py:315 -- The `process_trial_result` operation took 2.764 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187751)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_e8d19252_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-45-53/wandb/run-20230811_004615-e8d19252\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: Syncing run FSR_Trainable_e8d19252\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e8d19252\n",
      "2023-08-11 00:46:29,611\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.378 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:46:29,619\tWARNING util.py:315 -- The `process_trial_result` operation took 3.384 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:46:29,627\tWARNING util.py:315 -- Processing trial results took 3.394 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:46:29,631\tWARNING util.py:315 -- The `process_trial_result` operation took 3.398 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_80318608_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-46-08/wandb/run-20230811_004631-80318608\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: Syncing run FSR_Trainable_80318608\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/80318608\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:                mae_coord ▁█▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:                mae_force █▅▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:               mape_coord ▁█▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:               mape_force █▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:                   metric ▁▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:               rmse_coord ▁▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:               rmse_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:       time_since_restore ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:         time_this_iter_s ▂▁▂█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:             time_total_s ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:                timestamp ▁▄▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:               tmae_coord ▁███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:               tmae_force █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:              tmape_coord █▆▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:              tmape_force █▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:              trmse_coord ▁▅██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:              trmse_force █▅▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:                mae_coord 7.94244\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:                mae_force 1311.88437\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:               mape_coord 1.5382\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:               mape_force 2.347040563093095e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:                   metric 0.97607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:               rmse_coord 3.25277\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:               rmse_force 797.5406\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:       time_since_restore 15.95647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:         time_this_iter_s 5.10097\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:             time_total_s 15.95647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:                timestamp 1691682387\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:               tmae_coord 1.71657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:               tmae_force 0.54191\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:              tmape_coord 62491919939887.14\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:              tmape_force 1065556415109820.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:              trmse_coord 0.66975\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb:              trmse_force 0.30632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: 🚀 View run FSR_Trainable_e8d19252 at: https://wandb.ai/seokjin/FSR-prediction/runs/e8d19252\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=187988)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004615-e8d19252/logs\n",
      "2023-08-11 00:46:44,144\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.667 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:46:44,150\tWARNING util.py:315 -- The `process_trial_result` operation took 2.675 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:46:44,154\tWARNING util.py:315 -- Processing trial results took 2.678 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:46:44,157\tWARNING util.py:315 -- The `process_trial_result` operation took 2.681 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_ad7a4599_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-46-20/wandb/run-20230811_004645-ad7a4599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: Syncing run FSR_Trainable_ad7a4599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ad7a4599\n",
      "2023-08-11 00:46:56,961\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.024 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:46:56,967\tWARNING util.py:315 -- The `process_trial_result` operation took 3.031 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:46:56,970\tWARNING util.py:315 -- Processing trial results took 3.034 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:46:56,973\tWARNING util.py:315 -- The `process_trial_result` operation took 3.037 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_0263d11d_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-46-36/wandb/run-20230811_004659-0263d11d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: Syncing run FSR_Trainable_0263d11d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0263d11d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:                mae_coord ▁▂▂█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:                mae_force █▄▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:               mape_coord ▁▃▃█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:               mape_force █▄▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:                   metric ▂▁▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:               rmse_coord ▁▁▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:               rmse_force █▆▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:         time_this_iter_s █▃▁▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:                timestamp ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:               tmae_coord ▁▂▂█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:               tmae_force █▃▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:              tmape_coord ███▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:              tmape_force █▃▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:              trmse_coord ▁▁▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:              trmse_force █▆▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:                mae_coord 9.42992\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:                mae_force 1348.52054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:               mape_coord 1.7224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:               mape_force 2.409236179041336e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:                   metric 1.0975\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:               rmse_coord 3.83838\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:               rmse_force 882.04219\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:       time_since_restore 16.26685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:         time_this_iter_s 4.17775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:             time_total_s 16.26685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:                timestamp 1691682416\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:               tmae_coord 2.0284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:               tmae_force 0.52904\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:              tmape_coord 71866636825077.84\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:              tmape_force 1129489799243560.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:              trmse_coord 0.79652\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb:              trmse_force 0.30098\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: 🚀 View run FSR_Trainable_ad7a4599 at: https://wandb.ai/seokjin/FSR-prediction/runs/ad7a4599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188514)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004645-ad7a4599/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: Waiting for W&B process to finish... (success).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:                mae_coord █▇█▄▆▅▄▄▅▄▄▃▄▃▃▃▄▃▃▂▃▂▃▃▃▃▃▂▂▂▃▁▂▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:                mae_force █▇▆▄▄▃▂▂▃▁▂▃▃▃▃▃▃▂▂▂▂▂▁▂▁▁▁▁▂▁▂▂▂▁▃▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:               mape_coord █▇█▅▇▆▅▄▄▄▂▂▃▂▂▁▃▃▂▁▁▃▂▃▂▂▃▃▃▃▃▂▂▂▄▁▂▂▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:               mape_force ██▆▅▄▃▂▂▃▁▂▃▂▃▃▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▂▂▂▁▂▂▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:                   metric █▇▇▄▄▄▃▂▃▂▂▂▃▂▂▂▂▂▂▁▂▂▁▂▂▂▂▂▂▂▂▂▁▁▄▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:               rmse_coord ▇▆█▄▅▅▄▄▄▄▃▃▄▄▃▂▄▃▃▃▃▂▃▃▃▃▃▃▂▂▃▁▂▁▄▁▁▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:               rmse_force █▇▆▅▄▃▂▂▂▂▂▂▂▃▂▃▃▂▂▁▂▂▁▂▂▁▁▂▂▁▂▂▂▁▃▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:         time_this_iter_s ▄▅▂▁▃▁▂▂▅▆▃▂▂▃▃▃▅▅▄▂▂▁▁▂▂▃▃▃▅▃▃▅▅▃▄▂▆█▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:               tmae_coord ▇▇█▄▆▅▄▄▅▄▄▃▄▃▃▃▃▃▂▂▃▂▂▂▃▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:               tmae_force █▇▆▄▄▃▂▂▃▂▂▃▃▃▃▃▂▂▃▂▂▂▁▂▁▂▁▁▂▁▂▂▂▁▃▃▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:              tmape_coord ▅▇▄█▅▃▄▃▁▂▂▂▁▂▂▁▃▃▄▂▂▆▃▂▂▃▄▄▅▄▄▄▄▄▄▃▃▄▆▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:              tmape_force █▇▆▄▄▃▂▂▃▂▂▃▃▃▃▂▂▂▂▂▃▂▁▂▁▂▁▁▁▁▂▂▂▁▂▃▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:              trmse_coord ▇▆█▄▅▅▃▃▄▃▃▂▃▃▃▂▃▃▂▂▂▂▂▂▃▂▃▂▂▂▃▁▂▁▃▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=181836)\u001b[0m wandb:              trmse_force █▇▆▄▄▃▂▂▂▂▂▂▂▂▂▃▂▂▂▁▂▂▁▂▁▂▁▂▂▂▂▃▂▁▅▂▁▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004659-0263d11d/logs\n",
      "2023-08-11 00:47:13,157\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.468 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:47:13,162\tWARNING util.py:315 -- The `process_trial_result` operation took 2.474 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:47:13,163\tWARNING util.py:315 -- Processing trial results took 2.476 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:47:13,165\tWARNING util.py:315 -- The `process_trial_result` operation took 2.478 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: iterations_since_restore 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:                mae_coord 3.83395\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:                mae_force 1243.98911\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:               mape_coord 1.10314\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:               mape_force 496747604.65753\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:                   metric 6.20889\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:               rmse_coord 2.35646\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:               rmse_force 829.20975\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:       time_since_restore 3.17325\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:         time_this_iter_s 3.17325\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:             time_total_s 3.17325\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:                timestamp 1691682413\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:               tmae_coord 7.28164\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:               tmae_force 2.9087\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:              tmape_coord 522388982252142.2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:              tmape_force 32.867\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:       training_iteration 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:              trmse_coord 4.48525\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb:              trmse_force 1.72364\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: 🚀 View run FSR_Trainable_0263d11d at: https://wandb.ai/seokjin/FSR-prediction/runs/0263d11d\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188731)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004659-0263d11d/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_f58d2a52_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-46-50/wandb/run-20230811_004715-f58d2a52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: Syncing run FSR_Trainable_f58d2a52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f58d2a52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: 🚀 View run FSR_Trainable_f58d2a52 at: https://wandb.ai/seokjin/FSR-prediction/runs/f58d2a52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188990)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004715-f58d2a52/logs\n",
      "2023-08-11 00:47:26,073\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.277 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:47:26,077\tWARNING util.py:315 -- The `process_trial_result` operation took 2.282 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:47:26,079\tWARNING util.py:315 -- Processing trial results took 2.284 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:47:26,081\tWARNING util.py:315 -- The `process_trial_result` operation took 2.286 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_3fbd5160_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-47-08/wandb/run-20230811_004726-3fbd5160\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: Syncing run FSR_Trainable_3fbd5160\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3fbd5160\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:                mae_coord ▅█▇▂▅▃▆▅▅▄▄▃▃▂▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:                mae_force ▇██▆▅▄▇▃▃▃▂▂▁▂▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:               mape_coord ▃█▇▂▄▃▇▅▄▄▄▃▄▂▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:               mape_force ▇██▆▅▄▇▄▄▃▃▁▁▁▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:                   metric ▆█▇▃▆▄▆▃▂▃▂▂▂▁▁▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:               rmse_coord ▆█▇▁▇▄▅▄▄▄▃▃▃▂▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:               rmse_force ███▇▅▅▅▂▁▂▂▂▁▁▂▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:       time_since_restore ▁▂▂▃▃▄▄▅▅▆▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:         time_this_iter_s █▆▃▅▄▄▅▄▃▁▁▃▁▁▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:             time_total_s ▁▂▂▃▃▄▄▅▅▆▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:                timestamp ▁▂▂▃▄▄▅▅▆▆▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:               tmae_coord ▄█▇▂▆▃▆▅▄▄▄▃▃▂▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:               tmae_force ▇██▅▄▂█▃▂▂▂▁▁▁▁▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:              tmape_coord ▇▅▅█▂▅▁▂▁▂▂▂▂▁▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:              tmape_force ▇██▅▄▃▇▄▃▂▂▁▂▁▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:              trmse_coord ▅█▇▂▇▄▅▄▄▄▂▂▂▁▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:              trmse_force ▇██▆▄▄▇▂▁▃▂▂▂▁▂▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:                mae_coord 6.70781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:                mae_force 1159.67997\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:               mape_coord 1.40529\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:               mape_force 1.4225403280735862e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:                   metric 0.89507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:               rmse_coord 2.87786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:               rmse_force 840.9263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:       time_since_restore 56.53027\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:         time_this_iter_s 3.24482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:             time_total_s 56.53027\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:                timestamp 1691682445\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:               tmae_coord 1.44017\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:               tmae_force 0.47908\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:              tmape_coord 78472562234225.98\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:              tmape_force 747579957184123.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:              trmse_coord 0.58154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb:              trmse_force 0.31353\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: 🚀 View run FSR_Trainable_80318608 at: https://wandb.ai/seokjin/FSR-prediction/runs/80318608\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=188209)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004631-80318608/logs\n",
      "2023-08-11 00:47:37,623\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.544 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:47:37,627\tWARNING util.py:315 -- The `process_trial_result` operation took 2.549 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:47:37,632\tWARNING util.py:315 -- Processing trial results took 2.554 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:47:37,634\tWARNING util.py:315 -- The `process_trial_result` operation took 2.556 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_0daad9a2_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-47-18/wandb/run-20230811_004738-0daad9a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: Syncing run FSR_Trainable_0daad9a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0daad9a2\n",
      "2023-08-11 00:47:52,262\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.639 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:47:52,268\tWARNING util.py:315 -- The `process_trial_result` operation took 3.646 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:47:52,301\tWARNING util.py:315 -- Processing trial results took 3.678 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:47:52,304\tWARNING util.py:315 -- The `process_trial_result` operation took 3.682 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_580494a3_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-47-30/wandb/run-20230811_004753-580494a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: Syncing run FSR_Trainable_580494a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/580494a3\n",
      "2023-08-11 00:48:07,064\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.183 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:48:07,074\tWARNING util.py:315 -- The `process_trial_result` operation took 3.194 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:48:07,077\tWARNING util.py:315 -- Processing trial results took 3.197 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:48:07,086\tWARNING util.py:315 -- The `process_trial_result` operation took 3.205 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_22929497_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-47-42/wandb/run-20230811_004811-22929497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: Syncing run FSR_Trainable_22929497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/22929497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:                mae_coord █▇▆▅▅▄▄▃▃▃▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:                mae_force █▇▆▅▄▄▃▃▃▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:               mape_coord █▅▄▃▃▃▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:               mape_force █▇▆▅▄▄▃▃▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:                   metric █▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:               rmse_coord █▇▆▅▅▄▄▃▃▃▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:               rmse_force █▆▅▅▄▄▃▃▃▃▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:         time_this_iter_s █▂▆▃▂▃▂▄▁▄▃▃▂▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:                timestamp ▁▂▂▃▃▄▄▅▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:               tmae_coord █▆▅▅▄▄▃▃▃▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:               tmae_force █▇▆▅▄▄▃▃▃▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:              tmape_coord █▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:              tmape_force █▇▆▅▄▃▃▃▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:              trmse_coord █▆▅▅▄▄▄▃▃▃▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:              trmse_force █▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:                mae_coord 4.67138\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:                mae_force 1361.65016\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:               mape_coord 1.21916\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:               mape_force 2.683633673384846e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:                   metric 0.80099\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:               rmse_coord 2.49582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:               rmse_force 898.27818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:       time_since_restore 74.5158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:         time_this_iter_s 4.46208\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:             time_total_s 74.5158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:                timestamp 1691682542\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:               tmae_coord 1.01725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:               tmae_force 0.49322\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:              tmape_coord 83462723790010.12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:              tmape_force 1039858758853264.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:              trmse_coord 0.51166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb:              trmse_force 0.28933\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: 🚀 View run FSR_Trainable_580494a3 at: https://wandb.ai/seokjin/FSR-prediction/runs/580494a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189661)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004753-580494a3/logs\n",
      "2023-08-11 00:49:28,326\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 4.069 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:49:28,328\tWARNING util.py:315 -- The `process_trial_result` operation took 4.072 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:49:28,330\tWARNING util.py:315 -- Processing trial results took 4.074 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:49:28,333\tWARNING util.py:315 -- The `process_trial_result` operation took 4.078 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_573038ac_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-47-58/wandb/run-20230811_004928-573038ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: Syncing run FSR_Trainable_573038ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/573038ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:                mae_coord █▄▂▂▂▁▁▁▁▁▁▁▁▁▂▃▅▆▆▇▆▆▆▆▅▅▅▅▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:                mae_force ██▇▇▇▇▇▇▇▆▆▆▆▅▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:               mape_coord █▄▂▂▁▁▁▁▁▁▁▂▂▂▃▄▆▇▇███▇▇▇▇▆▆▆▆▆▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:               mape_force █▇▇▇▇▇▆▆▆▆▆▅▅▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:                   metric █▇▇▇▇▆▆▆▆▆▆▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:               rmse_coord █▅▄▄▃▃▃▃▃▃▃▃▃▂▃▃▄▅▅▅▅▅▄▄▃▃▃▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:               rmse_force █▇▇▇▇▇▇▇▆▆▆▆▆▅▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:         time_this_iter_s ▄▁▃▅▃▄▃▂▃▂▃▃▃▃▃▃▂▂▃▃▃▃▄▅█▆▂▂▁▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:               tmae_coord █▄▃▂▂▂▁▁▁▁▁▁▁▁▂▃▄▅▆▆▆▅▅▅▅▄▄▄▄▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:               tmae_force ██▇▇▇▇▇▇▆▆▆▆▅▅▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:              tmape_coord █▆▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:              tmape_force ██▇▇▇▇▆▆▆▆▆▅▅▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:              trmse_coord █▆▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▅▅▅▅▄▄▄▃▃▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:              trmse_force ██▇▇▇▇▇▆▆▆▆▆▅▅▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:                mae_coord 4.79424\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:                mae_force 1045.58468\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:               mape_coord 1.27489\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:               mape_force 1.756733346784795e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:                   metric 0.74935\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:               rmse_coord 2.49671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:               rmse_force 726.74124\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:       time_since_restore 152.56871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:         time_this_iter_s 4.51588\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:             time_total_s 152.56871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:                timestamp 1691682609\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:               tmae_coord 1.03638\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:               tmae_force 0.39007\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:              tmape_coord 80890459315895.95\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:              tmape_force 704061040224869.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:              trmse_coord 0.50558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb:              trmse_force 0.24377\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: 🚀 View run FSR_Trainable_0daad9a2 at: https://wandb.ai/seokjin/FSR-prediction/runs/0daad9a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189441)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004738-0daad9a2/logs\n",
      "2023-08-11 00:50:32,897\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.162 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:50:32,903\tWARNING util.py:315 -- The `process_trial_result` operation took 3.168 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:50:32,905\tWARNING util.py:315 -- Processing trial results took 3.170 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:50:32,912\tWARNING util.py:315 -- The `process_trial_result` operation took 3.177 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_8d1c2f35_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-49-18/wandb/run-20230811_005033-8d1c2f35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: Syncing run FSR_Trainable_8d1c2f35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8d1c2f35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:                mae_coord 3.85052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:                mae_force 1030.45725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:               mape_coord 1.16629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:               mape_force 6.3330841838725256e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:                   metric 39.72403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:               rmse_coord 2.36374\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:               rmse_force 905.38015\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:       time_since_restore 5.52543\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:         time_this_iter_s 5.52543\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:             time_total_s 5.52543\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:                timestamp 1691682629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:               tmae_coord 27.89166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:               tmae_force 5.5582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:              tmape_coord 1840595086734642.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:              tmape_force 2321338652300145.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:              trmse_coord 33.254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb:              trmse_force 6.47002\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: 🚀 View run FSR_Trainable_8d1c2f35 at: https://wandb.ai/seokjin/FSR-prediction/runs/8d1c2f35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190453)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_005033-8d1c2f35/logs\n",
      "2023-08-11 00:50:54,105\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.032 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:50:54,110\tWARNING util.py:315 -- The `process_trial_result` operation took 2.038 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:50:54,112\tWARNING util.py:315 -- Processing trial results took 2.040 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:50:54,113\tWARNING util.py:315 -- The `process_trial_result` operation took 2.041 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_3a9efe0a_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-50-24/wandb/run-20230811_005054-3a9efe0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: Syncing run FSR_Trainable_3a9efe0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3a9efe0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:                mae_coord 3.84098\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:                mae_force 1054.1399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:               mape_coord 1.15528\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:               mape_force 6.014625921571062e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:                   metric 39.69306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:               rmse_coord 2.37358\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:               rmse_force 879.34717\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:       time_since_restore 5.55498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:         time_this_iter_s 5.55498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:             time_total_s 5.55498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:                timestamp 1691682652\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:               tmae_coord 27.88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:               tmae_force 5.66274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:              tmape_coord 2088436773238550.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:              tmape_force 2251695618758733.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:              trmse_coord 33.16168\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb:              trmse_force 6.53139\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: 🚀 View run FSR_Trainable_3a9efe0a at: https://wandb.ai/seokjin/FSR-prediction/runs/3a9efe0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_005054-3a9efe0a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-08-11 00:51:18,668\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.922 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:51:18,669\tWARNING util.py:315 -- The `process_trial_result` operation took 1.924 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:51:18,673\tWARNING util.py:315 -- Processing trial results took 1.928 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:51:18,674\tWARNING util.py:315 -- The `process_trial_result` operation took 1.928 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_d50fa8ea_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-50-46/wandb/run-20230811_005118-d50fa8ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: Syncing run FSR_Trainable_d50fa8ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d50fa8ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:                mae_coord █▆▅▄▄▃▃▃▂▂▂▂▁▁▁▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:                mae_force ███▇▇▇▇▇▇▆▆▆▅▅▄▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:               mape_coord █▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:               mape_force ██▇▇▇▇▇▆▆▆▆▆▅▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:                   metric ██▇▇▇▇▆▆▆▆▆▅▅▄▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:               rmse_coord █▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:               rmse_force ██▇▇▇▇▇▇▇▆▆▆▅▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:         time_this_iter_s ▅█▅▁▂▁▁▁▂▂▁▂▃▃▂▂▂▃▁▁▂▃▂▂▂▃▂▁▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:               tmae_coord █▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▃▃▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:               tmae_force ██▇▇▇▇▆▆▆▆▆▅▅▄▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:              tmape_coord █▅▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:              tmape_force ██▇▇▇▆▆▆▆▆▅▅▅▄▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:              trmse_coord █▇▇▇▇▆▆▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:              trmse_force ██▇▇▇▇▇▆▆▆▆▆▅▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:                mae_coord 4.68979\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:                mae_force 1037.15746\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:               mape_coord 1.24008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:               mape_force 1.7867729508060052e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:                   metric 0.74548\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:               rmse_coord 2.46909\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:               rmse_force 718.45238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:       time_since_restore 151.59321\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:         time_this_iter_s 4.32423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:             time_total_s 151.59321\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:                timestamp 1691682720\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:               tmae_coord 1.02677\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:               tmae_force 0.38759\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:              tmape_coord 81019867906013.77\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:              tmape_force 709594152821738.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:              trmse_coord 0.50399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb:              trmse_force 0.2415\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: 🚀 View run FSR_Trainable_573038ac at: https://wandb.ai/seokjin/FSR-prediction/runs/573038ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190160)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004928-573038ac/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-08-11 00:52:25,578\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.801 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:52:25,582\tWARNING util.py:315 -- The `process_trial_result` operation took 2.806 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:52:25,592\tWARNING util.py:315 -- Processing trial results took 2.816 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:52:25,596\tWARNING util.py:315 -- The `process_trial_result` operation took 2.820 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_bbb73642_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-51-08/wandb/run-20230811_005224-bbb73642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: Syncing run FSR_Trainable_bbb73642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/bbb73642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:                mae_coord █▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:                mae_force █▇▆▅▅▄▄▄▄▃▃▃▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:               mape_coord █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:               mape_force █▇▆▅▅▄▄▄▃▃▃▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:                   metric █▆▅▄▄▄▄▃▃▃▃▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:               rmse_coord █▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:               rmse_force █▆▅▅▄▄▄▄▃▃▃▃▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:         time_this_iter_s █▃▄▃▃▁▁▄▁▅▄▂▃▂▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:                timestamp ▁▂▂▃▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:               tmae_coord █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:               tmae_force █▆▅▅▅▄▄▄▃▃▃▃▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:              tmape_coord █▆▅▄▃▃▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:              tmape_force █▆▅▅▄▄▄▃▃▃▃▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:              trmse_coord █▅▄▃▃▃▃▃▃▃▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:              trmse_force █▆▅▅▅▄▄▄▃▃▃▃▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:                mae_coord 4.66451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:                mae_force 1363.55938\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:               mape_coord 1.21272\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:               mape_force 2.651884759435659e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:                   metric 0.8056\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:               rmse_coord 2.49393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:               rmse_force 911.98564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:       time_since_restore 109.7475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:         time_this_iter_s 6.62783\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:             time_total_s 109.7475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:                timestamp 1691682781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:               tmae_coord 1.01892\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:               tmae_force 0.48939\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:              tmape_coord 83971400890878.16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:              tmape_force 1000970932693398.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:              trmse_coord 0.5108\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb:              trmse_force 0.2948\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: 🚀 View run FSR_Trainable_d50fa8ea at: https://wandb.ai/seokjin/FSR-prediction/runs/d50fa8ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=190926)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_005118-d50fa8ea/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:                mae_coord █▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:                mae_force █▆▅▄▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:               mape_coord █▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:               mape_force █▇▅▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:                   metric █▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:               rmse_coord █▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:               rmse_force █▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:         time_this_iter_s █▃▃▂▄▂▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:               tmae_coord █▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:               tmae_force ██▆▄▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:              tmape_coord █▃▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:              tmape_force ▃█▆▄▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:              trmse_coord █▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:              trmse_force █▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:                mae_coord 5.01754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:                mae_force 1426.72981\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:               mape_coord 1.2501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:               mape_force 2.8284650928719575e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:                   metric 0.83053\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:               rmse_coord 2.55998\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:               rmse_force 936.80281\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:       time_since_restore 54.86146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:         time_this_iter_s 6.27369\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:             time_total_s 54.86146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:                timestamp 1691682793\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:               tmae_coord 1.09739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:               tmae_force 0.51769\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:              tmape_coord 82600463466911.16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:              tmape_force 1099705181442238.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:              trmse_coord 0.52753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb:              trmse_force 0.30301\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: 🚀 View run FSR_Trainable_bbb73642 at: https://wandb.ai/seokjin/FSR-prediction/runs/bbb73642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191196)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_005224-bbb73642/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_08ece1c7_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-52-14/wandb/run-20230811_005324-08ece1c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: Syncing run FSR_Trainable_08ece1c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/08ece1c7\n",
      "2023-08-11 00:53:26,079\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.486 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:53:26,084\tWARNING util.py:315 -- The `process_trial_result` operation took 2.492 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:53:26,085\tWARNING util.py:315 -- Processing trial results took 2.493 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:53:26,086\tWARNING util.py:315 -- The `process_trial_result` operation took 2.495 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:53:35,433\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.652 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:53:35,438\tWARNING util.py:315 -- The `process_trial_result` operation took 2.658 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:53:35,440\tWARNING util.py:315 -- Processing trial results took 2.660 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:53:35,442\tWARNING util.py:315 -- The `process_trial_result` operation took 2.662 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_1a98df0c_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-53-15/wandb/run-20230811_005338-1a98df0c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Syncing run FSR_Trainable_1a98df0c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1a98df0c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: | 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:                mae_coord █▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:                mae_force █▆▃▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:               mape_coord █▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:               mape_force ▁█▅▄▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:                   metric █▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:               rmse_coord █▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:               rmse_force █▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:         time_this_iter_s █▃▄▁▁▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:                timestamp ▁▂▄▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:               tmae_coord █▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:               tmae_force ██▃▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:              tmape_coord ▁█▇▇▆▆▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:              tmape_force ▁█▄▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:              trmse_coord █▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:              trmse_force █▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:                mae_coord 4.95041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:                mae_force 1422.36124\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:               mape_coord 1.23723\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:               mape_force 2.771658034815889e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:                   metric 0.83157\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:               rmse_coord 2.56307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:               rmse_force 943.16593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:       time_since_restore 55.08242\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:         time_this_iter_s 6.38127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:             time_total_s 55.08242\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:                timestamp 1691682855\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:               tmae_coord 1.08289\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:               tmae_force 0.51341\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:              tmape_coord 82535475628403.95\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:              tmape_force 1072226511438861.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:              trmse_coord 0.52846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb:              trmse_force 0.30311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: 🚀 View run FSR_Trainable_08ece1c7 at: https://wandb.ai/seokjin/FSR-prediction/runs/08ece1c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191467)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_005324-08ece1c7/logs\n",
      "2023-08-11 00:54:35,942\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.289 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:54:35,948\tWARNING util.py:315 -- The `process_trial_result` operation took 2.296 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:54:35,950\tWARNING util.py:315 -- Processing trial results took 2.298 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:54:35,952\tWARNING util.py:315 -- The `process_trial_result` operation took 2.301 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_48c46c0b_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-53-30/wandb/run-20230811_005439-48c46c0b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: Syncing run FSR_Trainable_48c46c0b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/48c46c0b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:                mae_coord █▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▂▂▁▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:                mae_force ▇█▅▃▂▄▃▃▂▂▂▂▂▁▂▁▂▁▁▁▁▂▂▁▂▂▂▂▂▂▁▂▂▂▂▃▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:               mape_coord █▅▅▄▃▂▂▂▂▂▂▂▃▂▂▁▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:               mape_force ▅█▅▄▂▅▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▂▂▂▂▂▁▁▂▁▁▂▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:                   metric █▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▂▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:               rmse_coord █▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▁▁▁▂▂▁▁▁▁▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:               rmse_force █▇▄▃▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:         time_this_iter_s ▅▂▅▄▃▃▅▄▃▄█▇▂▃▁▄▂▂▃▃▄▃▃▃▅▃▃▃▂▄▅▁▂▁▁▃▂▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:               tmae_coord █▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▂▂▁▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:               tmae_force ▆█▅▄▃▅▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▂▂▂▂▂▃▂▃▂▁▂▂▂▂▃▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:              tmape_coord ▁█▆▅▄▃▄▂▂▃▂▃▃▃▄▃▃▃▃▃▃▃▄▃▃▄▄▃▄▃▃▄▃▂▃▃▄▂▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:              tmape_force ▃█▅▄▃▅▄▃▃▂▂▂▂▂▂▂▂▁▁▁▂▂▂▁▁▂▂▂▂▂▁▁▂▁▁▂▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:              trmse_coord █▄▄▃▃▂▂▂▂▂▂▂▂▁▂▁▂▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:              trmse_force █▇▄▃▂▃▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▂▁▁▂▂▂▂▂▂▁▂▂▂▂▂▃▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:                mae_coord 5.3052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:                mae_force 747.40268\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:               mape_coord 1.20196\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:               mape_force 8.2895352923858e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:                   metric 0.69355\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:               rmse_coord 2.6316\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:               rmse_force 546.4187\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:       time_since_restore 474.50818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:         time_this_iter_s 4.78671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:             time_total_s 474.50818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:                timestamp 1691682928\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:               tmae_coord 1.12348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:               tmae_force 0.27521\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:              tmape_coord 75149236785981.98\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:              tmape_force 353797499619630.44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:              trmse_coord 0.5119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb:              trmse_force 0.18165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: 🚀 View run FSR_Trainable_3fbd5160 at: https://wandb.ai/seokjin/FSR-prediction/runs/3fbd5160\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189208)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004726-3fbd5160/logs\n",
      "2023-08-11 00:55:50,361\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.666 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:55:50,364\tWARNING util.py:315 -- The `process_trial_result` operation took 2.670 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:55:50,367\tWARNING util.py:315 -- Processing trial results took 2.672 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:55:50,368\tWARNING util.py:315 -- The `process_trial_result` operation took 2.674 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_3433d7fa_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-54-30/wandb/run-20230811_005553-3433d7fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: Syncing run FSR_Trainable_3433d7fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3433d7fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:                mae_coord █▃▁▁▁▁▂▃▃▃▃▃▄▄▄▄▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:                mae_force ██▇▇▆▆▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:               mape_coord █▅▅▄▅▅▅▆▅▅▅▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:               mape_force ██▇▇▆▆▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:                   metric █▇▇▆▆▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:               rmse_coord █▄▃▃▃▃▂▂▂▁▁▁▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:               rmse_force ██▇▇▆▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:         time_this_iter_s ▇▄▂▃▂▄▂▇▁▁▂▂▄▂▁▅▄▄▃▂▃▄▃▃▂▂▁▃▁▂▂▃▆▃▃▂▃█▅▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:               tmae_coord █▃▁▁▁▁▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:               tmae_force █▇▇▇▆▅▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:              tmape_coord █▇▆▅▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:              tmape_force █▇▇▆▆▅▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:              trmse_coord █▅▄▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:              trmse_force █▇▇▇▆▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:                mae_coord 4.77914\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:                mae_force 759.21624\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:               mape_coord 1.11783\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:               mape_force 9.889178632389349e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:                   metric 0.68866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:               rmse_coord 2.46949\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:               rmse_force 551.22303\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:       time_since_restore 463.48656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:         time_this_iter_s 4.54836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:             time_total_s 463.48656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:                timestamp 1691682961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:               tmae_coord 1.02513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:               tmae_force 0.29285\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:              tmape_coord 75381671764227.56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:              tmape_force 432979227638978.44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:              trmse_coord 0.49712\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb:              trmse_force 0.19154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: 🚀 View run FSR_Trainable_22929497 at: https://wandb.ai/seokjin/FSR-prediction/runs/22929497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=189879)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_004811-22929497/logs\n",
      "2023-08-11 00:56:22,859\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.922 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:56:22,864\tWARNING util.py:315 -- The `process_trial_result` operation took 2.928 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:56:22,866\tWARNING util.py:315 -- Processing trial results took 2.930 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:56:22,877\tWARNING util.py:315 -- The `process_trial_result` operation took 2.941 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_fb304c25_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-55-44/wandb/run-20230811_005626-fb304c25\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: Syncing run FSR_Trainable_fb304c25\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fb304c25\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:                mae_coord ▁▃▂▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▁▁▁▁▁▂▂▃▄▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:                mae_force █▅▄▃▃▂▂▂▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:               mape_coord ██▇▇▇▆▅▅▅▄▄▄▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:               mape_force █▅▅▆▆▅▄▄▄▄▄▅▅▅▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:                   metric █▄▂▂▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:               rmse_coord ▂▂▁▂▂▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:               rmse_force █▅▃▂▂▁▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:         time_this_iter_s █▄▃▁▂▂▁▂▂▂▂▂▂▁▂▁▂▂▁▂▁▁▁▂▂▁▁▂▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:               tmae_coord ▂▂▁▃▃▂▂▂▂▂▂▂▂▃▃▃▂▂▂▂▂▃▃▃▄▄▅▅▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:               tmae_force ▆▆▇▆▅▅▅▆▆▇▇██████▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:              tmape_coord █▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:              tmape_force ▁▃▅▇▇▇▇▇▇▇███████▇▇▇▇▆▆▆▅▅▅▄▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:              trmse_coord ▄▂▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:              trmse_force █▄▄▃▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:                mae_coord 5.88433\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:                mae_force 737.74686\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:               mape_coord 1.26192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:               mape_force 1.23025171216837e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:                   metric 0.72531\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:               rmse_coord 2.72662\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:               rmse_force 455.37079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:       time_since_restore 70.58037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:         time_this_iter_s 2.18232\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:             time_total_s 70.58037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:                timestamp 1691683051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:               tmae_coord 1.24054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:               tmae_force 0.32835\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:              tmape_coord 75914930625022.28\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:              tmape_force 664670214653148.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:              trmse_coord 0.53671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb:              trmse_force 0.1886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: 🚀 View run FSR_Trainable_fb304c25 at: https://wandb.ai/seokjin/FSR-prediction/runs/fb304c25\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192503)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_005626-fb304c25/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:                mae_coord ▆▅▅▄▄▄▄▄▄▄▄▅▆▆▆▇████▇▇▇▆▇▅▅▅▄▃▃▃▃▂▅▂▃▄▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:                mae_force █▄▄▄▅▅▅▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▁▂▂▂▁▂▁▂▁▁▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:               mape_coord █▇▇▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▂▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:               mape_force ▆▅▆▆▇███▇▇▆▆▆▆▅▅▅▄▄▅▄▄▃▃▃▃▃▃▃▃▃▂▃▂▂▂▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:                   metric █▄▃▃▃▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▂▁▂▁▁▁▁▁▂▂▃▂▃▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:               rmse_coord ▄▂▁▁▁▂▂▂▃▃▄▄▄▅▅▆▆▇▇▆▆▇▇▆▇▅▆▆▅▅▅▄▅▅▆▅▇█▆▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:               rmse_force █▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:         time_this_iter_s ▇▅▂▄▄▃▃▄▃▂█▅▃▅▄▃▃▆▄▇▇▅▆▄▃▆█▄▃▂▂▃▂▂▂▂▁▂▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:               tmae_coord ▇▅▄▄▄▄▄▄▄▅▅▆▇▇▇████▇▇▇▆▆▆▄▅▅▄▃▃▃▃▃▅▂▄▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:               tmae_force ▅▅▆▆▇███▇▇▆▆▅▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:              tmape_coord █▇▇▆▅▅▅▅▅▅▅▄▄▄▃▃▂▂▂▂▂▁▁▁▁▂▁▁▁▁▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:              tmape_force ▂▄▆▇████▇▇▆▆▆▅▅▄▄▄▃▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:              trmse_coord █▃▁▁▁▂▂▂▃▃▄▄▅▅▅▅▆▆▆▅▅▅▅▄▆▃▅▄▃▃▃▃▄▄▆▃▆▇▅▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb:              trmse_force █▄▄▄▅▅▅▅▅▄▄▄▃▃▂▂▂▁▂▂▁▁▁▁▁▁▂▁▁▁▁▁▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_7d5d0f23_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-56-16/wandb/run-20230811_005759-7d5d0f23\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: Syncing run FSR_Trainable_7d5d0f23\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7d5d0f23\n",
      "2023-08-11 00:58:15,851\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.069 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:58:15,854\tWARNING util.py:315 -- The `process_trial_result` operation took 3.073 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:58:15,855\tWARNING util.py:315 -- Processing trial results took 3.073 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:58:15,856\tWARNING util.py:315 -- The `process_trial_result` operation took 3.075 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193065)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-08-11 00:58:17,933\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.023 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:58:17,937\tWARNING util.py:315 -- The `process_trial_result` operation took 2.027 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:58:17,938\tWARNING util.py:315 -- Processing trial results took 2.029 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:58:17,939\tWARNING util.py:315 -- The `process_trial_result` operation took 2.030 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193065)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193065)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193065)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193065)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_b1d49451_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-57-47/wandb/run-20230811_005817-b1d49451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193065)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193065)\u001b[0m wandb: Syncing run FSR_Trainable_b1d49451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193065)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193065)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b1d49451\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-08-11 00:58:38,272 E 181243 181243] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: f26c708f66ae615c627afbdfe06999fde4c91fcd5664d16b5f3733ca, IP: 172.26.215.93) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.26.215.93`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:                mae_coord ▂▃▄▅▄▄▃▂▂▁▂▂▃▂▂▄▄▅▄▄▆▅▄█▆▄▅▄▄▃▄▃▃▃▁▄▃▁▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:                mae_force █▅▃▄▅▅▅▄▄▃▃▃▂▂▂▂▂▁▂▂▁▂▁▂▂▂▁▂▁▁▁▂▂▁▁▂▂▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:               mape_coord █▇▆▅▄▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂▂▂▁▂▂▂▂▂▂▂▁▂▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:               mape_force █▆▅▅▆▆▆▆▅▅▄▄▃▃▃▃▃▂▃▃▂▃▂▂▂▂▂▂▂▁▁▂▂▁▂▂▂▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:                   metric █▅▄▅▆▆▆▅▄▃▃▂▂▁▂▁▁▁▃▁▂▂▁▄▂▂▂▃▃▃▄▃▃▃▂▅▄▁▁▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:               rmse_coord ▂▁▁▂▂▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▅▄▄█▆▅▄▅▆▆▇▆▆▆▄█▅▄▆▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:               rmse_force █▄▁▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▂▂▂▂▂▂▂▂▃▂▂▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:         time_this_iter_s ▄▃▂▂▂▂▂▂▂▄▃▂▃▂▁▃▆▂▂▁▁▂▁▁▂▁▁▁▁▁▂▆▅█▃▂▃▂▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:               tmae_coord ▂▃▄▅▅▅▄▄▃▂▂▂▃▂▂▄▄▄▄▄▆▅▄█▅▄▄▄▄▃▄▃▃▂▁▄▃▁▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:               tmae_force ▇▇▆▇███▇▆▆▅▄▄▃▃▃▃▂▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▁▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:              tmape_coord █▆▆▆▆▆▆▆▅▅▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:              tmape_force ▆▇▇▇███▇▆▆▅▅▄▃▄▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▁▂▂▁▂▂▂▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:              trmse_coord ▄▁▁▂▂▃▃▂▂▂▁▁▂▂▂▃▃▃▄▃▅▄▃█▅▄▃▅▅▅▆▅▄▄▃▆▅▃▃▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:              trmse_force █▆▅▆▇▇▆▆▅▄▄▃▂▂▂▁▁▁▂▁▁▂▁▂▂▂▁▂▂▂▃▂▃▂▂▃▃▁▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:                mae_coord 5.72483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:                mae_force 702.41281\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:               mape_coord 1.22078\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:               mape_force 8.499780395327258e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:                   metric 0.72792\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:               rmse_coord 2.79728\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:               rmse_force 503.36172\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:       time_since_restore 248.66463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:         time_this_iter_s 2.96304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:             time_total_s 248.66463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:                timestamp 1691683134\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:               tmae_coord 1.19137\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:               tmae_force 0.28378\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:              tmape_coord 69792407377401.93\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:              tmape_force 403640265630007.06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:              trmse_coord 0.53754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb:              trmse_force 0.19037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: 🚀 View run FSR_Trainable_48c46c0b at: https://wandb.ai/seokjin/FSR-prediction/runs/48c46c0b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=191963)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_005439-48c46c0b/logs\n",
      "2023-08-11 00:59:10,198\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.377 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:59:10,202\tWARNING util.py:315 -- The `process_trial_result` operation took 2.383 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:59:10,205\tWARNING util.py:315 -- Processing trial results took 2.386 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:59:10,207\tWARNING util.py:315 -- The `process_trial_result` operation took 2.388 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_839c76a6_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-58-09/wandb/run-20230811_005913-839c76a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: Syncing run FSR_Trainable_839c76a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/839c76a6\n",
      "2023-08-11 00:59:25,734\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.455 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:59:25,739\tWARNING util.py:315 -- The `process_trial_result` operation took 3.461 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:59:25,742\tWARNING util.py:315 -- Processing trial results took 3.465 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:59:25,746\tWARNING util.py:315 -- The `process_trial_result` operation took 3.469 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_cf85d82f_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-59-06/wandb/run-20230811_005929-cf85d82f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: Syncing run FSR_Trainable_cf85d82f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cf85d82f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:                mae_coord 26.46171\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:                mae_force 3859.28929\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:               mape_coord 4.47407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:               mape_force 5.415656583398532e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:                   metric 2.48227\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:               rmse_coord 9.45393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:               rmse_force 2304.19935\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:       time_since_restore 1.12285\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:         time_this_iter_s 1.12285\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:             time_total_s 1.12285\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:                timestamp 1691683162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:               tmae_coord 5.42994\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:               tmae_force 1.37647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:              tmape_coord 45015248970767.49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:              tmape_force 2313288229739749.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:              trmse_coord 1.77018\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb:              trmse_force 0.71209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: 🚀 View run FSR_Trainable_cf85d82f at: https://wandb.ai/seokjin/FSR-prediction/runs/cf85d82f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193563)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_005929-cf85d82f/logs\n",
      "2023-08-11 00:59:45,724\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.558 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:59:45,729\tWARNING util.py:315 -- The `process_trial_result` operation took 2.563 s, which may be a performance bottleneck.\n",
      "2023-08-11 00:59:45,732\tWARNING util.py:315 -- Processing trial results took 2.566 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 00:59:45,733\tWARNING util.py:315 -- The `process_trial_result` operation took 2.568 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_75852e34_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-59-21/wandb/run-20230811_005949-75852e34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: Syncing run FSR_Trainable_75852e34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75852e34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:                mae_coord 23.93408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:                mae_force 4668.96384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:               mape_coord 3.82597\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:               mape_force 8.30170349620074e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:                   metric 2.5763\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:               rmse_coord 7.6704\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:               rmse_force 2571.2921\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:       time_since_restore 1.29663\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:         time_this_iter_s 1.29663\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:             time_total_s 1.29663\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:                timestamp 1691683183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:               tmae_coord 5.39151\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:               tmae_force 1.83708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:              tmape_coord 34866315568670.793\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:              tmape_force 3969621566800217.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:              trmse_coord 1.71047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb:              trmse_force 0.86583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: 🚀 View run FSR_Trainable_75852e34 at: https://wandb.ai/seokjin/FSR-prediction/runs/75852e34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193804)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_005949-75852e34/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:                mae_coord █▃▃▅▇▇▆▆▅▅▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:                mae_force █▇▆▆▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:               mape_coord █▅▅▆▇▇▇▇▇▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:               mape_force █▇▆▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:                   metric █▇▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:               rmse_coord █▆▅▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:               rmse_force █▇▆▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:         time_this_iter_s ▇▃▂▂▂█▃▂▂▂▂▂▂▁▂▂▂▂▁▃▄▇▆█▅▃▃▆█▆▃▂▁▁▃▄▇▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:               tmae_coord █▄▃▄▆▆▅▅▄▄▄▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:               tmae_force █▇▆▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:              tmape_coord █▆▅▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:              tmape_force █▇▆▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:              trmse_coord █▆▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:              trmse_force █▇▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:                mae_coord 4.71034\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:                mae_force 683.52548\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:               mape_coord 1.1519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:               mape_force 9.461020114297356e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:                   metric 0.66007\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:               rmse_coord 2.44939\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:               rmse_force 482.88891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:       time_since_restore 243.69109\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:         time_this_iter_s 2.49823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:             time_total_s 243.69109\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:                timestamp 1691683206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:               tmae_coord 1.00609\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:               tmae_force 0.26379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:              tmape_coord 78779339352228.48\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:              tmape_force 406841124284061.7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:              trmse_coord 0.49072\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb:              trmse_force 0.16935\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: 🚀 View run FSR_Trainable_3433d7fa at: https://wandb.ai/seokjin/FSR-prediction/runs/3433d7fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192252)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_005553-3433d7fa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_770d6969_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_00-59-41/wandb/run-20230811_010015-770d6969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: Syncing run FSR_Trainable_770d6969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/770d6969\n",
      "2023-08-11 01:00:22,115\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.822 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:00:22,118\tWARNING util.py:315 -- The `process_trial_result` operation took 3.826 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:00:22,121\tWARNING util.py:315 -- Processing trial results took 3.830 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:00:22,123\tWARNING util.py:315 -- The `process_trial_result` operation took 3.832 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:                mae_coord █▇▄▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:                mae_force ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:               mape_coord ▁▅▇██▇▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:               mape_force ▁▂▄▅▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:                   metric █▇▅▄▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:               rmse_coord █▇▅▄▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:               rmse_force ▂▁▂▄▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:         time_this_iter_s █▂▄▂▁▃▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:               tmae_coord █▇▄▃▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:               tmae_force ▁▂▃▄▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:              tmape_coord ▁▂▃▅▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:              tmape_force ▁▂▃▅▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:              trmse_coord █▇▅▄▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:              trmse_force █▅▃▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:                mae_coord 5.51941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:                mae_force 1292.95382\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:               mape_coord 1.57133\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:               mape_force 2.2759232004263142e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:                   metric 0.83708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:               rmse_coord 2.57452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:               rmse_force 968.00334\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:       time_since_restore 148.67703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:         time_this_iter_s 18.71238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:             time_total_s 148.67703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:                timestamp 1691683224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:               tmae_coord 1.17993\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:               tmae_force 0.44311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:              tmape_coord 95548523225693.31\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:              tmape_force 753060088719904.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:              trmse_coord 0.528\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb:              trmse_force 0.30908\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: 🚀 View run FSR_Trainable_7d5d0f23 at: https://wandb.ai/seokjin/FSR-prediction/runs/7d5d0f23\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=192836)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_005759-7d5d0f23/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_08dbc938_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-00-04/wandb/run-20230811_010036-08dbc938\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: Syncing run FSR_Trainable_08dbc938\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/08dbc938\n",
      "2023-08-11 01:00:45,022\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.935 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:00:45,027\tWARNING util.py:315 -- The `process_trial_result` operation took 3.941 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:00:45,034\tWARNING util.py:315 -- Processing trial results took 3.948 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:00:45,038\tWARNING util.py:315 -- The `process_trial_result` operation took 3.952 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:                mae_coord 5.49974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:                mae_force 1272.07862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:               mape_coord 1.54442\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:               mape_force 7.119036202583974e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:                   metric 42.35777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:               rmse_coord 2.56798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:               rmse_force 941.89478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:       time_since_restore 16.89324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:         time_this_iter_s 16.89324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:             time_total_s 16.89324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:                timestamp 1691683241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:               tmae_coord 34.92168\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:               tmae_force 10.23769\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:              tmape_coord 2.233569661047873e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:              tmape_force 2.460742464789014e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:              trmse_coord 35.24118\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb:              trmse_force 7.1166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: 🚀 View run FSR_Trainable_08dbc938 at: https://wandb.ai/seokjin/FSR-prediction/runs/08dbc938\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194278)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010036-08dbc938/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_8198c6d1_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-00-24/wandb/run-20230811_010059-8198c6d1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: Syncing run FSR_Trainable_8198c6d1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8198c6d1\n",
      "2023-08-11 01:01:05,528\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.922 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:01:05,531\tWARNING util.py:315 -- The `process_trial_result` operation took 2.926 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:01:05,532\tWARNING util.py:315 -- Processing trial results took 2.928 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:01:05,534\tWARNING util.py:315 -- The `process_trial_result` operation took 2.929 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:                mae_coord 5.65497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:                mae_force 1067.53495\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:               mape_coord 1.50747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:               mape_force 496846675.03542\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:                   metric 7.27306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:               rmse_coord 2.8025\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:               rmse_force 796.86688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:       time_since_restore 16.62833\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:         time_this_iter_s 16.62833\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:             time_total_s 16.62833\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:                timestamp 1691683262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:               tmae_coord 10.04411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:               tmae_force 3.06129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:              tmape_coord 3952251722615985.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:              tmape_force 240.29962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:              trmse_coord 5.07031\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb:              trmse_force 2.20275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: 🚀 View run FSR_Trainable_8198c6d1 at: https://wandb.ai/seokjin/FSR-prediction/runs/8198c6d1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194518)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010059-8198c6d1/logs\n",
      "2023-08-11 01:01:12,235\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.379 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:01:12,238\tWARNING util.py:315 -- The `process_trial_result` operation took 2.386 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:01:12,242\tWARNING util.py:315 -- Processing trial results took 2.389 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:01:12,244\tWARNING util.py:315 -- The `process_trial_result` operation took 2.392 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_d3e59f10_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-00-46/wandb/run-20230811_010117-d3e59f10\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: Syncing run FSR_Trainable_d3e59f10\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d3e59f10\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:                mae_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:                mae_force ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:               mape_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:               mape_force ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:               rmse_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:               tmae_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:               tmae_force ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:              tmape_coord ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:              tmape_force ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:              trmse_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:              trmse_force ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:                mae_coord 8.36734\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:                mae_force 1397.33772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:               mape_coord 1.60743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:               mape_force 2.762155287781203e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:                   metric 0.9523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:               rmse_coord 3.2369\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:               rmse_force 878.56781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:       time_since_restore 7.73455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:         time_this_iter_s 4.03737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:             time_total_s 7.73455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:                timestamp 1691683276\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:               tmae_coord 1.78955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:               tmae_force 0.54054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:              tmape_coord 78031572851060.34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:              tmape_force 1231005913063527.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:              trmse_coord 0.65907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb:              trmse_force 0.29323\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: 🚀 View run FSR_Trainable_d3e59f10 at: https://wandb.ai/seokjin/FSR-prediction/runs/d3e59f10\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194756)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010117-d3e59f10/logs\n",
      "2023-08-11 01:01:28,190\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.770 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:01:28,195\tWARNING util.py:315 -- The `process_trial_result` operation took 2.776 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:01:28,198\tWARNING util.py:315 -- Processing trial results took 2.778 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:01:28,200\tWARNING util.py:315 -- The `process_trial_result` operation took 2.780 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_36b10ce7_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-01-06/wandb/run-20230811_010132-36b10ce7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Syncing run FSR_Trainable_36b10ce7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/36b10ce7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:                mae_coord ▄▆▁▁▄▄▄▄▄▅▅▆▆▇▇▇▇▇▇▇▇▇▇█▆▆▆▆▆▅▄▄▃▃▂▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:                mae_force █▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:               mape_coord ██▅▅▅▄▂▂▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:               mape_force █▆▅▅▅▅▅▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:                   metric █▅▄▃▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▂▂▁▁▁▁▁▂▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:               rmse_coord ▂▄▁▁▂▃▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█▆▆▆▇█▇▇▇▇▆▆▇▆▆▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:               rmse_force █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:         time_this_iter_s ▄▂▂▂▃▁▁▂▃▁▃▇▄▃▇▂▄▆▂▃▃▅▃▂▃▇▄▄▆█▄▇▃▂▅▇▅▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:               tmae_coord ██▃▃▆▇▇▇▇▇▇▇▇█████▇▇▇▇▇█▆▆▆▆▆▅▄▃▃▂▂▃▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:               tmae_force █▇▆▆▇▇▆▆▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:              tmape_coord █▆▆▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:              tmape_force ▇▇▆▇██▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:              trmse_coord █▄▁▁▄▆▇███████▇▇▇▇▆▆▆▆▆█▅▅▄▅▇▇▅▅▅▄▄▆▄▄▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:              trmse_force █▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:                mae_coord 5.18261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:                mae_force 641.09383\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:               mape_coord 1.17282\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:               mape_force 6.572758353674683e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:                   metric 0.66863\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:               rmse_coord 2.57804\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:               rmse_force 491.48956\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:       time_since_restore 116.86006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:         time_this_iter_s 1.09081\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:             time_total_s 116.86006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:                timestamp 1691683290\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:               tmae_coord 1.08474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:               tmae_force 0.24554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:              tmape_coord 73583120233485.33\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:              tmape_force 310087713416170.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:              trmse_coord 0.49959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb:              trmse_force 0.16904\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: 🚀 View run FSR_Trainable_839c76a6 at: https://wandb.ai/seokjin/FSR-prediction/runs/839c76a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=193345)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_005913-839c76a6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: | 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: / 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: / 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: - 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: - 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194991)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010132-36b10ce7/logs\n",
      "2023-08-11 01:01:42,317\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.267 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:01:42,323\tWARNING util.py:315 -- The `process_trial_result` operation took 3.274 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:01:42,326\tWARNING util.py:315 -- Processing trial results took 3.277 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:01:42,327\tWARNING util.py:315 -- The `process_trial_result` operation took 3.278 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_f4c384a8_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-01-24/wandb/run-20230811_010145-f4c384a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: Syncing run FSR_Trainable_f4c384a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f4c384a8\n",
      "2023-08-11 01:01:58,166\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.159 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:01:58,171\tWARNING util.py:315 -- The `process_trial_result` operation took 3.165 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:01:58,173\tWARNING util.py:315 -- Processing trial results took 3.167 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:01:58,175\tWARNING util.py:315 -- The `process_trial_result` operation took 3.169 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_cf94ab7d_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-01-38/wandb/run-20230811_010202-cf94ab7d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: Syncing run FSR_Trainable_cf94ab7d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cf94ab7d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:                mae_coord ▃▃▂▃▂█▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:                mae_force ▁▁▁▂▄█▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:               mape_coord ▁▂▃▃▄█▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:               mape_force ▁▂▃▄▆███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:                   metric █▇▄▅▃▇▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:               rmse_coord ██▆▇▄█▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:               rmse_force ▃▂▁▂▂█▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:       time_since_restore ▁▂▃▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:         time_this_iter_s ▅▆█▅▅▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:             time_total_s ▁▂▃▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:                timestamp ▁▂▄▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:               tmae_coord ▃▂▂▃▂█▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:               tmae_force ▁▁▂▂▄█▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:              tmape_coord ▁▂▃▃▄█▆▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:              tmape_force ▁▂▃▄▆███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:              trmse_coord ██▆▇▄▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:              trmse_force ▆▄▁▂▁█▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:                mae_coord 5.51642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:                mae_force 1304.52268\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:               mape_coord 1.56192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:               mape_force 2.2788559884960074e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:                   metric 0.83687\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:               rmse_coord 2.57792\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:               rmse_force 964.85718\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:       time_since_restore 106.30493\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:         time_this_iter_s 10.74479\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:             time_total_s 106.30493\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:                timestamp 1691683319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:               tmae_coord 1.18219\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:               tmae_force 0.44944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:              tmape_coord 99609607953879.39\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:              tmape_force 763979732479118.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:              trmse_coord 0.52635\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb:              trmse_force 0.31051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: 🚀 View run FSR_Trainable_770d6969 at: https://wandb.ai/seokjin/FSR-prediction/runs/770d6969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=194042)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010015-770d6969/logs\n",
      "2023-08-11 01:02:13,670\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.312 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:02:13,676\tWARNING util.py:315 -- The `process_trial_result` operation took 2.318 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:02:13,678\tWARNING util.py:315 -- Processing trial results took 2.320 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:02:13,680\tWARNING util.py:315 -- The `process_trial_result` operation took 2.322 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_86fbc03e_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-01-53/wandb/run-20230811_010217-86fbc03e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: Syncing run FSR_Trainable_86fbc03e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/86fbc03e\n",
      "2023-08-11 01:02:30,892\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.938 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:02:30,895\tWARNING util.py:315 -- The `process_trial_result` operation took 2.943 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:02:30,898\tWARNING util.py:315 -- Processing trial results took 2.946 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:02:30,900\tWARNING util.py:315 -- The `process_trial_result` operation took 2.947 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_dcd3bac3_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-02-10/wandb/run-20230811_010235-dcd3bac3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: Syncing run FSR_Trainable_dcd3bac3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/dcd3bac3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:                mae_coord ▅█▃▅▆▆▆▄▄▃▂▂▂▂▂▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▆▁▄▄▅▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:                mae_force █▆▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:               mape_coord ██▆▆▆▆▆▅▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▁▁▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:               mape_force █▇▇▆▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:                   metric █▆▃▃▃▃▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:               rmse_coord ▅▄▁▁▂▃▃▃▃▃▂▂▃▃▃▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▂▄▄▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:               rmse_force █▆▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:         time_this_iter_s ▇█▅▂▄▂▂▁▁▃▃▄▄▃▄▆▆▇▆▇█▅▃▆▇▆▄▅▂▂▃▃▂▂▁▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇█████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:               tmae_coord ▇█▄▅▆▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▅▁▃▃▄▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:               tmae_force █▇▇▆▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:              tmape_coord █▆▅▅▄▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▃▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:              tmape_force ▄▄█▇██████▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▂▃▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:              trmse_coord █▇▃▃▃▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▁▂▂▃▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:              trmse_force █▆▄▃▃▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:                mae_coord 5.75114\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:                mae_force 691.07984\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:               mape_coord 1.22888\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:               mape_force 9.600505444037928e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:                   metric 0.70117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:               rmse_coord 2.71969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:               rmse_force 472.00371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:       time_since_restore 59.94594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:         time_this_iter_s 0.75623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:             time_total_s 59.94594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:                timestamp 1691683385\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:               tmae_coord 1.19417\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:               tmae_force 0.27427\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:              tmape_coord 73022482709405.03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:              tmape_force 430839734204251.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:              trmse_coord 0.52634\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb:              trmse_force 0.17483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: 🚀 View run FSR_Trainable_cf94ab7d at: https://wandb.ai/seokjin/FSR-prediction/runs/cf94ab7d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195458)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010202-cf94ab7d/logs\n",
      "2023-08-11 01:03:21,703\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.166 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:03:21,707\tWARNING util.py:315 -- The `process_trial_result` operation took 2.171 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:03:21,709\tWARNING util.py:315 -- Processing trial results took 2.173 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:03:21,711\tWARNING util.py:315 -- The `process_trial_result` operation took 2.174 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_14a7a158_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-02-26/wandb/run-20230811_010325-14a7a158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: Syncing run FSR_Trainable_14a7a158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/14a7a158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:                mae_coord █▄▅▄▄▃▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:                mae_force █▇▆▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:               mape_coord █▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:               mape_force █▇▇▆▆▅▄▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:                   metric █▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:               rmse_coord █▄▆▅▃▂▁▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▅▅▅▅▅▅▅▅▅▅▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:               rmse_force █▆▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:         time_this_iter_s ▆▃▂▂▂▂█▆▄▂▁▂▂▄▄▆█▃█▅▂█▆▅▂▁▂▂▂▂▃▂▂▃▂▂▁▃▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:               tmae_coord █▆▆▅▄▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:               tmae_force █▇▇▇▆▅▅▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:              tmape_coord █▆▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:              tmape_force ▆▇████▇▇▆▆▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:              trmse_coord █▅▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:              trmse_force █▆▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:                mae_coord 4.83178\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:                mae_force 646.03668\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:               mape_coord 1.18762\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:               mape_force 8.561961902319293e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:                   metric 0.65184\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:               rmse_coord 2.4626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:               rmse_force 438.71915\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:       time_since_restore 87.40167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:         time_this_iter_s 0.91346\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:             time_total_s 87.40167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:                timestamp 1691683404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:               tmae_coord 1.0334\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:               tmae_force 0.26308\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:              tmape_coord 78945291046065.47\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:              tmape_force 429662607874678.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:              trmse_coord 0.49001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb:              trmse_force 0.16183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: 🚀 View run FSR_Trainable_f4c384a8 at: https://wandb.ai/seokjin/FSR-prediction/runs/f4c384a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195233)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010145-f4c384a8/logs\n",
      "2023-08-11 01:03:43,633\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.332 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:03:43,638\tWARNING util.py:315 -- The `process_trial_result` operation took 2.339 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:03:43,641\tWARNING util.py:315 -- Processing trial results took 2.342 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:03:43,644\tWARNING util.py:315 -- The `process_trial_result` operation took 2.344 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_683aa5cb_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-03-18/wandb/run-20230811_010347-683aa5cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: Syncing run FSR_Trainable_683aa5cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/683aa5cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:                mae_coord ▁▄▆▇▇█████▇▆▆▆▅▅▅▅▅▅▅▅▅▃▄▄▄▄▅▇▄▄▄▅▃▂▃▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:                mae_force █▅▄▃▃▃▃▃▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▂▂▁▁▁▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:               mape_coord █▆▇▇▇▆▅▅▅▅▄▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▄▁▂▂▃▂▂▂▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:               mape_force █▇▆▅▅▅▅▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:                   metric █▄▄▄▄▅▆▆▆▅▅▅▄▄▃▃▃▃▃▃▃▃▃▁▂▂▂▂▃▅▁▂▂▃▂▁▁▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:               rmse_coord ▁▂▂▃▃▄▅▅▅▅▅▄▄▄▄▄▅▅▅▅▅▅▅▄▄▄▅▄▆█▄▅▅▆▄▃▄▄▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:               rmse_force █▃▂▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:         time_this_iter_s ▄▂▃▂▅▆▃█▆▂▇▄▅▁▂▂▁▁▁▁▃▁▁▂▁▁▂▂▂▃▂▂▇▄▅▅▃▃▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:               tmae_coord ▂▆▇▇█████▇▇▆▆▅▄▅▅▄▄▄▄▄▄▂▃▃▃▃▄▆▃▃▃▄▃▁▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:               tmae_force ▇█▇▇▇█████▇▇▆▆▅▅▅▄▄▄▄▃▃▂▂▂▂▂▂▃▁▁▁▂▂▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:              tmape_coord █▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▂▂▃▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:              tmape_force ▃▇▇▇██████▇▇▆▆▅▅▅▄▄▄▄▃▄▃▂▂▂▂▁▂▂▁▁▁▂▂▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:              trmse_coord ▃▄▄▄▄▅▆▆▆▆▅▅▄▄▃▄▄▄▄▄▄▄▄▂▃▂▃▃▅█▂▄▄▆▂▁▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:              trmse_force █▄▃▃▃▄▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▂▃▁▂▁▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:                mae_coord 5.2328\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:                mae_force 649.03956\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:               mape_coord 1.20949\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:               mape_force 7.650518506017445e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:                   metric 0.67056\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:               rmse_coord 2.55412\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:               rmse_force 469.21897\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:       time_since_restore 90.63224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:         time_this_iter_s 0.97587\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:             time_total_s 90.63224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:                timestamp 1691683435\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:               tmae_coord 1.10206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:               tmae_force 0.26526\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:              tmape_coord 76858540893612.12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:              tmape_force 394008707144449.06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:              trmse_coord 0.49916\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb:              trmse_force 0.17141\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: 🚀 View run FSR_Trainable_86fbc03e at: https://wandb.ai/seokjin/FSR-prediction/runs/86fbc03e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195694)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010217-86fbc03e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:                mae_coord ▃█▅▅▃▂▁▁▁▂▂▃▃▃▃▄▄▅▅▅▆▇▇▇▇▇▇▆▆█▆▆▃█▁▇▄▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:                mae_force █▆▄▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:               mape_coord ██▇▇▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▂▁▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:               mape_force █▇▆▆▆▆▆▆▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:                   metric █▅▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▁▃▁▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:               rmse_coord ▄▄▂▂▁▁▁▁▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▇▅▅▄█▄▅▆▅▆▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:               rmse_force █▅▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:         time_this_iter_s ▅▄▅█▃▄▄▄▂▃▃▁▂▂▂▁▂▁▂▂▂▃▂▃▃▂▂▅▃▄▃▃▂▂▄▃▆▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:               tmae_coord ▅█▅▅▄▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▄▄▅▄▄▂▅▁▅▃▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:               tmae_force █▇▆▆▇▇▇▇▇▇▆▆▆▅▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▃▂▂▂▃▁▃▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:              tmape_coord █▆▆▆▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▂▁▂▁▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:              tmape_force ▅▅▆▇██████▇▇▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▃▁▃▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:              trmse_coord █▇▃▃▃▂▂▂▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▅▂▃▂▆▁▃▄▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:              trmse_force █▅▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▂▁▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:                mae_coord 5.47585\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:                mae_force 642.53316\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:               mape_coord 1.20156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:               mape_force 7.700455208415625e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:                   metric 0.67725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:               rmse_coord 2.63934\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:               rmse_force 461.76456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:       time_since_restore 89.41904\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:         time_this_iter_s 0.80321\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:             time_total_s 89.41904\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:                timestamp 1691683449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:               tmae_coord 1.14019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:               tmae_force 0.25952\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:              tmape_coord 75923458228967.56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:              tmape_force 388949516795420.06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:              trmse_coord 0.50842\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb:              trmse_force 0.16883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: 🚀 View run FSR_Trainable_dcd3bac3 at: https://wandb.ai/seokjin/FSR-prediction/runs/dcd3bac3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=195919)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010235-dcd3bac3/logs\n",
      "2023-08-11 01:04:15,865\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.482 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:04:15,868\tWARNING util.py:315 -- The `process_trial_result` operation took 2.485 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:04:15,871\tWARNING util.py:315 -- Processing trial results took 2.488 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:04:15,872\tWARNING util.py:315 -- The `process_trial_result` operation took 2.489 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_f90c4aea_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-03-40/wandb/run-20230811_010420-f90c4aea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: Syncing run FSR_Trainable_f90c4aea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f90c4aea\n",
      "2023-08-11 01:04:33,595\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.866 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:04:33,601\tWARNING util.py:315 -- The `process_trial_result` operation took 2.873 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:04:33,604\tWARNING util.py:315 -- Processing trial results took 2.877 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:04:33,607\tWARNING util.py:315 -- The `process_trial_result` operation took 2.879 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_170adff7_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-04-12/wandb/run-20230811_010438-170adff7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: Syncing run FSR_Trainable_170adff7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/170adff7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:                mae_coord ▆█▆▅▅▄▃▂▂▃▃▄▄▅▅▅▅▆▆▆▆▆▆▆▆▅▅▅▅▄▄▄▃▃▃▁▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:                mae_force █▄▃▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:               mape_coord ██▅▅▅▅▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:               mape_force █▄▄▅▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:                   metric █▆▃▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:               rmse_coord ▂▅▃▂▂▂▁▁▂▃▃▄▅▆▆▇▇▇███████▇▇▆▆▆▆▆▆▆▃▁▄▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:               rmse_force █▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:         time_this_iter_s ▃▂▁▁▁▁▃▂▂▃▂▂▂▃▂▄▂▂▁▃▂▂▂▁▂▃▄▂█▂▄▃▁▂▃▂▁▂▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:               tmae_coord ▆█▅▅▄▃▃▂▃▃▃▄▄▅▅▅▅▆▆▆▆▆▆▆▅▅▅▅▄▄▄▃▃▃▂▁▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:               tmae_force █▅▅▆██▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:              tmape_coord █▅▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:              tmape_force ▆▄▆▇███▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:              trmse_coord ██▅▄▃▃▃▃▃▄▄▄▅▆▆▆▆▇▇▇▇▇▇▇▇▆▆▆▅▅▅▅▅▅▁▁▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:              trmse_force █▅▃▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:                mae_coord 5.34303\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:                mae_force 674.39114\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:               mape_coord 1.18652\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:               mape_force 8.195087535405036e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:                   metric 0.67972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:               rmse_coord 2.58393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:               rmse_force 485.06645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:       time_since_restore 101.27824\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:         time_this_iter_s 0.91424\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:             time_total_s 101.27824\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:                timestamp 1691683516\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:               tmae_coord 1.12622\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:               tmae_force 0.26523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:              tmape_coord 73421143200746.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:              tmape_force 385182302368383.44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:              trmse_coord 0.50616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb:              trmse_force 0.17356\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: 🚀 View run FSR_Trainable_14a7a158 at: https://wandb.ai/seokjin/FSR-prediction/runs/14a7a158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196212)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010325-14a7a158/logs\n",
      "2023-08-11 01:05:34,491\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.437 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:05:34,496\tWARNING util.py:315 -- The `process_trial_result` operation took 2.444 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:05:34,498\tWARNING util.py:315 -- Processing trial results took 2.445 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:05:34,500\tWARNING util.py:315 -- The `process_trial_result` operation took 2.447 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_5ce6d566_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-04-29/wandb/run-20230811_010538-5ce6d566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: Syncing run FSR_Trainable_5ce6d566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5ce6d566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:                mae_coord █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:                mae_force █▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:               mape_coord █▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:               mape_force █▆▅▃▂▂▂▂▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:                   metric █▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:               rmse_coord █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:               rmse_force █▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:         time_this_iter_s ▅▄▂▂▂▃▃▅▄▂▃▅▂▃▄▂▂▄█▄▆▆█▂▁▂▆▂▁▄█▂▂▃▃▅▁▁▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:               tmae_coord █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:               tmae_force █▅▄▃▂▂▂▂▂▁▁▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:              tmape_coord ▁█████▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:              tmape_force █▇▆▄▃▃▂▂▂▂▂▂▂▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:              trmse_coord █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:              trmse_force █▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:                mae_coord 4.87937\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:                mae_force 688.20864\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:               mape_coord 1.12172\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:               mape_force 9.417446103326326e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:                   metric 0.65838\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:               rmse_coord 2.47244\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:               rmse_force 462.1405\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:       time_since_restore 101.47304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:         time_this_iter_s 0.95716\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:             time_total_s 101.47304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:                timestamp 1691683537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:               tmae_coord 1.0346\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:               tmae_force 0.27394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:              tmape_coord 75564988005328.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:              tmape_force 437289157375628.44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:              trmse_coord 0.48878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb:              trmse_force 0.1696\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: 🚀 View run FSR_Trainable_683aa5cb at: https://wandb.ai/seokjin/FSR-prediction/runs/683aa5cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196457)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010347-683aa5cb/logs\n",
      "2023-08-11 01:05:56,900\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.557 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:05:56,904\tWARNING util.py:315 -- The `process_trial_result` operation took 2.563 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:05:56,906\tWARNING util.py:315 -- Processing trial results took 2.565 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:05:56,909\tWARNING util.py:315 -- The `process_trial_result` operation took 2.568 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_d54e09c5_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-05-30/wandb/run-20230811_010600-d54e09c5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Syncing run FSR_Trainable_d54e09c5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d54e09c5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:                mae_coord █▇▄▃▂▂▂▂▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:                mae_force █▇▆▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:               mape_coord █▅▃▂▂▂▂▂▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:               mape_force █▆▇▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:                   metric █▆▅▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:               rmse_coord █▄▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:               rmse_force █▆▅▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:         time_this_iter_s ▄▄▃▂▄▅▂▄▅▇▂█▄▂▂▄▂▂▂▆▂▄▂▄▂▃▁▃▃▃▄▅▁▂▂▄▃▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:               tmae_coord █▇▅▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:               tmae_force █▇▇▆▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:              tmape_coord █▇▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:              tmape_force ▆▇█▆▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:              trmse_coord █▅▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:              trmse_force █▆▅▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:                mae_coord 4.83217\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:                mae_force 639.88677\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:               mape_coord 1.21167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:               mape_force 8.605521677631626e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:                   metric 0.65499\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:               rmse_coord 2.46451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:               rmse_force 428.97717\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:       time_since_restore 96.6735\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:         time_this_iter_s 0.90387\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:             time_total_s 96.6735\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:                timestamp 1691683565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:               tmae_coord 1.02878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:               tmae_force 0.26538\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:              tmape_coord 78708109554200.06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:              tmape_force 439774518677417.56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:              trmse_coord 0.49223\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb:              trmse_force 0.16276\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: 🚀 View run FSR_Trainable_f90c4aea at: https://wandb.ai/seokjin/FSR-prediction/runs/f90c4aea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196721)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010420-f90c4aea/logs\n",
      "2023-08-11 01:06:24,490\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.112 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:06:24,496\tWARNING util.py:315 -- The `process_trial_result` operation took 2.118 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:06:24,498\tWARNING util.py:315 -- Processing trial results took 2.121 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:06:24,502\tWARNING util.py:315 -- The `process_trial_result` operation took 2.125 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | Waiting for wandb.init()...762)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_5aadf73d_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-05-53/wandb/run-20230811_010628-5aadf73d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Syncing run FSR_Trainable_5aadf73d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5aadf73d\n",
      "wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:                mae_coord █▃▃▃▂▂▃▃▃▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:                mae_force █▆▄▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:               mape_coord █▆▆▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:               mape_force █▆▄▄▄▃▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:                   metric █▆▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:               rmse_coord █▂▁▂▁▁▁▁▃▃▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▄▄▃▄▅▄▆▄▄▅▅▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:               rmse_force █▇▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:         time_this_iter_s ▅▃▅█▃▆▃▁▂▃▁▂▂▃▂▂▃▂▂▁▂▃▂▃▂▁▃▂▄▂▂▂▂▃▂▂▁▂▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:               tmae_coord █▄▃▃▂▂▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:               tmae_force █▆▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:              tmape_coord █▆▅▅▅▅▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:              tmape_force █▆▅▆▆▆▆▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:              trmse_coord █▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:              trmse_force █▆▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:                mae_coord 5.31857\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:                mae_force 632.74373\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:               mape_coord 1.17394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:               mape_force 7.063560848814065e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:                   metric 0.67278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:               rmse_coord 2.58986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:               rmse_force 467.64616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:       time_since_restore 101.11365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:         time_this_iter_s 0.87788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:             time_total_s 101.11365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:                timestamp 1691683586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:               tmae_coord 1.12069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:               tmae_force 0.25094\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:              tmape_coord 74569538375438.42\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:              tmape_force 352458434843920.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:              trmse_coord 0.50676\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb:              trmse_force 0.16602\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: 🚀 View run FSR_Trainable_170adff7 at: https://wandb.ai/seokjin/FSR-prediction/runs/170adff7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=196951)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010438-170adff7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010438-170adff7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197762)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010628-5aadf73d/logs\n",
      "2023-08-11 01:06:41,163\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.136 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:06:41,167\tWARNING util.py:315 -- The `process_trial_result` operation took 2.140 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:06:41,169\tWARNING util.py:315 -- Processing trial results took 2.142 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:06:41,169\tWARNING util.py:315 -- The `process_trial_result` operation took 2.143 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_5a8ffac3_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-06-20/wandb/run-20230811_010645-5a8ffac3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: Syncing run FSR_Trainable_5a8ffac3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5a8ffac3\n",
      "2023-08-11 01:06:56,999\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.727 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:06:57,003\tWARNING util.py:315 -- The `process_trial_result` operation took 2.732 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:06:57,004\tWARNING util.py:315 -- Processing trial results took 2.733 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:06:57,005\tWARNING util.py:315 -- The `process_trial_result` operation took 2.735 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_1c686623_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-06-37/wandb/run-20230811_010701-1c686623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: Syncing run FSR_Trainable_1c686623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1c686623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:                mae_coord 4.26798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:                mae_force 933.21165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:               mape_coord 1.23191\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:               mape_force 464659162.79982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:                   metric 5.97432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:               rmse_coord 2.37793\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:               rmse_force 640.66331\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:       time_since_restore 1.38901\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:         time_this_iter_s 1.38901\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:             time_total_s 1.38901\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:                timestamp 1691683614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:               tmae_coord 7.85804\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:               tmae_force 2.45558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:              tmape_coord 1562127123714514.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:              tmape_force 103.26057\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:              trmse_coord 4.44522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb:              trmse_force 1.52909\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: 🚀 View run FSR_Trainable_1c686623 at: https://wandb.ai/seokjin/FSR-prediction/runs/1c686623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198240)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010701-1c686623/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:                mae_coord ▂█▆▄▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:                mae_force █▅▄▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:               mape_coord ▆█▇▆▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:               mape_force █▄▅▅▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:                   metric █▆▅▄▂▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:               rmse_coord ▃▆▄▄▁▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:               rmse_force █▆▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:         time_this_iter_s █▃▃▃▃▅▅▄▆▃▄▃▂▂▃▄▆▄▃▄▃▄▂▁▂▂▄▄▃▃▅▄▂▃▃▄█▆▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:               tmae_coord ▄█▄▃▁▁▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:               tmae_force █▅▇▇▅▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:              tmape_coord █▇▅▅▅▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:              tmape_force ▇▄▇█▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:              trmse_coord ▆█▄▃▁▁▁▂▃▃▄▄▅▅▅▆▆▆▆▆▆▆▆▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb:              trmse_force █▆▅▅▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "2023-08-11 01:07:14,744\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.266 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:07:14,756\tWARNING util.py:315 -- The `process_trial_result` operation took 2.278 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:07:14,758\tWARNING util.py:315 -- Processing trial results took 2.281 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:07:14,764\tWARNING util.py:315 -- The `process_trial_result` operation took 2.286 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: \\ 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_ad00c085_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-06-53/wandb/run-20230811_010718-ad00c085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb: Syncing run FSR_Trainable_ad00c085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ad00c085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:                mae_coord ▅▃▃▂▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:                mae_force █▅▂▁▂▃▃▄▄▅▅▅▆▆▆▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:               mape_coord ▇██▅▃▂▂▃▃▃▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:               mape_force ▆█▃▂▃▃▄▅▅▅▅▅▅▅▄▄▃▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:                   metric █▅▃▁▁▂▂▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:               rmse_coord ▄▃▂▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:               rmse_force █▃▂▁▁▂▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:         time_this_iter_s ▆▃▅▃▃▂▃▄▂▄▂▃▃▅▅▄▄█▄▄▃▂▃▂▃▂▂▁▂▂▅▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:               tmae_coord ▇▃▃▂▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:               tmae_force ▅▇▃▁▂▄▅▆▇▇█████▇▇▆▆▆▆▆▆▆▆▆▆▆▆▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:              tmape_coord ▇██▆▅▅▅▅▅▆▆▆▆▆▆▆▆▅▅▅▅▄▄▄▃▃▃▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:              tmape_force ▁█▆▅▅▆▇█████▇▇▆▆▅▅▅▅▅▅▅▅▅▅▅▅▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:              trmse_coord ▇▃▂▁▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:              trmse_force █▅▃▁▁▂▃▄▄▅▅▅▅▅▆▅▅▅▅▅▅▅▆▆▆▆▆▆▅▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:                mae_coord 6.04044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:                mae_force 810.79895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:               mape_coord 1.3371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:               mape_force 1.2877688575418376e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:                   metric 0.75303\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:               rmse_coord 2.78897\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:               rmse_force 518.75985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:       time_since_restore 29.82793\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:         time_this_iter_s 1.24428\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:             time_total_s 29.82793\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:                timestamp 1691683636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:               tmae_coord 1.26682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:               tmae_force 0.3596\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:              tmape_coord 76376930230479.83\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:              tmape_force 700707784018153.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:              trmse_coord 0.54448\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb:              trmse_force 0.20855\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: 🚀 View run FSR_Trainable_5a8ffac3 at: https://wandb.ai/seokjin/FSR-prediction/runs/5a8ffac3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010645-5a8ffac3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198012)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198491)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:                mae_coord ▅▁▂▅▆▇███▇████▇▇▇▆▆▆▅▅▅▅▄▄▄▄▄▄▄▄▄▃▂▄▃▄▆▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:                mae_force █▅▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:               mape_coord ▇▆██▆▅▄▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:               mape_force █▅▅▅▅▄▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:                   metric █▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▁▂▁▂▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:               rmse_coord ▄▁▁▃▄▅▆▆▆▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄▅▃▃▄▂▅█▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:               rmse_force █▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:         time_this_iter_s █▃▃▃▄▃▃▄▃▃▃▄▅▄▄▄▂▅▄▃▂▂▁▁▄▃▄▃▃▅▃▅▃▃▁▂▄▂▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:               tmae_coord ▆▂▃▆▆▇███▇██▇▇▇▆▆▆▅▅▅▄▄▃▃▃▃▃▂▂▂▃▃▂▁▂▂▂▅▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:               tmae_force █▅▅▆▅▅▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:              tmape_coord █▆▆▅▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:              tmape_force █▅▆▇▇▇▇▇▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:              trmse_coord █▃▃▅▆▆███▇▇▇▇▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▂▃▃▃▄▂▂▄▁▄▇▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:              trmse_force █▆▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: iterations_since_restore 100\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:                mae_coord 5.15352\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:                mae_force 647.31267\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:               mape_coord 1.19093\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:               mape_force 7.310654809537719e+17\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:                   metric 0.66346\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:               rmse_coord 2.55623\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:               rmse_force 473.54466\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:       time_since_restore 91.73369\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:         time_this_iter_s 0.88523\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:             time_total_s 91.73369\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:                timestamp 1691683641\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:               tmae_coord 1.07891\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:               tmae_force 0.24861\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:              tmape_coord 75567404686340.69\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:              tmape_force 335443993086738.4\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:       training_iteration 100\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:              trmse_coord 0.49757\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb:              trmse_force 0.16589\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: 🚀 View run FSR_Trainable_5ce6d566 at: https://wandb.ai/seokjin/FSR-prediction/runs/5ce6d566\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010538-5ce6d566/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2023-08-11 01:07:27,722\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.055 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:07:27,728\tWARNING util.py:315 -- The `process_trial_result` operation took 2.061 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:07:27,729\tWARNING util.py:315 -- Processing trial results took 2.063 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:07:27,732\tWARNING util.py:315 -- The `process_trial_result` operation took 2.066 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=197255)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_ea590b92_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-07-11/wandb/run-20230811_010730-ea590b92\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: Syncing run FSR_Trainable_ea590b92\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ea590b92\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: | 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: / 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: - 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 01:07:38,322\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.614 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:07:38,327\tWARNING util.py:315 -- The `process_trial_result` operation took 2.620 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:07:38,328\tWARNING util.py:315 -- Processing trial results took 2.622 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:07:38,329\tWARNING util.py:315 -- The `process_trial_result` operation took 2.623 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:                mae_coord 27.57373\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:                mae_force 2027.96006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:               mape_coord 4.65913\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:               mape_force 2.303188057593602e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:                   metric 2.26603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:               rmse_coord 9.09135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:               rmse_force 1588.61985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:       time_since_restore 1.72794\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:         time_this_iter_s 1.72794\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:             time_total_s 1.72794\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:                timestamp 1691683645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:               tmae_coord 5.89055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:               tmae_force 0.70744\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:              tmape_coord 28566838243352.58\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:              tmape_force 1154171223229453.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:              trmse_coord 1.82812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb:              trmse_force 0.43792\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: 🚀 View run FSR_Trainable_ea590b92 at: https://wandb.ai/seokjin/FSR-prediction/runs/ea590b92\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198735)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010730-ea590b92/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_739c4bd5_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-07-24/wandb/run-20230811_010741-739c4bd5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: Syncing run FSR_Trainable_739c4bd5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/739c4bd5\n",
      "2023-08-11 01:07:48,507\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.577 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:07:48,513\tWARNING util.py:315 -- The `process_trial_result` operation took 3.584 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:07:48,516\tWARNING util.py:315 -- Processing trial results took 3.587 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:07:48,517\tWARNING util.py:315 -- The `process_trial_result` operation took 3.588 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_a152f03f_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-07-34/wandb/run-20230811_010752-a152f03f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: Syncing run FSR_Trainable_a152f03f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a152f03f\n",
      "2023-08-11 01:07:58,942\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.395 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:07:58,948\tWARNING util.py:315 -- The `process_trial_result` operation took 2.404 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:07:58,949\tWARNING util.py:315 -- Processing trial results took 2.406 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:07:58,951\tWARNING util.py:315 -- The `process_trial_result` operation took 2.408 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_4d46109e_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-07-43/wandb/run-20230811_010802-4d46109e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: Syncing run FSR_Trainable_4d46109e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4d46109e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:                mae_coord 24.85182\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:                mae_force 1414.88373\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:               mape_coord 4.22355\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:               mape_force 2.3008949687057938e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:                   metric 2.05078\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:               rmse_coord 8.32346\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:               rmse_force 1021.35289\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:       time_since_restore 1.16815\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:         time_this_iter_s 1.16815\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:             time_total_s 1.16815\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:                timestamp 1691683676\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:               tmae_coord 5.37808\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:               tmae_force 0.49429\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:              tmape_coord 25757578556418.59\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:              tmape_force 741903876983639.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:              trmse_coord 1.71609\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb:              trmse_force 0.33469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: 🚀 View run FSR_Trainable_4d46109e at: https://wandb.ai/seokjin/FSR-prediction/runs/4d46109e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199330)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010802-4d46109e/logs\n",
      "2023-08-11 01:08:13,588\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.248 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:08:13,593\tWARNING util.py:315 -- The `process_trial_result` operation took 2.255 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:08:13,596\tWARNING util.py:315 -- Processing trial results took 2.257 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:08:13,598\tWARNING util.py:315 -- The `process_trial_result` operation took 2.259 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_12d3f88c_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-07-55/wandb/run-20230811_010817-12d3f88c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Syncing run FSR_Trainable_12d3f88c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/12d3f88c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:                mae_coord ██▇▆▄▃▃▃▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:                mae_force █▇▆▂▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:               mape_coord ██▇▆▅▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:               mape_force ▆█▇▃▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:                   metric █▅▃▂▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:               rmse_coord █▇▅▃▁▁▁▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:               rmse_force █▆▄▂▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:         time_this_iter_s █▅▄▂▁▂▃▂▃▅▃▂▃▂▂▁▃▅▂▃▂▁▅▅▃▄▂▃▅▂▅▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:                timestamp ▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:               tmae_coord █▇▆▅▃▂▁▂▂▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:               tmae_force ▃▇▇▃▁▂▂▃▄▅▆▆▇▇███████▇▇▇▆▆▆▅▅▄▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:              tmape_coord █▇▇▆▅▅▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:              tmape_force ▁▅▇▆▆▆▆▆▇▇▇██████████▇▇▇▇▇▆▆▆▆▆▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:              trmse_coord █▆▄▂▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:              trmse_force █▅▃▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:                mae_coord 5.50745\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:                mae_force 782.71311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:               mape_coord 1.23523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:               mape_force 1.306367793026674e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:                   metric 0.71721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:               rmse_coord 2.61629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:               rmse_force 477.63004\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:       time_since_restore 29.31878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:         time_this_iter_s 0.8323\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:             time_total_s 29.31878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:                timestamp 1691683703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:               tmae_coord 1.16595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:               tmae_force 0.35743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:              tmape_coord 80148114299528.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:              tmape_force 734429674191582.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:              trmse_coord 0.51552\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb:              trmse_force 0.20169\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: 🚀 View run FSR_Trainable_a152f03f at: https://wandb.ai/seokjin/FSR-prediction/runs/a152f03f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199150)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010752-a152f03f/logs\n",
      "2023-08-11 01:08:29,248\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.245 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:08:29,253\tWARNING util.py:315 -- The `process_trial_result` operation took 2.251 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:08:29,255\tWARNING util.py:315 -- Processing trial results took 2.252 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:08:29,257\tWARNING util.py:315 -- The `process_trial_result` operation took 2.254 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_cb5a0b86_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-08-10/wandb/run-20230811_010833-cb5a0b86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: Syncing run FSR_Trainable_cb5a0b86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cb5a0b86\n",
      "2023-08-11 01:08:45,270\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.065 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:08:45,272\tWARNING util.py:315 -- The `process_trial_result` operation took 2.068 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:08:45,273\tWARNING util.py:315 -- Processing trial results took 2.069 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:08:45,275\tWARNING util.py:315 -- The `process_trial_result` operation took 2.071 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_65119616_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-08-25/wandb/run-20230811_010850-65119616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: Syncing run FSR_Trainable_65119616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/65119616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:                mae_coord ▃█▇▆▃▃▃▂▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▄▅▄▃▄▅▅▄▅▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:                mae_force █▆▄▃▂▂▂▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:               mape_coord ███▇▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:               mape_force ██▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▃▃▁▃▃▃▃▁▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:                   metric █▆▄▃▂▂▂▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▂▂▁▁▂▂▁▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:               rmse_coord ▅▆▅▃▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▇▅▄▅▇█▆▇▅▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:               rmse_force █▅▃▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:       time_since_restore ▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:         time_this_iter_s ▅▃▂▁▂▃▆▄▃▃▂▄▄▄▃▂▄▃▂▂▆▄▃▄▄▃▃▄▆▅▃▄▂▃▃▅▄▆▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:             time_total_s ▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:               tmae_coord ▄█▆▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▂▂▃▄▂▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:               tmae_force ▆▇█▇▆▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:              tmape_coord █▆▆▆▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:              tmape_force ▂▅███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:              trmse_coord ██▄▂▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▅▄▃▃▅▅▄▅▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:              trmse_force █▆▄▄▃▃▃▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:                mae_coord 5.8043\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:                mae_force 704.01951\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:               mape_coord 1.24093\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:               mape_force 9.428455709878086e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:                   metric 0.7043\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:               rmse_coord 2.73028\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:               rmse_force 486.6715\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:       time_since_restore 59.83779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:         time_this_iter_s 1.54562\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:             time_total_s 59.83779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:                timestamp 1691683732\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:               tmae_coord 1.20357\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:               tmae_force 0.28174\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:              tmape_coord 76105181796015.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:              tmape_force 443164159231403.75\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:              trmse_coord 0.52612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb:              trmse_force 0.17818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: 🚀 View run FSR_Trainable_739c4bd5 at: https://wandb.ai/seokjin/FSR-prediction/runs/739c4bd5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=198964)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010741-739c4bd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: | 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:                mae_coord █▅▂▁▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▆▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:                mae_force █▆▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:               mape_coord █▆▅▄▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:               mape_force ██▇▅▆▆▅▅▄▃▃▃▃▃▃▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:                   metric █▅▂▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:               rmse_coord ▅▃▁▁▂▂▂▂▃▄▄▅▅▆▆▆▇▇▇▇████████████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:               rmse_force █▆▄▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:         time_this_iter_s ▄▄▃▃▂▁▂▁▂▁▂▂▃▅▄▂▆▄▃▇█▆▄▄▁▁▁▁▁▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:               tmae_coord █▅▂▁▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:               tmae_force ▆▇█▆▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▅▅▅▄▄▃▃▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:              tmape_coord █▆▆▆▇▆▆▅▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:              tmape_force ▁▃▆▇███▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▄▄▄▄▄▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:              trmse_coord █▄▂▁▁▂▂▃▃▄▄▅▅▆▆▆▇▇▇▇████████████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:              trmse_force █▆▅▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:                mae_coord 5.67946\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:                mae_force 747.5627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:               mape_coord 1.23693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:               mape_force 1.2480529963736335e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:                   metric 0.71555\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:               rmse_coord 2.65419\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:               rmse_force 455.56465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:       time_since_restore 34.12075\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:         time_this_iter_s 1.08131\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:             time_total_s 34.12075\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:                timestamp 1691683747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:               tmae_coord 1.20395\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:               tmae_force 0.3352\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:              tmape_coord 77106754713090.58\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:              tmape_force 678170941898821.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:              trmse_coord 0.52504\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb:              trmse_force 0.19051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: 🚀 View run FSR_Trainable_cb5a0b86 at: https://wandb.ai/seokjin/FSR-prediction/runs/cb5a0b86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199793)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010833-cb5a0b86/logs\n",
      "2023-08-11 01:09:15,207\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.525 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:09:15,210\tWARNING util.py:315 -- The `process_trial_result` operation took 2.529 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:09:15,212\tWARNING util.py:315 -- Processing trial results took 2.531 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:09:15,213\tWARNING util.py:315 -- The `process_trial_result` operation took 2.533 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_f9bd9eb4_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-08-41/wandb/run-20230811_010922-f9bd9eb4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: Syncing run FSR_Trainable_f9bd9eb4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f9bd9eb4\n",
      "2023-08-11 01:09:35,296\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.456 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:09:35,301\tWARNING util.py:315 -- The `process_trial_result` operation took 2.462 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:09:35,303\tWARNING util.py:315 -- Processing trial results took 2.463 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:09:35,305\tWARNING util.py:315 -- The `process_trial_result` operation took 2.466 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_ca89b0d0_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-09-10/wandb/run-20230811_010940-ca89b0d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: Syncing run FSR_Trainable_ca89b0d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ca89b0d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: | 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:                mae_coord 4.0089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:                mae_force 1154.84673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:               mape_coord 1.14775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:               mape_force 2.6133599811868588e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:                   metric 40.22243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:               rmse_coord 2.38092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:               rmse_force 994.54217\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:       time_since_restore 1.87592\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:         time_this_iter_s 1.87592\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:             time_total_s 1.87592\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:                timestamp 1691683772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:               tmae_coord 28.06565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:               tmae_force 5.99127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:              tmape_coord 3147145047508879.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:              tmape_force 1054346454077906.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:              trmse_coord 33.15274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb:              trmse_force 7.0697\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: 🚀 View run FSR_Trainable_ca89b0d0 at: https://wandb.ai/seokjin/FSR-prediction/runs/ca89b0d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200541)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010940-ca89b0d0/logs\n",
      "2023-08-11 01:10:00,424\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.969 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:10:00,429\tWARNING util.py:315 -- The `process_trial_result` operation took 1.974 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:10:00,430\tWARNING util.py:315 -- Processing trial results took 1.975 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:10:00,431\tWARNING util.py:315 -- The `process_trial_result` operation took 1.976 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_23bfb9b7_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-09-31/wandb/run-20230811_011004-23bfb9b7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: Syncing run FSR_Trainable_23bfb9b7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/23bfb9b7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:                mae_coord 3.92869\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:                mae_force 1240.35065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:               mape_coord 1.14618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:               mape_force 9.777631052353726e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:                   metric 39.5156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:               rmse_coord 2.37467\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:               rmse_force 1049.67154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:       time_since_restore 2.17463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:         time_this_iter_s 2.17463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:             time_total_s 2.17463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:                timestamp 1691683798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:               tmae_coord 28.10116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:               tmae_force 5.83398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:              tmape_coord 3018628451581468.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:              tmape_force 3690018139166867.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:              trmse_coord 33.12144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb:              trmse_force 6.39416\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: 🚀 View run FSR_Trainable_23bfb9b7 at: https://wandb.ai/seokjin/FSR-prediction/runs/23bfb9b7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200810)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_011004-23bfb9b7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:                mae_coord ▁▂▇▅▄▃▃▄▄▅▅▅▅▅▆▆▇▇██████████████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:                mae_force █▆▅▄▃▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:               mape_coord ▄▅██▇▇▇▆▅▅▅▅▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:               mape_force █▅▄▃▂▁▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:                   metric █▇▆▅▃▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:               rmse_coord ▁▁▅▃▂▁▂▃▃▄▄▄▄▅▅▅▆▆▇▇▇▇▇▇▇███████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:               rmse_force █▇▅▅▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:         time_this_iter_s ▆▆█▇▅▄▂▁▂▄▁▆▇▄▅▃▃▃▃▄▃▅▃▂▂▃▆▂▃▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:                timestamp ▁▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:               tmae_coord ▁▂█▆▄▃▃▄▄▄▄▅▅▅▆▆▇▇██████████████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:               tmae_force █▄▄▃▂▁▃▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:              tmape_coord ██▅▅▄▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:              tmape_force █▃▃▂▁▁▅▆▆▆▆▇▇▆▆▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:              trmse_coord ▄▄█▅▃▁▁▂▂▂▃▃▃▄▄▅▅▆▇▇▇▇▇▇▇▇▆▆▆▆▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:              trmse_force █▇▅▅▃▂▂▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:                mae_coord 5.72732\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:                mae_force 750.43348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:               mape_coord 1.2655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:               mape_force 1.1702299203710326e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:                   metric 0.71173\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:               rmse_coord 2.65271\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:               rmse_force 480.20513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:       time_since_restore 54.98874\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:         time_this_iter_s 1.45285\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:             time_total_s 54.98874\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:                timestamp 1691683816\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:               tmae_coord 1.21189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:               tmae_force 0.33524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:              tmape_coord 79562022869623.23\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:              tmape_force 677515110015287.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:              trmse_coord 0.52342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb:              trmse_force 0.18831\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: 🚀 View run FSR_Trainable_f9bd9eb4 at: https://wandb.ai/seokjin/FSR-prediction/runs/f9bd9eb4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200296)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010922-f9bd9eb4/logs\n",
      "2023-08-11 01:10:25,159\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.776 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:10:25,166\tWARNING util.py:315 -- The `process_trial_result` operation took 3.784 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:10:25,169\tWARNING util.py:315 -- Processing trial results took 3.788 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:10:25,171\tWARNING util.py:315 -- The `process_trial_result` operation took 3.789 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_e0ff0bca_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-09-56/wandb/run-20230811_011029-e0ff0bca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: Syncing run FSR_Trainable_e0ff0bca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e0ff0bca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:                mae_coord 25.02395\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:                mae_force 1756.92251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:               mape_coord 3.89332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:               mape_force 2.946877872182683e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:                   metric 2.20657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:               rmse_coord 8.12372\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:               rmse_force 1065.14446\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:       time_since_restore 3.88747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:         time_this_iter_s 3.88747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:             time_total_s 3.88747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:                timestamp 1691683821\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:               tmae_coord 5.6373\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:               tmae_force 0.72488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:              tmape_coord 23517056090839.445\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:              tmape_force 1497640098841474.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:              trmse_coord 1.82036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb:              trmse_force 0.3862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: 🚀 View run FSR_Trainable_e0ff0bca at: https://wandb.ai/seokjin/FSR-prediction/runs/e0ff0bca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201061)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_011029-e0ff0bca/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:                mae_coord ▇▇▆▄▄▃▂▃▃▃▄▅▆▇██████▇▇▇▆▆▆▅▅▄▄▃▃▃▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:                mae_force █▅▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:               mape_coord ██▆▅▄▄▃▃▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:               mape_force █▅▅▅▄▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:                   metric █▆▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:               rmse_coord ▇▆▃▂▂▂▁▁▂▂▃▄▅▆▇▇███████▇▇▇▇▆▆▆▆▅▅▅▅▅▅▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:               rmse_force █▆▅▅▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:         time_this_iter_s ▅▃▂▂▁▂▃▂▁▁▃▃▃▆▅▁▁▃▃▆▇▆▃▂▁█▅▃▃▅▅▃▂▅▄▃▃▇▄▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:               tmae_coord ██▇▅▄▄▃▄▄▄▅▅▆▆▆▇▇▆▆▆▆▆▅▅▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:               tmae_force █▅▅▄▄▄▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:              tmape_coord █▇▅▅▄▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:              tmape_force █▅▄▄▅▅▆▆▆▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:              trmse_coord ██▅▄▃▃▂▂▃▃▃▃▄▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb:              trmse_force █▅▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "2023-08-11 01:10:45,465\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.965 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:10:45,473\tWARNING util.py:315 -- The `process_trial_result` operation took 2.974 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:10:45,476\tWARNING util.py:315 -- Processing trial results took 2.977 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:10:45,479\tWARNING util.py:315 -- The `process_trial_result` operation took 2.979 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=199555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_4f5ed776_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-10-17/wandb/run-20230811_011047-4f5ed776\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: Syncing run FSR_Trainable_4f5ed776\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4f5ed776\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:                mae_coord 27.66525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:                mae_force 2399.12926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:               mape_coord 4.28174\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:               mape_force 2.7608839514191826e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:                   metric 2.49734\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:               rmse_coord 8.7146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:               rmse_force 1760.14558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:       time_since_restore 5.65688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:         time_this_iter_s 5.65688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:             time_total_s 5.65688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:                timestamp 1691683842\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:               tmae_coord 6.40817\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:               tmae_force 0.78876\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:              tmape_coord 25581304920265.98\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:              tmape_force 1127093720949531.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:              trmse_coord 2.01264\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb:              trmse_force 0.4847\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: 🚀 View run FSR_Trainable_4f5ed776 at: https://wandb.ai/seokjin/FSR-prediction/runs/4f5ed776\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201315)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_011047-4f5ed776/logs\n",
      "2023-08-11 01:10:56,286\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.817 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:10:56,290\tWARNING util.py:315 -- The `process_trial_result` operation took 2.823 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:10:56,293\tWARNING util.py:315 -- Processing trial results took 2.826 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:10:56,295\tWARNING util.py:315 -- The `process_trial_result` operation took 2.827 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201543)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201543)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201543)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201543)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201543)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201543)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201543)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201543)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201543)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_815ec5f7_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-10-37/wandb/run-20230811_011100-815ec5f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201543)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201543)\u001b[0m wandb: Syncing run FSR_Trainable_815ec5f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201543)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201543)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/815ec5f7\n",
      "2023-08-11 01:11:15,826\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.599 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:11:15,831\tWARNING util.py:315 -- The `process_trial_result` operation took 3.605 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:11:15,834\tWARNING util.py:315 -- Processing trial results took 3.609 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:11:15,836\tWARNING util.py:315 -- The `process_trial_result` operation took 3.611 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201776)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201776)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201776)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201776)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201776)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_21fdf89c_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-10-51/wandb/run-20230811_011122-21fdf89c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201776)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201776)\u001b[0m wandb: Syncing run FSR_Trainable_21fdf89c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201776)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=201776)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/21fdf89c\n",
      "2023-08-11 01:11:36,351\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.732 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:11:36,355\tWARNING util.py:315 -- The `process_trial_result` operation took 2.738 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:11:36,356\tWARNING util.py:315 -- Processing trial results took 2.740 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:11:36,358\tWARNING util.py:315 -- The `process_trial_result` operation took 2.742 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-08-11 01:11:38,302 E 181243 181243] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: f26c708f66ae615c627afbdfe06999fde4c91fcd5664d16b5f3733ca, IP: 172.26.215.93) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.26.215.93`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:                mae_coord ▁▄▇▇▆▆▆▆▆▇▇▇▇▇▇█████▇█▆▇█▅▆▆▃▄▇▄▆▇▆▆▆▆▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:                mae_force █▆▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:               mape_coord ▅██▆▄▄▃▃▃▃▄▄▄▄▄▄▄▄▄▃▃▃▂▃▃▂▂▂▁▁▃▂▃▄▄▄▄▃▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:               mape_force █▆▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:                   metric █▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▃▁▁▂▁▁▃▂▂▃▂▂▂▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:               rmse_coord ▁▁▃▃▂▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▆▄▆▆▃▄▅▃▄▇▅▆█▇▇▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:               rmse_force █▆▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:         time_this_iter_s ▄▄▇▂▁▃▄▅▃▁▂▅▄▂▂▂▂▂▂█▃▄▃▂▂▄▂▁▃▅▄▁▁▂▄▂▃▄▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:               tmae_coord ▂▆█▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▄▅▆▃▄▄▁▂▅▂▃▅▄▄▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:               tmae_force █▆▄▃▄▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:              tmape_coord ██▅▅▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:              tmape_force █▆▃▃▄▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:              trmse_coord ▅▅▆▅▅▅▅▄▄▄▄▄▄▄▃▃▄▄▄▄▄▅▂▆▇▁▃▄▁▂▇▃▅█▆▆▇▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:              trmse_force █▆▄▃▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▂▁▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:                mae_coord 5.45673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:                mae_force 648.15859\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:               mape_coord 1.24063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:               mape_force 7.014570713681682e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:                   metric 0.70451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:               rmse_coord 2.73209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:               rmse_force 484.05159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:       time_since_restore 171.9444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:         time_this_iter_s 1.97425\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:             time_total_s 171.9444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:                timestamp 1691683925\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:               tmae_coord 1.13576\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:               tmae_force 0.25311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:              tmape_coord 75183619987948.94\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:              tmape_force 322585663728367.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:              trmse_coord 0.52694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb:              trmse_force 0.17757\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: 🚀 View run FSR_Trainable_65119616 at: https://wandb.ai/seokjin/FSR-prediction/runs/65119616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=200023)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_010850-65119616/logs\n",
      "2023-08-11 01:12:11,480\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 5.208 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:12:11,489\tWARNING util.py:315 -- The `process_trial_result` operation took 5.217 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:12:11,492\tWARNING util.py:315 -- Processing trial results took 5.220 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:12:11,494\tWARNING util.py:315 -- The `process_trial_result` operation took 5.222 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=202149)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=202149)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=202149)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=202149)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=202149)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_3838379f_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-11-10/wandb/run-20230811_011216-3838379f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=202149)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=202149)\u001b[0m wandb: Syncing run FSR_Trainable_3838379f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=202149)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=202149)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3838379f\n",
      "2023-08-11 01:12:33,609\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.361 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:12:33,613\tWARNING util.py:315 -- The `process_trial_result` operation took 2.367 s, which may be a performance bottleneck.\n",
      "2023-08-11 01:12:33,614\tWARNING util.py:315 -- Processing trial results took 2.368 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 01:12:33,616\tWARNING util.py:315 -- The `process_trial_result` operation took 2.370 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=202385)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-08-11 01:12:38,303 E 181243 181243] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: f26c708f66ae615c627afbdfe06999fde4c91fcd5664d16b5f3733ca, IP: 172.26.215.93) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.26.215.93`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=202385)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=202385)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=202385)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=202385)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_9a152f5f_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-11-31/wandb/run-20230811_011237-9a152f5f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=202385)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=202385)\u001b[0m wandb: Syncing run FSR_Trainable_9a152f5f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=202385)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=202385)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9a152f5f\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "The Ray Tune run failed. Please inspect the previous error messages for a cause. After fixing the issue, you can restart the run from scratch or continue this run. To continue this run, you can use `tuner = Tuner.restore(\"/home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35\", trainable=...)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/tune_controller.py:815\u001b[0m, in \u001b[0;36mTuneController._schedule_trial_task.<locals>._on_result\u001b[0;34m(tracked_actor, *args, **kwargs)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     on_result(trial, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    816\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:735\u001b[0m, in \u001b[0;36m_TuneControllerBase._on_training_result\u001b[0;34m(self, trial, result)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[39mwith\u001b[39;00m warn_if_slow(\u001b[39m\"\u001b[39m\u001b[39mprocess_trial_result\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 735\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_trial_results(trial, result)\n\u001b[1;32m    736\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_execute_queued_decision(trial)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:748\u001b[0m, in \u001b[0;36m_TuneControllerBase._process_trial_results\u001b[0;34m(self, trial, results)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[39mwith\u001b[39;00m warn_if_slow(\u001b[39m\"\u001b[39m\u001b[39mprocess_trial_result\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 748\u001b[0m     decision \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_trial_result(trial, result)\n\u001b[1;32m    749\u001b[0m \u001b[39mif\u001b[39;00m decision \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    750\u001b[0m     \u001b[39m# If we didn't get a decision, this means a\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[39m# non-training future (e.g. a save) was scheduled.\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[39m# We do not allow processing more results then.\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:805\u001b[0m, in \u001b[0;36m_TuneControllerBase._process_trial_result\u001b[0;34m(self, trial, result)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[39mwith\u001b[39;00m warn_if_slow(\u001b[39m\"\u001b[39m\u001b[39mcallbacks.on_trial_result\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 805\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_callbacks\u001b[39m.\u001b[39;49mon_trial_result(\n\u001b[1;32m    806\u001b[0m         iteration\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iteration,\n\u001b[1;32m    807\u001b[0m         trials\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_trials,\n\u001b[1;32m    808\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m    809\u001b[0m         result\u001b[39m=\u001b[39;49mresult\u001b[39m.\u001b[39;49mcopy(),\n\u001b[1;32m    810\u001b[0m     )\n\u001b[1;32m    811\u001b[0m trial\u001b[39m.\u001b[39mupdate_last_result(result)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/callback.py:392\u001b[0m, in \u001b[0;36mCallbackList.on_trial_result\u001b[0;34m(self, **info)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callbacks:\n\u001b[0;32m--> 392\u001b[0m     callback\u001b[39m.\u001b[39;49mon_trial_result(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minfo)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/logger/logger.py:140\u001b[0m, in \u001b[0;36mLoggerCallback.on_trial_result\u001b[0;34m(self, iteration, trials, trial, result, **info)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_trial_result\u001b[39m(\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    134\u001b[0m     iteration: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minfo,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_trial_result(iteration, trial, result)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/air/integrations/wandb.py:679\u001b[0m, in \u001b[0;36mWandbLoggerCallback.log_trial_result\u001b[0;34m(self, iteration, trial, result)\u001b[0m\n\u001b[1;32m    678\u001b[0m result \u001b[39m=\u001b[39m _clean_log(result)\n\u001b[0;32m--> 679\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_trial_queues[trial]\u001b[39m.\u001b[39;49mput((_QueueItem\u001b[39m.\u001b[39;49mRESULT, result))\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/util/queue.py:105\u001b[0m, in \u001b[0;36mQueue.put\u001b[0;34m(self, item, block, timeout)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     ray\u001b[39m.\u001b[39;49mget(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor\u001b[39m.\u001b[39;49mput\u001b[39m.\u001b[39;49mremote(item, timeout))\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:18\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m auto_init_ray()\n\u001b[0;32m---> 18\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/_private/worker.py:2542\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2541\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2542\u001b[0m             \u001b[39mraise\u001b[39;00m value\n\u001b[1;32m   2544\u001b[0m \u001b[39mif\u001b[39;00m is_individual_id:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Task was killed due to the node running low on memory.\nMemory on the node (IP: 172.26.215.93, ID: f26c708f66ae615c627afbdfe06999fde4c91fcd5664d16b5f3733ca) where the task (actor ID: 022bd7e220b596903f1a71b901000000, name=_QueueActor.__init__, pid=202383, memory used=0.14GB) was running was 7.33GB / 7.72GB (0.950235), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dc7f3e41318be3b9b9f9075eef6d0e5024d789f2868c8e25a2bfa8c7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.26.215.93`. To see the logs of the worker, use `ray logs worker-dc7f3e41318be3b9b9f9075eef6d0e5024d789f2868c8e25a2bfa8c7*out -ip 172.26.215.93. Top 10 memory users:\nPID\tMEM(GB)\tCOMMAND\n100531\t0.55\t/home/seokj/.vscode-server/bin/74f6148eb9ea00507ec113ec51c489d6ffb4b771/node /home/seokj/.vscode-ser...\n200484\t0.54\t/home/seokj/workspace/.venv/bin/python -m ipykernel_launcher --ip=127.0.0.1 --stdin=9046 --control=9...\n202290\t0.38\tray::ResourceTrainable.train\n201433\t0.36\tray::ResourceTrainable.train\n201909\t0.36\tray::ResourceTrainable.train\n201683\t0.36\tray::ResourceTrainable.train\n133043\t0.30\t/home/seokj/workspace/.venv/bin/python -m ipykernel_launcher --ip=127.0.0.1 --stdin=9008 --control=9...\n133315\t0.28\t/home/seokj/workspace/.venv/bin/python -m ipykernel_launcher --ip=127.0.0.1 --stdin=9013 --control=9...\n100443\t0.25\t/home/seokj/.vscode-server/bin/74f6148eb9ea00507ec113ec51c489d6ffb4b771/node /home/seokj/.vscode-ser...\n133313\t0.23\t/home/seokj/workspace/.venv/bin/python -m ipykernel_launcher --ip=127.0.0.1 --stdin=9003 --control=9...\nRefer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/tuner.py:347\u001b[0m, in \u001b[0;36mTuner.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_local_tuner\u001b[39m.\u001b[39;49mfit()\n\u001b[1;32m    348\u001b[0m \u001b[39mexcept\u001b[39;00m TuneError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py:588\u001b[0m, in \u001b[0;36mTunerInternal.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_restored:\n\u001b[0;32m--> 588\u001b[0m     analysis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_internal(trainable, param_space)\n\u001b[1;32m    589\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py:712\u001b[0m, in \u001b[0;36mTunerInternal._fit_internal\u001b[0;34m(self, trainable, param_space)\u001b[0m\n\u001b[1;32m    699\u001b[0m args \u001b[39m=\u001b[39m {\n\u001b[1;32m    700\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tune_run_arguments(trainable),\n\u001b[1;32m    701\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tuner_kwargs,\n\u001b[1;32m    711\u001b[0m }\n\u001b[0;32m--> 712\u001b[0m analysis \u001b[39m=\u001b[39m run(\n\u001b[1;32m    713\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49margs,\n\u001b[1;32m    714\u001b[0m )\n\u001b[1;32m    715\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclear_remote_string_queue()\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/tune.py:1070\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, checkpoint_keep_all_ranks, checkpoint_upload_from_workers, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, trial_executor, local_dir, _experiment_checkpoint_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m   1068\u001b[0m     \u001b[39mnot\u001b[39;00m runner\u001b[39m.\u001b[39mis_finished() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m experiment_interrupted_event\u001b[39m.\u001b[39mis_set()\n\u001b[1;32m   1069\u001b[0m ):\n\u001b[0;32m-> 1070\u001b[0m     runner\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m   1071\u001b[0m     \u001b[39mif\u001b[39;00m has_verbosity(Verbosity\u001b[39m.\u001b[39mV1_EXPERIMENT):\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/tune_controller.py:256\u001b[0m, in \u001b[0;36mTuneController.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39m# Handle one event\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_actor_manager\u001b[39m.\u001b[39;49mnext(timeout\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m):\n\u001b[1;32m    257\u001b[0m     \u001b[39m# If there are no actors running, warn about potentially\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[39m# insufficient resources\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_actor_manager\u001b[39m.\u001b[39mnum_live_actors:\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/air/execution/_internal/actor_manager.py:224\u001b[0m, in \u001b[0;36mRayActorManager.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39melif\u001b[39;00m future \u001b[39min\u001b[39;00m actor_task_futures:\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_actor_task_events\u001b[39m.\u001b[39;49mresolve_future(future)\n\u001b[1;32m    225\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py:118\u001b[0m, in \u001b[0;36mRayEventManager.resolve_future\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m on_result:\n\u001b[0;32m--> 118\u001b[0m     on_result(result)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/air/execution/_internal/actor_manager.py:752\u001b[0m, in \u001b[0;36mRayActorManager._schedule_tracked_actor_task.<locals>.on_result\u001b[0;34m(result)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_result\u001b[39m(result: Any):\n\u001b[0;32m--> 752\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_actor_task_resolved(\n\u001b[1;32m    753\u001b[0m         tracked_actor_task\u001b[39m=\u001b[39;49mtracked_actor_task, result\u001b[39m=\u001b[39;49mresult\n\u001b[1;32m    754\u001b[0m     )\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/air/execution/_internal/actor_manager.py:300\u001b[0m, in \u001b[0;36mRayActorManager._actor_task_resolved\u001b[0;34m(self, tracked_actor_task, result)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mif\u001b[39;00m tracked_actor_task\u001b[39m.\u001b[39m_on_result:\n\u001b[0;32m--> 300\u001b[0m     tracked_actor_task\u001b[39m.\u001b[39;49m_on_result(tracked_actor, result)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/tune_controller.py:824\u001b[0m, in \u001b[0;36mTuneController._schedule_trial_task.<locals>._on_result\u001b[0;34m(tracked_actor, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[39mraise\u001b[39;00m TuneError(traceback\u001b[39m.\u001b[39mformat_exc())\n",
      "\u001b[0;31mTuneError\u001b[0m: Traceback (most recent call last):\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/tune_controller.py\", line 815, in _on_result\n    on_result(trial, *args, **kwargs)\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py\", line 735, in _on_training_result\n    self._process_trial_results(trial, result)\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py\", line 748, in _process_trial_results\n    decision = self._process_trial_result(trial, result)\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py\", line 805, in _process_trial_result\n    self._callbacks.on_trial_result(\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/callback.py\", line 392, in on_trial_result\n    callback.on_trial_result(**info)\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/logger/logger.py\", line 140, in on_trial_result\n    self.log_trial_result(iteration, trial, result)\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/air/integrations/wandb.py\", line 679, in log_trial_result\n    self._trial_queues[trial].put((_QueueItem.RESULT, result))\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/util/queue.py\", line 105, in put\n    ray.get(self.actor.put.remote(item, timeout))\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 2542, in get\n    raise value\nray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\nMemory on the node (IP: 172.26.215.93, ID: f26c708f66ae615c627afbdfe06999fde4c91fcd5664d16b5f3733ca) where the task (actor ID: 022bd7e220b596903f1a71b901000000, name=_QueueActor.__init__, pid=202383, memory used=0.14GB) was running was 7.33GB / 7.72GB (0.950235), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: dc7f3e41318be3b9b9f9075eef6d0e5024d789f2868c8e25a2bfa8c7) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.26.215.93`. To see the logs of the worker, use `ray logs worker-dc7f3e41318be3b9b9f9075eef6d0e5024d789f2868c8e25a2bfa8c7*out -ip 172.26.215.93. Top 10 memory users:\nPID\tMEM(GB)\tCOMMAND\n100531\t0.55\t/home/seokj/.vscode-server/bin/74f6148eb9ea00507ec113ec51c489d6ffb4b771/node /home/seokj/.vscode-ser...\n200484\t0.54\t/home/seokj/workspace/.venv/bin/python -m ipykernel_launcher --ip=127.0.0.1 --stdin=9046 --control=9...\n202290\t0.38\tray::ResourceTrainable.train\n201433\t0.36\tray::ResourceTrainable.train\n201909\t0.36\tray::ResourceTrainable.train\n201683\t0.36\tray::ResourceTrainable.train\n133043\t0.30\t/home/seokj/workspace/.venv/bin/python -m ipykernel_launcher --ip=127.0.0.1 --stdin=9008 --control=9...\n133315\t0.28\t/home/seokj/workspace/.venv/bin/python -m ipykernel_launcher --ip=127.0.0.1 --stdin=9013 --control=9...\n100443\t0.25\t/home/seokj/.vscode-server/bin/74f6148eb9ea00507ec113ec51c489d6ffb4b771/node /home/seokj/.vscode-ser...\n133313\t0.23\t/home/seokj/workspace/.venv/bin/python -m ipykernel_launcher --ip=127.0.0.1 --stdin=9003 --control=9...\nRefer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39;49mfit()\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/tuner.py:349\u001b[0m, in \u001b[0;36mTuner.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_tuner\u001b[39m.\u001b[39mfit()\n\u001b[1;32m    348\u001b[0m     \u001b[39mexcept\u001b[39;00m TuneError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 349\u001b[0m         \u001b[39mraise\u001b[39;00m TuneError(\n\u001b[1;32m    350\u001b[0m             _TUNER_FAILED_MSG\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    351\u001b[0m                 path\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_tuner\u001b[39m.\u001b[39mget_experiment_checkpoint_dir()\n\u001b[1;32m    352\u001b[0m             )\n\u001b[1;32m    353\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m     experiment_checkpoint_dir \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39mget(\n\u001b[1;32m    356\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remote_tuner\u001b[39m.\u001b[39mget_experiment_checkpoint_dir\u001b[39m.\u001b[39mremote()\n\u001b[1;32m    357\u001b[0m     )\n",
      "\u001b[0;31mTuneError\u001b[0m: The Ray Tune run failed. Please inspect the previous error messages for a cause. After fixing the issue, you can restart the run from scratch or continue this run. To continue this run, you can use `tuner = Tuner.restore(\"/home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35\", trainable=...)`."
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to read the results for 2 trials:\n",
      "- /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_48eada07_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-12-28\n",
      "- /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_57b5d6f7_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-45-46\n",
      "2023-08-11 03:31:22,078\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-08-11 03:31:23,247\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-08-11 03:33:29</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:05.50        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.3/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=17<br>Bracket: Iter 64.000: -0.6896651606278077 | Iter 32.000: -0.6927863841207486 | Iter 16.000: -0.6919857686755788 | Iter 8.000: -0.6916639432359264 | Iter 4.000: -0.7103094813534823 | Iter 2.000: -0.7739583547935092 | Iter 1.000: -0.8210422490873328<br>Logical resource usage: 1.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>imputer             </th><th>imputer_args/strateg\n",
       "y       </th><th>index_X             </th><th>index_y             </th><th>model         </th><th style=\"text-align: right;\">    model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">  model_args/num_layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  tmae_force</th><th style=\"text-align: right;\">  trmse_force</th><th style=\"text-align: right;\">  tmape_force</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_9a152f5f</td><td>TERMINATED</td><td>172.26.215.93:209202</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0990</td><td>sklearn.impute._3820</td><td>mean  </td><td>[&#x27;FSR_for_force_6540</td><td>[&#x27;force&#x27;, &#x27;x_co_6040</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000856839</td><td>sklearn.preproc_08d0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        8.06291 </td><td style=\"text-align: right;\">    0.38511 </td><td style=\"text-align: right;\">     0.235137</td><td style=\"text-align: right;\">  7.41493e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_21fdf89c</td><td>TERMINATED</td><td>172.26.215.93:209203</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_9bf0</td><td>sklearn.impute._1f70</td><td>mean  </td><td>[&#x27;FSR_for_force_f700</td><td>[&#x27;force&#x27;, &#x27;x_co_e2c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00154315 </td><td>sklearn.preproc_95f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      103.181   </td><td style=\"text-align: right;\">    0.263298</td><td style=\"text-align: right;\">     0.168299</td><td style=\"text-align: right;\">  4.10181e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_3838379f</td><td>TERMINATED</td><td>172.26.215.93:209204</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4db0</td><td>sklearn.impute._fc80</td><td>mean  </td><td>[&#x27;FSR_for_force_57c0</td><td>[&#x27;force&#x27;, &#x27;x_co_a800</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00172314 </td><td>sklearn.preproc_4c90</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       89.8499  </td><td style=\"text-align: right;\">    0.255212</td><td style=\"text-align: right;\">     0.165868</td><td style=\"text-align: right;\">  3.84505e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_815ec5f7</td><td>TERMINATED</td><td>172.26.215.93:209205</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_1bf0</td><td>sklearn.impute._9f20</td><td>mean  </td><td>[&#x27;FSR_for_force_aa80</td><td>[&#x27;force&#x27;, &#x27;x_co_c900</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00166242 </td><td>sklearn.preproc_2df0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      106.7     </td><td style=\"text-align: right;\">    0.252841</td><td style=\"text-align: right;\">     0.163021</td><td style=\"text-align: right;\">  3.92701e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_65119616</td><td>TERMINATED</td><td>172.26.215.93:199929</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_3e10</td><td>sklearn.impute._2dd0</td><td>mean  </td><td>[&#x27;FSR_for_force_7fc0</td><td>[&#x27;force&#x27;, &#x27;x_co_73c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000928866</td><td>sklearn.preproc_3bd0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      171.944   </td><td style=\"text-align: right;\">    0.253109</td><td style=\"text-align: right;\">     0.177573</td><td style=\"text-align: right;\">  3.22586e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_4f5ed776</td><td>TERMINATED</td><td>172.26.215.93:201182</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8030</td><td>sklearn.impute._74b0</td><td>mean  </td><td>[&#x27;FSR_for_force_ad80</td><td>[&#x27;force&#x27;, &#x27;x_co_6b80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000898585</td><td>sklearn.preproc_8e70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        5.65688 </td><td style=\"text-align: right;\">    0.788755</td><td style=\"text-align: right;\">     0.484698</td><td style=\"text-align: right;\">  1.12709e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_12d3f88c</td><td>TERMINATED</td><td>172.26.215.93:199460</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4090</td><td>sklearn.impute._de30</td><td>mean  </td><td>[&#x27;FSR_for_force_e800</td><td>[&#x27;force&#x27;, &#x27;x_co_8640</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000959015</td><td>sklearn.preproc_7f30</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      116.262   </td><td style=\"text-align: right;\">    0.269049</td><td style=\"text-align: right;\">     0.170632</td><td style=\"text-align: right;\">  4.0038e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_e0ff0bca</td><td>TERMINATED</td><td>172.26.215.93:200949</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_33f0</td><td>sklearn.impute._1e30</td><td>mean  </td><td>[&#x27;FSR_for_force_6fc0</td><td>[&#x27;force&#x27;, &#x27;x_co_eb00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00138584 </td><td>sklearn.preproc_31b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.88747 </td><td style=\"text-align: right;\">    0.724879</td><td style=\"text-align: right;\">     0.386201</td><td style=\"text-align: right;\">  1.49764e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_f9bd9eb4</td><td>TERMINATED</td><td>172.26.215.93:200214</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8c30</td><td>sklearn.impute._2510</td><td>mean  </td><td>[&#x27;FSR_for_force_9fc0</td><td>[&#x27;force&#x27;, &#x27;x_co_4d80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00123867 </td><td>sklearn.preproc_9ad0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       54.9887  </td><td style=\"text-align: right;\">    0.335242</td><td style=\"text-align: right;\">     0.188311</td><td style=\"text-align: right;\">  6.77515e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_23bfb9b7</td><td>TERMINATED</td><td>172.26.215.93:200699</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_d8f0</td><td>sklearn.impute._35f0</td><td>mean  </td><td>[&#x27;FSR_for_force_3cc0</td><td>[&#x27;force&#x27;, &#x27;x_co_3f80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0048073  </td><td>sklearn.preproc_d6b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.17463 </td><td style=\"text-align: right;\">    5.83398 </td><td style=\"text-align: right;\">     6.39416 </td><td style=\"text-align: right;\">  3.69002e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_ca89b0d0</td><td>TERMINATED</td><td>172.26.215.93:200436</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0b70</td><td>sklearn.impute._1e80</td><td>mean  </td><td>[&#x27;FSR_for_force_7d40</td><td>[&#x27;force&#x27;, &#x27;x_co_5180</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000976739</td><td>sklearn.preproc_09f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.87592 </td><td style=\"text-align: right;\">    5.99127 </td><td style=\"text-align: right;\">     7.0697  </td><td style=\"text-align: right;\">  1.05435e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_cb5a0b86</td><td>TERMINATED</td><td>172.26.215.93:199693</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_87b0</td><td>sklearn.impute._b0a0</td><td>mean  </td><td>[&#x27;FSR_for_force_2300</td><td>[&#x27;force&#x27;, &#x27;x_co_c6c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00356107 </td><td>sklearn.preproc_9710</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       34.1208  </td><td style=\"text-align: right;\">    0.335201</td><td style=\"text-align: right;\">     0.19051 </td><td style=\"text-align: right;\">  6.78171e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_739c4bd5</td><td>TERMINATED</td><td>172.26.215.93:198882</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_6430</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>[&#x27;FSR_for_force_b8c0</td><td>[&#x27;force&#x27;, &#x27;x_co_8ac0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00352884 </td><td>sklearn.preproc_61f0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       59.8378  </td><td style=\"text-align: right;\">    0.281742</td><td style=\"text-align: right;\">     0.178176</td><td style=\"text-align: right;\">  4.43164e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_a152f03f</td><td>TERMINATED</td><td>172.26.215.93:198965</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_5770</td><td>sklearn.impute._71e0</td><td>mean  </td><td>[&#x27;FSR_for_force_a6c0</td><td>[&#x27;force&#x27;, &#x27;x_co_82c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00345198 </td><td>sklearn.preproc_6610</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       29.3188  </td><td style=\"text-align: right;\">    0.357425</td><td style=\"text-align: right;\">     0.201691</td><td style=\"text-align: right;\">  7.3443e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_4d46109e</td><td>TERMINATED</td><td>172.26.215.93:199151</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_aa30</td><td>sklearn.impute._faa0</td><td>mean  </td><td>[&#x27;FSR_for_force_fe80</td><td>[&#x27;force&#x27;, &#x27;x_co_7500</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000201056</td><td>sklearn.preproc_a7f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.16815 </td><td style=\"text-align: right;\">    0.49429 </td><td style=\"text-align: right;\">     0.33469 </td><td style=\"text-align: right;\">  7.41904e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_ea590b92</td><td>TERMINATED</td><td>172.26.215.93:198622</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_dc50</td><td>sklearn.impute._3820</td><td>mean  </td><td>[&#x27;FSR_for_force_b180</td><td>[&#x27;force&#x27;, &#x27;x_co_05c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000448574</td><td>sklearn.preproc_d650</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.72794 </td><td style=\"text-align: right;\">    0.707441</td><td style=\"text-align: right;\">     0.437915</td><td style=\"text-align: right;\">  1.15417e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_5ce6d566</td><td>TERMINATED</td><td>172.26.215.93:197190</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_43f0</td><td>sklearn.impute._a920</td><td>mean  </td><td>[&#x27;FSR_for_force_2c00</td><td>[&#x27;force&#x27;, &#x27;x_co_fe40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00118859 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       91.7337  </td><td style=\"text-align: right;\">    0.248612</td><td style=\"text-align: right;\">     0.16589 </td><td style=\"text-align: right;\">  3.35444e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_5a8ffac3</td><td>TERMINATED</td><td>172.26.215.93:197906</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_16b0</td><td>sklearn.impute._de30</td><td>mean  </td><td>[&#x27;FSR_for_force_22c0</td><td>[&#x27;force&#x27;, &#x27;x_co_4840</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00690872 </td><td>sklearn.preproc_2610</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       29.8279  </td><td style=\"text-align: right;\">    0.359596</td><td style=\"text-align: right;\">     0.208549</td><td style=\"text-align: right;\">  7.00708e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_ad00c085</td><td>TERMINATED</td><td>172.26.215.93:198383</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_20d0</td><td>sklearn.impute._12a0</td><td>mean  </td><td>[&#x27;FSR_for_force_ac40</td><td>[&#x27;force&#x27;, &#x27;x_co_9c80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00707276 </td><td>sklearn.preproc_1e90</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.19177 </td><td style=\"text-align: right;\">    2.0798  </td><td style=\"text-align: right;\">     1.40659 </td><td style=\"text-align: right;\">114.954      </td></tr>\n",
       "<tr><td>FSR_Trainable_d54e09c5</td><td>TERMINATED</td><td>172.26.215.93:197433</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_1ef0</td><td>sklearn.impute._9250</td><td>mean  </td><td>[&#x27;FSR_for_force_fc00</td><td>[&#x27;force&#x27;, &#x27;x_co_7400</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00190057 </td><td>sklearn.preproc_2d90</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       59.249   </td><td style=\"text-align: right;\">    0.290744</td><td style=\"text-align: right;\">     0.174419</td><td style=\"text-align: right;\">  5.0318e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_1c686623</td><td>TERMINATED</td><td>172.26.215.93:198147</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4870</td><td>sklearn.impute._b8c0</td><td>mean  </td><td>[&#x27;FSR_for_force_c040</td><td>[&#x27;force&#x27;, &#x27;x_co_0680</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00602797 </td><td>sklearn.preproc_57d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.38901 </td><td style=\"text-align: right;\">    2.45558 </td><td style=\"text-align: right;\">     1.52909 </td><td style=\"text-align: right;\">103.261      </td></tr>\n",
       "<tr><td>FSR_Trainable_170adff7</td><td>TERMINATED</td><td>172.26.215.93:196857</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c930</td><td>sklearn.impute._fd70</td><td>mean  </td><td>[&#x27;FSR_for_force_bc00</td><td>[&#x27;force&#x27;, &#x27;x_co_10c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00113796 </td><td>sklearn.preproc_c870</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      101.114   </td><td style=\"text-align: right;\">    0.250942</td><td style=\"text-align: right;\">     0.166022</td><td style=\"text-align: right;\">  3.52458e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_5aadf73d</td><td>TERMINATED</td><td>172.26.215.93:197697</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_1890</td><td>sklearn.impute._25b0</td><td>mean  </td><td>[&#x27;FSR_for_force_7d00</td><td>[&#x27;force&#x27;, &#x27;x_co_7880</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000448327</td><td>sklearn.preproc_2730</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.66906 </td><td style=\"text-align: right;\">    0.710857</td><td style=\"text-align: right;\">     0.418041</td><td style=\"text-align: right;\">  1.4639e+15 </td></tr>\n",
       "<tr><td>FSR_Trainable_f90c4aea</td><td>TERMINATED</td><td>172.26.215.93:196649</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_09f0</td><td>sklearn.impute._0350</td><td>mean  </td><td>[&#x27;FSR_for_force_6740</td><td>[&#x27;force&#x27;, &#x27;x_co_7f40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00684209 </td><td>sklearn.preproc_0630</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       96.6735  </td><td style=\"text-align: right;\">    0.265384</td><td style=\"text-align: right;\">     0.162764</td><td style=\"text-align: right;\">  4.39775e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_683aa5cb</td><td>TERMINATED</td><td>172.26.215.93:196393</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_f450</td><td>sklearn.impute._2ba0</td><td>mean  </td><td>[&#x27;FSR_for_force_ae40</td><td>[&#x27;force&#x27;, &#x27;x_co_4040</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000406177</td><td>sklearn.preproc_f210</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      101.473   </td><td style=\"text-align: right;\">    0.273943</td><td style=\"text-align: right;\">     0.169602</td><td style=\"text-align: right;\">  4.37289e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_14a7a158</td><td>TERMINATED</td><td>172.26.215.93:196148</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_3570</td><td>sklearn.impute._2bf0</td><td>mean  </td><td>[&#x27;FSR_for_force_0380</td><td>[&#x27;force&#x27;, &#x27;x_co_d680</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00192089 </td><td>sklearn.preproc_3510</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      101.278   </td><td style=\"text-align: right;\">    0.265228</td><td style=\"text-align: right;\">     0.173557</td><td style=\"text-align: right;\">  3.85182e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_dcd3bac3</td><td>TERMINATED</td><td>172.26.215.93:195825</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_2010</td><td>sklearn.impute._3550</td><td>mean  </td><td>[&#x27;FSR_for_force_ac40</td><td>[&#x27;force&#x27;, &#x27;x_co_5640</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00201247 </td><td>sklearn.preproc_2f70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       89.419   </td><td style=\"text-align: right;\">    0.259522</td><td style=\"text-align: right;\">     0.168831</td><td style=\"text-align: right;\">  3.8895e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_86fbc03e</td><td>TERMINATED</td><td>172.26.215.93:195592</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_1530</td><td>sklearn.impute._3d20</td><td>mean  </td><td>[&#x27;FSR_for_force_2e00</td><td>[&#x27;force&#x27;, &#x27;x_co_8040</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00213324 </td><td>sklearn.preproc_1230</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       90.6322  </td><td style=\"text-align: right;\">    0.26526 </td><td style=\"text-align: right;\">     0.171408</td><td style=\"text-align: right;\">  3.94009e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_f4c384a8</td><td>TERMINATED</td><td>172.26.215.93:195127</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ef10</td><td>sklearn.impute._b3c0</td><td>mean  </td><td>[&#x27;FSR_for_force_f4c0</td><td>[&#x27;force&#x27;, &#x27;x_co_ae00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00687811 </td><td>sklearn.preproc_ee50</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       87.4017  </td><td style=\"text-align: right;\">    0.26308 </td><td style=\"text-align: right;\">     0.161834</td><td style=\"text-align: right;\">  4.29663e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_cf94ab7d</td><td>TERMINATED</td><td>172.26.215.93:195371</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_a1f0</td><td>sklearn.impute._8d00</td><td>mean  </td><td>[&#x27;FSR_for_force_e340</td><td>[&#x27;force&#x27;, &#x27;x_co_81c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00226659 </td><td>sklearn.preproc_b150</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       59.9459  </td><td style=\"text-align: right;\">    0.274265</td><td style=\"text-align: right;\">     0.174831</td><td style=\"text-align: right;\">  4.3084e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_770d6969</td><td>TERMINATED</td><td>172.26.215.93:193944</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4030</td><td>sklearn.impute._3870</td><td>mean  </td><td>[&#x27;FSR_for_force_5e00</td><td>[&#x27;force&#x27;, &#x27;x_co_3e40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00292305 </td><td>sklearn.preproc_fe10</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">      106.305   </td><td style=\"text-align: right;\">    0.449443</td><td style=\"text-align: right;\">     0.310513</td><td style=\"text-align: right;\">  7.6398e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_839c76a6</td><td>TERMINATED</td><td>172.26.215.93:193278</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_1e90</td><td>sklearn.impute._fcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_7700</td><td>[&#x27;force&#x27;, &#x27;x_co_08c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00224038 </td><td>sklearn.preproc_1c50</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      116.86    </td><td style=\"text-align: right;\">    0.245539</td><td style=\"text-align: right;\">     0.169041</td><td style=\"text-align: right;\">  3.10088e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_36b10ce7</td><td>TERMINATED</td><td>172.26.215.93:194875</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_9d10</td><td>sklearn.impute._bfa0</td><td>mean  </td><td>[&#x27;FSR_for_force_e740</td><td>[&#x27;force&#x27;, &#x27;x_co_6240</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00406642 </td><td>sklearn.preproc_ab50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.40836 </td><td style=\"text-align: right;\">    0.661676</td><td style=\"text-align: right;\">     0.393313</td><td style=\"text-align: right;\">  1.14325e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_d3e59f10</td><td>TERMINATED</td><td>172.26.215.93:194640</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_6c70</td><td>sklearn.impute._bcd0</td><td>mean  </td><td>[&#x27;FSR_for_force_f480</td><td>[&#x27;force&#x27;, &#x27;x_co_eb80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00406336 </td><td>sklearn.preproc_7bd0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        7.73455 </td><td style=\"text-align: right;\">    0.540537</td><td style=\"text-align: right;\">     0.293226</td><td style=\"text-align: right;\">  1.23101e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_8198c6d1</td><td>TERMINATED</td><td>172.26.215.93:194402</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cc30</td><td>sklearn.impute._2970</td><td>mean  </td><td>[&#x27;FSR_for_force_5780</td><td>[&#x27;force&#x27;, &#x27;x_co_7880</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00238581 </td><td>sklearn.preproc_db90</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       16.6283  </td><td style=\"text-align: right;\">    3.06129 </td><td style=\"text-align: right;\">     2.20275 </td><td style=\"text-align: right;\">240.3        </td></tr>\n",
       "<tr><td>FSR_Trainable_08dbc938</td><td>TERMINATED</td><td>172.26.215.93:194170</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>sklearn.impute._bf00</td><td>mean  </td><td>[&#x27;FSR_for_force_f440</td><td>[&#x27;force&#x27;, &#x27;x_co_a2c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00275751 </td><td>sklearn.preproc_5770</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       16.8932  </td><td style=\"text-align: right;\">   10.2377  </td><td style=\"text-align: right;\">     7.1166  </td><td style=\"text-align: right;\">  2.46074e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_7d5d0f23</td><td>TERMINATED</td><td>172.26.215.93:192745</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0ed0</td><td>sklearn.impute._3b40</td><td>mean  </td><td>[&#x27;FSR_for_force_bd80</td><td>[&#x27;force&#x27;, &#x27;x_co_fb00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00143578 </td><td>sklearn.preproc_0e10</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">      148.677   </td><td style=\"text-align: right;\">    0.443113</td><td style=\"text-align: right;\">     0.30908 </td><td style=\"text-align: right;\">  7.5306e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_3433d7fa</td><td>TERMINATED</td><td>172.26.215.93:192188</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_5dd0</td><td>sklearn.impute._b870</td><td>mean  </td><td>[&#x27;FSR_for_force_2fc0</td><td>[&#x27;force&#x27;, &#x27;x_co_3f40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00135528 </td><td>sklearn.preproc_6d30</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      243.691   </td><td style=\"text-align: right;\">    0.263792</td><td style=\"text-align: right;\">     0.169348</td><td style=\"text-align: right;\">  4.06841e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_75852e34</td><td>TERMINATED</td><td>172.26.215.93:193706</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f90</td><td>sklearn.impute._f870</td><td>mean  </td><td>[&#x27;FSR_for_force_f740</td><td>[&#x27;force&#x27;, &#x27;x_co_8c00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00106666 </td><td>sklearn.preproc_4ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.29663 </td><td style=\"text-align: right;\">    1.83708 </td><td style=\"text-align: right;\">     0.865826</td><td style=\"text-align: right;\">  3.96962e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_cf85d82f</td><td>TERMINATED</td><td>172.26.215.93:193475</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c450</td><td>sklearn.impute._2ce0</td><td>mean  </td><td>[&#x27;FSR_for_force_6ac0</td><td>[&#x27;force&#x27;, &#x27;x_co_7c40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00110628 </td><td>sklearn.preproc_c390</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.12285 </td><td style=\"text-align: right;\">    1.37647 </td><td style=\"text-align: right;\">     0.712087</td><td style=\"text-align: right;\">  2.31329e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_48c46c0b</td><td>TERMINATED</td><td>172.26.215.93:191902</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_6e50</td><td>sklearn.impute._35f0</td><td>mean  </td><td>[&#x27;FSR_for_force_aa00</td><td>[&#x27;force&#x27;, &#x27;x_co_62c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00163083 </td><td>sklearn.preproc_6c10</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      248.665   </td><td style=\"text-align: right;\">    0.283781</td><td style=\"text-align: right;\">     0.190372</td><td style=\"text-align: right;\">  4.0364e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_b1d49451</td><td>TERMINATED</td><td>172.26.215.93:192951</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8c30</td><td>sklearn.impute._a8d0</td><td>mean  </td><td>[&#x27;FSR_for_force_6780</td><td>[&#x27;force&#x27;, &#x27;x_co_7980</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00225083 </td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       33.086   </td><td style=\"text-align: right;\">    0.465332</td><td style=\"text-align: right;\">     0.312695</td><td style=\"text-align: right;\">  8.37049e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_1a98df0c</td><td>TERMINATED</td><td>172.26.215.93:191593</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4330</td><td>sklearn.impute._af10</td><td>mean  </td><td>[&#x27;FSR_for_force_f340</td><td>[&#x27;force&#x27;, &#x27;x_co_eb80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00123903 </td><td>sklearn.preproc_42d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      230.736   </td><td style=\"text-align: right;\">    0.25361 </td><td style=\"text-align: right;\">     0.174099</td><td style=\"text-align: right;\">  3.30162e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_fb304c25</td><td>TERMINATED</td><td>172.26.215.93:192441</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8330</td><td>sklearn.impute._a010</td><td>mean  </td><td>[&#x27;FSR_for_force_1d00</td><td>[&#x27;force&#x27;, &#x27;x_co_3b80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00148797 </td><td>sklearn.preproc_8270</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       70.5804  </td><td style=\"text-align: right;\">    0.328354</td><td style=\"text-align: right;\">     0.188602</td><td style=\"text-align: right;\">  6.6467e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_22929497</td><td>TERMINATED</td><td>172.26.215.93:189779</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c0f0</td><td>sklearn.impute._10c0</td><td>mean  </td><td>[&#x27;FSR_for_force_e640</td><td>[&#x27;force&#x27;, &#x27;x_co_6b80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000953432</td><td>sklearn.preproc_c030</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      463.487   </td><td style=\"text-align: right;\">    0.292846</td><td style=\"text-align: right;\">     0.191543</td><td style=\"text-align: right;\">  4.32979e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_3fbd5160</td><td>TERMINATED</td><td>172.26.215.93:189105</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_6fd0</td><td>sklearn.impute._8a30</td><td>mean  </td><td>[&#x27;FSR_for_force_bd00</td><td>[&#x27;force&#x27;, &#x27;x_co_eb80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00255546 </td><td>sklearn.preproc_7db0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      474.508   </td><td style=\"text-align: right;\">    0.27521 </td><td style=\"text-align: right;\">     0.181654</td><td style=\"text-align: right;\">  3.53797e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_08ece1c7</td><td>TERMINATED</td><td>172.26.215.93:191403</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4390</td><td>sklearn.impute._70f0</td><td>mean  </td><td>[&#x27;FSR_for_force_e640</td><td>[&#x27;force&#x27;, &#x27;x_co_8300</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000310297</td><td>sklearn.preproc_5230</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       55.0824  </td><td style=\"text-align: right;\">    0.513415</td><td style=\"text-align: right;\">     0.303107</td><td style=\"text-align: right;\">  1.07223e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_bbb73642</td><td>TERMINATED</td><td>172.26.215.93:191137</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_26d0</td><td>sklearn.impute._afb0</td><td>mean  </td><td>[&#x27;FSR_for_force_f140</td><td>[&#x27;force&#x27;, &#x27;x_co_0340</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00037373 </td><td>sklearn.preproc_23d0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       54.8615  </td><td style=\"text-align: right;\">    0.517691</td><td style=\"text-align: right;\">     0.303009</td><td style=\"text-align: right;\">  1.09971e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_d50fa8ea</td><td>TERMINATED</td><td>172.26.215.93:190863</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_aaf0</td><td>sklearn.impute._0990</td><td>mean  </td><td>[&#x27;FSR_for_force_7340</td><td>[&#x27;force&#x27;, &#x27;x_co_2c40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000682007</td><td>sklearn.preproc_ac10</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">      109.747   </td><td style=\"text-align: right;\">    0.489389</td><td style=\"text-align: right;\">     0.294802</td><td style=\"text-align: right;\">  1.00097e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_573038ac</td><td>TERMINATED</td><td>172.26.215.93:190101</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_9ef0</td><td>sklearn.impute._b640</td><td>mean  </td><td>[&#x27;FSR_for_force_e6c0</td><td>[&#x27;force&#x27;, &#x27;x_co_3980</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000686159</td><td>sklearn.preproc_9dd0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">      151.593   </td><td style=\"text-align: right;\">    0.387594</td><td style=\"text-align: right;\">     0.241495</td><td style=\"text-align: right;\">  7.09594e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_3a9efe0a</td><td>TERMINATED</td><td>172.26.215.93:190623</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_2850</td><td>sklearn.impute._bbe0</td><td>mean  </td><td>[&#x27;FSR_for_force_2d40</td><td>[&#x27;force&#x27;, &#x27;x_co_44c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000631839</td><td>sklearn.preproc_36f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        5.55498 </td><td style=\"text-align: right;\">    5.66274 </td><td style=\"text-align: right;\">     6.53139 </td><td style=\"text-align: right;\">  2.2517e+15 </td></tr>\n",
       "<tr><td>FSR_Trainable_8d1c2f35</td><td>TERMINATED</td><td>172.26.215.93:190394</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_2b50</td><td>sklearn.impute._ea10</td><td>mean  </td><td>[&#x27;FSR_for_force_6fc0</td><td>[&#x27;force&#x27;, &#x27;x_co_d440</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000691159</td><td>sklearn.preproc_3ab0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        5.52543 </td><td style=\"text-align: right;\">    5.5582  </td><td style=\"text-align: right;\">     6.47002 </td><td style=\"text-align: right;\">  2.32134e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_0daad9a2</td><td>TERMINATED</td><td>172.26.215.93:189324</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_f210</td><td>sklearn.impute._9c50</td><td>mean  </td><td>[&#x27;FSR_for_force_4c40</td><td>[&#x27;force&#x27;, &#x27;x_co_b400</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000758025</td><td>sklearn.preproc_efd0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">      152.569   </td><td style=\"text-align: right;\">    0.390066</td><td style=\"text-align: right;\">     0.243769</td><td style=\"text-align: right;\">  7.04061e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_580494a3</td><td>TERMINATED</td><td>172.26.215.93:189559</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0e70</td><td>sklearn.impute._1480</td><td>mean  </td><td>[&#x27;FSR_for_force_bc00</td><td>[&#x27;force&#x27;, &#x27;x_co_bd00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0007145  </td><td>sklearn.preproc_09f0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       74.5158  </td><td style=\"text-align: right;\">    0.493219</td><td style=\"text-align: right;\">     0.28933 </td><td style=\"text-align: right;\">  1.03986e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_80318608</td><td>TERMINATED</td><td>172.26.215.93:188109</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c150</td><td>sklearn.impute._3140</td><td>mean  </td><td>[&#x27;FSR_for_force_bf80</td><td>[&#x27;force&#x27;, &#x27;x_co_16c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00875213 </td><td>sklearn.preproc_c090</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       56.5303  </td><td style=\"text-align: right;\">    0.479076</td><td style=\"text-align: right;\">     0.313532</td><td style=\"text-align: right;\">  7.4758e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_f58d2a52</td><td>TERMINATED</td><td>172.26.215.93:188892</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_a2b0</td><td>sklearn.impute._2830</td><td>mean  </td><td>[&#x27;FSR_for_force_2900</td><td>[&#x27;force&#x27;, &#x27;x_co_3f00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0029268  </td><td>sklearn.preproc_a070</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.29969 </td><td style=\"text-align: right;\">    2.78256 </td><td style=\"text-align: right;\">     1.60301 </td><td style=\"text-align: right;\"> 44.8819     </td></tr>\n",
       "<tr><td>FSR_Trainable_043a5c07</td><td>TERMINATED</td><td>172.26.215.93:181673</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_55f0</td><td>sklearn.impute._b0f0</td><td>mean  </td><td>[&#x27;FSR_for_force_6300</td><td>[&#x27;force&#x27;, &#x27;x_co_0680</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00485025 </td><td>sklearn.preproc_9110</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">      400.072   </td><td style=\"text-align: right;\">    0.29724 </td><td style=\"text-align: right;\">     0.188638</td><td style=\"text-align: right;\">  4.43445e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_ad7a4599</td><td>TERMINATED</td><td>172.26.215.93:188323</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0330</td><td>sklearn.impute._a380</td><td>mean  </td><td>[&#x27;FSR_for_force_f5c0</td><td>[&#x27;force&#x27;, &#x27;x_co_6a40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00885971 </td><td>sklearn.preproc_0ed0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">       16.2668  </td><td style=\"text-align: right;\">    0.529038</td><td style=\"text-align: right;\">     0.300977</td><td style=\"text-align: right;\">  1.12949e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_0263d11d</td><td>TERMINATED</td><td>172.26.215.93:188629</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>sklearn.impute._1e80</td><td>mean  </td><td>[&#x27;FSR_for_force_22c0</td><td>[&#x27;force&#x27;, &#x27;x_co_8840</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00295366 </td><td>sklearn.preproc_1530</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.17325 </td><td style=\"text-align: right;\">    2.9087  </td><td style=\"text-align: right;\">     1.72364 </td><td style=\"text-align: right;\"> 32.867      </td></tr>\n",
       "<tr><td>FSR_Trainable_e8d19252</td><td>TERMINATED</td><td>172.26.215.93:187882</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_65b0</td><td>sklearn.impute._abf0</td><td>median</td><td>[&#x27;FSR_for_force_ae00</td><td>[&#x27;force&#x27;, &#x27;x_co_ad40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0110079  </td><td>sklearn.preproc_7510</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">       15.9565  </td><td style=\"text-align: right;\">    0.541914</td><td style=\"text-align: right;\">     0.306317</td><td style=\"text-align: right;\">  1.06556e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_f67d3b44</td><td>TERMINATED</td><td>172.26.215.93:187679</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_86f0</td><td>sklearn.impute._1340</td><td>median</td><td>[&#x27;FSR_for_force_2280</td><td>[&#x27;force&#x27;, &#x27;x_co_ccc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0131022  </td><td>sklearn.preproc_8630</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        9.27454 </td><td style=\"text-align: right;\">    0.463531</td><td style=\"text-align: right;\">     0.268788</td><td style=\"text-align: right;\">  8.4529e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_8c57c14c</td><td>TERMINATED</td><td>172.26.215.93:185813</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ec10</td><td>sklearn.impute._cee0</td><td>median</td><td>[&#x27;FSR_for_force_e740</td><td>[&#x27;force&#x27;, &#x27;x_co_e400</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0052025  </td><td>sklearn.preproc_e9d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      106.805   </td><td style=\"text-align: right;\">    0.312781</td><td style=\"text-align: right;\">     0.196293</td><td style=\"text-align: right;\">  4.62638e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_d220b1c8</td><td>TERMINATED</td><td>172.26.215.93:187407</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_1410</td><td>sklearn.impute._bbe0</td><td>median</td><td>[&#x27;FSR_for_force_fc00</td><td>[&#x27;force&#x27;, &#x27;x_co_fd40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0147145  </td><td>sklearn.preproc_22b0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       10.5259  </td><td style=\"text-align: right;\">    0.5315  </td><td style=\"text-align: right;\">     0.288344</td><td style=\"text-align: right;\">  1.12683e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_90abffb2</td><td>TERMINATED</td><td>172.26.215.93:187175</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c3f0</td><td>sklearn.impute._9c00</td><td>median</td><td>[&#x27;FSR_for_force_2740</td><td>[&#x27;force&#x27;, &#x27;x_co_cb40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0117539  </td><td>sklearn.preproc_d350</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       19.5351  </td><td style=\"text-align: right;\">    0.436289</td><td style=\"text-align: right;\">     0.266733</td><td style=\"text-align: right;\">  7.75968e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_af4ab174</td><td>TERMINATED</td><td>172.26.215.93:186930</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_1170</td><td>sklearn.impute._10c0</td><td>median</td><td>[&#x27;FSR_for_force_e7c0</td><td>[&#x27;force&#x27;, &#x27;x_co_0640</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00150899 </td><td>sklearn.preproc_20d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.55204 </td><td style=\"text-align: right;\">    1.11243 </td><td style=\"text-align: right;\">     0.617872</td><td style=\"text-align: right;\">  1.89249e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_d3008e43</td><td>TERMINATED</td><td>172.26.215.93:186699</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_7750</td><td>sklearn.impute._f230</td><td>median</td><td>[&#x27;FSR_for_force_8cc0</td><td>[&#x27;force&#x27;, &#x27;x_co_b180</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00170067 </td><td>sklearn.preproc_7450</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.69474 </td><td style=\"text-align: right;\">    6.95313 </td><td style=\"text-align: right;\">     6.91425 </td><td style=\"text-align: right;\">  5.13527e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_75e926e4</td><td>TERMINATED</td><td>172.26.215.93:186482</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_83f0</td><td>sklearn.impute._eb50</td><td>median</td><td>[&#x27;FSR_for_force_d240</td><td>[&#x27;force&#x27;, &#x27;x_co_f780</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00539943 </td><td>sklearn.preproc_9350</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.27228 </td><td style=\"text-align: right;\">    7.35641 </td><td style=\"text-align: right;\">     6.79468 </td><td style=\"text-align: right;\">  7.87585e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_dcea2390</td><td>TERMINATED</td><td>172.26.215.93:186252</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_b5d0</td><td>sklearn.impute._3370</td><td>median</td><td>[&#x27;FSR_for_force_a9c0</td><td>[&#x27;force&#x27;, &#x27;x_co_bc00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00600688 </td><td>sklearn.preproc_b390</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        8.75897 </td><td style=\"text-align: right;\">    0.580445</td><td style=\"text-align: right;\">     0.307894</td><td style=\"text-align: right;\">  1.35308e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_ce96e157</td><td>TERMINATED</td><td>172.26.215.93:186034</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ef70</td><td>sklearn.impute._5700</td><td>median</td><td>[&#x27;FSR_for_force_2f00</td><td>[&#x27;force&#x27;, &#x27;x_co_0100</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00471879 </td><td>sklearn.preproc_ed30</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        8.42941 </td><td style=\"text-align: right;\">    0.53255 </td><td style=\"text-align: right;\">     0.287865</td><td style=\"text-align: right;\">  1.21929e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_bb462b5a</td><td>TERMINATED</td><td>172.26.215.93:185581</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0ab0</td><td>sklearn.impute._6ec0</td><td>median</td><td>[&#x27;FSR_for_force_bf80</td><td>[&#x27;force&#x27;, &#x27;x_co_3040</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00396456 </td><td>sklearn.preproc_09f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.971077</td><td style=\"text-align: right;\">    2.76831 </td><td style=\"text-align: right;\">     1.69407 </td><td style=\"text-align: right;\"> 55.3187     </td></tr>\n",
       "<tr><td>FSR_Trainable_dfe69d9f</td><td>TERMINATED</td><td>172.26.215.93:185352</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_6af0</td><td>sklearn.impute._b9b0</td><td>median</td><td>[&#x27;FSR_for_force_3840</td><td>[&#x27;force&#x27;, &#x27;x_co_c980</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0932638  </td><td>sklearn.preproc_68b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.911505</td><td style=\"text-align: right;\">    3.69523 </td><td style=\"text-align: right;\">     2.29844 </td><td style=\"text-align: right;\">256.141      </td></tr>\n",
       "<tr><td>FSR_Trainable_b1203ce7</td><td>TERMINATED</td><td>172.26.215.93:185120</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_abb0</td><td>sklearn.impute._3230</td><td>median</td><td>[&#x27;FSR_for_force_18c0</td><td>[&#x27;force&#x27;, &#x27;x_co_4a00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0758109  </td><td>sklearn.preproc_5890</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        5.36449 </td><td style=\"text-align: right;\">    0.505987</td><td style=\"text-align: right;\">     0.293121</td><td style=\"text-align: right;\">  1.18024e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_c4772750</td><td>TERMINATED</td><td>172.26.215.93:184905</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0090</td><td>sklearn.impute._bcd0</td><td>median</td><td>[&#x27;FSR_for_force_4e40</td><td>[&#x27;force&#x27;, &#x27;x_co_7400</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0323133  </td><td>sklearn.preproc_ff30</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        9.87519 </td><td style=\"text-align: right;\">    0.529779</td><td style=\"text-align: right;\">     0.295031</td><td style=\"text-align: right;\">  1.01099e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_3409809b</td><td>TERMINATED</td><td>172.26.215.93:183943</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_6b50</td><td>sklearn.impute._11b0</td><td>median</td><td>[&#x27;FSR_for_force_a580</td><td>[&#x27;force&#x27;, &#x27;x_co_4b40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000137474</td><td>sklearn.preproc_7ab0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">       66.6879  </td><td style=\"text-align: right;\">    0.482375</td><td style=\"text-align: right;\">     0.32815 </td><td style=\"text-align: right;\">  7.30487e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_a9e43461</td><td>TERMINATED</td><td>172.26.215.93:184630</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cc90</td><td>sklearn.impute._0e40</td><td>median</td><td>[&#x27;FSR_for_force_6a40</td><td>[&#x27;force&#x27;, &#x27;x_co_0040</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0308372  </td><td>sklearn.preproc_cbd0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       22.3069  </td><td style=\"text-align: right;\">    0.457903</td><td style=\"text-align: right;\">     0.277612</td><td style=\"text-align: right;\">  7.68211e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_611b154e</td><td>TERMINATED</td><td>172.26.215.93:184387</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_37b0</td><td>sklearn.impute._2ba0</td><td>median</td><td>[&#x27;FSR_for_force_3040</td><td>[&#x27;force&#x27;, &#x27;x_co_4240</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0245677  </td><td>sklearn.preproc_3570</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       21.6874  </td><td style=\"text-align: right;\">    0.442225</td><td style=\"text-align: right;\">     0.270661</td><td style=\"text-align: right;\">  7.29168e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_ebb1cdcf</td><td>TERMINATED</td><td>172.26.215.93:184166</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_04b0</td><td>sklearn.impute._12a0</td><td>median</td><td>[&#x27;FSR_for_force_7c40</td><td>[&#x27;force&#x27;, &#x27;x_co_7480</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0288409  </td><td>sklearn.preproc_1350</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       10.3289  </td><td style=\"text-align: right;\">    0.594703</td><td style=\"text-align: right;\">     0.330343</td><td style=\"text-align: right;\">  1.38154e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_4cf41fd6</td><td>TERMINATED</td><td>172.26.215.93:183484</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_1cb0</td><td>sklearn.impute._3690</td><td>median</td><td>[&#x27;FSR_for_force_6d00</td><td>[&#x27;force&#x27;, &#x27;x_co_f000</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0182973  </td><td>sklearn.preproc_1830</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       18.5167  </td><td style=\"text-align: right;\">    0.604986</td><td style=\"text-align: right;\">     0.33657 </td><td style=\"text-align: right;\">  1.38589e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_863ef801</td><td>TERMINATED</td><td>172.26.215.93:183703</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_bd50</td><td>sklearn.impute._fdc0</td><td>median</td><td>[&#x27;FSR_for_force_5d40</td><td>[&#x27;force&#x27;, &#x27;x_co_6fc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.3093e-05 </td><td>sklearn.preproc_bb10</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.11362 </td><td style=\"text-align: right;\">    1.06633 </td><td style=\"text-align: right;\">     0.568791</td><td style=\"text-align: right;\">  1.94872e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_19e6058b</td><td>TERMINATED</td><td>172.26.215.93:183244</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_5110</td><td>sklearn.impute._bd70</td><td>median</td><td>[&#x27;FSR_for_force_5e00</td><td>[&#x27;force&#x27;, &#x27;x_co_b380</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        4.825e-05  </td><td>sklearn.preproc_6070</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">       12.1478  </td><td style=\"text-align: right;\">    0.932246</td><td style=\"text-align: right;\">     0.521179</td><td style=\"text-align: right;\">  1.37614e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_3ea710a0</td><td>TERMINATED</td><td>172.26.215.93:183021</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_85d0</td><td>sklearn.impute._3780</td><td>mean  </td><td>[&#x27;FSR_for_force_7dc0</td><td>[&#x27;force&#x27;, &#x27;x_co_dc40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        1.11597e-05</td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        5.08803 </td><td style=\"text-align: right;\">    3.49865 </td><td style=\"text-align: right;\">     2.05487 </td><td style=\"text-align: right;\"> 74.0941     </td></tr>\n",
       "<tr><td>FSR_Trainable_4be3dabe</td><td>TERMINATED</td><td>172.26.215.93:182789</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_ed90</td><td>sklearn.impute._9250</td><td>median</td><td>[&#x27;FSR_for_force_dfc0</td><td>[&#x27;force&#x27;, &#x27;x_co_cc40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        1.62917e-05</td><td>sklearn.preproc_fcf0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        4.02472 </td><td style=\"text-align: right;\">    3.33216 </td><td style=\"text-align: right;\">     2.08015 </td><td style=\"text-align: right;\"> 84.5943     </td></tr>\n",
       "<tr><td>FSR_Trainable_da121a16</td><td>TERMINATED</td><td>172.26.215.93:182561</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_9d70</td><td>sklearn.impute._9660</td><td>mean  </td><td>[&#x27;FSR_for_force_2700</td><td>[&#x27;force&#x27;, &#x27;x_co_9b80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00028746 </td><td>sklearn.preproc_acd0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        5.24813 </td><td style=\"text-align: right;\">    5.91984 </td><td style=\"text-align: right;\">     7.15308 </td><td style=\"text-align: right;\">  5.82121e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_b47983cf</td><td>TERMINATED</td><td>172.26.215.93:182329</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0090</td><td>sklearn.impute._3d70</td><td>mean  </td><td>[&#x27;FSR_for_force_6cc0</td><td>[&#x27;force&#x27;, &#x27;x_co_02c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        3.44594e-05</td><td>sklearn.preproc_0210</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        8.89746 </td><td style=\"text-align: right;\">    0.622707</td><td style=\"text-align: right;\">     0.346531</td><td style=\"text-align: right;\">  1.24973e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_3c320acd</td><td>TERMINATED</td><td>172.26.215.93:182021</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_44b0</td><td>sklearn.impute._e6f0</td><td>median</td><td>[&#x27;FSR_for_force_2ec0</td><td>[&#x27;force&#x27;, &#x27;x_co_0e00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        3.5019e-05 </td><td>sklearn.preproc_5410</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.25225 </td><td style=\"text-align: right;\">    6.24538 </td><td style=\"text-align: right;\">     7.20836 </td><td style=\"text-align: right;\">  1.35384e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_e5ebe0c9</td><td>TERMINATED</td><td>172.26.215.93:181599</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_27f0</td><td>sklearn.impute._9ac0</td><td>mean  </td><td>[&#x27;FSR_for_force_5980</td><td>[&#x27;force&#x27;, &#x27;x_co_fec0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        3.20846e-05</td><td>sklearn.preproc_3690</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">       27.5601  </td><td style=\"text-align: right;\">    5.87529 </td><td style=\"text-align: right;\">     7.1269  </td><td style=\"text-align: right;\">  6.26039e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_4d8df89d</td><td>TERMINATED</td><td>172.26.215.93:181837</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c630</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_ea80</td><td>[&#x27;force&#x27;, &#x27;x_co_8980</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000102234</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        7.6936  </td><td style=\"text-align: right;\">    0.546602</td><td style=\"text-align: right;\">     0.392256</td><td style=\"text-align: right;\">  5.36619e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_48eada07</td><td>TERMINATED</td><td>172.26.215.93:209206</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_d890</td><td>sklearn.impute._aab0</td><td>mean  </td><td>[&#x27;FSR_for_force_8080</td><td>[&#x27;force&#x27;, &#x27;x_co_eb80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00163449 </td><td>sklearn.preproc_e7f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        8.40614 </td><td style=\"text-align: right;\">    0.417023</td><td style=\"text-align: right;\">     0.235144</td><td style=\"text-align: right;\">  9.11523e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_57b5d6f7</td><td>TERMINATED</td><td>172.26.215.93:209207</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c450</td><td>sklearn.impute._3c80</td><td>mean  </td><td>[&#x27;FSR_for_force_f480</td><td>[&#x27;force&#x27;, &#x27;x_co_1ec0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000569516</td><td>sklearn.preproc_d3b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.70981 </td><td style=\"text-align: right;\">    0.555184</td><td style=\"text-align: right;\">     0.317063</td><td style=\"text-align: right;\">  1.25589e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_28ec7133</td><td>TERMINATED</td><td>172.26.215.93:210044</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_94d0</td><td>sklearn.impute._d250</td><td>mean  </td><td>[&#x27;FSR_for_force_4480</td><td>[&#x27;force&#x27;, &#x27;x_co_da40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000524816</td><td>sklearn.preproc_b750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        3.46317 </td><td style=\"text-align: right;\">    0.403903</td><td style=\"text-align: right;\">     0.269529</td><td style=\"text-align: right;\">  6.96675e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_da984e65</td><td>TERMINATED</td><td>172.26.215.93:210478</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_94d0</td><td>sklearn.impute._d250</td><td>mean  </td><td>[&#x27;FSR_for_force_1540</td><td>[&#x27;force&#x27;, &#x27;x_co_1240</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00158928 </td><td>sklearn.preproc_b750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.25215 </td><td style=\"text-align: right;\">    0.381518</td><td style=\"text-align: right;\">     0.265094</td><td style=\"text-align: right;\">  6.00315e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_35aed758</td><td>TERMINATED</td><td>172.26.215.93:210732</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_94d0</td><td>sklearn.impute._d250</td><td>mean  </td><td>[&#x27;FSR_for_force_cbc0</td><td>[&#x27;force&#x27;, &#x27;x_co_d280</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00124763 </td><td>sklearn.preproc_b750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.42322 </td><td style=\"text-align: right;\">    0.749304</td><td style=\"text-align: right;\">     0.408525</td><td style=\"text-align: right;\">  1.42901e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_46a66fbd</td><td>TERMINATED</td><td>172.26.215.93:210972</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_94d0</td><td>sklearn.impute._d250</td><td>mean  </td><td>[&#x27;FSR_for_force_f100</td><td>[&#x27;force&#x27;, &#x27;x_co_c5c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00209057 </td><td>sklearn.preproc_b750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.1758  </td><td style=\"text-align: right;\">    0.499856</td><td style=\"text-align: right;\">     0.307553</td><td style=\"text-align: right;\">  9.48777e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_4f7002fa</td><td>TERMINATED</td><td>172.26.215.93:211207</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_94d0</td><td>sklearn.impute._d250</td><td>mean  </td><td>[&#x27;FSR_for_force_67c0</td><td>[&#x27;force&#x27;, &#x27;x_co_7d40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00220199 </td><td>sklearn.preproc_b750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.17402 </td><td style=\"text-align: right;\">    0.759864</td><td style=\"text-align: right;\">     0.428248</td><td style=\"text-align: right;\">  1.6675e+15 </td></tr>\n",
       "<tr><td>FSR_Trainable_56b99d52</td><td>TERMINATED</td><td>172.26.215.93:211447</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_94d0</td><td>sklearn.impute._d250</td><td>mean  </td><td>[&#x27;FSR_for_force_c880</td><td>[&#x27;force&#x27;, &#x27;x_co_c400</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000792311</td><td>sklearn.preproc_b750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.23691 </td><td style=\"text-align: right;\">    0.610222</td><td style=\"text-align: right;\">     0.323643</td><td style=\"text-align: right;\">  1.46031e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_0e4322aa</td><td>TERMINATED</td><td>172.26.215.93:211539</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_94d0</td><td>sklearn.impute._d250</td><td>mean  </td><td>[&#x27;FSR_for_force_2b40</td><td>[&#x27;force&#x27;, &#x27;x_co_0900</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000776288</td><td>sklearn.preproc_b750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.31382 </td><td style=\"text-align: right;\">    0.623174</td><td style=\"text-align: right;\">     0.319515</td><td style=\"text-align: right;\">  1.53126e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_f81dfc2d</td><td>TERMINATED</td><td>172.26.215.93:211850</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_94d0</td><td>sklearn.impute._d250</td><td>mean  </td><td>[&#x27;FSR_for_force_8180</td><td>[&#x27;force&#x27;, &#x27;x_co_a1c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0010773  </td><td>sklearn.preproc_b750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.53968 </td><td style=\"text-align: right;\">    0.410443</td><td style=\"text-align: right;\">     0.278548</td><td style=\"text-align: right;\">  7.27021e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_119b1b07</td><td>TERMINATED</td><td>172.26.215.93:211941</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_94d0</td><td>sklearn.impute._d250</td><td>mean  </td><td>[&#x27;FSR_for_force_4bc0</td><td>[&#x27;force&#x27;, &#x27;x_co_8c00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00280391 </td><td>sklearn.preproc_b750</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.8806  </td><td style=\"text-align: right;\">    0.382535</td><td style=\"text-align: right;\">     0.204837</td><td style=\"text-align: right;\">  8.56557e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_0bca8ed6</td><td>TERMINATED</td><td>172.26.215.93:212252</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_94d0</td><td>sklearn.impute._d250</td><td>mean  </td><td>[&#x27;FSR_for_force_3e00</td><td>[&#x27;force&#x27;, &#x27;x_co_0e80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00290973 </td><td>sklearn.preproc_b750</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.71943 </td><td style=\"text-align: right;\">    0.396802</td><td style=\"text-align: right;\">     0.215915</td><td style=\"text-align: right;\">  9.10261e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_63255ad9</td><td>TERMINATED</td><td>172.26.215.93:212346</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_94d0</td><td>sklearn.impute._d250</td><td>mean  </td><td>[&#x27;FSR_for_force_e680</td><td>[&#x27;force&#x27;, &#x27;x_co_ed80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00119393 </td><td>sklearn.preproc_8450</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.04297 </td><td style=\"text-align: right;\">    2.37421 </td><td style=\"text-align: right;\">     1.52995 </td><td style=\"text-align: right;\"> 73.5442     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 03:31:23,301\tINFO experiment_state.py:435 -- A local experiment checkpoint was found and will be used to restore the previous experiment state.\n",
      "2023-08-11 03:31:23,307\tWARNING trial_runner.py:418 -- Attempting to resume experiment from /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35. This will ignore any new changes to the specification.\n",
      "2023-08-11 03:31:23,308\tINFO trial_runner.py:422 -- Using the newest experiment state file found within the experiment directory: experiment_state-2023-08-11_01-45-30.json\n",
      "2023-08-11 03:31:23,612\tINFO wandb.py:320 -- Already logged into W&B.\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=209203)\u001b[0m 2023-08-11 03:31:32,992\tINFO trainable.py:918 -- Restored on 172.26.215.93 from checkpoint: /tmp/checkpoint_tmp_76d6e12af35e42dda908e5b68b809d77\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=209203)\u001b[0m 2023-08-11 03:31:32,993\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 55, '_timesteps_total': None, '_time_total': 65.34051585197449, '_episodes_total': None}\n",
      "/home/seokj/workspace/.venv/lib/python3.10/site-packages/optuna/trial/_trial.py:494: UserWarning: The reported value is ignored because this `step` 41 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">  mae_coord</th><th style=\"text-align: right;\">  mae_force</th><th style=\"text-align: right;\">  mape_coord</th><th style=\"text-align: right;\">  mape_force</th><th style=\"text-align: right;\">  metric</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">  rmse_coord</th><th style=\"text-align: right;\">  rmse_force</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  tmae_coord</th><th style=\"text-align: right;\">  tmae_force</th><th style=\"text-align: right;\">  tmape_coord</th><th style=\"text-align: right;\">  tmape_force</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th><th style=\"text-align: right;\">  trmse_coord</th><th style=\"text-align: right;\">  trmse_force</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_0bca8ed6</td><td>2023-08-11_03-33-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    5.83276</td><td style=\"text-align: right;\">    868.916</td><td style=\"text-align: right;\">     1.55563</td><td style=\"text-align: right;\"> 1.66923e+18</td><td style=\"text-align: right;\">0.733683</td><td>172.26.215.93</td><td style=\"text-align: right;\">212252</td><td style=\"text-align: right;\">     2.59852</td><td style=\"text-align: right;\">     513.723</td><td style=\"text-align: right;\">             2.71943</td><td style=\"text-align: right;\">          0.558843</td><td style=\"text-align: right;\">       2.71943</td><td style=\"text-align: right;\"> 1691692404</td><td style=\"text-align: right;\">     1.22675</td><td style=\"text-align: right;\">    0.396802</td><td style=\"text-align: right;\">  8.80448e+13</td><td style=\"text-align: right;\">  9.10261e+14</td><td style=\"text-align: right;\">                   4</td><td>0bca8ed6  </td><td style=\"text-align: right;\">     0.517768</td><td style=\"text-align: right;\">     0.215915</td></tr>\n",
       "<tr><td>FSR_Trainable_0e4322aa</td><td>2023-08-11_03-32-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    7.578  </td><td style=\"text-align: right;\">   1651.95 </td><td style=\"text-align: right;\">     1.73146</td><td style=\"text-align: right;\"> 3.78392e+18</td><td style=\"text-align: right;\">1.01374 </td><td>172.26.215.93</td><td style=\"text-align: right;\">211539</td><td style=\"text-align: right;\">     3.15538</td><td style=\"text-align: right;\">     970.318</td><td style=\"text-align: right;\">             1.31382</td><td style=\"text-align: right;\">          1.31382 </td><td style=\"text-align: right;\">       1.31382</td><td style=\"text-align: right;\"> 1691692378</td><td style=\"text-align: right;\">     1.74873</td><td style=\"text-align: right;\">    0.623174</td><td style=\"text-align: right;\">  9.26633e+13</td><td style=\"text-align: right;\">  1.53126e+15</td><td style=\"text-align: right;\">                   1</td><td>0e4322aa  </td><td style=\"text-align: right;\">     0.694225</td><td style=\"text-align: right;\">     0.319515</td></tr>\n",
       "<tr><td>FSR_Trainable_119b1b07</td><td>2023-08-11_03-33-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    5.67262</td><td style=\"text-align: right;\">    858.792</td><td style=\"text-align: right;\">     1.49728</td><td style=\"text-align: right;\"> 1.58982e+18</td><td style=\"text-align: right;\">0.710309</td><td>172.26.215.93</td><td style=\"text-align: right;\">211941</td><td style=\"text-align: right;\">     2.54437</td><td style=\"text-align: right;\">     504.846</td><td style=\"text-align: right;\">             2.8806 </td><td style=\"text-align: right;\">          0.65383 </td><td style=\"text-align: right;\">       2.8806 </td><td style=\"text-align: right;\"> 1691692395</td><td style=\"text-align: right;\">     1.18513</td><td style=\"text-align: right;\">    0.382535</td><td style=\"text-align: right;\">  8.59213e+13</td><td style=\"text-align: right;\">  8.56557e+14</td><td style=\"text-align: right;\">                   4</td><td>119b1b07  </td><td style=\"text-align: right;\">     0.505472</td><td style=\"text-align: right;\">     0.204837</td></tr>\n",
       "<tr><td>FSR_Trainable_21fdf89c</td><td>2023-08-11_03-32-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        45</td><td style=\"text-align: right;\">    5.32435</td><td style=\"text-align: right;\">    651.537</td><td style=\"text-align: right;\">     1.19473</td><td style=\"text-align: right;\"> 8.34275e+17</td><td style=\"text-align: right;\">0.675416</td><td>172.26.215.93</td><td style=\"text-align: right;\">209203</td><td style=\"text-align: right;\">     2.60022</td><td style=\"text-align: right;\">     453.129</td><td style=\"text-align: right;\">            37.8405 </td><td style=\"text-align: right;\">          0.730253</td><td style=\"text-align: right;\">     103.181  </td><td style=\"text-align: right;\"> 1691692352</td><td style=\"text-align: right;\">     1.11918</td><td style=\"text-align: right;\">    0.263298</td><td style=\"text-align: right;\">  7.2956e+13 </td><td style=\"text-align: right;\">  4.10181e+14</td><td style=\"text-align: right;\">                 100</td><td>21fdf89c  </td><td style=\"text-align: right;\">     0.507117</td><td style=\"text-align: right;\">     0.168299</td></tr>\n",
       "<tr><td>FSR_Trainable_28ec7133</td><td>2023-08-11_03-31-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    5.30413</td><td style=\"text-align: right;\">   1140.46 </td><td style=\"text-align: right;\">     1.38129</td><td style=\"text-align: right;\"> 1.93086e+18</td><td style=\"text-align: right;\">0.784679</td><td>172.26.215.93</td><td style=\"text-align: right;\">210044</td><td style=\"text-align: right;\">     2.50295</td><td style=\"text-align: right;\">     840.436</td><td style=\"text-align: right;\">             3.46317</td><td style=\"text-align: right;\">          1.38829 </td><td style=\"text-align: right;\">       3.46317</td><td style=\"text-align: right;\"> 1691692318</td><td style=\"text-align: right;\">     1.14801</td><td style=\"text-align: right;\">    0.403903</td><td style=\"text-align: right;\">  9.4161e+13 </td><td style=\"text-align: right;\">  6.96675e+14</td><td style=\"text-align: right;\">                   2</td><td>28ec7133  </td><td style=\"text-align: right;\">     0.515151</td><td style=\"text-align: right;\">     0.269529</td></tr>\n",
       "<tr><td>FSR_Trainable_35aed758</td><td>2023-08-11_03-32-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   20.5306 </td><td style=\"text-align: right;\">   1862.51 </td><td style=\"text-align: right;\">     3.13937</td><td style=\"text-align: right;\"> 2.78963e+18</td><td style=\"text-align: right;\">1.87748 </td><td>172.26.215.93</td><td style=\"text-align: right;\">210732</td><td style=\"text-align: right;\">     7.2083 </td><td style=\"text-align: right;\">    1199.9  </td><td style=\"text-align: right;\">             1.42322</td><td style=\"text-align: right;\">          1.42322 </td><td style=\"text-align: right;\">       1.42322</td><td style=\"text-align: right;\"> 1691692342</td><td style=\"text-align: right;\">     4.43144</td><td style=\"text-align: right;\">    0.749304</td><td style=\"text-align: right;\">  3.34478e+13</td><td style=\"text-align: right;\">  1.42901e+15</td><td style=\"text-align: right;\">                   1</td><td>35aed758  </td><td style=\"text-align: right;\">     1.46896 </td><td style=\"text-align: right;\">     0.408525</td></tr>\n",
       "<tr><td>FSR_Trainable_3838379f</td><td>2023-08-11_03-32-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        60</td><td style=\"text-align: right;\">    5.41718</td><td style=\"text-align: right;\">    643.069</td><td style=\"text-align: right;\">     1.17597</td><td style=\"text-align: right;\"> 7.70419e+17</td><td style=\"text-align: right;\">0.674217</td><td>172.26.215.93</td><td style=\"text-align: right;\">209204</td><td style=\"text-align: right;\">     2.62062</td><td style=\"text-align: right;\">     465.954</td><td style=\"text-align: right;\">            45.312  </td><td style=\"text-align: right;\">          0.679527</td><td style=\"text-align: right;\">      89.8499 </td><td style=\"text-align: right;\"> 1691692362</td><td style=\"text-align: right;\">     1.13167</td><td style=\"text-align: right;\">    0.255212</td><td style=\"text-align: right;\">  7.56335e+13</td><td style=\"text-align: right;\">  3.84505e+14</td><td style=\"text-align: right;\">                 100</td><td>3838379f  </td><td style=\"text-align: right;\">     0.508349</td><td style=\"text-align: right;\">     0.165868</td></tr>\n",
       "<tr><td>FSR_Trainable_46a66fbd</td><td>2023-08-11_03-32-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   19.1657 </td><td style=\"text-align: right;\">   1342.19 </td><td style=\"text-align: right;\">     3.33368</td><td style=\"text-align: right;\"> 2.21604e+18</td><td style=\"text-align: right;\">1.73027 </td><td>172.26.215.93</td><td style=\"text-align: right;\">210972</td><td style=\"text-align: right;\">     6.91445</td><td style=\"text-align: right;\">     945.257</td><td style=\"text-align: right;\">             1.1758 </td><td style=\"text-align: right;\">          1.1758  </td><td style=\"text-align: right;\">       1.1758 </td><td style=\"text-align: right;\"> 1691692353</td><td style=\"text-align: right;\">     4.20424</td><td style=\"text-align: right;\">    0.499856</td><td style=\"text-align: right;\">  6.46556e+13</td><td style=\"text-align: right;\">  9.48777e+14</td><td style=\"text-align: right;\">                   1</td><td>46a66fbd  </td><td style=\"text-align: right;\">     1.42271 </td><td style=\"text-align: right;\">     0.307553</td></tr>\n",
       "<tr><td>FSR_Trainable_48eada07</td><td>2023-08-11_03-31-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    5.55037</td><td style=\"text-align: right;\">    902.659</td><td style=\"text-align: right;\">     1.50565</td><td style=\"text-align: right;\"> 1.6327e+18 </td><td style=\"text-align: right;\">0.742966</td><td>172.26.215.93</td><td style=\"text-align: right;\">209206</td><td style=\"text-align: right;\">     2.53547</td><td style=\"text-align: right;\">     538.484</td><td style=\"text-align: right;\">             8.40614</td><td style=\"text-align: right;\">          1.55633 </td><td style=\"text-align: right;\">       8.40614</td><td style=\"text-align: right;\"> 1691692319</td><td style=\"text-align: right;\">     1.17553</td><td style=\"text-align: right;\">    0.417023</td><td style=\"text-align: right;\">  8.88168e+13</td><td style=\"text-align: right;\">  9.11523e+14</td><td style=\"text-align: right;\">                   4</td><td>48eada07  </td><td style=\"text-align: right;\">     0.507821</td><td style=\"text-align: right;\">     0.235144</td></tr>\n",
       "<tr><td>FSR_Trainable_4f7002fa</td><td>2023-08-11_03-32-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   19.6853 </td><td style=\"text-align: right;\">   1776.34 </td><td style=\"text-align: right;\">     2.85575</td><td style=\"text-align: right;\"> 3.48467e+18</td><td style=\"text-align: right;\">1.84951 </td><td>172.26.215.93</td><td style=\"text-align: right;\">211207</td><td style=\"text-align: right;\">     6.59445</td><td style=\"text-align: right;\">    1079.48 </td><td style=\"text-align: right;\">             1.17402</td><td style=\"text-align: right;\">          1.17402 </td><td style=\"text-align: right;\">       1.17402</td><td style=\"text-align: right;\"> 1691692363</td><td style=\"text-align: right;\">     4.36987</td><td style=\"text-align: right;\">    0.759864</td><td style=\"text-align: right;\">  3.43276e+13</td><td style=\"text-align: right;\">  1.6675e+15 </td><td style=\"text-align: right;\">                   1</td><td>4f7002fa  </td><td style=\"text-align: right;\">     1.42126 </td><td style=\"text-align: right;\">     0.428248</td></tr>\n",
       "<tr><td>FSR_Trainable_56b99d52</td><td>2023-08-11_03-32-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    8.4377 </td><td style=\"text-align: right;\">   1638.42 </td><td style=\"text-align: right;\">     1.70871</td><td style=\"text-align: right;\"> 3.65603e+18</td><td style=\"text-align: right;\">0.977625</td><td>172.26.215.93</td><td style=\"text-align: right;\">211447</td><td style=\"text-align: right;\">     3.24555</td><td style=\"text-align: right;\">     975.726</td><td style=\"text-align: right;\">             1.23691</td><td style=\"text-align: right;\">          1.23691 </td><td style=\"text-align: right;\">       1.23691</td><td style=\"text-align: right;\"> 1691692371</td><td style=\"text-align: right;\">     1.80313</td><td style=\"text-align: right;\">    0.610222</td><td style=\"text-align: right;\">  6.47849e+13</td><td style=\"text-align: right;\">  1.46031e+15</td><td style=\"text-align: right;\">                   1</td><td>56b99d52  </td><td style=\"text-align: right;\">     0.653982</td><td style=\"text-align: right;\">     0.323643</td></tr>\n",
       "<tr><td>FSR_Trainable_57b5d6f7</td><td>2023-08-11_03-31-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    6.06442</td><td style=\"text-align: right;\">   1455.58 </td><td style=\"text-align: right;\">     1.48483</td><td style=\"text-align: right;\"> 2.94098e+18</td><td style=\"text-align: right;\">0.913194</td><td>172.26.215.93</td><td style=\"text-align: right;\">209207</td><td style=\"text-align: right;\">     2.74935</td><td style=\"text-align: right;\">     950.249</td><td style=\"text-align: right;\">             2.70981</td><td style=\"text-align: right;\">          2.70981 </td><td style=\"text-align: right;\">       2.70981</td><td style=\"text-align: right;\"> 1691692298</td><td style=\"text-align: right;\">     1.38482</td><td style=\"text-align: right;\">    0.555184</td><td style=\"text-align: right;\">  9.43607e+13</td><td style=\"text-align: right;\">  1.25589e+15</td><td style=\"text-align: right;\">                   1</td><td>57b5d6f7  </td><td style=\"text-align: right;\">     0.596131</td><td style=\"text-align: right;\">     0.317063</td></tr>\n",
       "<tr><td>FSR_Trainable_63255ad9</td><td>2023-08-11_03-33-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    4.05816</td><td style=\"text-align: right;\">   1017.73 </td><td style=\"text-align: right;\">     1.16706</td><td style=\"text-align: right;\"> 3.62278e+08</td><td style=\"text-align: right;\">5.96778 </td><td>172.26.215.93</td><td style=\"text-align: right;\">212346</td><td style=\"text-align: right;\">     2.33394</td><td style=\"text-align: right;\">     706.923</td><td style=\"text-align: right;\">             1.04297</td><td style=\"text-align: right;\">          1.04297 </td><td style=\"text-align: right;\">       1.04297</td><td style=\"text-align: right;\"> 1691692407</td><td style=\"text-align: right;\">     7.61867</td><td style=\"text-align: right;\">    2.37421 </td><td style=\"text-align: right;\">  1.18493e+15</td><td style=\"text-align: right;\"> 73.5442     </td><td style=\"text-align: right;\">                   1</td><td>63255ad9  </td><td style=\"text-align: right;\">     4.43782 </td><td style=\"text-align: right;\">     1.52995 </td></tr>\n",
       "<tr><td>FSR_Trainable_815ec5f7</td><td>2023-08-11_03-32-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        30</td><td style=\"text-align: right;\">    5.25623</td><td style=\"text-align: right;\">    624.895</td><td style=\"text-align: right;\">     1.16403</td><td style=\"text-align: right;\"> 7.90198e+17</td><td style=\"text-align: right;\">0.66507 </td><td>172.26.215.93</td><td style=\"text-align: right;\">209205</td><td style=\"text-align: right;\">     2.58241</td><td style=\"text-align: right;\">     445.954</td><td style=\"text-align: right;\">            26.1805 </td><td style=\"text-align: right;\">          0.788434</td><td style=\"text-align: right;\">     106.7    </td><td style=\"text-align: right;\"> 1691692340</td><td style=\"text-align: right;\">     1.10904</td><td style=\"text-align: right;\">    0.252841</td><td style=\"text-align: right;\">  7.39845e+13</td><td style=\"text-align: right;\">  3.92701e+14</td><td style=\"text-align: right;\">                 100</td><td>815ec5f7  </td><td style=\"text-align: right;\">     0.502049</td><td style=\"text-align: right;\">     0.163021</td></tr>\n",
       "<tr><td>FSR_Trainable_9a152f5f</td><td>2023-08-11_03-31-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    5.04504</td><td style=\"text-align: right;\">   1029.53 </td><td style=\"text-align: right;\">     1.25748</td><td style=\"text-align: right;\"> 1.84853e+18</td><td style=\"text-align: right;\">0.741098</td><td>172.26.215.93</td><td style=\"text-align: right;\">209202</td><td style=\"text-align: right;\">     2.49755</td><td style=\"text-align: right;\">     713.192</td><td style=\"text-align: right;\">             8.06291</td><td style=\"text-align: right;\">          1.21448 </td><td style=\"text-align: right;\">       8.06291</td><td style=\"text-align: right;\"> 1691692315</td><td style=\"text-align: right;\">     1.0886 </td><td style=\"text-align: right;\">    0.38511 </td><td style=\"text-align: right;\">  8.97847e+13</td><td style=\"text-align: right;\">  7.41493e+14</td><td style=\"text-align: right;\">                   4</td><td>9a152f5f  </td><td style=\"text-align: right;\">     0.505961</td><td style=\"text-align: right;\">     0.235137</td></tr>\n",
       "<tr><td>FSR_Trainable_da984e65</td><td>2023-08-11_03-32-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    5.10328</td><td style=\"text-align: right;\">   1087.95 </td><td style=\"text-align: right;\">     1.36607</td><td style=\"text-align: right;\"> 1.73066e+18</td><td style=\"text-align: right;\">0.774546</td><td>172.26.215.93</td><td style=\"text-align: right;\">210478</td><td style=\"text-align: right;\">     2.49174</td><td style=\"text-align: right;\">     816.706</td><td style=\"text-align: right;\">             2.25215</td><td style=\"text-align: right;\">          0.87896 </td><td style=\"text-align: right;\">       2.25215</td><td style=\"text-align: right;\"> 1691692331</td><td style=\"text-align: right;\">     1.12001</td><td style=\"text-align: right;\">    0.381518</td><td style=\"text-align: right;\">  9.32926e+13</td><td style=\"text-align: right;\">  6.00315e+14</td><td style=\"text-align: right;\">                   2</td><td>da984e65  </td><td style=\"text-align: right;\">     0.509453</td><td style=\"text-align: right;\">     0.265094</td></tr>\n",
       "<tr><td>FSR_Trainable_f81dfc2d</td><td>2023-08-11_03-33-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    5.44489</td><td style=\"text-align: right;\">   1123.63 </td><td style=\"text-align: right;\">     1.39277</td><td style=\"text-align: right;\"> 1.91243e+18</td><td style=\"text-align: right;\">0.796983</td><td>172.26.215.93</td><td style=\"text-align: right;\">211850</td><td style=\"text-align: right;\">     2.52828</td><td style=\"text-align: right;\">     832.749</td><td style=\"text-align: right;\">             1.53968</td><td style=\"text-align: right;\">          0.553916</td><td style=\"text-align: right;\">       1.53968</td><td style=\"text-align: right;\"> 1691692388</td><td style=\"text-align: right;\">     1.17633</td><td style=\"text-align: right;\">    0.410443</td><td style=\"text-align: right;\">  9.3949e+13 </td><td style=\"text-align: right;\">  7.27021e+14</td><td style=\"text-align: right;\">                   2</td><td>f81dfc2d  </td><td style=\"text-align: right;\">     0.518436</td><td style=\"text-align: right;\">     0.278548</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seokj/workspace/.venv/lib/python3.10/site-packages/optuna/trial/_trial.py:494: UserWarning: The reported value is ignored because this `step` 1 is already reported.\n",
      "  warnings.warn(\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=209205)\u001b[0m 2023-08-11 03:31:33,552\tINFO trainable.py:918 -- Restored on 172.26.215.93 from checkpoint: /tmp/checkpoint_tmp_ead7f66d7d38485bb5534b922360ae07\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=209205)\u001b[0m 2023-08-11 03:31:33,552\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 70, '_timesteps_total': None, '_time_total': 80.51905107498169, '_episodes_total': None}\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_9a152f5f_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-11-31/wandb/run-20230811_033145-9a152f5f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Syncing run FSR_Trainable_9a152f5f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9a152f5f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "/home/seokj/workspace/.venv/lib/python3.10/site-packages/optuna/trial/_trial.py:494: UserWarning: The reported value is ignored because this `step` 42 is already reported.\n",
      "  warnings.warn(\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:  $ pip install wandb --upgrade\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: Tracking run with wandb version 0.15.4\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_57b5d6f7_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_01-45-46/wandb/run-20230811_033146-57b5d6f7\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: Syncing run FSR_Trainable_57b5d6f7\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/57b5d6f7\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "/home/seokj/workspace/.venv/lib/python3.10/site-packages/optuna/trial/_trial.py:494: UserWarning: The reported value is ignored because this `step` 43 is already reported.\n",
      "  warnings.warn(\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:                mae_coord 6.06442\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:                mae_force 1455.58041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:               mape_coord 1.48483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:               mape_force 2.940982868877274e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:                   metric 0.91319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:               rmse_coord 2.74935\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:               rmse_force 950.24854\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:       time_since_restore 2.70981\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:         time_this_iter_s 2.70981\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:             time_total_s 2.70981\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:                timestamp 1691692298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:               tmae_coord 1.38482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:               tmae_force 0.55518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:              tmape_coord 94360687125603.92\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:              tmape_force 1255893883970400.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:              trmse_coord 0.59613\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb:              trmse_force 0.31706\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: 🚀 View run FSR_Trainable_57b5d6f7 at: https://wandb.ai/seokjin/FSR-prediction/runs/57b5d6f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209788)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033146-57b5d6f7/logs\n",
      "2023-08-11 03:31:57,319\tWARNING worker.py:2019 -- WARNING: 32 PYTHON worker processes have been started on node: 9904e5a8b936b23333822c713498556e4f6efc573b1f85ab5370d81f with address: 172.26.215.93. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)03 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:                mae_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:                mae_force █▇▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:               mape_coord █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:               mape_force █▇▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:                   metric █▆▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:               rmse_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:               rmse_force █▆▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:         time_this_iter_s █▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:                timestamp ▁▅██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:               tmae_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:               tmae_force █▇▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:              tmape_coord ▆█▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:              tmape_force █▇▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:              trmse_coord █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb:              trmse_force █▆▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209481)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-9a152f5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_28ec7133_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_03-31-36/wandb/run-20230811_033201-28ec7133\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Syncing run FSR_Trainable_28ec7133\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/28ec7133\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:                mae_coord ▁█▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:                mae_force █▇▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:               mape_coord █▆▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:               mape_force ▆█▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:                   metric █▅▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:               rmse_coord ▆█▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:               rmse_force █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:       time_since_restore ▁▂▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:         time_this_iter_s █▁▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:             time_total_s ▁▂▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:                timestamp ▁▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:               tmae_coord ▁█▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:               tmae_force ▁▆▄█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:              tmape_coord █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:              tmape_force ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:              trmse_coord ▅█▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:              trmse_force █▄▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:                mae_coord 5.55037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:                mae_force 902.6586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:               mape_coord 1.50565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:               mape_force 1.63270258638373e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:                   metric 0.74297\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:               rmse_coord 2.53547\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:               rmse_force 538.48433\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:       time_since_restore 8.40614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:         time_this_iter_s 1.55633\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:             time_total_s 8.40614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:                timestamp 1691692319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:               tmae_coord 1.17553\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:               tmae_force 0.41702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:              tmape_coord 88816825898671.39\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:              tmape_force 911522839968202.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:              trmse_coord 0.50782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb:              trmse_force 0.23514\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb: 🚀 View run FSR_Trainable_48eada07 at: https://wandb.ai/seokjin/FSR-prediction/runs/48eada07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033146-48eada07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:                mae_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:               mape_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:               rmse_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:               tmae_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:              tmape_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:              trmse_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209757)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210340)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033201-28ec7133/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_da984e65_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_03-31-53/wandb/run-20230811_033214-da984e65\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: Syncing run FSR_Trainable_da984e65\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/da984e65\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:                mae_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:               mape_coord ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:               rmse_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:               tmae_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:              tmape_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:              trmse_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:                mae_coord 5.10328\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:                mae_force 1087.94832\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:               mape_coord 1.36607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:               mape_force 1.7306625378587743e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:                   metric 0.77455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:               rmse_coord 2.49174\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:               rmse_force 816.70553\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:       time_since_restore 2.25215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:         time_this_iter_s 0.87896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:             time_total_s 2.25215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:                timestamp 1691692331\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:               tmae_coord 1.12001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:               tmae_force 0.38152\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:              tmape_coord 93292604483368.39\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:              tmape_force 600315017898528.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:              trmse_coord 0.50945\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb:              trmse_force 0.26509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: 🚀 View run FSR_Trainable_da984e65 at: https://wandb.ai/seokjin/FSR-prediction/runs/da984e65\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210599)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033214-da984e65/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 03:32:24,206\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.016 s, which may be a performance bottleneck.\n",
      "2023-08-11 03:32:24,210\tWARNING util.py:315 -- The `process_trial_result` operation took 2.021 s, which may be a performance bottleneck.\n",
      "2023-08-11 03:32:24,215\tWARNING util.py:315 -- Processing trial results took 2.025 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 03:32:24,218\tWARNING util.py:315 -- The `process_trial_result` operation took 2.029 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:                mae_coord ▄▃▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃█▂▃▃▁▃▃▁▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:                mae_force ▇▇▆▆▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂█▂▃▃▂▂▂▁▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:               mape_coord ▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃█▂▃▂▂▂▃▂▃▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:               mape_force █▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▇▁▂▂▁▁▂▁▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:                   metric ▃▃▂▂▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂█▂▃▃▁▃▃▁▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:               rmse_coord ▂▂▁▁▁▁▁▁▁▁▂▂▂▂▂▁▂▂▂█▁▃▃▁▃▃▁▃▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:               rmse_force ▃▃▃▃▃▄▃▃▃▃▃▃▃▃▂▂▂▂▂█▂▃▃▂▂▂▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:         time_this_iter_s █▁▃▂▂▂▁▁▂▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:                timestamp ▁▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇█████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:               tmae_coord ▄▃▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃█▂▃▂▁▃▃▁▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:               tmae_force █▇▆▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▃▆▂▃▃▂▂▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:              tmape_coord ▇▆▆▅▄▄▃▃▃▃▂▂▂▂▂▃▃▃▃▂▃▁▅▆▄▄▅▃█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:              tmape_force █▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▅▂▃▂▁▂▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:              trmse_coord ▂▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂█▂▃▃▁▃▃▁▃▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:              trmse_force ▅▄▄▄▄▄▄▄▄▄▄▄▃▃▃▂▂▃▂█▂▃▃▁▂▂▁▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb: iterations_since_restore 30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:                mae_coord 5.25623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:                mae_force 624.89512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:               mape_coord 1.16403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:               mape_force 7.901984284192381e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:                   metric 0.66507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:               rmse_coord 2.58241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:               rmse_force 445.95446\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:       time_since_restore 26.18048\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:         time_this_iter_s 0.78843\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:             time_total_s 106.69953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:                timestamp 1691692340\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:               tmae_coord 1.10904\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:               tmae_force 0.25284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:              tmape_coord 73984545127221.31\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:              tmape_force 392701113906275.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:              trmse_coord 0.50205\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb:              trmse_force 0.16302\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb: 🚀 View run FSR_Trainable_815ec5f7 at: https://wandb.ai/seokjin/FSR-prediction/runs/815ec5f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209488)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-815ec5f7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_35aed758_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_03-32-07/wandb/run-20230811_033230-35aed758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: Syncing run FSR_Trainable_35aed758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/35aed758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 03:32:35,184\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.988 s, which may be a performance bottleneck.\n",
      "2023-08-11 03:32:35,187\tWARNING util.py:315 -- The `process_trial_result` operation took 1.992 s, which may be a performance bottleneck.\n",
      "2023-08-11 03:32:35,190\tWARNING util.py:315 -- Processing trial results took 1.995 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 03:32:35,193\tWARNING util.py:315 -- The `process_trial_result` operation took 1.998 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:                mae_coord 20.53065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:                mae_force 1862.50881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:               mape_coord 3.13937\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:               mape_force 2.789633065224219e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:                   metric 1.87748\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:               rmse_coord 7.2083\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:               rmse_force 1199.89978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:       time_since_restore 1.42322\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:         time_this_iter_s 1.42322\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:             time_total_s 1.42322\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:                timestamp 1691692342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:               tmae_coord 4.43144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:               tmae_force 0.7493\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:              tmape_coord 33447842241706.61\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:              tmape_force 1429006712066583.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:              trmse_coord 1.46896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb:              trmse_force 0.40853\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: 🚀 View run FSR_Trainable_35aed758 at: https://wandb.ai/seokjin/FSR-prediction/runs/35aed758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=210837)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033230-35aed758/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033230-35aed758/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb:                mae_coord ▆▆▆▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▆█▂▂▃▄▄▄▃▃▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb:                mae_force █████▇▇▇▆▆▅▅▅▅▅▅▅▅▄▄▄▄▄▄▅▅▆▁▄▃▄▄▃▂▁▄▄▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb:               mape_coord ▆▆▆▆▆▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▅▅▅█▁▂▃▄▄▄▃▄▂▂▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb:               mape_force ███▇▇▇▇▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▄▁▃▃▃▃▂▂▁▃▂▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb:                   metric ▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▆█▁▃▃▄▅▄▃▃▄▄▄▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb:               rmse_coord ▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▄▄▅█▁▂▃▄▅▅▄▃▄▄▄▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb:               rmse_force ▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▇█▃▆▅▆▆▅▄▃▆▆▅▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb:         time_this_iter_s █▃▂▂▂▁▁▂▁▁▁▁▁▁▂▂▁▂▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb:                timestamp ▁▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb:               tmae_coord ██████████▇▇▇▇▇▇▆▆▆▅▅▅▅▅▄▅▇▁▂▃▃▄▃▂▃▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb:               tmae_force ██▇▇▇▇▆▆▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▄▁▂▂▂▂▂▁▁▂▂▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb:              tmape_coord █▇▇▇▆▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▁▁▃▃▃▂▂▁▂▃▂▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb:              tmape_force ██▇▇▇▇▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▄▂▂▂▂▂▂▂▁▂▂▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb:              trmse_coord ▆▆▆▆▆▆▆▆▆▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▅█▁▂▃▄▅▅▄▄▄▄▄▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb:              trmse_force ▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▅▅▆▆█▁▄▃▄▄▃▂▂▄▄▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_46a66fbd_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_03-32-20/wandb/run-20230811_033239-46a66fbd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: Syncing run FSR_Trainable_46a66fbd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/46a66fbd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209478)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:                mae_coord 19.16573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:                mae_force 1342.19375\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:               mape_coord 3.33368\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:               mape_force 2.2160370824698294e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:                   metric 1.73027\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:               rmse_coord 6.91445\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:               rmse_force 945.25671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:       time_since_restore 1.1758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:         time_this_iter_s 1.1758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:             time_total_s 1.1758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:                timestamp 1691692353\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:               tmae_coord 4.20424\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:               tmae_force 0.49986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:              tmape_coord 64655563143830.98\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:              tmape_force 948777408632356.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:              trmse_coord 1.42271\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb:              trmse_force 0.30755\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: 🚀 View run FSR_Trainable_46a66fbd at: https://wandb.ai/seokjin/FSR-prediction/runs/46a66fbd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211067)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033239-46a66fbd/logs\n",
      "2023-08-11 03:32:44,783\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.743 s, which may be a performance bottleneck.\n",
      "2023-08-11 03:32:44,785\tWARNING util.py:315 -- The `process_trial_result` operation took 1.747 s, which may be a performance bottleneck.\n",
      "2023-08-11 03:32:44,789\tWARNING util.py:315 -- Processing trial results took 1.750 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 03:32:44,792\tWARNING util.py:315 -- The `process_trial_result` operation took 1.754 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb:                mae_coord ▅▅▆▆▇▇▇▇▆▆▆▆▆▆▆▆▅▅▅▅▅▅▄▄▅▄▅▄▄▆▄▂▂▂▃▃▅▄█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb:                mae_force ██▇▇▇▆▆▆▆▆▅▅▅▅▅▅▅▅▅▄▄▃▃▃▂▂▂▃▂▃▃▁▁▁▁▁▂▂▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb:               mape_coord ▂▂▂▂▂▂▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▃▂▃▃▃▄▃▃▅▄▂▂▃▃▄▅▄█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb:               mape_force ████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▂▁▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb:                   metric ▅▅▅▅▄▄▄▃▃▄▄▃▄▄▃▃▃▃▃▃▃▃▁▂▃▂▃▃▃▅▄▁▁▁▂▃▅▄█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb:               rmse_coord ▁▁▂▂▂▂▂▂▂▂▃▂▃▃▃▃▃▃▃▃▂▃▂▃▃▃▄▃▃▅▄▁▂▂▃▄▆▄█▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb:               rmse_force ▃▃▂▂▂▂▁▁▁▂▂▁▂▂▃▃▃▃▄▄▄▃▂▃▃▄▄▅▄▆▆▂▄▄▄▅▆██▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb:         time_this_iter_s █▃▂▃▂▂▂▂▂▂▂▃▂▂▂▁▂▂▂▁▂▂▂▂▁▁▁▁▁▁▂▂▁▂▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb:                timestamp ▁▂▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇██████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb:               tmae_coord ▇▇██████▇▇▇▇▆▇▆▆▆▆▆▆▅▅▄▄▄▄▄▃▄▅▄▃▂▂▂▃▄▃▇▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb:               tmae_force ██▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▃▂▃▂▂▁▁▁▁▂▁▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb:              tmape_coord ███▇▆▅▄▅▅▅▄▄▃▃▂▂▂▂▁▁▁▁▃▁▁▁▁▂▂▁▁▅▄▄▄▅▅▅▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb:              tmape_force ██▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▃▂▂▁▁▁▁▂▁▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb:              trmse_coord ▃▃▃▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▃▄▃▃▆▄▂▂▂▃▄▆▄█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb:              trmse_force █▇▆▆▆▅▅▄▄▄▄▃▄▄▄▄▄▄▄▃▃▃▂▃▃▃▃▄▃▅▄▁▁▁▂▃▄▃█▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033145-3838379f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_4f7002fa_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_03-32-32/wandb/run-20230811_033247-4f7002fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: Syncing run FSR_Trainable_4f7002fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4f7002fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=209484)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:                mae_coord 19.68525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:                mae_force 1776.34392\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:               mape_coord 2.85575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:               mape_force 3.484666594918504e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:                   metric 1.84951\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:               rmse_coord 6.59445\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:               rmse_force 1079.4763\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:       time_since_restore 1.17402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:         time_this_iter_s 1.17402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:             time_total_s 1.17402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:                timestamp 1691692363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:               tmae_coord 4.36987\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:               tmae_force 0.75986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:              tmape_coord 34327618182012.16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:              tmape_force 1667497540899213.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:              trmse_coord 1.42126\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb:              trmse_force 0.42825\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: 🚀 View run FSR_Trainable_4f7002fa at: https://wandb.ai/seokjin/FSR-prediction/runs/4f7002fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033247-4f7002fa/logs\n",
      "2023-08-11 03:32:53,369\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.827 s, which may be a performance bottleneck.\n",
      "2023-08-11 03:32:53,376\tWARNING util.py:315 -- The `process_trial_result` operation took 1.834 s, which may be a performance bottleneck.\n",
      "2023-08-11 03:32:53,378\tWARNING util.py:315 -- Processing trial results took 1.836 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 03:32:53,380\tWARNING util.py:315 -- The `process_trial_result` operation took 1.838 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211305)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_56b99d52_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_03-32-41/wandb/run-20230811_033255-56b99d52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: Syncing run FSR_Trainable_56b99d52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/56b99d52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 03:32:59,839\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.719 s, which may be a performance bottleneck.\n",
      "2023-08-11 03:32:59,843\tWARNING util.py:315 -- The `process_trial_result` operation took 1.724 s, which may be a performance bottleneck.\n",
      "2023-08-11 03:32:59,844\tWARNING util.py:315 -- Processing trial results took 1.726 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 03:32:59,847\tWARNING util.py:315 -- The `process_trial_result` operation took 1.729 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:                mae_coord 8.4377\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:                mae_force 1638.41552\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:               mape_coord 1.70871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:               mape_force 3.6560264308769265e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:                   metric 0.97763\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:               rmse_coord 3.24555\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:               rmse_force 975.7264\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:       time_since_restore 1.23691\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:         time_this_iter_s 1.23691\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:             time_total_s 1.23691\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:                timestamp 1691692371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:               tmae_coord 1.80313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:               tmae_force 0.61022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:              tmape_coord 64784949505236.62\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:              tmape_force 1460305779937677.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:              trmse_coord 0.65398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb:              trmse_force 0.32364\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: 🚀 View run FSR_Trainable_56b99d52 at: https://wandb.ai/seokjin/FSR-prediction/runs/56b99d52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033255-56b99d52/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211538)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_0e4322aa_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_03-32-50/wandb/run-20230811_033302-0e4322aa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Syncing run FSR_Trainable_0e4322aa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0e4322aa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033302-0e4322aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211717)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-08-11 03:33:07,710\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.810 s, which may be a performance bottleneck.\n",
      "2023-08-11 03:33:07,714\tWARNING util.py:315 -- The `process_trial_result` operation took 1.815 s, which may be a performance bottleneck.\n",
      "2023-08-11 03:33:07,716\tWARNING util.py:315 -- Processing trial results took 1.818 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 03:33:07,722\tWARNING util.py:315 -- The `process_trial_result` operation took 1.823 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_f81dfc2d_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_03-32-56/wandb/run-20230811_033310-f81dfc2d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: Syncing run FSR_Trainable_f81dfc2d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f81dfc2d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 03:33:13,867\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.561 s, which may be a performance bottleneck.\n",
      "2023-08-11 03:33:13,869\tWARNING util.py:315 -- The `process_trial_result` operation took 1.564 s, which may be a performance bottleneck.\n",
      "2023-08-11 03:33:13,872\tWARNING util.py:315 -- Processing trial results took 1.566 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 03:33:13,873\tWARNING util.py:315 -- The `process_trial_result` operation took 1.567 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:                mae_coord ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:               mape_coord ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:               rmse_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:               tmae_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:              tmape_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:              trmse_coord █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:                mae_coord 5.44489\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:                mae_force 1123.62942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:               mape_coord 1.39277\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:               mape_force 1.91243198587334e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:                   metric 0.79698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:               rmse_coord 2.52828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:               rmse_force 832.74931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:       time_since_restore 1.53968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:         time_this_iter_s 0.55392\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:             time_total_s 1.53968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:                timestamp 1691692388\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:               tmae_coord 1.17633\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:               tmae_force 0.41044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:              tmape_coord 93948972133726.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:              tmape_force 727020969770247.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:              trmse_coord 0.51844\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb:              trmse_force 0.27855\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: 🚀 View run FSR_Trainable_f81dfc2d at: https://wandb.ai/seokjin/FSR-prediction/runs/f81dfc2d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=211939)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033310-f81dfc2d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_119b1b07_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_03-33-04/wandb/run-20230811_033316-119b1b07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Syncing run FSR_Trainable_119b1b07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/119b1b07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:                mae_coord ▁▂▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:                mae_force █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:               mape_coord █▇▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:               mape_force █▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:                   metric █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:               rmse_coord █▅▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:               rmse_force █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:       time_since_restore ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:         time_this_iter_s █▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:             time_total_s ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:                timestamp ▁▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:               tmae_coord ▆▃▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:               tmae_force ▅▁▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:              tmape_coord █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:              tmape_force ▁▁▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:              trmse_coord █▅▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb:              trmse_force █▃▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033316-119b1b07/logs\n",
      "2023-08-11 03:33:22,641\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.763 s, which may be a performance bottleneck.\n",
      "2023-08-11 03:33:22,645\tWARNING util.py:315 -- The `process_trial_result` operation took 1.768 s, which may be a performance bottleneck.\n",
      "2023-08-11 03:33:22,646\tWARNING util.py:315 -- Processing trial results took 1.770 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 03:33:22,648\tWARNING util.py:315 -- The `process_trial_result` operation took 1.772 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_0bca8ed6_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-11_03-33-11/wandb/run-20230811_033324-0bca8ed6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: Syncing run FSR_Trainable_0bca8ed6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0bca8ed6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 03:33:28,997\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.648 s, which may be a performance bottleneck.\n",
      "2023-08-11 03:33:29,002\tWARNING util.py:315 -- The `process_trial_result` operation took 1.653 s, which may be a performance bottleneck.\n",
      "2023-08-11 03:33:29,005\tWARNING util.py:315 -- Processing trial results took 1.656 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 03:33:29,008\tWARNING util.py:315 -- The `process_trial_result` operation took 1.659 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:                mae_coord ▁█▆▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:                mae_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:               mape_coord █▆▆▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:               mape_force ▁█▆▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:                   metric █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:               rmse_coord ▆█▇▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:               rmse_force █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:         time_this_iter_s █▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:                timestamp ▁▆▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:               tmae_coord ▂█▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:               tmae_force ▁█▇▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:              tmape_coord █▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:              tmape_force ▁▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:              trmse_coord ▆█▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:              trmse_force █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:                mae_coord 5.83276\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:                mae_force 868.91579\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:               mape_coord 1.55563\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:               mape_force 1.6692275133661076e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:                   metric 0.73368\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:               rmse_coord 2.59852\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:               rmse_force 513.72288\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:       time_since_restore 2.71943\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:         time_this_iter_s 0.55884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:             time_total_s 2.71943\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:                timestamp 1691692404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:               tmae_coord 1.22675\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:               tmae_force 0.3968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:              tmape_coord 88044788013220.62\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:              tmape_force 910261353004986.9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:              trmse_coord 0.51777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb:              trmse_force 0.21591\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: 🚀 View run FSR_Trainable_0bca8ed6 at: https://wandb.ai/seokjin/FSR-prediction/runs/0bca8ed6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212344)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033324-0bca8ed6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35/FSR_Trainable_63255ad9_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Sim_2023-08-11_03-33-19/wandb/run-20230811_033330-63255ad9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: Syncing run FSR_Trainable_63255ad9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/63255ad9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:                mae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:               mape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:               rmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:               tmae_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:              tmape_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:              trmse_coord ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:                mae_coord 4.05816\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:                mae_force 1017.72919\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:               mape_coord 1.16706\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:               mape_force 362278185.08904\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:                   metric 5.96778\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:               rmse_coord 2.33394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:               rmse_force 706.92284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:       time_since_restore 1.04297\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:         time_this_iter_s 1.04297\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:             time_total_s 1.04297\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:                timestamp 1691692407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:               tmae_coord 7.61867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:               tmae_force 2.37421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:              tmape_coord 1184925132681855.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:              tmape_force 73.54423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:              trmse_coord 4.43782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb:              trmse_force 1.52995\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: 🚀 View run FSR_Trainable_63255ad9 at: https://wandb.ai/seokjin/FSR-prediction/runs/63255ad9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=212523)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_033330-63255ad9/logs\n",
      "2023-08-11 03:33:39,345\tINFO tune.py:1111 -- Total run time: 136.09 seconds (125.47 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.restore(\n",
    "    path='/home/seokj/ray_results/FSR_Trainable_2023-08-11_00-39-35',\n",
    "    trainable=FSR_Trainable\n",
    ").fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
