{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1\n",
    "Model = All\n",
    "Optimizer = All\n",
    "Scaler = All\n",
    "Imputer = All\n",
    "\n",
    "Index_X = FSR_for_force, FSR_for_coord\n",
    "Index_y = force, x_coord, y_coord\n",
    "Data = Splited by Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.LSTM', 'fsr_model.CNN_LSTM', 'fsr_model.ANN'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    imputer = trial.suggest_categorical('imputer', ['sklearn.impute.SimpleImputer'])\n",
    "    if imputer == 'sklearn.impute.SimpleImputer':\n",
    "        trial.suggest_categorical('imputer_args/strategy', [\n",
    "            'mean',\n",
    "            'median',\n",
    "        ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': ['FSR_for_force', 'FSR_for_coord'],\n",
    "        'index_y': ['force', 'x_coord', 'y_coord'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_time'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-04 20:42:59,719] A new study created in memory with name: optuna\n",
      "2023-07-04 20:43:01,836\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-07-04 20:43:03,184\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-04 20:52:33</td></tr>\n",
       "<tr><td>Running for: </td><td>00:09:30.72        </td></tr>\n",
       "<tr><td>Memory:      </td><td>6.4/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=33<br>Bracket: Iter 64.000: -116.37239699642794 | Iter 32.000: -116.59661736321198 | Iter 16.000: -117.24627595882114 | Iter 8.000: -123.5965640958535 | Iter 4.000: -160.94495736834435 | Iter 2.000: -198.43871064741538 | Iter 1.000: -240.10463432738518<br>Logical resource usage: 8.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  ... 20 more trials not shown (20 TERMINATED)\n",
       "  Number of errored trials: 2<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_f408f767</td><td style=\"text-align: right;\">           1</td><td>/home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_f408f767_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-46-47/error.txt</td></tr>\n",
       "<tr><td>FSR_Trainable_35e41ca1</td><td style=\"text-align: right;\">           1</td><td>/home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_35e41ca1_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-47-00/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                  </th><th>criterion       </th><th>data_loader         </th><th>imputer             </th><th>imputer_args/strateg\n",
       "y       </th><th>index_X             </th><th>index_y             </th><th>model             </th><th style=\"text-align: right;\">    model_args/cnn_hidde\n",
       "n_size</th><th style=\"text-align: right;\">  model_args/cnn_num_l\n",
       "ayer</th><th style=\"text-align: right;\">    model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">    model_args/lstm_hidd\n",
       "en_size</th><th style=\"text-align: right;\">  model_args/lstm_num_\n",
       "layer</th><th style=\"text-align: right;\">  model_args/num_layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">       mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_60e26f1d</td><td>RUNNING   </td><td>172.26.215.93:1288246</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>median</td><td>[&#x27;FSR_for_force_4200</td><td>[&#x27;force&#x27;, &#x27;x_co_1180</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0114461  </td><td>sklearn.preproc_0030</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">        41.3026 </td><td style=\"text-align: right;\">115.753</td><td style=\"text-align: right;\"> 34.2146</td><td style=\"text-align: right;\">5.70834e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_82a7261c</td><td>RUNNING   </td><td>172.26.215.93:1287998</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>median</td><td>[&#x27;FSR_for_force_a780</td><td>[&#x27;force&#x27;, &#x27;x_co_b700</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0124305  </td><td>sklearn.preproc_0030</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">        49.2042 </td><td style=\"text-align: right;\">115.715</td><td style=\"text-align: right;\"> 34.167 </td><td style=\"text-align: right;\">5.66738e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_da8060d0</td><td>RUNNING   </td><td>172.26.215.93:1288639</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>median</td><td>[&#x27;FSR_for_force_cf80</td><td>[&#x27;force&#x27;, &#x27;x_co_6c00</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0123249  </td><td>sklearn.preproc_0030</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">        24.715  </td><td style=\"text-align: right;\">115.737</td><td style=\"text-align: right;\"> 34.2176</td><td style=\"text-align: right;\">5.72302e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f809912e</td><td>RUNNING   </td><td>172.26.215.93:1288328</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>median</td><td>[&#x27;FSR_for_force_5940</td><td>[&#x27;force&#x27;, &#x27;x_co_8a80</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0122417  </td><td>sklearn.preproc_0030</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">        35.5362 </td><td style=\"text-align: right;\">115.725</td><td style=\"text-align: right;\"> 34.135 </td><td style=\"text-align: right;\">5.68399e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_c830dd62</td><td>PENDING   </td><td>                     </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>median</td><td>[&#x27;FSR_for_force_5940</td><td>[&#x27;force&#x27;, &#x27;x_co_4340</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0125651  </td><td>sklearn.preproc_0030</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>FSR_Trainable_080964f7</td><td>TERMINATED</td><td>172.26.215.93:1282179</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>median</td><td>[&#x27;FSR_for_force_fc00</td><td>[&#x27;force&#x27;, &#x27;x_co_c300</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">                      </td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        4.98475e-05</td><td>sklearn.preproc_0030</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.70954</td><td style=\"text-align: right;\">253.112</td><td style=\"text-align: right;\"> 87.8893</td><td style=\"text-align: right;\">2.58478e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_0bb9589b</td><td>TERMINATED</td><td>172.26.215.93:1280791</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>mean  </td><td>[&#x27;FSR_for_force_ad40</td><td>[&#x27;force&#x27;, &#x27;x_co_82c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">                      </td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00231327 </td><td>sklearn.preproc_0570</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.55712</td><td style=\"text-align: right;\">303.55 </td><td style=\"text-align: right;\">103.333 </td><td style=\"text-align: right;\">1.20117e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_142de23b</td><td>TERMINATED</td><td>172.26.215.93:1280118</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>median</td><td>[&#x27;FSR_for_force_aa40</td><td>[&#x27;force&#x27;, &#x27;x_co_1200</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000866863</td><td>sklearn.preproc_0030</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        56.3033 </td><td style=\"text-align: right;\">187.469</td><td style=\"text-align: right;\"> 63.999 </td><td style=\"text-align: right;\">1.95731e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_158a9b24</td><td>TERMINATED</td><td>172.26.215.93:1287516</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>median</td><td>[&#x27;FSR_for_force_ac00</td><td>[&#x27;force&#x27;, &#x27;x_co_e6c0</td><td>fsr_model.LSTM    </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00235101 </td><td>sklearn.preproc_0450</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.3226 </td><td style=\"text-align: right;\">273.071</td><td style=\"text-align: right;\"> 78.1401</td><td style=\"text-align: right;\">1.66788e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_186e250c</td><td>TERMINATED</td><td>172.26.215.93:1285525</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>median</td><td>[&#x27;FSR_for_force_bd00</td><td>[&#x27;force&#x27;, &#x27;x_co_ab80</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0037968  </td><td>sklearn.preproc_0030</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3.32757</td><td style=\"text-align: right;\">198.56 </td><td style=\"text-align: right;\"> 62.1483</td><td style=\"text-align: right;\">2.00945e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_321e144e</td><td>TERMINATED</td><td>172.26.215.93:1283947</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>mean  </td><td>[&#x27;FSR_for_force_e680</td><td>[&#x27;force&#x27;, &#x27;x_co_7340</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00716098 </td><td>sklearn.preproc_0030</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       127.52   </td><td style=\"text-align: right;\">116.119</td><td style=\"text-align: right;\"> 34.7176</td><td style=\"text-align: right;\">5.81367e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_3b4317a1</td><td>TERMINATED</td><td>172.26.215.93:1286781</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>median</td><td>[&#x27;FSR_for_force_75c0</td><td>[&#x27;force&#x27;, &#x27;x_co_6480</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0158404  </td><td>sklearn.preproc_0030</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        56.2853 </td><td style=\"text-align: right;\">115.729</td><td style=\"text-align: right;\"> 34.1745</td><td style=\"text-align: right;\">5.68493e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_3bccdca1</td><td>TERMINATED</td><td>172.26.215.93:1281003</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>mean  </td><td>[&#x27;FSR_for_force_3180</td><td>[&#x27;force&#x27;, &#x27;x_co_1580</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">                      </td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000239983</td><td>sklearn.preproc_0570</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        34.8754 </td><td style=\"text-align: right;\">265.736</td><td style=\"text-align: right;\"> 88.5896</td><td style=\"text-align: right;\">1.34205e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_3d14e8cc</td><td>TERMINATED</td><td>172.26.215.93:1282285</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>median</td><td>[&#x27;FSR_for_force_8d40</td><td>[&#x27;force&#x27;, &#x27;x_co_af40</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        1.03311e-05</td><td>sklearn.preproc_0030</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         1.82089</td><td style=\"text-align: right;\">260.906</td><td style=\"text-align: right;\"> 86.2233</td><td style=\"text-align: right;\">2.38614e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_3d64b0e0</td><td>TERMINATED</td><td>172.26.215.93:1282656</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>median</td><td>[&#x27;FSR_for_force_f600</td><td>[&#x27;force&#x27;, &#x27;x_co_b140</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0770087  </td><td>sklearn.preproc_0030</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         1.65823</td><td style=\"text-align: right;\">289.261</td><td style=\"text-align: right;\"> 83.0913</td><td style=\"text-align: right;\">4.42367e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_492fcbfc</td><td>TERMINATED</td><td>172.26.215.93:1284628</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>median</td><td>[&#x27;FSR_for_force_1080</td><td>[&#x27;force&#x27;, &#x27;x_co_3c80</td><td>fsr_model.LSTM    </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00679893 </td><td>sklearn.preproc_0450</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.56005</td><td style=\"text-align: right;\">272.701</td><td style=\"text-align: right;\"> 79.3613</td><td style=\"text-align: right;\">2.57134e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_4a07e61a</td><td>TERMINATED</td><td>172.26.215.93:1282470</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>median</td><td>[&#x27;FSR_for_force_b240</td><td>[&#x27;force&#x27;, &#x27;x_co_c6c0</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0792266  </td><td>sklearn.preproc_0030</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        77.3991 </td><td style=\"text-align: right;\">134.366</td><td style=\"text-align: right;\"> 41.8324</td><td style=\"text-align: right;\">1.07913e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_4cf3ab8c</td><td>TERMINATED</td><td>172.26.215.93:1281492</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>mean  </td><td>[&#x27;FSR_for_force_bac0</td><td>[&#x27;force&#x27;, &#x27;x_co_2480</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">                      </td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00151449 </td><td>sklearn.preproc_0450</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.30501</td><td style=\"text-align: right;\">277.333</td><td style=\"text-align: right;\"> 84.5335</td><td style=\"text-align: right;\">8.33956e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_35e41ca1</td><td>ERROR     </td><td>172.26.215.93:1284386</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>median</td><td>[&#x27;FSR_for_force_25c0</td><td>[&#x27;force&#x27;, &#x27;x_co_2d40</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00629155 </td><td>sklearn.preproc_0450</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>FSR_Trainable_f408f767</td><td>ERROR     </td><td>172.26.215.93:1284165</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_01b0</td><td>sklearn.impute._bc30</td><td>median</td><td>[&#x27;FSR_for_force_9c40</td><td>[&#x27;force&#x27;, &#x27;x_co_9900</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00764131 </td><td>sklearn.preproc_0450</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">           </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 20:43:03,229\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th>iterations_since_restore  </th><th>mae               </th><th>mape                  </th><th>node_ip      </th><th style=\"text-align: right;\">    pid</th><th>rmse              </th><th>time_since_restore  </th><th>time_this_iter_s   </th><th>time_total_s      </th><th style=\"text-align: right;\">  timestamp</th><th>training_iteration  </th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_080964f7</td><td>2023-07-04_20-45-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>87.88925355887365 </td><td>258477813.18996572    </td><td>172.26.215.93</td><td style=\"text-align: right;\">1282179</td><td>253.11161678918478</td><td>3.7095401287078857  </td><td>1.5254478454589844 </td><td>3.7095401287078857</td><td style=\"text-align: right;\"> 1688471100</td><td>2                   </td><td>080964f7  </td></tr>\n",
       "<tr><td>FSR_Trainable_0bb9589b</td><td>2023-07-04_20-43-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>103.33253812863518</td><td>1.2011708262964786e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">1280791</td><td>303.549719722094  </td><td>4.557123899459839   </td><td>4.557123899459839  </td><td>4.557123899459839 </td><td style=\"text-align: right;\"> 1688471028</td><td>1                   </td><td>0bb9589b  </td></tr>\n",
       "<tr><td>FSR_Trainable_142de23b</td><td>2023-07-04_20-44-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>63.99897568372405 </td><td>195730699.13554934    </td><td>172.26.215.93</td><td style=\"text-align: right;\">1280118</td><td>187.46875582098357</td><td>56.30327796936035   </td><td>0.7838723659515381 </td><td>56.30327796936035 </td><td style=\"text-align: right;\"> 1688471066</td><td>100                 </td><td>142de23b  </td></tr>\n",
       "<tr><td>FSR_Trainable_158a9b24</td><td>2023-07-04_20-51-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>78.1401118598211  </td><td>1667884911386210.2    </td><td>172.26.215.93</td><td style=\"text-align: right;\">1287516</td><td>273.071486715629  </td><td>1.3225984573364258  </td><td>1.3225984573364258 </td><td>1.3225984573364258</td><td style=\"text-align: right;\"> 1688471479</td><td>1                   </td><td>158a9b24  </td></tr>\n",
       "<tr><td>FSR_Trainable_186e250c</td><td>2023-07-04_20-48-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td>4                         </td><td>62.14826543167562 </td><td>200944916.6501046     </td><td>172.26.215.93</td><td style=\"text-align: right;\">1285525</td><td>198.56037283933927</td><td>3.327568769454956   </td><td>0.7832024097442627 </td><td>3.327568769454956 </td><td style=\"text-align: right;\"> 1688471294</td><td>4                   </td><td>186e250c  </td></tr>\n",
       "<tr><td>FSR_Trainable_321e144e</td><td>2023-07-04_20-49-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>34.71763334577879 </td><td>58136711.54522443     </td><td>172.26.215.93</td><td style=\"text-align: right;\">1283947</td><td>116.11913454059426</td><td>127.51993680000305  </td><td>1.3507211208343506 </td><td>127.51993680000305</td><td style=\"text-align: right;\"> 1688471353</td><td>100                 </td><td>321e144e  </td></tr>\n",
       "<tr><td>FSR_Trainable_35e41ca1</td><td>2023-07-04_20-47-11</td><td>      </td><td>DESKTOP-0P789CI</td><td>                          </td><td>                  </td><td>                      </td><td>172.26.215.93</td><td style=\"text-align: right;\">1284386</td><td>                  </td><td>                    </td><td>                   </td><td>                  </td><td style=\"text-align: right;\"> 1688471231</td><td>                    </td><td>35e41ca1  </td></tr>\n",
       "<tr><td>FSR_Trainable_3b4317a1</td><td>2023-07-04_20-51-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>34.174464503710645</td><td>56849282.711112       </td><td>172.26.215.93</td><td style=\"text-align: right;\">1286781</td><td>115.72935052730104</td><td>56.28534007072449   </td><td>0.41617798805236816</td><td>56.28534007072449 </td><td style=\"text-align: right;\"> 1688471477</td><td>100                 </td><td>3b4317a1  </td></tr>\n",
       "<tr><td>FSR_Trainable_3bccdca1</td><td>2023-07-04_20-44-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>88.58957719991976 </td><td>1.3420464201248728e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">1281003</td><td>265.73623273999453</td><td>34.87538170814514   </td><td>16.770854473114014 </td><td>34.87538170814514 </td><td style=\"text-align: right;\"> 1688471071</td><td>2                   </td><td>3bccdca1  </td></tr>\n",
       "<tr><td>FSR_Trainable_3d14e8cc</td><td>2023-07-04_20-45-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>86.22325217411463 </td><td>238614097.20951423    </td><td>172.26.215.93</td><td style=\"text-align: right;\">1282285</td><td>260.90629425253775</td><td>1.820894718170166   </td><td>0.702256441116333  </td><td>1.820894718170166 </td><td style=\"text-align: right;\"> 1688471105</td><td>2                   </td><td>3d14e8cc  </td></tr>\n",
       "<tr><td>FSR_Trainable_3d64b0e0</td><td>2023-07-04_20-45-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>83.09127036675132 </td><td>442367002.22985375    </td><td>172.26.215.93</td><td style=\"text-align: right;\">1282656</td><td>289.2610664800665 </td><td>1.6582276821136475  </td><td>0.8608765602111816 </td><td>1.6582276821136475</td><td style=\"text-align: right;\"> 1688471120</td><td>2                   </td><td>3d64b0e0  </td></tr>\n",
       "<tr><td>FSR_Trainable_492fcbfc</td><td>2023-07-04_20-47-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>79.36132731953761 </td><td>2571339748067080.0    </td><td>172.26.215.93</td><td style=\"text-align: right;\">1284628</td><td>272.7009465221442 </td><td>4.5600481033325195  </td><td>4.5600481033325195 </td><td>4.5600481033325195</td><td style=\"text-align: right;\"> 1688471246</td><td>1                   </td><td>492fcbfc  </td></tr>\n",
       "<tr><td>FSR_Trainable_4a07e61a</td><td>2023-07-04_20-46-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>41.83235407847383 </td><td>107912818.33691563    </td><td>172.26.215.93</td><td style=\"text-align: right;\">1282470</td><td>134.36598540612167</td><td>77.39911651611328   </td><td>1.244603157043457  </td><td>77.39911651611328 </td><td style=\"text-align: right;\"> 1688471200</td><td>100                 </td><td>4a07e61a  </td></tr>\n",
       "<tr><td>FSR_Trainable_4cf3ab8c</td><td>2023-07-04_20-44-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>84.53353858604444 </td><td>8339559348502262.0    </td><td>172.26.215.93</td><td style=\"text-align: right;\">1281492</td><td>277.3331773005103 </td><td>3.3050060272216797  </td><td>3.3050060272216797 </td><td>3.3050060272216797</td><td style=\"text-align: right;\"> 1688471068</td><td>1                   </td><td>4cf3ab8c  </td></tr>\n",
       "<tr><td>FSR_Trainable_52702ada</td><td>2023-07-04_20-45-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>83.08415272434306 </td><td>318442938.4784588     </td><td>172.26.215.93</td><td style=\"text-align: right;\">1283389</td><td>280.3618072924798 </td><td>0.8023619651794434  </td><td>0.8023619651794434 </td><td>0.8023619651794434</td><td style=\"text-align: right;\"> 1688471149</td><td>1                   </td><td>52702ada  </td></tr>\n",
       "<tr><td>FSR_Trainable_584f3bff</td><td>2023-07-04_20-48-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td>8                         </td><td>49.43511397354561 </td><td>151833459.3622395     </td><td>172.26.215.93</td><td style=\"text-align: right;\">1285305</td><td>158.89534724836315</td><td>5.8551318645477295  </td><td>0.7443196773529053 </td><td>5.8551318645477295</td><td style=\"text-align: right;\"> 1688471284</td><td>8                   </td><td>584f3bff  </td></tr>\n",
       "<tr><td>FSR_Trainable_5cde80c1</td><td>2023-07-04_20-50-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>34.642430963347515</td><td>56391723.70661163     </td><td>172.26.215.93</td><td style=\"text-align: right;\">1285992</td><td>116.33357760136768</td><td>130.60733366012573  </td><td>0.8468847274780273 </td><td>130.60733366012573</td><td style=\"text-align: right;\"> 1688471457</td><td>100                 </td><td>5cde80c1  </td></tr>\n",
       "<tr><td>FSR_Trainable_60e26f1d</td><td>2023-07-04_20-52-35</td><td>False </td><td>DESKTOP-0P789CI</td><td>72                        </td><td>34.21478076536187 </td><td>57084121.675554425    </td><td>172.26.215.93</td><td style=\"text-align: right;\">1288246</td><td>115.75206820360893</td><td>42.97810125350952   </td><td>0.5375659465789795 </td><td>42.97810125350952 </td><td style=\"text-align: right;\"> 1688471555</td><td>72                  </td><td>60e26f1d  </td></tr>\n",
       "<tr><td>FSR_Trainable_723c61c3</td><td>2023-07-04_20-46-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>34.738431057274674</td><td>53909970.87936962     </td><td>172.26.215.93</td><td style=\"text-align: right;\">1282814</td><td>116.77248875792823</td><td>61.729610204696655  </td><td>0.7263872623443604 </td><td>61.729610204696655</td><td style=\"text-align: right;\"> 1688471196</td><td>100                 </td><td>723c61c3  </td></tr>\n",
       "<tr><td>FSR_Trainable_7b11dcc4</td><td>2023-07-04_20-43-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>122.82917623677194</td><td>2.0785718887174973e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">1280291</td><td>309.68637856762666</td><td>2.0177721977233887  </td><td>2.0177721977233887 </td><td>2.0177721977233887</td><td style=\"text-align: right;\"> 1688471005</td><td>1                   </td><td>7b11dcc4  </td></tr>\n",
       "<tr><td>FSR_Trainable_82a7261c</td><td>2023-07-04_20-52-36</td><td>False </td><td>DESKTOP-0P789CI</td><td>90                        </td><td>34.1682956658942  </td><td>56684407.4969716      </td><td>172.26.215.93</td><td style=\"text-align: right;\">1287998</td><td>115.71336950597775</td><td>52.13287544250488   </td><td>0.5709850788116455 </td><td>52.13287544250488 </td><td style=\"text-align: right;\"> 1688471556</td><td>90                  </td><td>82a7261c  </td></tr>\n",
       "<tr><td>FSR_Trainable_830e686e</td><td>2023-07-04_20-44-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>85.59838778280368 </td><td>3827304549987100.0    </td><td>172.26.215.93</td><td style=\"text-align: right;\">1281720</td><td>285.3529113275368 </td><td>3.787994146347046   </td><td>3.787994146347046  </td><td>3.787994146347046 </td><td style=\"text-align: right;\"> 1688471081</td><td>1                   </td><td>830e686e  </td></tr>\n",
       "<tr><td>FSR_Trainable_8bfd69f3</td><td>2023-07-04_20-50-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>34.1663054688584  </td><td>56466118.89176852     </td><td>172.26.215.93</td><td style=\"text-align: right;\">1286482</td><td>115.72731877542468</td><td>69.98263764381409   </td><td>0.5225434303283691 </td><td>69.98263764381409 </td><td style=\"text-align: right;\"> 1688471448</td><td>100                 </td><td>8bfd69f3  </td></tr>\n",
       "<tr><td>FSR_Trainable_91d5c340</td><td>2023-07-04_20-46-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>34.08802881189565 </td><td>57477534.85497649     </td><td>172.26.215.93</td><td style=\"text-align: right;\">1283165</td><td>115.550149775422  </td><td>59.309326171875     </td><td>0.49799299240112305</td><td>59.309326171875   </td><td style=\"text-align: right;\"> 1688471203</td><td>100                 </td><td>91d5c340  </td></tr>\n",
       "<tr><td>FSR_Trainable_945308e5</td><td>2023-07-04_20-47-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>78.49525268241634 </td><td>2483544143063949.5    </td><td>172.26.215.93</td><td style=\"text-align: right;\">1284834</td><td>272.568166370633  </td><td>5.074037075042725   </td><td>5.074037075042725  </td><td>5.074037075042725 </td><td style=\"text-align: right;\"> 1688471257</td><td>1                   </td><td>945308e5  </td></tr>\n",
       "<tr><td>FSR_Trainable_a26c4162</td><td>2023-07-04_20-44-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td>64                        </td><td>64.8577280948805  </td><td>185493348.3646991     </td><td>172.26.215.93</td><td style=\"text-align: right;\">1280473</td><td>199.7376838642984 </td><td>51.51094031333923   </td><td>0.588238000869751  </td><td>51.51094031333923 </td><td style=\"text-align: right;\"> 1688471073</td><td>64                  </td><td>a26c4162  </td></tr>\n",
       "<tr><td>FSR_Trainable_a9cd7e4c</td><td>2023-07-04_20-43-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td>4                         </td><td>80.5953972709799  </td><td>8.060119483614216e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">1280045</td><td>269.24041134374414</td><td>8.802748203277588   </td><td>2.3015856742858887 </td><td>8.802748203277588 </td><td style=\"text-align: right;\"> 1688471001</td><td>4                   </td><td>a9cd7e4c  </td></tr>\n",
       "<tr><td>FSR_Trainable_af8c57dd</td><td>2023-07-04_20-49-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>34.10317929034049 </td><td>57334252.63569394     </td><td>172.26.215.93</td><td style=\"text-align: right;\">1285061</td><td>115.56261512290595</td><td>65.25826215744019   </td><td>0.6802351474761963 </td><td>65.25826215744019 </td><td style=\"text-align: right;\"> 1688471340</td><td>100                 </td><td>af8c57dd  </td></tr>\n",
       "<tr><td>FSR_Trainable_b00530f1</td><td>2023-07-04_20-44-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>129.62837145298278</td><td>2.249949495221995e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">1281960</td><td>320.1914162417939 </td><td>2.025010824203491   </td><td>2.025010824203491  </td><td>2.025010824203491 </td><td style=\"text-align: right;\"> 1688471087</td><td>1                   </td><td>b00530f1  </td></tr>\n",
       "<tr><td>FSR_Trainable_b00c6ffe</td><td>2023-07-04_20-50-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td>64                        </td><td>34.707243524258494</td><td>56161401.8616141      </td><td>172.26.215.93</td><td style=\"text-align: right;\">1286282</td><td>116.41797195165725</td><td>87.53545784950256   </td><td>0.953106164932251  </td><td>87.53545784950256 </td><td style=\"text-align: right;\"> 1688471449</td><td>64                  </td><td>b00c6ffe  </td></tr>\n",
       "<tr><td>FSR_Trainable_b1028f5c</td><td>2023-07-04_20-51-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>76.95984169685256 </td><td>3006360318102523.5    </td><td>172.26.215.93</td><td style=\"text-align: right;\">1287748</td><td>261.4467588594753 </td><td>1.3561232089996338  </td><td>1.3561232089996338 </td><td>1.3561232089996338</td><td style=\"text-align: right;\"> 1688471489</td><td>1                   </td><td>b1028f5c  </td></tr>\n",
       "<tr><td>FSR_Trainable_c4c7a955</td><td>2023-07-04_20-51-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td>32                        </td><td>35.38294605837806 </td><td>70377662.71599828     </td><td>172.26.215.93</td><td style=\"text-align: right;\">1287281</td><td>117.49759596891903</td><td>13.709495782852173  </td><td>0.309983491897583  </td><td>13.709495782852173</td><td style=\"text-align: right;\"> 1688471485</td><td>32                  </td><td>c4c7a955  </td></tr>\n",
       "<tr><td>FSR_Trainable_c8ddac7f</td><td>2023-07-04_20-44-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>109.61120215547179</td><td>309682530.89714235    </td><td>172.26.215.93</td><td style=\"text-align: right;\">1281229</td><td>319.6956493519295 </td><td>0.7678759098052979  </td><td>0.7678759098052979 </td><td>0.7678759098052979</td><td style=\"text-align: right;\"> 1688471048</td><td>1                   </td><td>c8ddac7f  </td></tr>\n",
       "<tr><td>FSR_Trainable_d460a84c</td><td>2023-07-04_20-49-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td>64                        </td><td>35.01294709795704 </td><td>55755816.966328956    </td><td>172.26.215.93</td><td style=\"text-align: right;\">1285761</td><td>116.79878149451497</td><td>87.66737246513367   </td><td>1.0437486171722412 </td><td>87.66737246513367 </td><td style=\"text-align: right;\"> 1688471397</td><td>64                  </td><td>d460a84c  </td></tr>\n",
       "<tr><td>FSR_Trainable_da8060d0</td><td>2023-07-04_20-52-35</td><td>False </td><td>DESKTOP-0P789CI</td><td>44                        </td><td>34.21793541525404 </td><td>57233107.82500709     </td><td>172.26.215.93</td><td style=\"text-align: right;\">1288639</td><td>115.73690630820563</td><td>26.425716161727905  </td><td>0.5747475624084473 </td><td>26.425716161727905</td><td style=\"text-align: right;\"> 1688471555</td><td>44                  </td><td>da8060d0  </td></tr>\n",
       "<tr><td>FSR_Trainable_eff46477</td><td>2023-07-04_20-47-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>34.12323370651054 </td><td>57725179.9170471      </td><td>172.26.215.93</td><td style=\"text-align: right;\">1283621</td><td>115.54393031507085</td><td>55.75926995277405   </td><td>0.4329702854156494 </td><td>55.75926995277405 </td><td style=\"text-align: right;\"> 1688471228</td><td>100                 </td><td>eff46477  </td></tr>\n",
       "<tr><td>FSR_Trainable_f408f767</td><td>2023-07-04_20-47-00</td><td>      </td><td>DESKTOP-0P789CI</td><td>                          </td><td>                  </td><td>                      </td><td>172.26.215.93</td><td style=\"text-align: right;\">1284165</td><td>                  </td><td>                    </td><td>                   </td><td>                  </td><td style=\"text-align: right;\"> 1688471220</td><td>                    </td><td>f408f767  </td></tr>\n",
       "<tr><td>FSR_Trainable_f809912e</td><td>2023-07-04_20-52-37</td><td>False </td><td>DESKTOP-0P789CI</td><td>65                        </td><td>34.136415345233075</td><td>56854051.83215881     </td><td>172.26.215.93</td><td style=\"text-align: right;\">1288328</td><td>115.722863606939  </td><td>39.485928773880005  </td><td>0.5781309604644775 </td><td>39.485928773880005</td><td style=\"text-align: right;\"> 1688471557</td><td>65                  </td><td>f809912e  </td></tr>\n",
       "<tr><td>FSR_Trainable_ff0030d7</td><td>2023-07-04_20-50-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>96.12064947863512 </td><td>289846076.58978176    </td><td>172.26.215.93</td><td style=\"text-align: right;\">1287080</td><td>283.23737885423014</td><td>0.6893796920776367  </td><td>0.6893796920776367 </td><td>0.6893796920776367</td><td style=\"text-align: right;\"> 1688471459</td><td>1                   </td><td>ff0030d7  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_a9cd7e4c_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-04_20-43-03/wandb/run-20230704_204314-a9cd7e4c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: Syncing run FSR_Trainable_a9cd7e4c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9cd7e4c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_142de23b_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-04_20-43-08/wandb/run-20230704_204322-142de23b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: Syncing run FSR_Trainable_142de23b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/142de23b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb:                      mae ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb:                     mape ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb:                     rmse █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb:       time_since_restore ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb:         time_this_iter_s █▁▅▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb:             time_total_s ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb:                timestamp ▁▄▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb:                      mae 80.5954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb:                     mape 8.060119483614216e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb:                     rmse 269.24041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb:       time_since_restore 8.80275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb:         time_this_iter_s 2.30159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb:             time_total_s 8.80275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb:                timestamp 1688471001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: 🚀 View run FSR_Trainable_a9cd7e4c at: https://wandb.ai/seokjin/FSR-prediction/runs/a9cd7e4c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280117)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204314-a9cd7e4c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_7b11dcc4_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-04_20-43-15/wandb/run-20230704_204329-7b11dcc4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: Syncing run FSR_Trainable_7b11dcc4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7b11dcc4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb:                      mae 122.82918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb:                     mape 2.0785718887174973e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb:                     rmse 309.68638\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb:       time_since_restore 2.01777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb:         time_this_iter_s 2.01777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb:             time_total_s 2.01777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb:                timestamp 1688471005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: 🚀 View run FSR_Trainable_7b11dcc4 at: https://wandb.ai/seokjin/FSR-prediction/runs/7b11dcc4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280472)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204329-7b11dcc4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_a26c4162_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-04_20-43-23/wandb/run-20230704_204337-a26c4162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Syncing run FSR_Trainable_a26c4162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a26c4162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_0bb9589b_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-04_20-43-31/wandb/run-20230704_204351-0bb9589b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: Syncing run FSR_Trainable_0bb9589b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0bb9589b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb:                      mae 103.33254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb:                     mape 1.2011708262964786e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb:                     rmse 303.54972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb:       time_since_restore 4.55712\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb:         time_this_iter_s 4.55712\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb:             time_total_s 4.55712\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb:                timestamp 1688471028\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: 🚀 View run FSR_Trainable_0bb9589b at: https://wandb.ai/seokjin/FSR-prediction/runs/0bb9589b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280886)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204351-0bb9589b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_3bccdca1_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-04_20-43-43/wandb/run-20230704_204402-3bccdca1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: Syncing run FSR_Trainable_3bccdca1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3bccdca1\n",
      "2023-07-04 20:44:11,378\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.784 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:44:11,384\tWARNING util.py:315 -- The `process_trial_result` operation took 2.790 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:44:11,386\tWARNING util.py:315 -- Processing trial results took 2.792 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:44:11,389\tWARNING util.py:315 -- The `process_trial_result` operation took 2.795 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-04 20:44:14,630\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.890 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:44:14,631\tWARNING util.py:315 -- The `process_trial_result` operation took 1.893 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:44:14,633\tWARNING util.py:315 -- Processing trial results took 1.895 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:44:14,635\tWARNING util.py:315 -- The `process_trial_result` operation took 1.897 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_c8ddac7f_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-04_20-43-54/wandb/run-20230704_204414-c8ddac7f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: Syncing run FSR_Trainable_c8ddac7f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c8ddac7f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb:                      mae 109.6112\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb:                     mape 309682530.89714\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb:                     rmse 319.69565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb:       time_since_restore 0.76788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb:         time_this_iter_s 0.76788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb:             time_total_s 0.76788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb:                timestamp 1688471048\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: 🚀 View run FSR_Trainable_c8ddac7f at: https://wandb.ai/seokjin/FSR-prediction/runs/c8ddac7f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281330)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204414-c8ddac7f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-04 20:44:30,458\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.405 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:44:30,464\tWARNING util.py:315 -- The `process_trial_result` operation took 2.413 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:44:30,465\tWARNING util.py:315 -- Processing trial results took 2.415 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:44:30,467\tWARNING util.py:315 -- The `process_trial_result` operation took 2.416 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb:                      mae █▇▇▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb:                     mape █▇▇▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb:                     rmse █▇▇▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb:         time_this_iter_s ▅▃▃▁▂▅▂▂▁▁▅▃▇▅▇▂▂▅▅▄▃▂▃▆▆▅▅▅█▄▄▅▄▄▅▃▄▄▄▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb:                      mae 63.99898\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb:                     mape 195730699.13555\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb:                     rmse 187.46876\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb:       time_since_restore 56.30328\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb:         time_this_iter_s 0.78387\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb:             time_total_s 56.30328\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb:                timestamp 1688471066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: 🚀 View run FSR_Trainable_142de23b at: https://wandb.ai/seokjin/FSR-prediction/runs/142de23b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280290)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204322-142de23b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_4cf3ab8c_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-04_20-44-07/wandb/run-20230704_204432-4cf3ab8c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb: Syncing run FSR_Trainable_4cf3ab8c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4cf3ab8c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281109)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Waiting for W&B process to finish... (success).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb:                      mae 84.53354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb:                     mape 8339559348502262.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb:                     rmse 277.33318\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb:       time_since_restore 3.30501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb:         time_this_iter_s 3.30501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb:             time_total_s 3.30501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb:                timestamp 1688471068\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb: 🚀 View run FSR_Trainable_4cf3ab8c at: https://wandb.ai/seokjin/FSR-prediction/runs/4cf3ab8c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281576)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204432-4cf3ab8c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb:                      mae ▅▆▃▃▃▂▂▁▄▅▃▂▇▃▅▅▅▅▂▂▄▄▆█▅▇▇▃███▅▆▆▅▅▆▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb:                     mape ▅▇▃▃▄▂▂▁▄▅▃▂▆▃▆▅▆▇▃▂▃▄▄▇▅▇█▅▇██▆▆▆▆▆▆▆▆▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb:                     rmse ▅▇▃▂▂▂▂▁▄▅▂▂▇▃▅▃▅▅▁▁▃▄▅▇▆▇▇▂▇█▇▅▆▆▅▄▅▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb:         time_this_iter_s ▆▅▄▇▇▇▃▁▁▇▆▅▄▃▂▄█▆▆▄▆▅█▄▅▅▆▅▄▆▄▄▅▄▄▇▆▄▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "2023-07-04 20:44:43,198\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.903 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:44:43,204\tWARNING util.py:315 -- The `process_trial_result` operation took 1.909 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:44:43,206\tWARNING util.py:315 -- Processing trial results took 1.911 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:44:43,207\tWARNING util.py:315 -- The `process_trial_result` operation took 1.913 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1280660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_830e686e_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-04_20-44-24/wandb/run-20230704_204443-830e686e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb: Syncing run FSR_Trainable_830e686e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/830e686e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb:                      mae 85.59839\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb:                     mape 3827304549987100.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb:                     rmse 285.35291\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb:       time_since_restore 3.78799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb:         time_this_iter_s 3.78799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb:             time_total_s 3.78799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb:                timestamp 1688471081\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb: 🚀 View run FSR_Trainable_830e686e at: https://wandb.ai/seokjin/FSR-prediction/runs/830e686e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204443-830e686e/logs\n",
      "2023-07-04 20:44:50,119\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.224 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:44:50,127\tWARNING util.py:315 -- The `process_trial_result` operation took 2.232 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:44:50,129\tWARNING util.py:315 -- Processing trial results took 2.234 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:44:50,131\tWARNING util.py:315 -- The `process_trial_result` operation took 2.236 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1281842)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_b00530f1_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-44-37/wandb/run-20230704_204452-b00530f1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb: Syncing run FSR_Trainable_b00530f1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b00530f1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb:                      mae 129.62837\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb:                     mape 2.249949495221995e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb:                     rmse 320.19142\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb:       time_since_restore 2.02501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb:         time_this_iter_s 2.02501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb:             time_total_s 2.02501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb:                timestamp 1688471087\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb: 🚀 View run FSR_Trainable_b00530f1 at: https://wandb.ai/seokjin/FSR-prediction/runs/b00530f1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204452-b00530f1/logs\n",
      "2023-07-04 20:44:58,621\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.796 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:44:58,625\tWARNING util.py:315 -- The `process_trial_result` operation took 1.801 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:44:58,627\tWARNING util.py:315 -- Processing trial results took 1.802 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:44:58,629\tWARNING util.py:315 -- The `process_trial_result` operation took 1.804 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282062)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_080964f7_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-44-45/wandb/run-20230704_204500-080964f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb: Syncing run FSR_Trainable_080964f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/080964f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-04 20:45:04,862\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.914 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:45:04,866\tWARNING util.py:315 -- The `process_trial_result` operation took 1.919 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:45:04,867\tWARNING util.py:315 -- Processing trial results took 1.920 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:45:04,868\tWARNING util.py:315 -- The `process_trial_result` operation took 1.921 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb:                      mae 87.88925\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb:                     mape 258477813.18997\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb:                     rmse 253.11162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb:       time_since_restore 3.70954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb:         time_this_iter_s 1.52545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb:             time_total_s 3.70954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb:                timestamp 1688471100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb: 🚀 View run FSR_Trainable_080964f7 at: https://wandb.ai/seokjin/FSR-prediction/runs/080964f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204500-080964f7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282284)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_3d14e8cc_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-44-54/wandb/run-20230704_204507-3d14e8cc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: Syncing run FSR_Trainable_3d14e8cc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3d14e8cc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-04 20:45:11,873\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.964 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:45:11,878\tWARNING util.py:315 -- The `process_trial_result` operation took 1.971 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:45:11,880\tWARNING util.py:315 -- Processing trial results took 1.972 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:45:11,882\tWARNING util.py:315 -- The `process_trial_result` operation took 1.975 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282655)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282655)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282655)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_4a07e61a_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-45-01/wandb/run-20230704_204517-4a07e61a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282655)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282655)\u001b[0m wandb: Syncing run FSR_Trainable_4a07e61a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282655)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282655)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4a07e61a\n",
      "2023-07-04 20:45:19,273\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.340 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:45:19,277\tWARNING util.py:315 -- The `process_trial_result` operation took 2.346 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:45:19,279\tWARNING util.py:315 -- Processing trial results took 2.347 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:45:19,282\tWARNING util.py:315 -- The `process_trial_result` operation took 2.350 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb:                      mae 86.22325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb:                     mape 238614097.20951\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb:                     rmse 260.90629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb:       time_since_restore 1.82089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb:         time_this_iter_s 0.70226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb:             time_total_s 1.82089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb:                timestamp 1688471105\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: 🚀 View run FSR_Trainable_3d14e8cc at: https://wandb.ai/seokjin/FSR-prediction/runs/3d14e8cc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282469)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204507-3d14e8cc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-04 20:45:27,035\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.181 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:45:27,037\tWARNING util.py:315 -- The `process_trial_result` operation took 2.183 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:45:27,039\tWARNING util.py:315 -- Processing trial results took 2.185 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:45:27,041\tWARNING util.py:315 -- The `process_trial_result` operation took 2.188 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb:                      mae 83.09127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb:                     mape 442367002.22985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb:                     rmse 289.26107\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb:       time_since_restore 1.65823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb:         time_this_iter_s 0.86088\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb:             time_total_s 1.65823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb:                timestamp 1688471120\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: 🚀 View run FSR_Trainable_3d64b0e0 at: https://wandb.ai/seokjin/FSR-prediction/runs/3d64b0e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282812)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204522-3d64b0e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_723c61c3_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-45-16/wandb/run-20230704_204529-723c61c3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: Syncing run FSR_Trainable_723c61c3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/723c61c3\n",
      "2023-07-04 20:45:38,668\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.502 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:45:38,673\tWARNING util.py:315 -- The `process_trial_result` operation took 2.509 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:45:38,676\tWARNING util.py:315 -- Processing trial results took 2.511 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:45:38,678\tWARNING util.py:315 -- The `process_trial_result` operation took 2.514 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_91d5c340_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-45-24/wandb/run-20230704_204541-91d5c340\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: Syncing run FSR_Trainable_91d5c340\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/91d5c340\n",
      "2023-07-04 20:45:51,521\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.262 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:45:51,525\tWARNING util.py:315 -- The `process_trial_result` operation took 2.267 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:45:51,527\tWARNING util.py:315 -- Processing trial results took 2.269 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:45:51,529\tWARNING util.py:315 -- The `process_trial_result` operation took 2.272 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_52702ada_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-45-35/wandb/run-20230704_204554-52702ada\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: Syncing run FSR_Trainable_52702ada\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/52702ada\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb:                      mae 83.08415\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb:                     mape 318442938.47846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb:                     rmse 280.36181\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb:       time_since_restore 0.80236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb:         time_this_iter_s 0.80236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb:             time_total_s 0.80236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb:                timestamp 1688471149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: 🚀 View run FSR_Trainable_52702ada at: https://wandb.ai/seokjin/FSR-prediction/runs/52702ada\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283482)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204554-52702ada/logs\n",
      "2023-07-04 20:46:06,688\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.210 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:46:06,692\tWARNING util.py:315 -- The `process_trial_result` operation took 2.215 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:46:06,697\tWARNING util.py:315 -- Processing trial results took 2.220 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:46:06,701\tWARNING util.py:315 -- The `process_trial_result` operation took 2.224 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_eff46477_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-45-48/wandb/run-20230704_204610-eff46477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: Syncing run FSR_Trainable_eff46477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/eff46477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb:                      mae █▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb:                     mape █▆▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb:                     rmse █▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb:         time_this_iter_s ▆▂▂▂▁▂▃█▇▄▃▃▂▁▄▃▆▃▆▄▆▃▄▄▃▅▄▂▃▃▃▂▄▆▃▄▅▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb:                      mae 34.73843\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb:                     mape 53909970.87937\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb:                     rmse 116.77249\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb:       time_since_restore 61.72961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb:         time_this_iter_s 0.72639\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb:             time_total_s 61.72961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb:                timestamp 1688471196\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: 🚀 View run FSR_Trainable_723c61c3 at: https://wandb.ai/seokjin/FSR-prediction/runs/723c61c3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283028)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204529-723c61c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282655)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204529-723c61c3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282655)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282655)\u001b[0m wandb:                      mae ██▆▃▂▂▂▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282655)\u001b[0m wandb:                     mape ▇█▆▂▁▁▂▂▂▃▃▃▃▂▂▃▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282655)\u001b[0m wandb:                     rmse ██▆▃▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282655)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282655)\u001b[0m wandb:         time_this_iter_s █▂▁▁▄▂▁▃▃▃▂▂▄▅▄▃▃▃▅▃▄▅▃▃▄▄▃▄▃▃▃▃▃▅▃▅▅▅▆▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282655)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282655)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1282655)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204517-4a07e61a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb:                      mae █▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb:                     mape █▆▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb:                     rmse █▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb:         time_this_iter_s █▄▅▂▁▁▁▁▃▃▃▄▂▁▃▂▂█▂▂▃▁▂▂▂▂▁▁▄▅▂▄▃▃▇▆▇▆▅▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: iterations_since_restore 100\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb:                      mae 34.08803\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb:                     mape 57477534.85498\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb:                     rmse 115.55015\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb:       time_since_restore 59.30933\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb:         time_this_iter_s 0.49799\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb:             time_total_s 59.30933\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb:                timestamp 1688471203\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb:       training_iteration 100\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: 🚀 View run FSR_Trainable_91d5c340 at: https://wandb.ai/seokjin/FSR-prediction/runs/91d5c340\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283258)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204541-91d5c340/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2023-07-04 20:46:51,467\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.536 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:46:51,473\tWARNING util.py:315 -- The `process_trial_result` operation took 2.542 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:46:51,474\tWARNING util.py:315 -- Processing trial results took 2.543 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:46:51,478\tWARNING util.py:315 -- The `process_trial_result` operation took 2.547 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_321e144e_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-46-03/wandb/run-20230704_204654-321e144e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: Syncing run FSR_Trainable_321e144e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/321e144e\n",
      "2023-07-04 20:47:00,857\tERROR tune_controller.py:873 -- Trial task failed for trial FSR_Trainable_f408f767\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 2540, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::ResourceTrainable.train()\u001b[39m (pid=1284165, ip=172.26.215.93, actor_id=ac7c70f63dd3fbb3f916341901000000, repr=<ray.tune.trainable.util.FSR_Trainable object at 0x7fb9c44f4e50>)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 386, in train\n",
      "    result = self.step()\n",
      "  File \"/home/seokj/workspace/FSR-prediction/fsr_trainable.py\", line 72, in step\n",
      "    mae.append(sklearn.metrics.mean_absolute_error(y, pred))\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 196, in mean_absolute_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284253)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284253)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284253)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_f408f767_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-46-47/wandb/run-20230704_204706-f408f767\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284253)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284253)\u001b[0m wandb: Syncing run FSR_Trainable_f408f767\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284253)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284253)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f408f767\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284253)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284253)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284253)\u001b[0m wandb: \\ 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284253)\u001b[0m wandb: | 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284253)\u001b[0m wandb: 🚀 View run FSR_Trainable_f408f767 at: https://wandb.ai/seokjin/FSR-prediction/runs/f408f767\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284253)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284253)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204706-f408f767/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb:                      mae █▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb:                     mape █▆▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb:                     rmse █▅▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb:         time_this_iter_s ▆▅▄▃▄▃▃▃▃▃▃▄▄▄▃▃▄▆▅▅█▆▆▂▁▃▁▁▄▄▃▂▂▃▂▂▅▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb:                      mae 34.12323\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb:                     mape 57725179.91705\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb:                     rmse 115.54393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb:       time_since_restore 55.75927\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb:         time_this_iter_s 0.43297\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb:             time_total_s 55.75927\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb:                timestamp 1688471228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: \n",
      "2023-07-04 20:47:12,639\tERROR tune_controller.py:873 -- Trial task failed for trial FSR_Trainable_35e41ca1\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 2540, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::ResourceTrainable.train()\u001b[39m (pid=1284386, ip=172.26.215.93, actor_id=07ebda7ef27a0de90529877201000000, repr=<ray.tune.trainable.util.FSR_Trainable object at 0x7fddb094ceb0>)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 386, in train\n",
      "    result = self.step()\n",
      "  File \"/home/seokj/workspace/FSR-prediction/fsr_trainable.py\", line 72, in step\n",
      "    mae.append(sklearn.metrics.mean_absolute_error(y, pred))\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 196, in mean_absolute_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284486)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1283722)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284486)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284486)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_35e41ca1_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-47-00/wandb/run-20230704_204717-35e41ca1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284486)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284486)\u001b[0m wandb: Syncing run FSR_Trainable_35e41ca1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284486)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284486)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/35e41ca1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284486)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284486)\u001b[0m wandb: - 0.004 MB of 0.007 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284486)\u001b[0m wandb: \\ 0.004 MB of 0.007 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284486)\u001b[0m wandb: 🚀 View run FSR_Trainable_35e41ca1 at: https://wandb.ai/seokjin/FSR-prediction/runs/35e41ca1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284486)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284486)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204717-35e41ca1/logs\n",
      "2023-07-04 20:47:28,761\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.769 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:47:28,766\tWARNING util.py:315 -- The `process_trial_result` operation took 1.775 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:47:28,769\tWARNING util.py:315 -- Processing trial results took 1.777 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:47:28,771\tWARNING util.py:315 -- The `process_trial_result` operation took 1.779 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_492fcbfc_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-47-12/wandb/run-20230704_204728-492fcbfc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: Syncing run FSR_Trainable_492fcbfc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/492fcbfc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb:                      mae 79.36133\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb:                     mape 2571339748067080.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb:                     rmse 272.70095\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb:       time_since_restore 4.56005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb:         time_this_iter_s 4.56005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb:             time_total_s 4.56005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb:                timestamp 1688471246\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: 🚀 View run FSR_Trainable_492fcbfc at: https://wandb.ai/seokjin/FSR-prediction/runs/492fcbfc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284715)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204728-492fcbfc/logs\n",
      "2023-07-04 20:47:39,122\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.855 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:47:39,127\tWARNING util.py:315 -- The `process_trial_result` operation took 1.861 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:47:39,129\tWARNING util.py:315 -- Processing trial results took 1.863 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:47:39,131\tWARNING util.py:315 -- The `process_trial_result` operation took 1.865 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_945308e5_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-47-22/wandb/run-20230704_204739-945308e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: Syncing run FSR_Trainable_945308e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/945308e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb:                      mae 78.49525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb:                     mape 2483544143063949.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb:                     rmse 272.56817\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb:       time_since_restore 5.07404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb:         time_this_iter_s 5.07404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb:             time_total_s 5.07404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb:                timestamp 1688471257\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: 🚀 View run FSR_Trainable_945308e5 at: https://wandb.ai/seokjin/FSR-prediction/runs/945308e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284942)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204739-945308e5/logs\n",
      "2023-07-04 20:47:46,579\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.872 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:47:46,582\tWARNING util.py:315 -- The `process_trial_result` operation took 2.876 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:47:46,584\tWARNING util.py:315 -- Processing trial results took 2.878 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:47:46,587\tWARNING util.py:315 -- The `process_trial_result` operation took 2.881 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_af8c57dd_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-47-32/wandb/run-20230704_204749-af8c57dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: Syncing run FSR_Trainable_af8c57dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/af8c57dd\n",
      "2023-07-04 20:47:58,908\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.333 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:47:58,914\tWARNING util.py:315 -- The `process_trial_result` operation took 2.340 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:47:58,916\tWARNING util.py:315 -- Processing trial results took 2.342 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:47:58,918\tWARNING util.py:315 -- The `process_trial_result` operation took 2.344 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_584f3bff_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-47-43/wandb/run-20230704_204802-584f3bff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: Syncing run FSR_Trainable_584f3bff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/584f3bff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb:                      mae █▆▄▃▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb:                     mape █▆▅▄▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb:                     rmse █▆▅▄▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb:         time_this_iter_s ▇█▆▂▁▂▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb:                timestamp ▁▄▅▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb:                      mae 49.43511\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb:                     mape 151833459.36224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb:                     rmse 158.89535\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb:       time_since_restore 5.85513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb:         time_this_iter_s 0.74432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb:             time_total_s 5.85513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb:                timestamp 1688471284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: 🚀 View run FSR_Trainable_584f3bff at: https://wandb.ai/seokjin/FSR-prediction/runs/584f3bff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285395)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204802-584f3bff/logs\n",
      "2023-07-04 20:48:12,084\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.472 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:48:12,086\tWARNING util.py:315 -- The `process_trial_result` operation took 2.475 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:48:12,091\tWARNING util.py:315 -- Processing trial results took 2.480 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:48:12,093\tWARNING util.py:315 -- The `process_trial_result` operation took 2.482 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_186e250c_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-47-55/wandb/run-20230704_204815-186e250c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: Syncing run FSR_Trainable_186e250c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/186e250c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb:                      mae █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb:                     mape █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb:                     rmse █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb:         time_this_iter_s █▄▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb:                      mae 62.14827\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb:                     mape 200944916.6501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb:                     rmse 198.56037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb:       time_since_restore 3.32757\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb:         time_this_iter_s 0.7832\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb:             time_total_s 3.32757\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb:                timestamp 1688471294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: 🚀 View run FSR_Trainable_186e250c at: https://wandb.ai/seokjin/FSR-prediction/runs/186e250c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285629)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204815-186e250c/logs\n",
      "2023-07-04 20:48:25,032\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.711 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:48:25,034\tWARNING util.py:315 -- The `process_trial_result` operation took 1.713 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:48:25,035\tWARNING util.py:315 -- Processing trial results took 1.714 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:48:25,037\tWARNING util.py:315 -- The `process_trial_result` operation took 1.717 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_d460a84c_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-48-08/wandb/run-20230704_204828-d460a84c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: Syncing run FSR_Trainable_d460a84c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d460a84c\n",
      "2023-07-04 20:48:39,830\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.927 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:48:39,834\tWARNING util.py:315 -- The `process_trial_result` operation took 1.932 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:48:39,836\tWARNING util.py:315 -- Processing trial results took 1.934 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:48:39,837\tWARNING util.py:315 -- The `process_trial_result` operation took 1.935 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_5cde80c1_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-48-21/wandb/run-20230704_204843-5cde80c1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb: Syncing run FSR_Trainable_5cde80c1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cde80c1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb:                      mae █▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb:                     mape █▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb:                     rmse █▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb:         time_this_iter_s ▄▄▄▂▂▂▁▄▅▄▄▃▂▂▅▅▂▂▂▂▂▅▄▄▄▄▆▆▅▄▅█▄▄▃▃▄▃▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb:                      mae 34.10318\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb:                     mape 57334252.63569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb:                     rmse 115.56262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb:       time_since_restore 65.25826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb:         time_this_iter_s 0.68024\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb:             time_total_s 65.25826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb:                timestamp 1688471340\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: 🚀 View run FSR_Trainable_af8c57dd at: https://wandb.ai/seokjin/FSR-prediction/runs/af8c57dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285166)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204749-af8c57dd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-04 20:49:17,426\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.055 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:49:17,428\tWARNING util.py:315 -- The `process_trial_result` operation took 2.058 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:49:17,434\tWARNING util.py:315 -- Processing trial results took 2.063 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:49:17,435\tWARNING util.py:315 -- The `process_trial_result` operation took 2.065 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb:                      mae █▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb:                     mape █▁▃▄▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb:                     rmse █▄▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb:         time_this_iter_s ▄▃▂▂▄▂▂▄▁▁▃▄▂█▃▁▄▃▃▃▆▄▂▄▅▃▄▆▄▅▅▆▇▅▅▄▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb:                      mae 34.71763\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb:                     mape 58136711.54522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb:                     rmse 116.11913\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb:       time_since_restore 127.51994\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb:         time_this_iter_s 1.35072\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb:             time_total_s 127.51994\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb:                timestamp 1688471353\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: 🚀 View run FSR_Trainable_321e144e at: https://wandb.ai/seokjin/FSR-prediction/runs/321e144e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1284030)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204654-321e144e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_b00c6ffe_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-48-36/wandb/run-20230704_204921-b00c6ffe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb: Syncing run FSR_Trainable_b00c6ffe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b00c6ffe\n",
      "2023-07-04 20:49:33,739\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.162 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:49:33,744\tWARNING util.py:315 -- The `process_trial_result` operation took 2.168 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:49:33,746\tWARNING util.py:315 -- Processing trial results took 2.170 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:49:33,747\tWARNING util.py:315 -- The `process_trial_result` operation took 2.171 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_8bfd69f3_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-49-13/wandb/run-20230704_204938-8bfd69f3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Syncing run FSR_Trainable_8bfd69f3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8bfd69f3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb:                      mae █▆▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb:                     mape █▆▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb:                     rmse █▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb:         time_this_iter_s ▇▄▃▃▂▅▅▅▅▇▃▃▃▃▃▂▃▃▃▃▂▃▃▅▆▅▄▅▃█▃▂▃▂▂▃▂▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb:                      mae 35.01295\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb:                     mape 55755816.96633\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb:                     rmse 116.79878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb:       time_since_restore 87.66737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb:         time_this_iter_s 1.04375\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb:             time_total_s 87.66737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb:                timestamp 1688471397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: 🚀 View run FSR_Trainable_d460a84c at: https://wandb.ai/seokjin/FSR-prediction/runs/d460a84c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1285861)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204828-d460a84c/logs\n",
      "2023-07-04 20:50:16,506\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.306 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:50:16,508\tWARNING util.py:315 -- The `process_trial_result` operation took 3.310 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:50:16,511\tWARNING util.py:315 -- Processing trial results took 3.312 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:50:16,513\tWARNING util.py:315 -- The `process_trial_result` operation took 3.314 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_3b4317a1_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-49-30/wandb/run-20230704_205021-3b4317a1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: Syncing run FSR_Trainable_3b4317a1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3b4317a1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb:                      mae █▅▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb:                     mape █▆▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb:                     rmse █▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb:         time_this_iter_s ▄▃▆▃▃▂▃▃▂▂▂▂▂▂▂▂▂▁▂▃▂▃▃▃█▃▃▄▂▂▂▂▂▂▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb:                      mae 34.70724\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb:                     mape 56161401.86161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb:                     rmse 116.41797\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb:       time_since_restore 87.53546\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb:         time_this_iter_s 0.95311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb:             time_total_s 87.53546\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb:                timestamp 1688471449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb: 🚀 View run FSR_Trainable_b00c6ffe at: https://wandb.ai/seokjin/FSR-prediction/runs/b00c6ffe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286343)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204921-b00c6ffe/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb:                      mae █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb:                     mape █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb:                     rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb:         time_this_iter_s ▄▄▃▃▃▂▂▂▂▂▂▃▂▁▁▂▄▂▃▄▂▆█▂▃▄▂▂▁▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb: Waiting for W&B process to finish... (success).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-04 20:51:01,754\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.482 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:51:01,761\tWARNING util.py:315 -- The `process_trial_result` operation took 2.490 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:51:01,763\tWARNING util.py:315 -- Processing trial results took 2.492 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:51:01,765\tWARNING util.py:315 -- The `process_trial_result` operation took 2.495 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb:                      mae █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb:                     mape █▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb:                     rmse █▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb:         time_this_iter_s ▅▄▆▃▂▃▃▃▃▃▃▄█▄▃▄▃▂▂▂▃▃▂▄▄▃█▃▆▃▃▃▂▂▂▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb:                      mae 34.64243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb:                     mape 56391723.70661\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb:                     rmse 116.33358\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb:       time_since_restore 130.60733\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb:         time_this_iter_s 0.84688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb:             time_total_s 130.60733\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb:                timestamp 1688471457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb: 🚀 View run FSR_Trainable_5cde80c1 at: https://wandb.ai/seokjin/FSR-prediction/runs/5cde80c1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_204843-5cde80c1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286083)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_ff0030d7_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-50-11/wandb/run-20230704_205104-ff0030d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb: Syncing run FSR_Trainable_ff0030d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ff0030d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb:                      mae 96.12065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb:                     mape 289846076.58978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb:                     rmse 283.23738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb:       time_since_restore 0.68938\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb:         time_this_iter_s 0.68938\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb:             time_total_s 0.68938\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb:                timestamp 1688471459\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb: 🚀 View run FSR_Trainable_ff0030d7 at: https://wandb.ai/seokjin/FSR-prediction/runs/ff0030d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_205104-ff0030d7/logs\n",
      "2023-07-04 20:51:10,970\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.068 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:51:10,974\tWARNING util.py:315 -- The `process_trial_result` operation took 2.072 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:51:10,975\tWARNING util.py:315 -- Processing trial results took 2.074 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:51:10,977\tWARNING util.py:315 -- The `process_trial_result` operation took 2.076 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287151)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_c4c7a955_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-50-58/wandb/run-20230704_205113-c4c7a955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: Syncing run FSR_Trainable_c4c7a955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4c7a955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb:                      mae █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb:                     mape █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb:                     rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb:         time_this_iter_s █▄▂▄▆▂▂▂▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▁▁▁▁▁▁▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb:                timestamp ▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb:                      mae 34.17446\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb:                     mape 56849282.71111\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb:                     rmse 115.72935\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb:       time_since_restore 56.28534\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb:         time_this_iter_s 0.41618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb:             time_total_s 56.28534\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb:                timestamp 1688471477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: 🚀 View run FSR_Trainable_3b4317a1 at: https://wandb.ai/seokjin/FSR-prediction/runs/3b4317a1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1286846)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_205021-3b4317a1/logs\n",
      "2023-07-04 20:51:21,470\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.681 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:51:21,476\tWARNING util.py:315 -- The `process_trial_result` operation took 1.688 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:51:21,478\tWARNING util.py:315 -- Processing trial results took 1.690 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:51:21,480\tWARNING util.py:315 -- The `process_trial_result` operation took 1.692 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_158a9b24_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-51-08/wandb/run-20230704_205124-158a9b24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: Syncing run FSR_Trainable_158a9b24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/158a9b24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb:                      mae 78.14011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb:                     mape 1667884911386210.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb:                     rmse 273.07149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb:       time_since_restore 1.3226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb:         time_this_iter_s 1.3226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb:             time_total_s 1.3226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb:                timestamp 1688471479\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: 🚀 View run FSR_Trainable_158a9b24 at: https://wandb.ai/seokjin/FSR-prediction/runs/158a9b24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287614)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_205124-158a9b24/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb:                      mae █▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb:                     mape █▆▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb:                     rmse █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb:         time_this_iter_s ▇▄▃▄▅▄▄▃▄▃▃▃▃▅▄▁▁▁▆█▃▃▄▂▃▃▁▂▂▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb:                timestamp ▁▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇██████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_205113-c4c7a955/logs\n",
      "2023-07-04 20:51:30,838\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.731 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:51:30,843\tWARNING util.py:315 -- The `process_trial_result` operation took 1.736 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:51:30,845\tWARNING util.py:315 -- Processing trial results took 1.738 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:51:30,847\tWARNING util.py:315 -- The `process_trial_result` operation took 1.740 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_b1028f5c_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-51-18/wandb/run-20230704_205132-b1028f5c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb: Syncing run FSR_Trainable_b1028f5c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b1028f5c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287384)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb:                      mae 76.95984\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb:                     mape 3006360318102523.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb:                     rmse 261.44676\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb:       time_since_restore 1.35612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb:         time_this_iter_s 1.35612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb:             time_total_s 1.35612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb:                timestamp 1688471489\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb: 🚀 View run FSR_Trainable_b1028f5c at: https://wandb.ai/seokjin/FSR-prediction/runs/b1028f5c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb: Find logs at: ./wandb/run-20230704_205132-b1028f5c/logs\n",
      "2023-07-04 20:51:37,921\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.820 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:51:37,924\tWARNING util.py:315 -- The `process_trial_result` operation took 1.823 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:51:37,928\tWARNING util.py:315 -- Processing trial results took 1.827 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:51:37,932\tWARNING util.py:315 -- The `process_trial_result` operation took 1.831 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288107)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288107)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288107)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_82a7261c_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-51-27/wandb/run-20230704_205140-82a7261c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288107)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288107)\u001b[0m wandb: Syncing run FSR_Trainable_82a7261c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288107)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288107)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/82a7261c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1287876)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-04 20:51:47,353\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.302 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:51:47,358\tWARNING util.py:315 -- The `process_trial_result` operation took 2.307 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:51:47,360\tWARNING util.py:315 -- Processing trial results took 2.309 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:51:47,362\tWARNING util.py:315 -- The `process_trial_result` operation took 2.311 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288327)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288327)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288327)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_60e26f1d_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-51-35/wandb/run-20230704_205150-60e26f1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288327)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288327)\u001b[0m wandb: Syncing run FSR_Trainable_60e26f1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288327)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288327)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/60e26f1d\n",
      "2023-07-04 20:51:55,887\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.165 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:51:55,889\tWARNING util.py:315 -- The `process_trial_result` operation took 2.167 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:51:55,891\tWARNING util.py:315 -- Processing trial results took 2.169 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:51:55,893\tWARNING util.py:315 -- The `process_trial_result` operation took 2.172 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288508)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288508)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288508)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_f809912e_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-51-44/wandb/run-20230704_205159-f809912e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288508)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288508)\u001b[0m wandb: Syncing run FSR_Trainable_f809912e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288508)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288508)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f809912e\n",
      "2023-07-04 20:52:08,689\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.032 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:52:08,692\tWARNING util.py:315 -- The `process_trial_result` operation took 2.036 s, which may be a performance bottleneck.\n",
      "2023-07-04 20:52:08,695\tWARNING util.py:315 -- Processing trial results took 2.039 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-04 20:52:08,696\tWARNING util.py:315 -- The `process_trial_result` operation took 2.040 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288734)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288734)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288734)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-04_20-42-59/FSR_Trainable_da8060d0_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-04_20-51-52/wandb/run-20230704_205211-da8060d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288734)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288734)\u001b[0m wandb: Syncing run FSR_Trainable_da8060d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288734)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1288734)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/da8060d0\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=1000,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
