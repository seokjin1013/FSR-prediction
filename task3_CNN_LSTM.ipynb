{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task3\n",
    "\n",
    "Index_X = FSR_for_coord\n",
    "\n",
    "Index_y = x_coord, y_coord\n",
    "\n",
    "Data = Splited by Time\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-07-19_03-37-48/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-07-19_03-37-48\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "0.5428"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.CNN_LSTM'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    imputer = trial.suggest_categorical('imputer', ['sklearn.impute.SimpleImputer'])\n",
    "    if imputer == 'sklearn.impute.SimpleImputer':\n",
    "        trial.suggest_categorical('imputer_args/strategy', [\n",
    "            'mean',\n",
    "            'median',\n",
    "        ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': 'FSR_for_coord',\n",
    "        'index_y': ['x_coord', 'y_coord'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_time'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-19 03:37:48,215] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 03:37:50,467\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "2023-07-19 03:37:53,015\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-19 04:33:46</td></tr>\n",
       "<tr><td>Running for: </td><td>00:55:53.41        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.9/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=100<br>Bracket: Iter 64.000: -0.6131295946306705 | Iter 32.000: -0.6341992304798885 | Iter 16.000: -0.6436834024413107 | Iter 8.000: -0.6548212354968348 | Iter 4.000: -0.6690396754080681 | Iter 2.000: -0.6876400670061709 | Iter 1.000: -0.6984624681674466<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>imputer             </th><th>imputer_args/strateg\n",
       "y       </th><th>index_X      </th><th>index_y             </th><th>model             </th><th style=\"text-align: right;\">    model_args/cnn_hidde\n",
       "n_size</th><th style=\"text-align: right;\">  model_args/cnn_num_l\n",
       "ayer</th><th style=\"text-align: right;\">    model_args/lstm_hidd\n",
       "en_size</th><th style=\"text-align: right;\">  model_args/lstm_num_\n",
       "layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">     mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_5d3e9619</td><td>TERMINATED</td><td>172.26.215.93:397455</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3740</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000147708</td><td>sklearn.preproc_c810</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        92.8851 </td><td style=\"text-align: right;\">0.727399</td><td style=\"text-align: right;\">0.387785</td><td style=\"text-align: right;\">0.10113  </td></tr>\n",
       "<tr><td>FSR_Trainable_5a37feb1</td><td>TERMINATED</td><td>172.26.215.93:397527</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9a40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000311436</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       165.166  </td><td style=\"text-align: right;\">0.697381</td><td style=\"text-align: right;\">0.346834</td><td style=\"text-align: right;\">0.0999267</td></tr>\n",
       "<tr><td>FSR_Trainable_22e5e3fc</td><td>TERMINATED</td><td>172.26.215.93:397684</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6180</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0244534  </td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.92258</td><td style=\"text-align: right;\">0.767972</td><td style=\"text-align: right;\">0.457914</td><td style=\"text-align: right;\">0.111012 </td></tr>\n",
       "<tr><td>FSR_Trainable_b32d5df2</td><td>TERMINATED</td><td>172.26.215.93:397860</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c280</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000874611</td><td>sklearn.preproc_c7b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        15.9915 </td><td style=\"text-align: right;\">0.797122</td><td style=\"text-align: right;\">0.467   </td><td style=\"text-align: right;\">0.119898 </td></tr>\n",
       "<tr><td>FSR_Trainable_a00696f1</td><td>TERMINATED</td><td>172.26.215.93:398177</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3500</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0466261  </td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        17.3507 </td><td style=\"text-align: right;\">0.724108</td><td style=\"text-align: right;\">0.440095</td><td style=\"text-align: right;\">0.112994 </td></tr>\n",
       "<tr><td>FSR_Trainable_498b72ce</td><td>TERMINATED</td><td>172.26.215.93:398421</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5940</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00814335 </td><td>sklearn.preproc_c7b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.55176</td><td style=\"text-align: right;\">2.34301 </td><td style=\"text-align: right;\">2.06331 </td><td style=\"text-align: right;\">0.307365 </td></tr>\n",
       "<tr><td>FSR_Trainable_19714ab0</td><td>TERMINATED</td><td>172.26.215.93:398645</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d540</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00378494 </td><td>sklearn.preproc_c7b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         5.72931</td><td style=\"text-align: right;\">0.744661</td><td style=\"text-align: right;\">0.449026</td><td style=\"text-align: right;\">0.116743 </td></tr>\n",
       "<tr><td>FSR_Trainable_01a407a0</td><td>TERMINATED</td><td>172.26.215.93:398881</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3540</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0744466  </td><td>sklearn.preproc_c810</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       425.579  </td><td style=\"text-align: right;\">0.710463</td><td style=\"text-align: right;\">0.392197</td><td style=\"text-align: right;\">0.107882 </td></tr>\n",
       "<tr><td>FSR_Trainable_1f71a826</td><td>TERMINATED</td><td>172.26.215.93:399106</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3140</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">8</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        3.41472e-05</td><td>sklearn.preproc_c810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        13.5167 </td><td style=\"text-align: right;\">0.725783</td><td style=\"text-align: right;\">0.352572</td><td style=\"text-align: right;\">0.0912364</td></tr>\n",
       "<tr><td>FSR_Trainable_cc28ba53</td><td>TERMINATED</td><td>172.26.215.93:399397</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1a80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000347381</td><td>sklearn.preproc_c810</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       296.685  </td><td style=\"text-align: right;\">0.689728</td><td style=\"text-align: right;\">0.355039</td><td style=\"text-align: right;\">0.10278  </td></tr>\n",
       "<tr><td>FSR_Trainable_2285c2f9</td><td>TERMINATED</td><td>172.26.215.93:399583</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__35c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        4.39172e-05</td><td>sklearn.preproc_c7b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.16505</td><td style=\"text-align: right;\">2.63241 </td><td style=\"text-align: right;\">2.20404 </td><td style=\"text-align: right;\">0.389105 </td></tr>\n",
       "<tr><td>FSR_Trainable_1c20a35b</td><td>TERMINATED</td><td>172.26.215.93:399833</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__cdc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00293805 </td><td>sklearn.preproc_c7b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.76655</td><td style=\"text-align: right;\">0.774017</td><td style=\"text-align: right;\">0.462785</td><td style=\"text-align: right;\">0.118133 </td></tr>\n",
       "<tr><td>FSR_Trainable_f002d9d6</td><td>TERMINATED</td><td>172.26.215.93:400087</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0f80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00082816 </td><td>sklearn.preproc_c810</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       339.103  </td><td style=\"text-align: right;\">0.673771</td><td style=\"text-align: right;\">0.364974</td><td style=\"text-align: right;\">0.103994 </td></tr>\n",
       "<tr><td>FSR_Trainable_286ac475</td><td>TERMINATED</td><td>172.26.215.93:400332</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b800</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0048071  </td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.89294</td><td style=\"text-align: right;\">0.723472</td><td style=\"text-align: right;\">0.41242 </td><td style=\"text-align: right;\">0.109588 </td></tr>\n",
       "<tr><td>FSR_Trainable_836bac6d</td><td>TERMINATED</td><td>172.26.215.93:400563</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b880</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        1.16275e-05</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         9.51376</td><td style=\"text-align: right;\">0.699673</td><td style=\"text-align: right;\">0.3706  </td><td style=\"text-align: right;\">0.110789 </td></tr>\n",
       "<tr><td>FSR_Trainable_79618f04</td><td>TERMINATED</td><td>172.26.215.93:400797</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ef00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000839374</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.9221 </td><td style=\"text-align: right;\">0.701939</td><td style=\"text-align: right;\">0.359561</td><td style=\"text-align: right;\">0.10206  </td></tr>\n",
       "<tr><td>FSR_Trainable_30156eff</td><td>TERMINATED</td><td>172.26.215.93:401026</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1840</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        1.057e-05  </td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         8.10773</td><td style=\"text-align: right;\">0.700494</td><td style=\"text-align: right;\">0.365159</td><td style=\"text-align: right;\">0.109027 </td></tr>\n",
       "<tr><td>FSR_Trainable_191e5a09</td><td>TERMINATED</td><td>172.26.215.93:401261</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4c80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        1.27078e-05</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         7.66508</td><td style=\"text-align: right;\">0.701787</td><td style=\"text-align: right;\">0.371327</td><td style=\"text-align: right;\">0.11029  </td></tr>\n",
       "<tr><td>FSR_Trainable_b47f87c1</td><td>TERMINATED</td><td>172.26.215.93:401495</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5040</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000243666</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       123.703  </td><td style=\"text-align: right;\">0.663622</td><td style=\"text-align: right;\">0.343251</td><td style=\"text-align: right;\">0.0988394</td></tr>\n",
       "<tr><td>FSR_Trainable_d8d12509</td><td>TERMINATED</td><td>172.26.215.93:401810</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3e40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000211895</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       142.08   </td><td style=\"text-align: right;\">0.597032</td><td style=\"text-align: right;\">0.323218</td><td style=\"text-align: right;\">0.0910123</td></tr>\n",
       "<tr><td>FSR_Trainable_86c33214</td><td>TERMINATED</td><td>172.26.215.93:401987</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__18c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000240673</td><td>sklearn.preproc_c810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.80937</td><td style=\"text-align: right;\">0.71471 </td><td style=\"text-align: right;\">0.411662</td><td style=\"text-align: right;\">0.115454 </td></tr>\n",
       "<tr><td>FSR_Trainable_863d9411</td><td>TERMINATED</td><td>172.26.215.93:402243</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5b40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000221209</td><td>sklearn.preproc_c810</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        84.4651 </td><td style=\"text-align: right;\">0.698204</td><td style=\"text-align: right;\">0.341537</td><td style=\"text-align: right;\">0.100926 </td></tr>\n",
       "<tr><td>FSR_Trainable_28425721</td><td>TERMINATED</td><td>172.26.215.93:402523</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f940</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        9.94935e-05</td><td>sklearn.preproc_c810</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       336.193  </td><td style=\"text-align: right;\">0.672012</td><td style=\"text-align: right;\">0.361363</td><td style=\"text-align: right;\">0.104155 </td></tr>\n",
       "<tr><td>FSR_Trainable_22aa24f4</td><td>TERMINATED</td><td>172.26.215.93:402708</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__01c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00159262 </td><td>sklearn.preproc_c810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.89781</td><td style=\"text-align: right;\">0.713109</td><td style=\"text-align: right;\">0.38883 </td><td style=\"text-align: right;\">0.104349 </td></tr>\n",
       "<tr><td>FSR_Trainable_8c53b999</td><td>TERMINATED</td><td>172.26.215.93:402983</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__35c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00168146 </td><td>sklearn.preproc_c810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.5217 </td><td style=\"text-align: right;\">0.709323</td><td style=\"text-align: right;\">0.406784</td><td style=\"text-align: right;\">0.120899 </td></tr>\n",
       "<tr><td>FSR_Trainable_468d66ee</td><td>TERMINATED</td><td>172.26.215.93:403167</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1dc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000518088</td><td>sklearn.preproc_c810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        15.6652 </td><td style=\"text-align: right;\">0.699585</td><td style=\"text-align: right;\">0.36354 </td><td style=\"text-align: right;\">0.104151 </td></tr>\n",
       "<tr><td>FSR_Trainable_2511f06e</td><td>TERMINATED</td><td>172.26.215.93:403387</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0800</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000508279</td><td>sklearn.preproc_c810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         8.89197</td><td style=\"text-align: right;\">0.701145</td><td style=\"text-align: right;\">0.356608</td><td style=\"text-align: right;\">0.101973 </td></tr>\n",
       "<tr><td>FSR_Trainable_8c01a1f0</td><td>TERMINATED</td><td>172.26.215.93:403670</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3ac0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000423569</td><td>sklearn.preproc_c810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.03796</td><td style=\"text-align: right;\">0.707731</td><td style=\"text-align: right;\">0.38751 </td><td style=\"text-align: right;\">0.110844 </td></tr>\n",
       "<tr><td>FSR_Trainable_4451a342</td><td>TERMINATED</td><td>172.26.215.93:403860</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8280</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000133737</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       120.869  </td><td style=\"text-align: right;\">0.614803</td><td style=\"text-align: right;\">0.323856</td><td style=\"text-align: right;\">0.0915023</td></tr>\n",
       "<tr><td>FSR_Trainable_f03a8d9b</td><td>TERMINATED</td><td>172.26.215.93:404101</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7780</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000117111</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       122.409  </td><td style=\"text-align: right;\">0.627957</td><td style=\"text-align: right;\">0.339342</td><td style=\"text-align: right;\">0.0959611</td></tr>\n",
       "<tr><td>FSR_Trainable_9f64d024</td><td>TERMINATED</td><td>172.26.215.93:404316</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f900</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000145389</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       121.67   </td><td style=\"text-align: right;\">0.609048</td><td style=\"text-align: right;\">0.326038</td><td style=\"text-align: right;\">0.0919085</td></tr>\n",
       "<tr><td>FSR_Trainable_ee2bdf4e</td><td>TERMINATED</td><td>172.26.215.93:404664</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f540</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000127709</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       122.909  </td><td style=\"text-align: right;\">0.613109</td><td style=\"text-align: right;\">0.331387</td><td style=\"text-align: right;\">0.0929432</td></tr>\n",
       "<tr><td>FSR_Trainable_f5ff39b5</td><td>TERMINATED</td><td>172.26.215.93:404865</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c380</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000106576</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       102.862  </td><td style=\"text-align: right;\">0.646239</td><td style=\"text-align: right;\">0.350629</td><td style=\"text-align: right;\">0.096778 </td></tr>\n",
       "<tr><td>FSR_Trainable_1740aa9e</td><td>TERMINATED</td><td>172.26.215.93:405088</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__db00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000103149</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.50745</td><td style=\"text-align: right;\">0.710538</td><td style=\"text-align: right;\">0.38506 </td><td style=\"text-align: right;\">0.100481 </td></tr>\n",
       "<tr><td>FSR_Trainable_ffedeb7a</td><td>TERMINATED</td><td>172.26.215.93:405321</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1c00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000116219</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.41982</td><td style=\"text-align: right;\">0.726704</td><td style=\"text-align: right;\">0.416396</td><td style=\"text-align: right;\">0.105935 </td></tr>\n",
       "<tr><td>FSR_Trainable_832961ff</td><td>TERMINATED</td><td>172.26.215.93:405558</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__bdc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        9.78879e-05</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.70163</td><td style=\"text-align: right;\">0.699276</td><td style=\"text-align: right;\">0.361893</td><td style=\"text-align: right;\">0.102216 </td></tr>\n",
       "<tr><td>FSR_Trainable_24c7aee5</td><td>TERMINATED</td><td>172.26.215.93:405799</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f300</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        8.41897e-05</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       119.935  </td><td style=\"text-align: right;\">0.634233</td><td style=\"text-align: right;\">0.337601</td><td style=\"text-align: right;\">0.0972058</td></tr>\n",
       "<tr><td>FSR_Trainable_cfadcfb5</td><td>TERMINATED</td><td>172.26.215.93:406098</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e200</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000166233</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       116.214  </td><td style=\"text-align: right;\">0.61944 </td><td style=\"text-align: right;\">0.32982 </td><td style=\"text-align: right;\">0.0931078</td></tr>\n",
       "<tr><td>FSR_Trainable_d4909022</td><td>TERMINATED</td><td>172.26.215.93:406289</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c280</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000195318</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       196.798  </td><td style=\"text-align: right;\">0.607436</td><td style=\"text-align: right;\">0.321222</td><td style=\"text-align: right;\">0.0896438</td></tr>\n",
       "<tr><td>FSR_Trainable_36edc5db</td><td>TERMINATED</td><td>172.26.215.93:406511</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c7c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000187197</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       201.077  </td><td style=\"text-align: right;\">0.61619 </td><td style=\"text-align: right;\">0.324787</td><td style=\"text-align: right;\">0.0922675</td></tr>\n",
       "<tr><td>FSR_Trainable_e3457318</td><td>TERMINATED</td><td>172.26.215.93:406825</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4b00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000181876</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       159.59   </td><td style=\"text-align: right;\">0.605138</td><td style=\"text-align: right;\">0.317955</td><td style=\"text-align: right;\">0.09101  </td></tr>\n",
       "<tr><td>FSR_Trainable_dfc34fb3</td><td>TERMINATED</td><td>172.26.215.93:407093</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5fc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000176676</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        19.3675 </td><td style=\"text-align: right;\">0.675982</td><td style=\"text-align: right;\">0.352372</td><td style=\"text-align: right;\">0.104247 </td></tr>\n",
       "<tr><td>FSR_Trainable_48e09294</td><td>TERMINATED</td><td>172.26.215.93:407338</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__dc40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        5.8239e-05 </td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         4.84059</td><td style=\"text-align: right;\">0.699913</td><td style=\"text-align: right;\">0.361921</td><td style=\"text-align: right;\">0.103508 </td></tr>\n",
       "<tr><td>FSR_Trainable_7d33338d</td><td>TERMINATED</td><td>172.26.215.93:407569</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c040</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        5.2201e-05 </td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.86671</td><td style=\"text-align: right;\">0.705989</td><td style=\"text-align: right;\">0.37229 </td><td style=\"text-align: right;\">0.103706 </td></tr>\n",
       "<tr><td>FSR_Trainable_90a4ec38</td><td>TERMINATED</td><td>172.26.215.93:407773</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5340</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        2.81083e-05</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.30662</td><td style=\"text-align: right;\">0.699376</td><td style=\"text-align: right;\">0.361821</td><td style=\"text-align: right;\">0.102532 </td></tr>\n",
       "<tr><td>FSR_Trainable_e2404c92</td><td>TERMINATED</td><td>172.26.215.93:408019</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d600</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00022734 </td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       123.853  </td><td style=\"text-align: right;\">0.615037</td><td style=\"text-align: right;\">0.326587</td><td style=\"text-align: right;\">0.0927203</td></tr>\n",
       "<tr><td>FSR_Trainable_f763ff5d</td><td>TERMINATED</td><td>172.26.215.93:408231</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f380</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000309973</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        59.4515 </td><td style=\"text-align: right;\">0.653478</td><td style=\"text-align: right;\">0.359818</td><td style=\"text-align: right;\">0.100525 </td></tr>\n",
       "<tr><td>FSR_Trainable_194e8d7e</td><td>TERMINATED</td><td>172.26.215.93:408468</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7340</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000180081</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       126.297  </td><td style=\"text-align: right;\">0.613356</td><td style=\"text-align: right;\">0.328628</td><td style=\"text-align: right;\">0.0928234</td></tr>\n",
       "<tr><td>FSR_Trainable_00ffbd6d</td><td>TERMINATED</td><td>172.26.215.93:408731</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9300</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000337446</td><td>sklearn.preproc_c7b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.08404</td><td style=\"text-align: right;\">0.78265 </td><td style=\"text-align: right;\">0.496665</td><td style=\"text-align: right;\">0.116913 </td></tr>\n",
       "<tr><td>FSR_Trainable_597a6791</td><td>TERMINATED</td><td>172.26.215.93:408968</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__bd00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000325092</td><td>sklearn.preproc_c7b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.54601</td><td style=\"text-align: right;\">1.58128 </td><td style=\"text-align: right;\">1.21924 </td><td style=\"text-align: right;\">0.192536 </td></tr>\n",
       "<tr><td>FSR_Trainable_2f435512</td><td>TERMINATED</td><td>172.26.215.93:409202</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1b00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        6.57981e-05</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.93869</td><td style=\"text-align: right;\">0.718201</td><td style=\"text-align: right;\">0.395656</td><td style=\"text-align: right;\">0.110584 </td></tr>\n",
       "<tr><td>FSR_Trainable_cd21359b</td><td>TERMINATED</td><td>172.26.215.93:409388</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8040</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        6.53579e-05</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.95351</td><td style=\"text-align: right;\">0.697576</td><td style=\"text-align: right;\">0.348156</td><td style=\"text-align: right;\">0.10296  </td></tr>\n",
       "<tr><td>FSR_Trainable_1de51a04</td><td>TERMINATED</td><td>172.26.215.93:409613</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1d00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000149133</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         8.05633</td><td style=\"text-align: right;\">0.698649</td><td style=\"text-align: right;\">0.348605</td><td style=\"text-align: right;\">0.10178  </td></tr>\n",
       "<tr><td>FSR_Trainable_b912fe9e</td><td>TERMINATED</td><td>172.26.215.93:409840</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b400</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000160011</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.9004 </td><td style=\"text-align: right;\">0.687911</td><td style=\"text-align: right;\">0.354437</td><td style=\"text-align: right;\">0.104763 </td></tr>\n",
       "<tr><td>FSR_Trainable_b8de1926</td><td>TERMINATED</td><td>172.26.215.93:410074</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ccc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000149036</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        53.6015 </td><td style=\"text-align: right;\">0.642141</td><td style=\"text-align: right;\">0.341978</td><td style=\"text-align: right;\">0.0979116</td></tr>\n",
       "<tr><td>FSR_Trainable_a728f25d</td><td>TERMINATED</td><td>172.26.215.93:410301</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b4c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000242501</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       121.59   </td><td style=\"text-align: right;\">0.600593</td><td style=\"text-align: right;\">0.323537</td><td style=\"text-align: right;\">0.089476 </td></tr>\n",
       "<tr><td>FSR_Trainable_9d94984e</td><td>TERMINATED</td><td>172.26.215.93:410554</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7280</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000583929</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       120.838  </td><td style=\"text-align: right;\">0.592788</td><td style=\"text-align: right;\">0.316869</td><td style=\"text-align: right;\">0.0860986</td></tr>\n",
       "<tr><td>FSR_Trainable_74dd520f</td><td>TERMINATED</td><td>172.26.215.93:410829</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__13c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        3.40302e-05</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         4.04128</td><td style=\"text-align: right;\">0.698729</td><td style=\"text-align: right;\">0.358937</td><td style=\"text-align: right;\">0.107706 </td></tr>\n",
       "<tr><td>FSR_Trainable_d98df421</td><td>TERMINATED</td><td>172.26.215.93:411038</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c5c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000531975</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       168.451  </td><td style=\"text-align: right;\">0.617172</td><td style=\"text-align: right;\">0.314442</td><td style=\"text-align: right;\">0.0839548</td></tr>\n",
       "<tr><td>FSR_Trainable_3ea89d29</td><td>TERMINATED</td><td>172.26.215.93:411248</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ea80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000631863</td><td>sklearn.preproc_c7b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.67394</td><td style=\"text-align: right;\">0.753685</td><td style=\"text-align: right;\">0.44435 </td><td style=\"text-align: right;\">0.114148 </td></tr>\n",
       "<tr><td>FSR_Trainable_f602708e</td><td>TERMINATED</td><td>172.26.215.93:411489</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__28c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00060308 </td><td>sklearn.preproc_c7b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.58073</td><td style=\"text-align: right;\">0.743553</td><td style=\"text-align: right;\">0.427947</td><td style=\"text-align: right;\">0.110138 </td></tr>\n",
       "<tr><td>FSR_Trainable_ec364354</td><td>TERMINATED</td><td>172.26.215.93:411720</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b700</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00025332 </td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.94898</td><td style=\"text-align: right;\">0.701559</td><td style=\"text-align: right;\">0.357594</td><td style=\"text-align: right;\">0.101283 </td></tr>\n",
       "<tr><td>FSR_Trainable_ed0248b3</td><td>TERMINATED</td><td>172.26.215.93:411950</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5740</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000267143</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         4.25965</td><td style=\"text-align: right;\">0.69951 </td><td style=\"text-align: right;\">0.358524</td><td style=\"text-align: right;\">0.10232  </td></tr>\n",
       "<tr><td>FSR_Trainable_8ad86243</td><td>TERMINATED</td><td>172.26.215.93:412201</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b740</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00042498 </td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.06443</td><td style=\"text-align: right;\">0.701778</td><td style=\"text-align: right;\">0.368618</td><td style=\"text-align: right;\">0.10393  </td></tr>\n",
       "<tr><td>FSR_Trainable_24576498</td><td>TERMINATED</td><td>172.26.215.93:412414</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6900</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000434165</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        41.2024 </td><td style=\"text-align: right;\">0.671593</td><td style=\"text-align: right;\">0.364064</td><td style=\"text-align: right;\">0.0983938</td></tr>\n",
       "<tr><td>FSR_Trainable_e7085d33</td><td>TERMINATED</td><td>172.26.215.93:412632</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4280</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000205996</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       130.447  </td><td style=\"text-align: right;\">0.599781</td><td style=\"text-align: right;\">0.320918</td><td style=\"text-align: right;\">0.0887209</td></tr>\n",
       "<tr><td>FSR_Trainable_db1779a3</td><td>TERMINATED</td><td>172.26.215.93:412858</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4540</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000396939</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       455.664  </td><td style=\"text-align: right;\">0.571899</td><td style=\"text-align: right;\">0.293412</td><td style=\"text-align: right;\">0.0805268</td></tr>\n",
       "<tr><td>FSR_Trainable_333e936e</td><td>TERMINATED</td><td>172.26.215.93:413144</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f480</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000378172</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       166.367  </td><td style=\"text-align: right;\">0.614115</td><td style=\"text-align: right;\">0.316502</td><td style=\"text-align: right;\">0.0869951</td></tr>\n",
       "<tr><td>FSR_Trainable_4c44a518</td><td>TERMINATED</td><td>172.26.215.93:413416</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f880</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00072274 </td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       123.712  </td><td style=\"text-align: right;\">0.59032 </td><td style=\"text-align: right;\">0.313479</td><td style=\"text-align: right;\">0.0861087</td></tr>\n",
       "<tr><td>FSR_Trainable_477fb7ee</td><td>TERMINATED</td><td>172.26.215.93:413673</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0380</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.001106   </td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">        89.2282 </td><td style=\"text-align: right;\">0.632891</td><td style=\"text-align: right;\">0.361398</td><td style=\"text-align: right;\">0.0929614</td></tr>\n",
       "<tr><td>FSR_Trainable_0385c1d6</td><td>TERMINATED</td><td>172.26.215.93:413962</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e6c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0011023  </td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        12.9393 </td><td style=\"text-align: right;\">0.664275</td><td style=\"text-align: right;\">0.366897</td><td style=\"text-align: right;\">0.103896 </td></tr>\n",
       "<tr><td>FSR_Trainable_ac5a6022</td><td>TERMINATED</td><td>172.26.215.93:414169</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e600</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000271072</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       148.198  </td><td style=\"text-align: right;\">0.620217</td><td style=\"text-align: right;\">0.342214</td><td style=\"text-align: right;\">0.0915835</td></tr>\n",
       "<tr><td>FSR_Trainable_d030706f</td><td>TERMINATED</td><td>172.26.215.93:414385</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__eac0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000850628</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.73864</td><td style=\"text-align: right;\">0.712504</td><td style=\"text-align: right;\">0.385686</td><td style=\"text-align: right;\">0.106078 </td></tr>\n",
       "<tr><td>FSR_Trainable_def25e2f</td><td>TERMINATED</td><td>172.26.215.93:414610</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b480</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000697431</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.52596</td><td style=\"text-align: right;\">0.701607</td><td style=\"text-align: right;\">0.380963</td><td style=\"text-align: right;\">0.116937 </td></tr>\n",
       "<tr><td>FSR_Trainable_8ab3a6de</td><td>TERMINATED</td><td>172.26.215.93:414855</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e080</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000220739</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       124.426  </td><td style=\"text-align: right;\">0.607672</td><td style=\"text-align: right;\">0.325723</td><td style=\"text-align: right;\">0.0927704</td></tr>\n",
       "<tr><td>FSR_Trainable_0066a45c</td><td>TERMINATED</td><td>172.26.215.93:415079</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__a500</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000288906</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       123.165  </td><td style=\"text-align: right;\">0.59877 </td><td style=\"text-align: right;\">0.317751</td><td style=\"text-align: right;\">0.0908703</td></tr>\n",
       "<tr><td>FSR_Trainable_4ad05cbc</td><td>TERMINATED</td><td>172.26.215.93:415440</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7300</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000204832</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        51.4648 </td><td style=\"text-align: right;\">0.635934</td><td style=\"text-align: right;\">0.33997 </td><td style=\"text-align: right;\">0.0955855</td></tr>\n",
       "<tr><td>FSR_Trainable_8094e3b1</td><td>TERMINATED</td><td>172.26.215.93:415639</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3e00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000194337</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        12.3399 </td><td style=\"text-align: right;\">0.67583 </td><td style=\"text-align: right;\">0.345561</td><td style=\"text-align: right;\">0.102345 </td></tr>\n",
       "<tr><td>FSR_Trainable_ae002d89</td><td>TERMINATED</td><td>172.26.215.93:415853</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7540</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000361832</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       169.972  </td><td style=\"text-align: right;\">0.601174</td><td style=\"text-align: right;\">0.31689 </td><td style=\"text-align: right;\">0.0852393</td></tr>\n",
       "<tr><td>FSR_Trainable_71b9e672</td><td>TERMINATED</td><td>172.26.215.93:416080</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e640</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000386968</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       172.241  </td><td style=\"text-align: right;\">0.588474</td><td style=\"text-align: right;\">0.305813</td><td style=\"text-align: right;\">0.0857329</td></tr>\n",
       "<tr><td>FSR_Trainable_73205455</td><td>TERMINATED</td><td>172.26.215.93:416365</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5280</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000380693</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.25772</td><td style=\"text-align: right;\">0.700035</td><td style=\"text-align: right;\">0.348038</td><td style=\"text-align: right;\">0.101167 </td></tr>\n",
       "<tr><td>FSR_Trainable_725f2efb</td><td>TERMINATED</td><td>172.26.215.93:416550</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c440</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000388193</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        18.2052 </td><td style=\"text-align: right;\">0.676048</td><td style=\"text-align: right;\">0.341295</td><td style=\"text-align: right;\">0.100947 </td></tr>\n",
       "<tr><td>FSR_Trainable_2efed0e1</td><td>TERMINATED</td><td>172.26.215.93:416773</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2000</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000466949</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       510.822  </td><td style=\"text-align: right;\">0.551657</td><td style=\"text-align: right;\">0.281329</td><td style=\"text-align: right;\">0.0755517</td></tr>\n",
       "<tr><td>FSR_Trainable_6dca7ca5</td><td>TERMINATED</td><td>172.26.215.93:417056</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__a440</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00050535 </td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       514.745  </td><td style=\"text-align: right;\">0.569729</td><td style=\"text-align: right;\">0.288519</td><td style=\"text-align: right;\">0.0772115</td></tr>\n",
       "<tr><td>FSR_Trainable_e9ddb955</td><td>TERMINATED</td><td>172.26.215.93:417356</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7c40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000489868</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       279.772  </td><td style=\"text-align: right;\">0.586452</td><td style=\"text-align: right;\">0.302331</td><td style=\"text-align: right;\">0.0812595</td></tr>\n",
       "<tr><td>FSR_Trainable_f451ac61</td><td>TERMINATED</td><td>172.26.215.93:417533</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2740</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000317943</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       280.051  </td><td style=\"text-align: right;\">0.574104</td><td style=\"text-align: right;\">0.300648</td><td style=\"text-align: right;\">0.0821559</td></tr>\n",
       "<tr><td>FSR_Trainable_e9eabb15</td><td>TERMINATED</td><td>172.26.215.93:417983</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ed40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000511286</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       287.922  </td><td style=\"text-align: right;\">0.581144</td><td style=\"text-align: right;\">0.29302 </td><td style=\"text-align: right;\">0.0810606</td></tr>\n",
       "<tr><td>FSR_Trainable_84bae381</td><td>TERMINATED</td><td>172.26.215.93:418166</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4080</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000713766</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        45.8493 </td><td style=\"text-align: right;\">0.647353</td><td style=\"text-align: right;\">0.354841</td><td style=\"text-align: right;\">0.0971294</td></tr>\n",
       "<tr><td>FSR_Trainable_a802a962</td><td>TERMINATED</td><td>172.26.215.93:418484</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6480</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000495995</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       295.175  </td><td style=\"text-align: right;\">0.57742 </td><td style=\"text-align: right;\">0.298926</td><td style=\"text-align: right;\">0.0809895</td></tr>\n",
       "<tr><td>FSR_Trainable_57517084</td><td>TERMINATED</td><td>172.26.215.93:418693</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4f80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000571599</td><td>sklearn.preproc_c810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        12.0271 </td><td style=\"text-align: right;\">0.674006</td><td style=\"text-align: right;\">0.355715</td><td style=\"text-align: right;\">0.101439 </td></tr>\n",
       "<tr><td>FSR_Trainable_ca3746b5</td><td>TERMINATED</td><td>172.26.215.93:418955</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9c00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000738947</td><td>sklearn.preproc_c810</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        23.7157 </td><td style=\"text-align: right;\">0.66428 </td><td style=\"text-align: right;\">0.355313</td><td style=\"text-align: right;\">0.099413 </td></tr>\n",
       "<tr><td>FSR_Trainable_0133600f</td><td>TERMINATED</td><td>172.26.215.93:419134</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7e00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000489138</td><td>sklearn.preproc_c7b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.65977</td><td style=\"text-align: right;\">2.12064 </td><td style=\"text-align: right;\">1.83596 </td><td style=\"text-align: right;\">0.283736 </td></tr>\n",
       "<tr><td>FSR_Trainable_2b850322</td><td>TERMINATED</td><td>172.26.215.93:419408</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__42c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000506888</td><td>sklearn.preproc_c7b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.28284</td><td style=\"text-align: right;\">1.91813 </td><td style=\"text-align: right;\">1.70749 </td><td style=\"text-align: right;\">0.26907  </td></tr>\n",
       "<tr><td>FSR_Trainable_942275ec</td><td>TERMINATED</td><td>172.26.215.93:419592</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d180</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000968265</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       549.917  </td><td style=\"text-align: right;\">0.565224</td><td style=\"text-align: right;\">0.285481</td><td style=\"text-align: right;\">0.0786126</td></tr>\n",
       "<tr><td>FSR_Trainable_9288852c</td><td>TERMINATED</td><td>172.26.215.93:419813</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c180</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000317241</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        11.3715 </td><td style=\"text-align: right;\">0.688613</td><td style=\"text-align: right;\">0.342291</td><td style=\"text-align: right;\">0.0994743</td></tr>\n",
       "<tr><td>FSR_Trainable_0d66a628</td><td>TERMINATED</td><td>172.26.215.93:420096</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d440</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000299791</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        12.5836 </td><td style=\"text-align: right;\">0.69272 </td><td style=\"text-align: right;\">0.34375 </td><td style=\"text-align: right;\">0.100363 </td></tr>\n",
       "<tr><td>FSR_Trainable_551b247c</td><td>TERMINATED</td><td>172.26.215.93:420334</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d900</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000910391</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       533.071  </td><td style=\"text-align: right;\">0.555174</td><td style=\"text-align: right;\">0.278152</td><td style=\"text-align: right;\">0.0708917</td></tr>\n",
       "<tr><td>FSR_Trainable_b531ab6b</td><td>TERMINATED</td><td>172.26.215.93:420606</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6a00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000967112</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       302.451  </td><td style=\"text-align: right;\">0.561751</td><td style=\"text-align: right;\">0.289774</td><td style=\"text-align: right;\">0.077495 </td></tr>\n",
       "<tr><td>FSR_Trainable_797a94cd</td><td>TERMINATED</td><td>172.26.215.93:420878</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ad40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000936676</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       483.112  </td><td style=\"text-align: right;\">0.586456</td><td style=\"text-align: right;\">0.286204</td><td style=\"text-align: right;\">0.0717553</td></tr>\n",
       "<tr><td>FSR_Trainable_ca336f36</td><td>TERMINATED</td><td>172.26.215.93:421242</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c9f0</td><td>sklearn.impute._f9b0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c9c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000966341</td><td>sklearn.preproc_c570</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       349.579  </td><td style=\"text-align: right;\">0.542824</td><td style=\"text-align: right;\">0.275663</td><td style=\"text-align: right;\">0.0738985</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 03:37:53,074\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">     mape</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_0066a45c</td><td>2023-07-19_04-08-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.317751</td><td style=\"text-align: right;\">0.0908703</td><td>172.26.215.93</td><td style=\"text-align: right;\">415079</td><td style=\"text-align: right;\">0.59877 </td><td style=\"text-align: right;\">           123.165  </td><td style=\"text-align: right;\">           1.41631</td><td style=\"text-align: right;\">     123.165  </td><td style=\"text-align: right;\"> 1689707293</td><td style=\"text-align: right;\">                 100</td><td>0066a45c  </td></tr>\n",
       "<tr><td>FSR_Trainable_00ffbd6d</td><td>2023-07-19_03-57-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.496665</td><td style=\"text-align: right;\">0.116913 </td><td>172.26.215.93</td><td style=\"text-align: right;\">408731</td><td style=\"text-align: right;\">0.78265 </td><td style=\"text-align: right;\">             5.08404</td><td style=\"text-align: right;\">           5.08404</td><td style=\"text-align: right;\">       5.08404</td><td style=\"text-align: right;\"> 1689706629</td><td style=\"text-align: right;\">                   1</td><td>00ffbd6d  </td></tr>\n",
       "<tr><td>FSR_Trainable_0133600f</td><td>2023-07-19_04-19-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">1.83596 </td><td style=\"text-align: right;\">0.283736 </td><td>172.26.215.93</td><td style=\"text-align: right;\">419134</td><td style=\"text-align: right;\">2.12064 </td><td style=\"text-align: right;\">             8.65977</td><td style=\"text-align: right;\">           8.65977</td><td style=\"text-align: right;\">       8.65977</td><td style=\"text-align: right;\"> 1689707966</td><td style=\"text-align: right;\">                   1</td><td>0133600f  </td></tr>\n",
       "<tr><td>FSR_Trainable_01a407a0</td><td>2023-07-19_03-46-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.392197</td><td style=\"text-align: right;\">0.107882 </td><td>172.26.215.93</td><td style=\"text-align: right;\">398881</td><td style=\"text-align: right;\">0.710463</td><td style=\"text-align: right;\">           425.579  </td><td style=\"text-align: right;\">           4.30644</td><td style=\"text-align: right;\">     425.579  </td><td style=\"text-align: right;\"> 1689705996</td><td style=\"text-align: right;\">                 100</td><td>01a407a0  </td></tr>\n",
       "<tr><td>FSR_Trainable_0385c1d6</td><td>2023-07-19_04-05-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.366897</td><td style=\"text-align: right;\">0.103896 </td><td>172.26.215.93</td><td style=\"text-align: right;\">413962</td><td style=\"text-align: right;\">0.664275</td><td style=\"text-align: right;\">            12.9393 </td><td style=\"text-align: right;\">           1.39779</td><td style=\"text-align: right;\">      12.9393 </td><td style=\"text-align: right;\"> 1689707117</td><td style=\"text-align: right;\">                   8</td><td>0385c1d6  </td></tr>\n",
       "<tr><td>FSR_Trainable_0d66a628</td><td>2023-07-19_04-20-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.34375 </td><td style=\"text-align: right;\">0.100363 </td><td>172.26.215.93</td><td style=\"text-align: right;\">420096</td><td style=\"text-align: right;\">0.69272 </td><td style=\"text-align: right;\">            12.5836 </td><td style=\"text-align: right;\">           5.97127</td><td style=\"text-align: right;\">      12.5836 </td><td style=\"text-align: right;\"> 1689708047</td><td style=\"text-align: right;\">                   2</td><td>0d66a628  </td></tr>\n",
       "<tr><td>FSR_Trainable_1740aa9e</td><td>2023-07-19_03-50-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.38506 </td><td style=\"text-align: right;\">0.100481 </td><td>172.26.215.93</td><td style=\"text-align: right;\">405088</td><td style=\"text-align: right;\">0.710538</td><td style=\"text-align: right;\">             1.50745</td><td style=\"text-align: right;\">           1.50745</td><td style=\"text-align: right;\">       1.50745</td><td style=\"text-align: right;\"> 1689706252</td><td style=\"text-align: right;\">                   1</td><td>1740aa9e  </td></tr>\n",
       "<tr><td>FSR_Trainable_191e5a09</td><td>2023-07-19_03-42-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.371327</td><td style=\"text-align: right;\">0.11029  </td><td>172.26.215.93</td><td style=\"text-align: right;\">401261</td><td style=\"text-align: right;\">0.701787</td><td style=\"text-align: right;\">             7.66508</td><td style=\"text-align: right;\">           1.73652</td><td style=\"text-align: right;\">       7.66508</td><td style=\"text-align: right;\"> 1689705774</td><td style=\"text-align: right;\">                   4</td><td>191e5a09  </td></tr>\n",
       "<tr><td>FSR_Trainable_194e8d7e</td><td>2023-07-19_03-59-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.328628</td><td style=\"text-align: right;\">0.0928234</td><td>172.26.215.93</td><td style=\"text-align: right;\">408468</td><td style=\"text-align: right;\">0.613356</td><td style=\"text-align: right;\">           126.297  </td><td style=\"text-align: right;\">           1.10968</td><td style=\"text-align: right;\">     126.297  </td><td style=\"text-align: right;\"> 1689706748</td><td style=\"text-align: right;\">                 100</td><td>194e8d7e  </td></tr>\n",
       "<tr><td>FSR_Trainable_19714ab0</td><td>2023-07-19_03-39-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.449026</td><td style=\"text-align: right;\">0.116743 </td><td>172.26.215.93</td><td style=\"text-align: right;\">398645</td><td style=\"text-align: right;\">0.744661</td><td style=\"text-align: right;\">             5.72931</td><td style=\"text-align: right;\">           2.68103</td><td style=\"text-align: right;\">       5.72931</td><td style=\"text-align: right;\"> 1689705554</td><td style=\"text-align: right;\">                   2</td><td>19714ab0  </td></tr>\n",
       "<tr><td>FSR_Trainable_1c20a35b</td><td>2023-07-19_03-40-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.462785</td><td style=\"text-align: right;\">0.118133 </td><td>172.26.215.93</td><td style=\"text-align: right;\">399833</td><td style=\"text-align: right;\">0.774017</td><td style=\"text-align: right;\">             6.76655</td><td style=\"text-align: right;\">           6.76655</td><td style=\"text-align: right;\">       6.76655</td><td style=\"text-align: right;\"> 1689705629</td><td style=\"text-align: right;\">                   1</td><td>1c20a35b  </td></tr>\n",
       "<tr><td>FSR_Trainable_1de51a04</td><td>2023-07-19_03-58-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.348605</td><td style=\"text-align: right;\">0.10178  </td><td>172.26.215.93</td><td style=\"text-align: right;\">409613</td><td style=\"text-align: right;\">0.698649</td><td style=\"text-align: right;\">             8.05633</td><td style=\"text-align: right;\">           3.53187</td><td style=\"text-align: right;\">       8.05633</td><td style=\"text-align: right;\"> 1689706691</td><td style=\"text-align: right;\">                   2</td><td>1de51a04  </td></tr>\n",
       "<tr><td>FSR_Trainable_1f71a826</td><td>2023-07-19_03-39-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.352572</td><td style=\"text-align: right;\">0.0912364</td><td>172.26.215.93</td><td style=\"text-align: right;\">399106</td><td style=\"text-align: right;\">0.725783</td><td style=\"text-align: right;\">            13.5167 </td><td style=\"text-align: right;\">           6.5831 </td><td style=\"text-align: right;\">      13.5167 </td><td style=\"text-align: right;\"> 1689705586</td><td style=\"text-align: right;\">                   2</td><td>1f71a826  </td></tr>\n",
       "<tr><td>FSR_Trainable_2285c2f9</td><td>2023-07-19_03-40-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.20404 </td><td style=\"text-align: right;\">0.389105 </td><td>172.26.215.93</td><td style=\"text-align: right;\">399583</td><td style=\"text-align: right;\">2.63241 </td><td style=\"text-align: right;\">             3.16505</td><td style=\"text-align: right;\">           3.16505</td><td style=\"text-align: right;\">       3.16505</td><td style=\"text-align: right;\"> 1689705609</td><td style=\"text-align: right;\">                   1</td><td>2285c2f9  </td></tr>\n",
       "<tr><td>FSR_Trainable_22aa24f4</td><td>2023-07-19_03-47-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.38883 </td><td style=\"text-align: right;\">0.104349 </td><td>172.26.215.93</td><td style=\"text-align: right;\">402708</td><td style=\"text-align: right;\">0.713109</td><td style=\"text-align: right;\">             8.89781</td><td style=\"text-align: right;\">           8.89781</td><td style=\"text-align: right;\">       8.89781</td><td style=\"text-align: right;\"> 1689706025</td><td style=\"text-align: right;\">                   1</td><td>22aa24f4  </td></tr>\n",
       "<tr><td>FSR_Trainable_22e5e3fc</td><td>2023-07-19_03-38-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.457914</td><td style=\"text-align: right;\">0.111012 </td><td>172.26.215.93</td><td style=\"text-align: right;\">397684</td><td style=\"text-align: right;\">0.767972</td><td style=\"text-align: right;\">             3.92258</td><td style=\"text-align: right;\">           3.92258</td><td style=\"text-align: right;\">       3.92258</td><td style=\"text-align: right;\"> 1689705498</td><td style=\"text-align: right;\">                   1</td><td>22e5e3fc  </td></tr>\n",
       "<tr><td>FSR_Trainable_24576498</td><td>2023-07-19_04-01-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.364064</td><td style=\"text-align: right;\">0.0983938</td><td>172.26.215.93</td><td style=\"text-align: right;\">412414</td><td style=\"text-align: right;\">0.671593</td><td style=\"text-align: right;\">            41.2024 </td><td style=\"text-align: right;\">           4.78295</td><td style=\"text-align: right;\">      41.2024 </td><td style=\"text-align: right;\"> 1689706902</td><td style=\"text-align: right;\">                   8</td><td>24576498  </td></tr>\n",
       "<tr><td>FSR_Trainable_24c7aee5</td><td>2023-07-19_03-53-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.337601</td><td style=\"text-align: right;\">0.0972058</td><td>172.26.215.93</td><td style=\"text-align: right;\">405799</td><td style=\"text-align: right;\">0.634233</td><td style=\"text-align: right;\">           119.935  </td><td style=\"text-align: right;\">           1.15825</td><td style=\"text-align: right;\">     119.935  </td><td style=\"text-align: right;\"> 1689706427</td><td style=\"text-align: right;\">                 100</td><td>24c7aee5  </td></tr>\n",
       "<tr><td>FSR_Trainable_2511f06e</td><td>2023-07-19_03-47-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.356608</td><td style=\"text-align: right;\">0.101973 </td><td>172.26.215.93</td><td style=\"text-align: right;\">403387</td><td style=\"text-align: right;\">0.701145</td><td style=\"text-align: right;\">             8.89197</td><td style=\"text-align: right;\">           4.27179</td><td style=\"text-align: right;\">       8.89197</td><td style=\"text-align: right;\"> 1689706071</td><td style=\"text-align: right;\">                   2</td><td>2511f06e  </td></tr>\n",
       "<tr><td>FSR_Trainable_28425721</td><td>2023-07-19_03-52-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.361363</td><td style=\"text-align: right;\">0.104155 </td><td>172.26.215.93</td><td style=\"text-align: right;\">402523</td><td style=\"text-align: right;\">0.672012</td><td style=\"text-align: right;\">           336.193  </td><td style=\"text-align: right;\">           9.8746 </td><td style=\"text-align: right;\">     336.193  </td><td style=\"text-align: right;\"> 1689706347</td><td style=\"text-align: right;\">                  32</td><td>28425721  </td></tr>\n",
       "<tr><td>FSR_Trainable_286ac475</td><td>2023-07-19_03-41-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.41242 </td><td style=\"text-align: right;\">0.109588 </td><td>172.26.215.93</td><td style=\"text-align: right;\">400332</td><td style=\"text-align: right;\">0.723472</td><td style=\"text-align: right;\">             7.89294</td><td style=\"text-align: right;\">           3.71115</td><td style=\"text-align: right;\">       7.89294</td><td style=\"text-align: right;\"> 1689705691</td><td style=\"text-align: right;\">                   2</td><td>286ac475  </td></tr>\n",
       "<tr><td>FSR_Trainable_2b850322</td><td>2023-07-19_04-19-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">1.70749 </td><td style=\"text-align: right;\">0.26907  </td><td>172.26.215.93</td><td style=\"text-align: right;\">419408</td><td style=\"text-align: right;\">1.91813 </td><td style=\"text-align: right;\">             8.28284</td><td style=\"text-align: right;\">           8.28284</td><td style=\"text-align: right;\">       8.28284</td><td style=\"text-align: right;\"> 1689707988</td><td style=\"text-align: right;\">                   1</td><td>2b850322  </td></tr>\n",
       "<tr><td>FSR_Trainable_2efed0e1</td><td>2023-07-19_04-18-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.281329</td><td style=\"text-align: right;\">0.0755517</td><td>172.26.215.93</td><td style=\"text-align: right;\">416773</td><td style=\"text-align: right;\">0.551657</td><td style=\"text-align: right;\">           510.822  </td><td style=\"text-align: right;\">           5.27446</td><td style=\"text-align: right;\">     510.822  </td><td style=\"text-align: right;\"> 1689707907</td><td style=\"text-align: right;\">                 100</td><td>2efed0e1  </td></tr>\n",
       "<tr><td>FSR_Trainable_2f435512</td><td>2023-07-19_03-57-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.395656</td><td style=\"text-align: right;\">0.110584 </td><td>172.26.215.93</td><td style=\"text-align: right;\">409202</td><td style=\"text-align: right;\">0.718201</td><td style=\"text-align: right;\">             1.93869</td><td style=\"text-align: right;\">           1.93869</td><td style=\"text-align: right;\">       1.93869</td><td style=\"text-align: right;\"> 1689706662</td><td style=\"text-align: right;\">                   1</td><td>2f435512  </td></tr>\n",
       "<tr><td>FSR_Trainable_30156eff</td><td>2023-07-19_03-42-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.365159</td><td style=\"text-align: right;\">0.109027 </td><td>172.26.215.93</td><td style=\"text-align: right;\">401026</td><td style=\"text-align: right;\">0.700494</td><td style=\"text-align: right;\">             8.10773</td><td style=\"text-align: right;\">           1.83583</td><td style=\"text-align: right;\">       8.10773</td><td style=\"text-align: right;\"> 1689705754</td><td style=\"text-align: right;\">                   4</td><td>30156eff  </td></tr>\n",
       "<tr><td>FSR_Trainable_333e936e</td><td>2023-07-19_04-04-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.316502</td><td style=\"text-align: right;\">0.0869951</td><td>172.26.215.93</td><td style=\"text-align: right;\">413144</td><td style=\"text-align: right;\">0.614115</td><td style=\"text-align: right;\">           166.367  </td><td style=\"text-align: right;\">           1.63805</td><td style=\"text-align: right;\">     166.367  </td><td style=\"text-align: right;\"> 1689707089</td><td style=\"text-align: right;\">                 100</td><td>333e936e  </td></tr>\n",
       "<tr><td>FSR_Trainable_36edc5db</td><td>2023-07-19_03-56-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.324787</td><td style=\"text-align: right;\">0.0922675</td><td>172.26.215.93</td><td style=\"text-align: right;\">406511</td><td style=\"text-align: right;\">0.61619 </td><td style=\"text-align: right;\">           201.077  </td><td style=\"text-align: right;\">           2.01705</td><td style=\"text-align: right;\">     201.077  </td><td style=\"text-align: right;\"> 1689706591</td><td style=\"text-align: right;\">                 100</td><td>36edc5db  </td></tr>\n",
       "<tr><td>FSR_Trainable_3ea89d29</td><td>2023-07-19_03-59-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.44435 </td><td style=\"text-align: right;\">0.114148 </td><td>172.26.215.93</td><td style=\"text-align: right;\">411248</td><td style=\"text-align: right;\">0.753685</td><td style=\"text-align: right;\">             1.67394</td><td style=\"text-align: right;\">           1.67394</td><td style=\"text-align: right;\">       1.67394</td><td style=\"text-align: right;\"> 1689706786</td><td style=\"text-align: right;\">                   1</td><td>3ea89d29  </td></tr>\n",
       "<tr><td>FSR_Trainable_4451a342</td><td>2023-07-19_03-50-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.323856</td><td style=\"text-align: right;\">0.0915023</td><td>172.26.215.93</td><td style=\"text-align: right;\">403860</td><td style=\"text-align: right;\">0.614803</td><td style=\"text-align: right;\">           120.869  </td><td style=\"text-align: right;\">           1.0494 </td><td style=\"text-align: right;\">     120.869  </td><td style=\"text-align: right;\"> 1689706213</td><td style=\"text-align: right;\">                 100</td><td>4451a342  </td></tr>\n",
       "<tr><td>FSR_Trainable_468d66ee</td><td>2023-07-19_03-47-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.36354 </td><td style=\"text-align: right;\">0.104151 </td><td>172.26.215.93</td><td style=\"text-align: right;\">403167</td><td style=\"text-align: right;\">0.699585</td><td style=\"text-align: right;\">            15.6652 </td><td style=\"text-align: right;\">           3.57578</td><td style=\"text-align: right;\">      15.6652 </td><td style=\"text-align: right;\"> 1689706066</td><td style=\"text-align: right;\">                   4</td><td>468d66ee  </td></tr>\n",
       "<tr><td>FSR_Trainable_477fb7ee</td><td>2023-07-19_04-05-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.361398</td><td style=\"text-align: right;\">0.0929614</td><td>172.26.215.93</td><td style=\"text-align: right;\">413673</td><td style=\"text-align: right;\">0.632891</td><td style=\"text-align: right;\">            89.2282 </td><td style=\"text-align: right;\">           1.31212</td><td style=\"text-align: right;\">      89.2282 </td><td style=\"text-align: right;\"> 1689707116</td><td style=\"text-align: right;\">                  64</td><td>477fb7ee  </td></tr>\n",
       "<tr><td>FSR_Trainable_48e09294</td><td>2023-07-19_03-55-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.361921</td><td style=\"text-align: right;\">0.103508 </td><td>172.26.215.93</td><td style=\"text-align: right;\">407338</td><td style=\"text-align: right;\">0.699913</td><td style=\"text-align: right;\">             4.84059</td><td style=\"text-align: right;\">           2.1846 </td><td style=\"text-align: right;\">       4.84059</td><td style=\"text-align: right;\"> 1689706532</td><td style=\"text-align: right;\">                   2</td><td>48e09294  </td></tr>\n",
       "<tr><td>FSR_Trainable_498b72ce</td><td>2023-07-19_03-38-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.06331 </td><td style=\"text-align: right;\">0.307365 </td><td>172.26.215.93</td><td style=\"text-align: right;\">398421</td><td style=\"text-align: right;\">2.34301 </td><td style=\"text-align: right;\">             5.55176</td><td style=\"text-align: right;\">           5.55176</td><td style=\"text-align: right;\">       5.55176</td><td style=\"text-align: right;\"> 1689705538</td><td style=\"text-align: right;\">                   1</td><td>498b72ce  </td></tr>\n",
       "<tr><td>FSR_Trainable_4ad05cbc</td><td>2023-07-19_04-09-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.33997 </td><td style=\"text-align: right;\">0.0955855</td><td>172.26.215.93</td><td style=\"text-align: right;\">415440</td><td style=\"text-align: right;\">0.635934</td><td style=\"text-align: right;\">            51.4648 </td><td style=\"text-align: right;\">           1.28676</td><td style=\"text-align: right;\">      51.4648 </td><td style=\"text-align: right;\"> 1689707348</td><td style=\"text-align: right;\">                  32</td><td>4ad05cbc  </td></tr>\n",
       "<tr><td>FSR_Trainable_4c44a518</td><td>2023-07-19_04-05-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.313479</td><td style=\"text-align: right;\">0.0861087</td><td>172.26.215.93</td><td style=\"text-align: right;\">413416</td><td style=\"text-align: right;\">0.59032 </td><td style=\"text-align: right;\">           123.712  </td><td style=\"text-align: right;\">           1.26395</td><td style=\"text-align: right;\">     123.712  </td><td style=\"text-align: right;\"> 1689707104</td><td style=\"text-align: right;\">                 100</td><td>4c44a518  </td></tr>\n",
       "<tr><td>FSR_Trainable_551b247c</td><td>2023-07-19_04-30-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.278152</td><td style=\"text-align: right;\">0.0708917</td><td>172.26.215.93</td><td style=\"text-align: right;\">420334</td><td style=\"text-align: right;\">0.555174</td><td style=\"text-align: right;\">           533.071  </td><td style=\"text-align: right;\">           4.6049 </td><td style=\"text-align: right;\">     533.071  </td><td style=\"text-align: right;\"> 1689708612</td><td style=\"text-align: right;\">                 100</td><td>551b247c  </td></tr>\n",
       "<tr><td>FSR_Trainable_57517084</td><td>2023-07-19_04-18-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.355715</td><td style=\"text-align: right;\">0.101439 </td><td>172.26.215.93</td><td style=\"text-align: right;\">418693</td><td style=\"text-align: right;\">0.674006</td><td style=\"text-align: right;\">            12.0271 </td><td style=\"text-align: right;\">           2.72884</td><td style=\"text-align: right;\">      12.0271 </td><td style=\"text-align: right;\"> 1689707935</td><td style=\"text-align: right;\">                   4</td><td>57517084  </td></tr>\n",
       "<tr><td>FSR_Trainable_597a6791</td><td>2023-07-19_03-57-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">1.21924 </td><td style=\"text-align: right;\">0.192536 </td><td>172.26.215.93</td><td style=\"text-align: right;\">408968</td><td style=\"text-align: right;\">1.58128 </td><td style=\"text-align: right;\">             6.54601</td><td style=\"text-align: right;\">           6.54601</td><td style=\"text-align: right;\">       6.54601</td><td style=\"text-align: right;\"> 1689706649</td><td style=\"text-align: right;\">                   1</td><td>597a6791  </td></tr>\n",
       "<tr><td>FSR_Trainable_5a37feb1</td><td>2023-07-19_03-41-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.346834</td><td style=\"text-align: right;\">0.0999267</td><td>172.26.215.93</td><td style=\"text-align: right;\">397527</td><td style=\"text-align: right;\">0.697381</td><td style=\"text-align: right;\">           165.166  </td><td style=\"text-align: right;\">           1.4895 </td><td style=\"text-align: right;\">     165.166  </td><td style=\"text-align: right;\"> 1689705669</td><td style=\"text-align: right;\">                 100</td><td>5a37feb1  </td></tr>\n",
       "<tr><td>FSR_Trainable_5d3e9619</td><td>2023-07-19_03-39-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.387785</td><td style=\"text-align: right;\">0.10113  </td><td>172.26.215.93</td><td style=\"text-align: right;\">397455</td><td style=\"text-align: right;\">0.727399</td><td style=\"text-align: right;\">            92.8851 </td><td style=\"text-align: right;\">           0.83246</td><td style=\"text-align: right;\">      92.8851 </td><td style=\"text-align: right;\"> 1689705590</td><td style=\"text-align: right;\">                 100</td><td>5d3e9619  </td></tr>\n",
       "<tr><td>FSR_Trainable_6dca7ca5</td><td>2023-07-19_04-18-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.288519</td><td style=\"text-align: right;\">0.0772115</td><td>172.26.215.93</td><td style=\"text-align: right;\">417056</td><td style=\"text-align: right;\">0.569729</td><td style=\"text-align: right;\">           514.745  </td><td style=\"text-align: right;\">           5.22693</td><td style=\"text-align: right;\">     514.745  </td><td style=\"text-align: right;\"> 1689707934</td><td style=\"text-align: right;\">                 100</td><td>6dca7ca5  </td></tr>\n",
       "<tr><td>FSR_Trainable_71b9e672</td><td>2023-07-19_04-12-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.305813</td><td style=\"text-align: right;\">0.0857329</td><td>172.26.215.93</td><td style=\"text-align: right;\">416080</td><td style=\"text-align: right;\">0.588474</td><td style=\"text-align: right;\">           172.241  </td><td style=\"text-align: right;\">           1.75564</td><td style=\"text-align: right;\">     172.241  </td><td style=\"text-align: right;\"> 1689707520</td><td style=\"text-align: right;\">                 100</td><td>71b9e672  </td></tr>\n",
       "<tr><td>FSR_Trainable_725f2efb</td><td>2023-07-19_04-09-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.341295</td><td style=\"text-align: right;\">0.100947 </td><td>172.26.215.93</td><td style=\"text-align: right;\">416550</td><td style=\"text-align: right;\">0.676048</td><td style=\"text-align: right;\">            18.2052 </td><td style=\"text-align: right;\">           4.06018</td><td style=\"text-align: right;\">      18.2052 </td><td style=\"text-align: right;\"> 1689707392</td><td style=\"text-align: right;\">                   4</td><td>725f2efb  </td></tr>\n",
       "<tr><td>FSR_Trainable_73205455</td><td>2023-07-19_04-09-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.348038</td><td style=\"text-align: right;\">0.101167 </td><td>172.26.215.93</td><td style=\"text-align: right;\">416365</td><td style=\"text-align: right;\">0.700035</td><td style=\"text-align: right;\">             4.25772</td><td style=\"text-align: right;\">           4.25772</td><td style=\"text-align: right;\">       4.25772</td><td style=\"text-align: right;\"> 1689707365</td><td style=\"text-align: right;\">                   1</td><td>73205455  </td></tr>\n",
       "<tr><td>FSR_Trainable_74dd520f</td><td>2023-07-19_03-59-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.358937</td><td style=\"text-align: right;\">0.107706 </td><td>172.26.215.93</td><td style=\"text-align: right;\">410829</td><td style=\"text-align: right;\">0.698729</td><td style=\"text-align: right;\">             4.04128</td><td style=\"text-align: right;\">           1.75482</td><td style=\"text-align: right;\">       4.04128</td><td style=\"text-align: right;\"> 1689706765</td><td style=\"text-align: right;\">                   2</td><td>74dd520f  </td></tr>\n",
       "<tr><td>FSR_Trainable_79618f04</td><td>2023-07-19_03-42-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.359561</td><td style=\"text-align: right;\">0.10206  </td><td>172.26.215.93</td><td style=\"text-align: right;\">400797</td><td style=\"text-align: right;\">0.701939</td><td style=\"text-align: right;\">             5.9221 </td><td style=\"text-align: right;\">           1.21854</td><td style=\"text-align: right;\">       5.9221 </td><td style=\"text-align: right;\"> 1689705732</td><td style=\"text-align: right;\">                   4</td><td>79618f04  </td></tr>\n",
       "<tr><td>FSR_Trainable_797a94cd</td><td>2023-07-19_04-31-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.286204</td><td style=\"text-align: right;\">0.0717553</td><td>172.26.215.93</td><td style=\"text-align: right;\">420878</td><td style=\"text-align: right;\">0.586456</td><td style=\"text-align: right;\">           483.112  </td><td style=\"text-align: right;\">           3.53882</td><td style=\"text-align: right;\">     483.112  </td><td style=\"text-align: right;\"> 1689708716</td><td style=\"text-align: right;\">                 100</td><td>797a94cd  </td></tr>\n",
       "<tr><td>FSR_Trainable_7d33338d</td><td>2023-07-19_03-55-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.37229 </td><td style=\"text-align: right;\">0.103706 </td><td>172.26.215.93</td><td style=\"text-align: right;\">407569</td><td style=\"text-align: right;\">0.705989</td><td style=\"text-align: right;\">             1.86671</td><td style=\"text-align: right;\">           1.86671</td><td style=\"text-align: right;\">       1.86671</td><td style=\"text-align: right;\"> 1689706544</td><td style=\"text-align: right;\">                   1</td><td>7d33338d  </td></tr>\n",
       "<tr><td>FSR_Trainable_8094e3b1</td><td>2023-07-19_04-08-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.345561</td><td style=\"text-align: right;\">0.102345 </td><td>172.26.215.93</td><td style=\"text-align: right;\">415639</td><td style=\"text-align: right;\">0.67583 </td><td style=\"text-align: right;\">            12.3399 </td><td style=\"text-align: right;\">           2.22664</td><td style=\"text-align: right;\">      12.3399 </td><td style=\"text-align: right;\"> 1689707321</td><td style=\"text-align: right;\">                   4</td><td>8094e3b1  </td></tr>\n",
       "<tr><td>FSR_Trainable_832961ff</td><td>2023-07-19_03-51-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.361893</td><td style=\"text-align: right;\">0.102216 </td><td>172.26.215.93</td><td style=\"text-align: right;\">405558</td><td style=\"text-align: right;\">0.699276</td><td style=\"text-align: right;\">             2.70163</td><td style=\"text-align: right;\">           1.27918</td><td style=\"text-align: right;\">       2.70163</td><td style=\"text-align: right;\"> 1689706286</td><td style=\"text-align: right;\">                   2</td><td>832961ff  </td></tr>\n",
       "<tr><td>FSR_Trainable_836bac6d</td><td>2023-07-19_03-41-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.3706  </td><td style=\"text-align: right;\">0.110789 </td><td>172.26.215.93</td><td style=\"text-align: right;\">400563</td><td style=\"text-align: right;\">0.699673</td><td style=\"text-align: right;\">             9.51376</td><td style=\"text-align: right;\">           2.10114</td><td style=\"text-align: right;\">       9.51376</td><td style=\"text-align: right;\"> 1689705713</td><td style=\"text-align: right;\">                   4</td><td>836bac6d  </td></tr>\n",
       "<tr><td>FSR_Trainable_84bae381</td><td>2023-07-19_04-18-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.354841</td><td style=\"text-align: right;\">0.0971294</td><td>172.26.215.93</td><td style=\"text-align: right;\">418166</td><td style=\"text-align: right;\">0.647353</td><td style=\"text-align: right;\">            45.8493 </td><td style=\"text-align: right;\">           3.10129</td><td style=\"text-align: right;\">      45.8493 </td><td style=\"text-align: right;\"> 1689707886</td><td style=\"text-align: right;\">                  16</td><td>84bae381  </td></tr>\n",
       "<tr><td>FSR_Trainable_863d9411</td><td>2023-07-19_03-47-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.341537</td><td style=\"text-align: right;\">0.100926 </td><td>172.26.215.93</td><td style=\"text-align: right;\">402243</td><td style=\"text-align: right;\">0.698204</td><td style=\"text-align: right;\">            84.4651 </td><td style=\"text-align: right;\">          10.8987 </td><td style=\"text-align: right;\">      84.4651 </td><td style=\"text-align: right;\"> 1689706030</td><td style=\"text-align: right;\">                   8</td><td>863d9411  </td></tr>\n",
       "<tr><td>FSR_Trainable_86c33214</td><td>2023-07-19_03-45-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.411662</td><td style=\"text-align: right;\">0.115454 </td><td>172.26.215.93</td><td style=\"text-align: right;\">401987</td><td style=\"text-align: right;\">0.71471 </td><td style=\"text-align: right;\">             2.80937</td><td style=\"text-align: right;\">           2.80937</td><td style=\"text-align: right;\">       2.80937</td><td style=\"text-align: right;\"> 1689705929</td><td style=\"text-align: right;\">                   1</td><td>86c33214  </td></tr>\n",
       "<tr><td>FSR_Trainable_8ab3a6de</td><td>2023-07-19_04-08-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.325723</td><td style=\"text-align: right;\">0.0927704</td><td>172.26.215.93</td><td style=\"text-align: right;\">414855</td><td style=\"text-align: right;\">0.607672</td><td style=\"text-align: right;\">           124.426  </td><td style=\"text-align: right;\">           1.22654</td><td style=\"text-align: right;\">     124.426  </td><td style=\"text-align: right;\"> 1689707281</td><td style=\"text-align: right;\">                 100</td><td>8ab3a6de  </td></tr>\n",
       "<tr><td>FSR_Trainable_8ad86243</td><td>2023-07-19_04-00-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.368618</td><td style=\"text-align: right;\">0.10393  </td><td>172.26.215.93</td><td style=\"text-align: right;\">412201</td><td style=\"text-align: right;\">0.701778</td><td style=\"text-align: right;\">             3.06443</td><td style=\"text-align: right;\">           3.06443</td><td style=\"text-align: right;\">       3.06443</td><td style=\"text-align: right;\"> 1689706849</td><td style=\"text-align: right;\">                   1</td><td>8ad86243  </td></tr>\n",
       "<tr><td>FSR_Trainable_8c01a1f0</td><td>2023-07-19_03-48-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.38751 </td><td style=\"text-align: right;\">0.110844 </td><td>172.26.215.93</td><td style=\"text-align: right;\">403670</td><td style=\"text-align: right;\">0.707731</td><td style=\"text-align: right;\">             4.03796</td><td style=\"text-align: right;\">           4.03796</td><td style=\"text-align: right;\">       4.03796</td><td style=\"text-align: right;\"> 1689706080</td><td style=\"text-align: right;\">                   1</td><td>8c01a1f0  </td></tr>\n",
       "<tr><td>FSR_Trainable_8c53b999</td><td>2023-07-19_03-47-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.406784</td><td style=\"text-align: right;\">0.120899 </td><td>172.26.215.93</td><td style=\"text-align: right;\">402983</td><td style=\"text-align: right;\">0.709323</td><td style=\"text-align: right;\">             7.5217 </td><td style=\"text-align: right;\">           7.5217 </td><td style=\"text-align: right;\">       7.5217 </td><td style=\"text-align: right;\"> 1689706045</td><td style=\"text-align: right;\">                   1</td><td>8c53b999  </td></tr>\n",
       "<tr><td>FSR_Trainable_90a4ec38</td><td>2023-07-19_03-56-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.361821</td><td style=\"text-align: right;\">0.102532 </td><td>172.26.215.93</td><td style=\"text-align: right;\">407773</td><td style=\"text-align: right;\">0.699376</td><td style=\"text-align: right;\">             3.30662</td><td style=\"text-align: right;\">           1.52513</td><td style=\"text-align: right;\">       3.30662</td><td style=\"text-align: right;\"> 1689706563</td><td style=\"text-align: right;\">                   2</td><td>90a4ec38  </td></tr>\n",
       "<tr><td>FSR_Trainable_9288852c</td><td>2023-07-19_04-20-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.342291</td><td style=\"text-align: right;\">0.0994743</td><td>172.26.215.93</td><td style=\"text-align: right;\">419813</td><td style=\"text-align: right;\">0.688613</td><td style=\"text-align: right;\">            11.3715 </td><td style=\"text-align: right;\">           5.18402</td><td style=\"text-align: right;\">      11.3715 </td><td style=\"text-align: right;\"> 1689708019</td><td style=\"text-align: right;\">                   2</td><td>9288852c  </td></tr>\n",
       "<tr><td>FSR_Trainable_942275ec</td><td>2023-07-19_04-29-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.285481</td><td style=\"text-align: right;\">0.0786126</td><td>172.26.215.93</td><td style=\"text-align: right;\">419592</td><td style=\"text-align: right;\">0.565224</td><td style=\"text-align: right;\">           549.917  </td><td style=\"text-align: right;\">           5.45722</td><td style=\"text-align: right;\">     549.917  </td><td style=\"text-align: right;\"> 1689708561</td><td style=\"text-align: right;\">                 100</td><td>942275ec  </td></tr>\n",
       "<tr><td>FSR_Trainable_9d94984e</td><td>2023-07-19_04-01-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.316869</td><td style=\"text-align: right;\">0.0860986</td><td>172.26.215.93</td><td style=\"text-align: right;\">410554</td><td style=\"text-align: right;\">0.592788</td><td style=\"text-align: right;\">           120.838  </td><td style=\"text-align: right;\">           1.43057</td><td style=\"text-align: right;\">     120.838  </td><td style=\"text-align: right;\"> 1689706864</td><td style=\"text-align: right;\">                 100</td><td>9d94984e  </td></tr>\n",
       "<tr><td>FSR_Trainable_9f64d024</td><td>2023-07-19_03-50-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.326038</td><td style=\"text-align: right;\">0.0919085</td><td>172.26.215.93</td><td style=\"text-align: right;\">404316</td><td style=\"text-align: right;\">0.609048</td><td style=\"text-align: right;\">           121.67   </td><td style=\"text-align: right;\">           1.27473</td><td style=\"text-align: right;\">     121.67   </td><td style=\"text-align: right;\"> 1689706237</td><td style=\"text-align: right;\">                 100</td><td>9f64d024  </td></tr>\n",
       "<tr><td>FSR_Trainable_a00696f1</td><td>2023-07-19_03-38-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.440095</td><td style=\"text-align: right;\">0.112994 </td><td>172.26.215.93</td><td style=\"text-align: right;\">398177</td><td style=\"text-align: right;\">0.724108</td><td style=\"text-align: right;\">            17.3507 </td><td style=\"text-align: right;\">           7.18514</td><td style=\"text-align: right;\">      17.3507 </td><td style=\"text-align: right;\"> 1689705536</td><td style=\"text-align: right;\">                   2</td><td>a00696f1  </td></tr>\n",
       "<tr><td>FSR_Trainable_a728f25d</td><td>2023-07-19_04-00-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.323537</td><td style=\"text-align: right;\">0.089476 </td><td>172.26.215.93</td><td style=\"text-align: right;\">410301</td><td style=\"text-align: right;\">0.600593</td><td style=\"text-align: right;\">           121.59   </td><td style=\"text-align: right;\">           1.416  </td><td style=\"text-align: right;\">     121.59   </td><td style=\"text-align: right;\"> 1689706848</td><td style=\"text-align: right;\">                 100</td><td>a728f25d  </td></tr>\n",
       "<tr><td>FSR_Trainable_a802a962</td><td>2023-07-19_04-23-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.298926</td><td style=\"text-align: right;\">0.0809895</td><td>172.26.215.93</td><td style=\"text-align: right;\">418484</td><td style=\"text-align: right;\">0.57742 </td><td style=\"text-align: right;\">           295.175  </td><td style=\"text-align: right;\">           3.10946</td><td style=\"text-align: right;\">     295.175  </td><td style=\"text-align: right;\"> 1689708210</td><td style=\"text-align: right;\">                 100</td><td>a802a962  </td></tr>\n",
       "<tr><td>FSR_Trainable_ac5a6022</td><td>2023-07-19_04-07-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.342214</td><td style=\"text-align: right;\">0.0915835</td><td>172.26.215.93</td><td style=\"text-align: right;\">414169</td><td style=\"text-align: right;\">0.620217</td><td style=\"text-align: right;\">           148.198  </td><td style=\"text-align: right;\">           1.40416</td><td style=\"text-align: right;\">     148.198  </td><td style=\"text-align: right;\"> 1689707278</td><td style=\"text-align: right;\">                 100</td><td>ac5a6022  </td></tr>\n",
       "<tr><td>FSR_Trainable_ae002d89</td><td>2023-07-19_04-11-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.31689 </td><td style=\"text-align: right;\">0.0852393</td><td>172.26.215.93</td><td style=\"text-align: right;\">415853</td><td style=\"text-align: right;\">0.601174</td><td style=\"text-align: right;\">           169.972  </td><td style=\"text-align: right;\">           1.65667</td><td style=\"text-align: right;\">     169.972  </td><td style=\"text-align: right;\"> 1689707507</td><td style=\"text-align: right;\">                 100</td><td>ae002d89  </td></tr>\n",
       "<tr><td>FSR_Trainable_b32d5df2</td><td>2023-07-19_03-38-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.467   </td><td style=\"text-align: right;\">0.119898 </td><td>172.26.215.93</td><td style=\"text-align: right;\">397860</td><td style=\"text-align: right;\">0.797122</td><td style=\"text-align: right;\">            15.9915 </td><td style=\"text-align: right;\">          15.9915 </td><td style=\"text-align: right;\">      15.9915 </td><td style=\"text-align: right;\"> 1689705519</td><td style=\"text-align: right;\">                   1</td><td>b32d5df2  </td></tr>\n",
       "<tr><td>FSR_Trainable_b47f87c1</td><td>2023-07-19_03-45-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.343251</td><td style=\"text-align: right;\">0.0988394</td><td>172.26.215.93</td><td style=\"text-align: right;\">401495</td><td style=\"text-align: right;\">0.663622</td><td style=\"text-align: right;\">           123.703  </td><td style=\"text-align: right;\">           1.20028</td><td style=\"text-align: right;\">     123.703  </td><td style=\"text-align: right;\"> 1689705913</td><td style=\"text-align: right;\">                 100</td><td>b47f87c1  </td></tr>\n",
       "<tr><td>FSR_Trainable_b531ab6b</td><td>2023-07-19_04-27-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.289774</td><td style=\"text-align: right;\">0.077495 </td><td>172.26.215.93</td><td style=\"text-align: right;\">420606</td><td style=\"text-align: right;\">0.561751</td><td style=\"text-align: right;\">           302.451  </td><td style=\"text-align: right;\">           2.96484</td><td style=\"text-align: right;\">     302.451  </td><td style=\"text-align: right;\"> 1689708456</td><td style=\"text-align: right;\">                 100</td><td>b531ab6b  </td></tr>\n",
       "<tr><td>FSR_Trainable_b8de1926</td><td>2023-07-19_03-59-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.341978</td><td style=\"text-align: right;\">0.0979116</td><td>172.26.215.93</td><td style=\"text-align: right;\">410074</td><td style=\"text-align: right;\">0.642141</td><td style=\"text-align: right;\">            53.6015 </td><td style=\"text-align: right;\">           2.06358</td><td style=\"text-align: right;\">      53.6015 </td><td style=\"text-align: right;\"> 1689706762</td><td style=\"text-align: right;\">                  32</td><td>b8de1926  </td></tr>\n",
       "<tr><td>FSR_Trainable_b912fe9e</td><td>2023-07-19_03-58-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.354437</td><td style=\"text-align: right;\">0.104763 </td><td>172.26.215.93</td><td style=\"text-align: right;\">409840</td><td style=\"text-align: right;\">0.687911</td><td style=\"text-align: right;\">             5.9004 </td><td style=\"text-align: right;\">           1.19245</td><td style=\"text-align: right;\">       5.9004 </td><td style=\"text-align: right;\"> 1689706700</td><td style=\"text-align: right;\">                   4</td><td>b912fe9e  </td></tr>\n",
       "<tr><td>FSR_Trainable_ca336f36</td><td>2023-07-19_04-33-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.275663</td><td style=\"text-align: right;\">0.0738985</td><td>172.26.215.93</td><td style=\"text-align: right;\">421242</td><td style=\"text-align: right;\">0.542824</td><td style=\"text-align: right;\">           349.579  </td><td style=\"text-align: right;\">           2.46339</td><td style=\"text-align: right;\">     349.579  </td><td style=\"text-align: right;\"> 1689708826</td><td style=\"text-align: right;\">                 100</td><td>ca336f36  </td></tr>\n",
       "<tr><td>FSR_Trainable_ca3746b5</td><td>2023-07-19_04-19-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.355313</td><td style=\"text-align: right;\">0.099413 </td><td>172.26.215.93</td><td style=\"text-align: right;\">418955</td><td style=\"text-align: right;\">0.66428 </td><td style=\"text-align: right;\">            23.7157 </td><td style=\"text-align: right;\">           2.82887</td><td style=\"text-align: right;\">      23.7157 </td><td style=\"text-align: right;\"> 1689707972</td><td style=\"text-align: right;\">                   8</td><td>ca3746b5  </td></tr>\n",
       "<tr><td>FSR_Trainable_cc28ba53</td><td>2023-07-19_03-45-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.355039</td><td style=\"text-align: right;\">0.10278  </td><td>172.26.215.93</td><td style=\"text-align: right;\">399397</td><td style=\"text-align: right;\">0.689728</td><td style=\"text-align: right;\">           296.685  </td><td style=\"text-align: right;\">           2.82662</td><td style=\"text-align: right;\">     296.685  </td><td style=\"text-align: right;\"> 1689705904</td><td style=\"text-align: right;\">                 100</td><td>cc28ba53  </td></tr>\n",
       "<tr><td>FSR_Trainable_cd21359b</td><td>2023-07-19_03-58-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.348156</td><td style=\"text-align: right;\">0.10296  </td><td>172.26.215.93</td><td style=\"text-align: right;\">409388</td><td style=\"text-align: right;\">0.697576</td><td style=\"text-align: right;\">             7.95351</td><td style=\"text-align: right;\">           3.68666</td><td style=\"text-align: right;\">       7.95351</td><td style=\"text-align: right;\"> 1689706680</td><td style=\"text-align: right;\">                   2</td><td>cd21359b  </td></tr>\n",
       "<tr><td>FSR_Trainable_cfadcfb5</td><td>2023-07-19_03-54-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.32982 </td><td style=\"text-align: right;\">0.0931078</td><td>172.26.215.93</td><td style=\"text-align: right;\">406098</td><td style=\"text-align: right;\">0.61944 </td><td style=\"text-align: right;\">           116.214  </td><td style=\"text-align: right;\">           1.19978</td><td style=\"text-align: right;\">     116.214  </td><td style=\"text-align: right;\"> 1689706483</td><td style=\"text-align: right;\">                 100</td><td>cfadcfb5  </td></tr>\n",
       "<tr><td>FSR_Trainable_d030706f</td><td>2023-07-19_04-05-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.385686</td><td style=\"text-align: right;\">0.106078 </td><td>172.26.215.93</td><td style=\"text-align: right;\">414385</td><td style=\"text-align: right;\">0.712504</td><td style=\"text-align: right;\">             2.73864</td><td style=\"text-align: right;\">           2.73864</td><td style=\"text-align: right;\">       2.73864</td><td style=\"text-align: right;\"> 1689707130</td><td style=\"text-align: right;\">                   1</td><td>d030706f  </td></tr>\n",
       "<tr><td>FSR_Trainable_d4909022</td><td>2023-07-19_03-56-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.321222</td><td style=\"text-align: right;\">0.0896438</td><td>172.26.215.93</td><td style=\"text-align: right;\">406289</td><td style=\"text-align: right;\">0.607436</td><td style=\"text-align: right;\">           196.798  </td><td style=\"text-align: right;\">           1.80368</td><td style=\"text-align: right;\">     196.798  </td><td style=\"text-align: right;\"> 1689706573</td><td style=\"text-align: right;\">                 100</td><td>d4909022  </td></tr>\n",
       "<tr><td>FSR_Trainable_d8d12509</td><td>2023-07-19_03-47-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.323218</td><td style=\"text-align: right;\">0.0910123</td><td>172.26.215.93</td><td style=\"text-align: right;\">401810</td><td style=\"text-align: right;\">0.597032</td><td style=\"text-align: right;\">           142.08   </td><td style=\"text-align: right;\">           1.18767</td><td style=\"text-align: right;\">     142.08   </td><td style=\"text-align: right;\"> 1689706072</td><td style=\"text-align: right;\">                 100</td><td>d8d12509  </td></tr>\n",
       "<tr><td>FSR_Trainable_d98df421</td><td>2023-07-19_04-02-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.314442</td><td style=\"text-align: right;\">0.0839548</td><td>172.26.215.93</td><td style=\"text-align: right;\">411038</td><td style=\"text-align: right;\">0.617172</td><td style=\"text-align: right;\">           168.451  </td><td style=\"text-align: right;\">           1.70021</td><td style=\"text-align: right;\">     168.451  </td><td style=\"text-align: right;\"> 1689706962</td><td style=\"text-align: right;\">                 100</td><td>d98df421  </td></tr>\n",
       "<tr><td>FSR_Trainable_db1779a3</td><td>2023-07-19_04-09-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.293412</td><td style=\"text-align: right;\">0.0805268</td><td>172.26.215.93</td><td style=\"text-align: right;\">412858</td><td style=\"text-align: right;\">0.571899</td><td style=\"text-align: right;\">           455.664  </td><td style=\"text-align: right;\">           4.98491</td><td style=\"text-align: right;\">     455.664  </td><td style=\"text-align: right;\"> 1689707354</td><td style=\"text-align: right;\">                 100</td><td>db1779a3  </td></tr>\n",
       "<tr><td>FSR_Trainable_def25e2f</td><td>2023-07-19_04-05-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.380963</td><td style=\"text-align: right;\">0.116937 </td><td>172.26.215.93</td><td style=\"text-align: right;\">414610</td><td style=\"text-align: right;\">0.701607</td><td style=\"text-align: right;\">             1.52596</td><td style=\"text-align: right;\">           1.52596</td><td style=\"text-align: right;\">       1.52596</td><td style=\"text-align: right;\"> 1689707139</td><td style=\"text-align: right;\">                   1</td><td>def25e2f  </td></tr>\n",
       "<tr><td>FSR_Trainable_dfc34fb3</td><td>2023-07-19_03-55-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.352372</td><td style=\"text-align: right;\">0.104247 </td><td>172.26.215.93</td><td style=\"text-align: right;\">407093</td><td style=\"text-align: right;\">0.675982</td><td style=\"text-align: right;\">            19.3675 </td><td style=\"text-align: right;\">           2.16074</td><td style=\"text-align: right;\">      19.3675 </td><td style=\"text-align: right;\"> 1689706514</td><td style=\"text-align: right;\">                   8</td><td>dfc34fb3  </td></tr>\n",
       "<tr><td>FSR_Trainable_e2404c92</td><td>2023-07-19_03-58-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.326587</td><td style=\"text-align: right;\">0.0927203</td><td>172.26.215.93</td><td style=\"text-align: right;\">408019</td><td style=\"text-align: right;\">0.615037</td><td style=\"text-align: right;\">           123.853  </td><td style=\"text-align: right;\">           1.29886</td><td style=\"text-align: right;\">     123.853  </td><td style=\"text-align: right;\"> 1689706716</td><td style=\"text-align: right;\">                 100</td><td>e2404c92  </td></tr>\n",
       "<tr><td>FSR_Trainable_e3457318</td><td>2023-07-19_03-56-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.317955</td><td style=\"text-align: right;\">0.09101  </td><td>172.26.215.93</td><td style=\"text-align: right;\">406825</td><td style=\"text-align: right;\">0.605138</td><td style=\"text-align: right;\">           159.59   </td><td style=\"text-align: right;\">           1.70923</td><td style=\"text-align: right;\">     159.59   </td><td style=\"text-align: right;\"> 1689706612</td><td style=\"text-align: right;\">                 100</td><td>e3457318  </td></tr>\n",
       "<tr><td>FSR_Trainable_e7085d33</td><td>2023-07-19_04-03-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.320918</td><td style=\"text-align: right;\">0.0887209</td><td>172.26.215.93</td><td style=\"text-align: right;\">412632</td><td style=\"text-align: right;\">0.599781</td><td style=\"text-align: right;\">           130.447  </td><td style=\"text-align: right;\">           1.18915</td><td style=\"text-align: right;\">     130.447  </td><td style=\"text-align: right;\"> 1689707010</td><td style=\"text-align: right;\">                 100</td><td>e7085d33  </td></tr>\n",
       "<tr><td>FSR_Trainable_e9ddb955</td><td>2023-07-19_04-16-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.302331</td><td style=\"text-align: right;\">0.0812595</td><td>172.26.215.93</td><td style=\"text-align: right;\">417356</td><td style=\"text-align: right;\">0.586452</td><td style=\"text-align: right;\">           279.772  </td><td style=\"text-align: right;\">           2.61312</td><td style=\"text-align: right;\">     279.772  </td><td style=\"text-align: right;\"> 1689707810</td><td style=\"text-align: right;\">                 100</td><td>e9ddb955  </td></tr>\n",
       "<tr><td>FSR_Trainable_e9eabb15</td><td>2023-07-19_04-22-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.29302 </td><td style=\"text-align: right;\">0.0810606</td><td>172.26.215.93</td><td style=\"text-align: right;\">417983</td><td style=\"text-align: right;\">0.581144</td><td style=\"text-align: right;\">           287.922  </td><td style=\"text-align: right;\">           2.71421</td><td style=\"text-align: right;\">     287.922  </td><td style=\"text-align: right;\"> 1689708129</td><td style=\"text-align: right;\">                 100</td><td>e9eabb15  </td></tr>\n",
       "<tr><td>FSR_Trainable_ec364354</td><td>2023-07-19_04-00-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.357594</td><td style=\"text-align: right;\">0.101283 </td><td>172.26.215.93</td><td style=\"text-align: right;\">411720</td><td style=\"text-align: right;\">0.701559</td><td style=\"text-align: right;\">             1.94898</td><td style=\"text-align: right;\">           1.94898</td><td style=\"text-align: right;\">       1.94898</td><td style=\"text-align: right;\"> 1689706816</td><td style=\"text-align: right;\">                   1</td><td>ec364354  </td></tr>\n",
       "<tr><td>FSR_Trainable_ed0248b3</td><td>2023-07-19_04-00-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.358524</td><td style=\"text-align: right;\">0.10232  </td><td>172.26.215.93</td><td style=\"text-align: right;\">411950</td><td style=\"text-align: right;\">0.69951 </td><td style=\"text-align: right;\">             4.25965</td><td style=\"text-align: right;\">           2.02172</td><td style=\"text-align: right;\">       4.25965</td><td style=\"text-align: right;\"> 1689706835</td><td style=\"text-align: right;\">                   2</td><td>ed0248b3  </td></tr>\n",
       "<tr><td>FSR_Trainable_ee2bdf4e</td><td>2023-07-19_03-52-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.331387</td><td style=\"text-align: right;\">0.0929432</td><td>172.26.215.93</td><td style=\"text-align: right;\">404664</td><td style=\"text-align: right;\">0.613109</td><td style=\"text-align: right;\">           122.909  </td><td style=\"text-align: right;\">           1.26381</td><td style=\"text-align: right;\">     122.909  </td><td style=\"text-align: right;\"> 1689706358</td><td style=\"text-align: right;\">                 100</td><td>ee2bdf4e  </td></tr>\n",
       "<tr><td>FSR_Trainable_f002d9d6</td><td>2023-07-19_03-46-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.364974</td><td style=\"text-align: right;\">0.103994 </td><td>172.26.215.93</td><td style=\"text-align: right;\">400087</td><td style=\"text-align: right;\">0.673771</td><td style=\"text-align: right;\">           339.103  </td><td style=\"text-align: right;\">           3.29655</td><td style=\"text-align: right;\">     339.103  </td><td style=\"text-align: right;\"> 1689705992</td><td style=\"text-align: right;\">                 100</td><td>f002d9d6  </td></tr>\n",
       "<tr><td>FSR_Trainable_f03a8d9b</td><td>2023-07-19_03-50-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.339342</td><td style=\"text-align: right;\">0.0959611</td><td>172.26.215.93</td><td style=\"text-align: right;\">404101</td><td style=\"text-align: right;\">0.627957</td><td style=\"text-align: right;\">           122.409  </td><td style=\"text-align: right;\">           1.23341</td><td style=\"text-align: right;\">     122.409  </td><td style=\"text-align: right;\"> 1689706225</td><td style=\"text-align: right;\">                 100</td><td>f03a8d9b  </td></tr>\n",
       "<tr><td>FSR_Trainable_f451ac61</td><td>2023-07-19_04-17-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.300648</td><td style=\"text-align: right;\">0.0821559</td><td>172.26.215.93</td><td style=\"text-align: right;\">417533</td><td style=\"text-align: right;\">0.574104</td><td style=\"text-align: right;\">           280.051  </td><td style=\"text-align: right;\">           2.63517</td><td style=\"text-align: right;\">     280.051  </td><td style=\"text-align: right;\"> 1689707823</td><td style=\"text-align: right;\">                 100</td><td>f451ac61  </td></tr>\n",
       "<tr><td>FSR_Trainable_f5ff39b5</td><td>2023-07-19_03-52-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.350629</td><td style=\"text-align: right;\">0.096778 </td><td>172.26.215.93</td><td style=\"text-align: right;\">404865</td><td style=\"text-align: right;\">0.646239</td><td style=\"text-align: right;\">           102.862  </td><td style=\"text-align: right;\">           1.00806</td><td style=\"text-align: right;\">     102.862  </td><td style=\"text-align: right;\"> 1689706350</td><td style=\"text-align: right;\">                 100</td><td>f5ff39b5  </td></tr>\n",
       "<tr><td>FSR_Trainable_f602708e</td><td>2023-07-19_04-00-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.427947</td><td style=\"text-align: right;\">0.110138 </td><td>172.26.215.93</td><td style=\"text-align: right;\">411489</td><td style=\"text-align: right;\">0.743553</td><td style=\"text-align: right;\">             1.58073</td><td style=\"text-align: right;\">           1.58073</td><td style=\"text-align: right;\">       1.58073</td><td style=\"text-align: right;\"> 1689706801</td><td style=\"text-align: right;\">                   1</td><td>f602708e  </td></tr>\n",
       "<tr><td>FSR_Trainable_f763ff5d</td><td>2023-07-19_03-57-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.359818</td><td style=\"text-align: right;\">0.100525 </td><td>172.26.215.93</td><td style=\"text-align: right;\">408231</td><td style=\"text-align: right;\">0.653478</td><td style=\"text-align: right;\">            59.4515 </td><td style=\"text-align: right;\">           1.7022 </td><td style=\"text-align: right;\">      59.4515 </td><td style=\"text-align: right;\"> 1689706653</td><td style=\"text-align: right;\">                  32</td><td>f763ff5d  </td></tr>\n",
       "<tr><td>FSR_Trainable_ffedeb7a</td><td>2023-07-19_03-51-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.416396</td><td style=\"text-align: right;\">0.105935 </td><td>172.26.215.93</td><td style=\"text-align: right;\">405321</td><td style=\"text-align: right;\">0.726704</td><td style=\"text-align: right;\">             1.41982</td><td style=\"text-align: right;\">           1.41982</td><td style=\"text-align: right;\">       1.41982</td><td style=\"text-align: right;\"> 1689706267</td><td style=\"text-align: right;\">                   1</td><td>ffedeb7a  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_5d3e9619_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_03-37-53/wandb/run-20230719_033804-5d3e9619\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Syncing run FSR_Trainable_5d3e9619\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5d3e9619\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_5a37feb1_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_03-37-58/wandb/run-20230719_033813-5a37feb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: Syncing run FSR_Trainable_5a37feb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5a37feb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_22e5e3fc_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_03-38-06/wandb/run-20230719_033821-22e5e3fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: Syncing run FSR_Trainable_22e5e3fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/22e5e3fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb:                      mae 0.45791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb:                     mape 0.11101\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb:                     rmse 0.76797\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb:       time_since_restore 3.92258\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb:         time_this_iter_s 3.92258\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb:             time_total_s 3.92258\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb:                timestamp 1689705498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: 🚀 View run FSR_Trainable_22e5e3fc at: https://wandb.ai/seokjin/FSR-prediction/runs/22e5e3fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397859)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033821-22e5e3fc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_b32d5df2_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_03-38-14/wandb/run-20230719_033833-b32d5df2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: Syncing run FSR_Trainable_b32d5df2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b32d5df2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_a00696f1_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_03-38-23/wandb/run-20230719_033845-a00696f1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: Syncing run FSR_Trainable_a00696f1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a00696f1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb:                      mae 0.467\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb:                     mape 0.1199\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb:                     rmse 0.79712\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb:       time_since_restore 15.99147\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb:         time_this_iter_s 15.99147\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb:             time_total_s 15.99147\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb:                timestamp 1689705519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: 🚀 View run FSR_Trainable_b32d5df2 at: https://wandb.ai/seokjin/FSR-prediction/runs/b32d5df2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398055)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033833-b32d5df2/logs\n",
      "2023-07-19 03:38:48,968\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.612 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:38:48,971\tWARNING util.py:315 -- The `process_trial_result` operation took 1.616 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:38:48,973\tWARNING util.py:315 -- Processing trial results took 1.618 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:38:48,974\tWARNING util.py:315 -- The `process_trial_result` operation took 1.619 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb:                      mae 0.44009\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb:                     mape 0.11299\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb:                     rmse 0.72411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb:       time_since_restore 17.35071\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb:         time_this_iter_s 7.18514\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb:             time_total_s 17.35071\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb:                timestamp 1689705536\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: 🚀 View run FSR_Trainable_a00696f1 at: https://wandb.ai/seokjin/FSR-prediction/runs/a00696f1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398273)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033845-a00696f1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 03:39:01,419\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.547 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:39:01,423\tWARNING util.py:315 -- The `process_trial_result` operation took 2.552 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:39:01,425\tWARNING util.py:315 -- Processing trial results took 2.554 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:39:01,427\tWARNING util.py:315 -- The `process_trial_result` operation took 2.555 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_498b72ce_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_03-38-37/wandb/run-20230719_033901-498b72ce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Syncing run FSR_Trainable_498b72ce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/498b72ce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033901-498b72ce/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033901-498b72ce/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033901-498b72ce/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033901-498b72ce/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033901-498b72ce/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033901-498b72ce/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033901-498b72ce/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033901-498b72ce/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033901-498b72ce/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033901-498b72ce/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033901-498b72ce/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033901-498b72ce/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033901-498b72ce/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398514)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033901-498b72ce/logs\n",
      "2023-07-19 03:39:11,977\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.125 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:39:11,982\tWARNING util.py:315 -- The `process_trial_result` operation took 2.131 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:39:11,984\tWARNING util.py:315 -- Processing trial results took 2.133 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:39:11,987\tWARNING util.py:315 -- The `process_trial_result` operation took 2.135 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_19714ab0_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_03-38-53/wandb/run-20230719_033913-19714ab0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: Syncing run FSR_Trainable_19714ab0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/19714ab0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb:                      mae 0.44903\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb:                     mape 0.11674\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb:                     rmse 0.74466\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb:       time_since_restore 5.72931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb:         time_this_iter_s 2.68103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb:             time_total_s 5.72931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb:                timestamp 1689705554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: 🚀 View run FSR_Trainable_19714ab0 at: https://wandb.ai/seokjin/FSR-prediction/runs/19714ab0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398763)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033913-19714ab0/logs\n",
      "2023-07-19 03:39:25,150\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.028 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:39:25,156\tWARNING util.py:315 -- The `process_trial_result` operation took 2.034 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:39:25,157\tWARNING util.py:315 -- Processing trial results took 2.036 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:39:25,161\tWARNING util.py:315 -- The `process_trial_result` operation took 2.039 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_01a407a0_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_03-39-06/wandb/run-20230719_033925-01a407a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Syncing run FSR_Trainable_01a407a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/01a407a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_1f71a826_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_03-39-17/wandb/run-20230719_033937-1f71a826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: Syncing run FSR_Trainable_1f71a826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1f71a826\n",
      "2023-07-19 03:39:39,512\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.984 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:39:39,517\tWARNING util.py:315 -- The `process_trial_result` operation took 2.991 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:39:39,519\tWARNING util.py:315 -- Processing trial results took 2.992 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:39:39,521\tWARNING util.py:315 -- The `process_trial_result` operation took 2.994 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb:                      mae 0.35257\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb:                     mape 0.09124\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb:                     rmse 0.72578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb:       time_since_restore 13.5167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb:         time_this_iter_s 6.5831\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb:             time_total_s 13.5167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb:                timestamp 1689705586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: 🚀 View run FSR_Trainable_1f71a826 at: https://wandb.ai/seokjin/FSR-prediction/runs/1f71a826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399213)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033937-1f71a826/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb:                      mae ███████▇▇▇▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb:                     mape ██▇▆▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb:                     rmse ▃▄▄▆▇▇██▇▆▅▅▅▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb:         time_this_iter_s ▆▂▁▂▄▃▂▄▂▃█▅▅▄▄▅▄▃▃▄▅▄▃▂▂▄▃▂▆▆▅▄▄▅▄▅▅▅▅▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: \n",
      "2023-07-19 03:40:01,363\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.075 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:40:01,369\tWARNING util.py:315 -- The `process_trial_result` operation took 2.082 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:40:01,371\tWARNING util.py:315 -- Processing trial results took 2.084 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:40:01,373\tWARNING util.py:315 -- The `process_trial_result` operation took 2.086 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397526)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_cc28ba53_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-39-29/wandb/run-20230719_034002-cc28ba53\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: Syncing run FSR_Trainable_cc28ba53\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cc28ba53\n",
      "2023-07-19 03:40:11,387\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.248 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:40:11,390\tWARNING util.py:315 -- The `process_trial_result` operation took 2.251 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:40:11,394\tWARNING util.py:315 -- Processing trial results took 2.255 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:40:11,395\tWARNING util.py:315 -- The `process_trial_result` operation took 2.257 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_2285c2f9_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-39-55/wandb/run-20230719_034013-2285c2f9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: Syncing run FSR_Trainable_2285c2f9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2285c2f9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb:                      mae 2.20404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb:                     mape 0.38911\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb:                     rmse 2.63241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb:       time_since_restore 3.16505\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb:         time_this_iter_s 3.16505\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb:             time_total_s 3.16505\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb:                timestamp 1689705609\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: 🚀 View run FSR_Trainable_2285c2f9 at: https://wandb.ai/seokjin/FSR-prediction/runs/2285c2f9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399683)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034013-2285c2f9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_1c20a35b_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-40-05/wandb/run-20230719_034030-1c20a35b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: Syncing run FSR_Trainable_1c20a35b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1c20a35b\n",
      "2023-07-19 03:40:31,342\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.692 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:40:31,347\tWARNING util.py:315 -- The `process_trial_result` operation took 1.698 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:40:31,349\tWARNING util.py:315 -- Processing trial results took 1.700 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:40:31,351\tWARNING util.py:315 -- The `process_trial_result` operation took 1.703 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb:                      mae 0.46278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb:                     mape 0.11813\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb:                     rmse 0.77402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb:       time_since_restore 6.76655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb:         time_this_iter_s 6.76655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb:             time_total_s 6.76655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb:                timestamp 1689705629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: 🚀 View run FSR_Trainable_1c20a35b at: https://wandb.ai/seokjin/FSR-prediction/runs/1c20a35b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399911)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034030-1c20a35b/logs\n",
      "2023-07-19 03:40:48,118\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.018 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:40:48,122\tWARNING util.py:315 -- The `process_trial_result` operation took 2.022 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:40:48,124\tWARNING util.py:315 -- Processing trial results took 2.024 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:40:48,126\tWARNING util.py:315 -- The `process_trial_result` operation took 2.026 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_f002d9d6_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-40-22/wandb/run-20230719_034049-f002d9d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: Syncing run FSR_Trainable_f002d9d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f002d9d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb:                      mae ███▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb:                     mape ███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb:                     rmse ███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb:         time_this_iter_s ▆▂▅▃▅█▃▆▃▃▅▃▃▄▂▇▄▆▄▆▄▃▁▅▃▇▅▃▄▅▃▄▃▆▃▃▄▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb:                      mae 0.34683\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb:                     mape 0.09993\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb:                     rmse 0.69738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb:       time_since_restore 165.16559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb:         time_this_iter_s 1.4895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb:             time_total_s 165.16559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb:                timestamp 1689705669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: 🚀 View run FSR_Trainable_5a37feb1 at: https://wandb.ai/seokjin/FSR-prediction/runs/5a37feb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=397683)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_033813-5a37feb1/logs\n",
      "2023-07-19 03:41:27,315\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.548 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:41:27,320\tWARNING util.py:315 -- The `process_trial_result` operation took 2.554 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:41:27,322\tWARNING util.py:315 -- Processing trial results took 2.556 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:41:27,324\tWARNING util.py:315 -- The `process_trial_result` operation took 2.558 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_286ac475_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-40-42/wandb/run-20230719_034128-286ac475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: Syncing run FSR_Trainable_286ac475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/286ac475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb:                      mae 0.41242\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb:                     mape 0.10959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb:                     rmse 0.72347\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb:       time_since_restore 7.89294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb:         time_this_iter_s 3.71115\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb:             time_total_s 7.89294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb:                timestamp 1689705691\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: 🚀 View run FSR_Trainable_286ac475 at: https://wandb.ai/seokjin/FSR-prediction/runs/286ac475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400389)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034128-286ac475/logs\n",
      "2023-07-19 03:41:47,019\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.075 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:41:47,024\tWARNING util.py:315 -- The `process_trial_result` operation took 2.081 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:41:47,025\tWARNING util.py:315 -- Processing trial results took 2.082 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:41:47,027\tWARNING util.py:315 -- The `process_trial_result` operation took 2.084 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_836bac6d_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-41-20/wandb/run-20230719_034149-836bac6d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: Syncing run FSR_Trainable_836bac6d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/836bac6d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb:                      mae █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb:                     mape █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb:                     rmse █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb:         time_this_iter_s █▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb:                      mae 0.3706\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb:                     mape 0.11079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb:                     rmse 0.69967\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb:       time_since_restore 9.51376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb:         time_this_iter_s 2.10114\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb:             time_total_s 9.51376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb:                timestamp 1689705713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: 🚀 View run FSR_Trainable_836bac6d at: https://wandb.ai/seokjin/FSR-prediction/runs/836bac6d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400620)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034149-836bac6d/logs\n",
      "2023-07-19 03:42:08,704\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.411 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:42:08,710\tWARNING util.py:315 -- The `process_trial_result` operation took 2.418 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:42:08,714\tWARNING util.py:315 -- Processing trial results took 2.421 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:42:08,717\tWARNING util.py:315 -- The `process_trial_result` operation took 2.425 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_79618f04_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-41-42/wandb/run-20230719_034211-79618f04\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: Syncing run FSR_Trainable_79618f04\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/79618f04\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb:                      mae █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb:                     mape █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb:                     rmse █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb:         time_this_iter_s █▃▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb:                timestamp ▁▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb:                      mae 0.35956\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb:                     mape 0.10206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb:                     rmse 0.70194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb:       time_since_restore 5.9221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb:         time_this_iter_s 1.21854\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb:             time_total_s 5.9221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb:                timestamp 1689705732\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: 🚀 View run FSR_Trainable_79618f04 at: https://wandb.ai/seokjin/FSR-prediction/runs/79618f04\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400853)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034211-79618f04/logs\n",
      "2023-07-19 03:42:28,584\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.109 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:42:28,589\tWARNING util.py:315 -- The `process_trial_result` operation took 2.115 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:42:28,591\tWARNING util.py:315 -- Processing trial results took 2.117 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:42:28,593\tWARNING util.py:315 -- The `process_trial_result` operation took 2.119 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_30156eff_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-42-04/wandb/run-20230719_034230-30156eff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: Syncing run FSR_Trainable_30156eff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/30156eff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb:                      mae █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb:                     mape █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb:                     rmse █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb:         time_this_iter_s █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb:                      mae 0.36516\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb:                     mape 0.10903\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb:                     rmse 0.70049\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb:       time_since_restore 8.10773\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb:         time_this_iter_s 1.83583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb:             time_total_s 8.10773\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb:                timestamp 1689705754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: 🚀 View run FSR_Trainable_30156eff at: https://wandb.ai/seokjin/FSR-prediction/runs/30156eff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401085)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034230-30156eff/logs\n",
      "2023-07-19 03:42:49,184\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.514 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:42:49,196\tWARNING util.py:315 -- The `process_trial_result` operation took 2.527 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:42:49,202\tWARNING util.py:315 -- Processing trial results took 2.533 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:42:49,205\tWARNING util.py:315 -- The `process_trial_result` operation took 2.537 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_191e5a09_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-42-24/wandb/run-20230719_034252-191e5a09\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: Syncing run FSR_Trainable_191e5a09\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/191e5a09\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb:                      mae █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb:                     mape █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb:                     rmse █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb:         time_this_iter_s █▆▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb:                      mae 0.37133\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb:                     mape 0.11029\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb:                     rmse 0.70179\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb:       time_since_restore 7.66508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb:         time_this_iter_s 1.73652\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb:             time_total_s 7.66508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb:                timestamp 1689705774\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: 🚀 View run FSR_Trainable_191e5a09 at: https://wandb.ai/seokjin/FSR-prediction/runs/191e5a09\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401318)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034252-191e5a09/logs\n",
      "2023-07-19 03:43:09,645\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.183 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:43:09,651\tWARNING util.py:315 -- The `process_trial_result` operation took 2.190 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:43:09,653\tWARNING util.py:315 -- Processing trial results took 2.192 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:43:09,657\tWARNING util.py:315 -- The `process_trial_result` operation took 2.196 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_b47f87c1_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-42-44/wandb/run-20230719_034312-b47f87c1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: Syncing run FSR_Trainable_b47f87c1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b47f87c1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb:                      mae █▆▄▃▂▁▁▂▃▃▄▄▅▅▅▅▆▆▆▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb:                     mape █▇▇▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb:                     rmse ███▇▇▇▇▆▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb:         time_this_iter_s █▅▃▄▂▁▂▂▂▃▁█▆▅█▂▅▃▆▄▂█▂▁▅▂▂▃▂▁▂▂▁▂▃▂▂▂▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb:                      mae 0.35504\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb:                     mape 0.10278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb:                     rmse 0.68973\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb:       time_since_restore 296.68465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb:         time_this_iter_s 2.82662\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb:             time_total_s 296.68465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb:                timestamp 1689705904\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: 🚀 View run FSR_Trainable_cc28ba53 at: https://wandb.ai/seokjin/FSR-prediction/runs/cc28ba53\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=399470)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034002-cc28ba53/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb:                      mae █▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb:                     mape █▇▆▅▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb:                     rmse █▇▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb:         time_this_iter_s █▄▂▃▂▄▁▁▃▂▃▃▂▂▂▂▁▃▂▃▂▃▃▃▂▂▃▁▃▂▃▃▄▃▂▂▂▂▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb:                      mae 0.34325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb:                     mape 0.09884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb:                     rmse 0.66362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb:       time_since_restore 123.70329\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb:         time_this_iter_s 1.20028\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb:             time_total_s 123.70329\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb:                timestamp 1689705913\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: 🚀 View run FSR_Trainable_b47f87c1 at: https://wandb.ai/seokjin/FSR-prediction/runs/b47f87c1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401551)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034312-b47f87c1/logs\n",
      "2023-07-19 03:45:19,463\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.246 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:45:19,467\tWARNING util.py:315 -- The `process_trial_result` operation took 2.250 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:45:19,469\tWARNING util.py:315 -- Processing trial results took 2.252 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:45:19,471\tWARNING util.py:315 -- The `process_trial_result` operation took 2.254 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_d8d12509_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-43-05/wandb/run-20230719_034522-d8d12509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: Syncing run FSR_Trainable_d8d12509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d8d12509\n",
      "2023-07-19 03:45:31,247\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.150 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:45:31,251\tWARNING util.py:315 -- The `process_trial_result` operation took 2.155 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:45:31,255\tWARNING util.py:315 -- Processing trial results took 2.159 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:45:31,258\tWARNING util.py:315 -- The `process_trial_result` operation took 2.163 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_86c33214_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-45-15/wandb/run-20230719_034533-86c33214\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: Syncing run FSR_Trainable_86c33214\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/86c33214\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb:                      mae 0.41166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb:                     mape 0.11545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb:                     rmse 0.71471\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb:       time_since_restore 2.80937\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb:         time_this_iter_s 2.80937\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb:             time_total_s 2.80937\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb:                timestamp 1689705929\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: 🚀 View run FSR_Trainable_86c33214 at: https://wandb.ai/seokjin/FSR-prediction/runs/86c33214\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402093)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034533-86c33214/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_863d9411_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-45-26/wandb/run-20230719_034551-863d9411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Syncing run FSR_Trainable_863d9411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/863d9411\n",
      "2023-07-19 03:45:56,970\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.604 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:45:56,973\tWARNING util.py:315 -- The `process_trial_result` operation took 1.608 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:45:56,974\tWARNING util.py:315 -- Processing trial results took 1.609 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:45:56,976\tWARNING util.py:315 -- The `process_trial_result` operation took 1.611 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb:                      mae ▁▂▃▃▃▃▃▅▃▄▆▆▃▃▃▃▃▃▃▃█▃▄▃▃▃▄▃▃▂▃▂▂▂▂▂▄▂▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb:                     mape ▄▃▄▃▃▃▄▇▄▅▆▅▂▂▂▂▂▃▃▄█▃▃▃▁▂▃▃▂▂▂▂▂▂▁▂▄▃▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb:                     rmse █▆▅▇▇▆▆▇▆▆▆▆▄▄▄▄▄▄▄▄█▄▄▃▃▃▄▃▂▁▂▂▁▁▂▂▄▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb:         time_this_iter_s ▆▁▁▁▁▄▁▇▁▅▂▆▃▅▃▁▂▂▃▂▂▂▁▂▁▃▂▂▂▂▂▂▃▃▅▄█▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb:                      mae 0.36497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb:                     mape 0.10399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb:                     rmse 0.67377\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb:       time_since_restore 339.10271\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb:         time_this_iter_s 3.29655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb:             time_total_s 339.10271\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb:                timestamp 1689705992\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: 🚀 View run FSR_Trainable_f002d9d6 at: https://wandb.ai/seokjin/FSR-prediction/runs/f002d9d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=400143)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034049-f002d9d6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb:                      mae ▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▃▃▅▆▇▇▇▇██▇▇▆▇▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb:                     mape ▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▄▄▅▅▆▇▇▇▇███▇▇█▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb:                     rmse ▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▁▂▂▄▄▅▆▆▇▇█▇▇▅▆▆▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb:         time_this_iter_s █▄▃▁▅▂▁▂▃▃▃▃▃▆▂▃▂▃▃▃▃▂▂▂▃▂▂▂▂▂▃▃▃▃▄▄▄▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=398990)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_28425721_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-45-43/wandb/run-20230719_034650-28425721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: Syncing run FSR_Trainable_28425721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/28425721\n",
      "2023-07-19 03:46:56,596\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.656 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:46:56,599\tWARNING util.py:315 -- The `process_trial_result` operation took 1.660 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:46:56,600\tWARNING util.py:315 -- Processing trial results took 1.661 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:46:56,601\tWARNING util.py:315 -- The `process_trial_result` operation took 1.662 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_22aa24f4_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-46-42/wandb/run-20230719_034704-22aa24f4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: Syncing run FSR_Trainable_22aa24f4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/22aa24f4\n",
      "2023-07-19 03:47:07,790\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.198 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:47:07,794\tWARNING util.py:315 -- The `process_trial_result` operation took 2.203 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:47:07,796\tWARNING util.py:315 -- Processing trial results took 2.205 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:47:07,800\tWARNING util.py:315 -- The `process_trial_result` operation took 2.208 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb:                      mae 0.38883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb:                     mape 0.10435\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb:                     rmse 0.71311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb:       time_since_restore 8.89781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb:         time_this_iter_s 8.89781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb:             time_total_s 8.89781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb:                timestamp 1689706025\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: 🚀 View run FSR_Trainable_22aa24f4 at: https://wandb.ai/seokjin/FSR-prediction/runs/22aa24f4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402810)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034704-22aa24f4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb:                      mae █▆▄▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb:                     mape █▅▃▂▁▁▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb:                     rmse █▇▆▅▅▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb:         time_this_iter_s █▆▁▁▂▅▄▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "2023-07-19 03:47:27,233\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.014 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:47:27,239\tWARNING util.py:315 -- The `process_trial_result` operation took 2.021 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:47:27,242\tWARNING util.py:315 -- Processing trial results took 2.023 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:47:27,243\tWARNING util.py:315 -- The `process_trial_result` operation took 2.025 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_8c53b999_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-46-56/wandb/run-20230719_034725-8c53b999\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: Syncing run FSR_Trainable_8c53b999\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8c53b999\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb:                      mae 0.40678\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb:                     mape 0.1209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb:                     rmse 0.70932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb:       time_since_restore 7.5217\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb:         time_this_iter_s 7.5217\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb:             time_total_s 7.5217\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb:                timestamp 1689706045\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: 🚀 View run FSR_Trainable_8c53b999 at: https://wandb.ai/seokjin/FSR-prediction/runs/8c53b999\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403053)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034725-8c53b999/logs\n",
      "2023-07-19 03:47:35,879\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.285 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:47:35,882\tWARNING util.py:315 -- The `process_trial_result` operation took 2.289 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:47:35,885\tWARNING util.py:315 -- Processing trial results took 2.291 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:47:35,886\tWARNING util.py:315 -- The `process_trial_result` operation took 2.293 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_468d66ee_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-47-17/wandb/run-20230719_034736-468d66ee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: Syncing run FSR_Trainable_468d66ee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/468d66ee\n",
      "2023-07-19 03:47:46,878\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.121 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:47:46,882\tWARNING util.py:315 -- The `process_trial_result` operation took 2.125 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:47:46,884\tWARNING util.py:315 -- Processing trial results took 2.127 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:47:46,887\tWARNING util.py:315 -- The `process_trial_result` operation took 2.130 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_2511f06e_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-47-29/wandb/run-20230719_034747-2511f06e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: Syncing run FSR_Trainable_2511f06e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2511f06e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb:                      mae ▃▁█▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb:                     mape ▂▁█▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb:                     rmse ▃▁█▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb:         time_this_iter_s █▁▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb:                timestamp ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb:                      mae 0.36354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb:                     mape 0.10415\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb:                     rmse 0.69958\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb:       time_since_restore 15.66517\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb:         time_this_iter_s 3.57578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb:             time_total_s 15.66517\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb:                timestamp 1689706066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: 🚀 View run FSR_Trainable_468d66ee at: https://wandb.ai/seokjin/FSR-prediction/runs/468d66ee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403269)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034736-468d66ee/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034747-2511f06e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403493)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb:                      mae █▇▇▇▇▇▇▇▆▆▆▆▅▆▅▅▆▅▅▅▆▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb:                     mape █▆▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb:                     rmse █▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb:         time_this_iter_s █▃▂▄▃▄▃▅▄▄▃▅▄▃▂▃▂▃▃▂▃▂▃▇▇▅▇▅▅▃▁▁▄▅▆▅▄▇▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb:                      mae 0.32322\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb:                     mape 0.09101\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb:                     rmse 0.59703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb:       time_since_restore 142.08029\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb:         time_this_iter_s 1.18767\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb:             time_total_s 142.08029\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb:                timestamp 1689706072\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: 🚀 View run FSR_Trainable_d8d12509 at: https://wandb.ai/seokjin/FSR-prediction/runs/d8d12509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034522-d8d12509/logs\n",
      "2023-07-19 03:48:02,335\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.772 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:48:02,339\tWARNING util.py:315 -- The `process_trial_result` operation took 1.776 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:48:02,343\tWARNING util.py:315 -- Processing trial results took 1.781 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:48:02,345\tWARNING util.py:315 -- The `process_trial_result` operation took 1.782 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=401870)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_8c01a1f0_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-47-40/wandb/run-20230719_034802-8c01a1f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: Syncing run FSR_Trainable_8c01a1f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8c01a1f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 03:48:08,885\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.955 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:48:08,889\tWARNING util.py:315 -- The `process_trial_result` operation took 1.960 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:48:08,891\tWARNING util.py:315 -- Processing trial results took 1.962 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:48:08,895\tWARNING util.py:315 -- The `process_trial_result` operation took 1.966 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb:                      mae 0.38751\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb:                     mape 0.11084\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb:                     rmse 0.70773\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb:       time_since_restore 4.03796\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb:         time_this_iter_s 4.03796\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb:             time_total_s 4.03796\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb:                timestamp 1689706080\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: 🚀 View run FSR_Trainable_8c01a1f0 at: https://wandb.ai/seokjin/FSR-prediction/runs/8c01a1f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403745)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034802-8c01a1f0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_4451a342_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-47-56/wandb/run-20230719_034811-4451a342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: Syncing run FSR_Trainable_4451a342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4451a342\n",
      "2023-07-19 03:48:20,469\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.889 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:48:20,475\tWARNING util.py:315 -- The `process_trial_result` operation took 1.896 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:48:20,478\tWARNING util.py:315 -- Processing trial results took 1.899 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:48:20,480\tWARNING util.py:315 -- The `process_trial_result` operation took 1.901 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_f03a8d9b_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-48-05/wandb/run-20230719_034823-f03a8d9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: Syncing run FSR_Trainable_f03a8d9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f03a8d9b\n",
      "2023-07-19 03:48:33,435\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.610 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:48:33,437\tWARNING util.py:315 -- The `process_trial_result` operation took 1.612 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:48:33,439\tWARNING util.py:315 -- Processing trial results took 1.615 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:48:33,441\tWARNING util.py:315 -- The `process_trial_result` operation took 1.616 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_9f64d024_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-48-17/wandb/run-20230719_034836-9f64d024\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: Syncing run FSR_Trainable_9f64d024\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9f64d024\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb:                      mae █▇▇▇▆▆▆▆▆▆▆▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▃▂▂▂▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb:                     mape ███▇▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb:                     rmse █▇▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb:         time_this_iter_s ▇▂▁▂▅▃▃▅▅▄▃▃▃▄▃▃▃▄▃▃▃▃▃█▇▃▄▃▃▅▃▃▄▃▃▅▃▄▅▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb:                      mae 0.32386\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb:                     mape 0.0915\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb:                     rmse 0.6148\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb:       time_since_restore 120.86862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb:         time_this_iter_s 1.0494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb:             time_total_s 120.86862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb:                timestamp 1689706213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: 🚀 View run FSR_Trainable_4451a342 at: https://wandb.ai/seokjin/FSR-prediction/runs/4451a342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=403963)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034811-4451a342/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 03:50:28,458\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.237 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:50:28,464\tWARNING util.py:315 -- The `process_trial_result` operation took 2.244 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:50:28,466\tWARNING util.py:315 -- Processing trial results took 2.246 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:50:28,468\tWARNING util.py:315 -- The `process_trial_result` operation took 2.248 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb:                      mae █▅▃▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▂▁▂▂▂▁▁▁▁▁▁▂▁▁▁▂▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb:                     mape █▆▆▅▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb:                     rmse █▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb:         time_this_iter_s █▄▃▃▄▃▂▂▂▂▁▂▂▃▂▂▃▁▃█▅▃▃▂▃▄▂▂▂▁▃▄▃▂▂▃▂▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb:                      mae 0.33934\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb:                     mape 0.09596\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb:                     rmse 0.62796\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb:       time_since_restore 122.4088\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb:         time_this_iter_s 1.23341\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb:             time_total_s 122.4088\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb:                timestamp 1689706225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: 🚀 View run FSR_Trainable_f03a8d9b at: https://wandb.ai/seokjin/FSR-prediction/runs/f03a8d9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404188)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034823-f03a8d9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_ee2bdf4e_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-48-30/wandb/run-20230719_035031-ee2bdf4e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: Syncing run FSR_Trainable_ee2bdf4e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ee2bdf4e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 03:50:41,417\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.694 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:50:41,420\tWARNING util.py:315 -- The `process_trial_result` operation took 1.699 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:50:41,422\tWARNING util.py:315 -- Processing trial results took 1.700 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:50:41,423\tWARNING util.py:315 -- The `process_trial_result` operation took 1.702 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb:                      mae █▇▆▅▅▅▅▅▅▅▅▅▄▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▄▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb:                     mape █▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▃▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb:                     rmse █▇▆▅▅▅▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb:         time_this_iter_s █▄▂▃▂▂▂▂▁▂▂▂▂▂▂▃▇▄▂▃▂▂▃▂▂▃▄▂▂▂▂▄▂▂▁▁▆▄▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb:                      mae 0.32604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb:                     mape 0.09191\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb:                     rmse 0.60905\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb:       time_since_restore 121.67005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb:         time_this_iter_s 1.27473\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb:             time_total_s 121.67005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb:                timestamp 1689706237\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: 🚀 View run FSR_Trainable_9f64d024 at: https://wandb.ai/seokjin/FSR-prediction/runs/9f64d024\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404402)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034836-9f64d024/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_f5ff39b5_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-50-24/wandb/run-20230719_035044-f5ff39b5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Syncing run FSR_Trainable_f5ff39b5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f5ff39b5\n",
      "2023-07-19 03:50:54,624\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.162 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:50:54,628\tWARNING util.py:315 -- The `process_trial_result` operation took 2.167 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:50:54,632\tWARNING util.py:315 -- Processing trial results took 2.170 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:50:54,634\tWARNING util.py:315 -- The `process_trial_result` operation took 2.172 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_1740aa9e_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-50-38/wandb/run-20230719_035057-1740aa9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: Syncing run FSR_Trainable_1740aa9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1740aa9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb:                      mae 0.38506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb:                     mape 0.10048\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb:                     rmse 0.71054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb:       time_since_restore 1.50745\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb:         time_this_iter_s 1.50745\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb:             time_total_s 1.50745\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb:                timestamp 1689706252\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: 🚀 View run FSR_Trainable_1740aa9e at: https://wandb.ai/seokjin/FSR-prediction/runs/1740aa9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405180)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035057-1740aa9e/logs\n",
      "2023-07-19 03:51:09,842\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.190 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:51:09,861\tWARNING util.py:315 -- The `process_trial_result` operation took 2.210 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:51:09,863\tWARNING util.py:315 -- Processing trial results took 2.212 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:51:09,878\tWARNING util.py:315 -- The `process_trial_result` operation took 2.227 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_ffedeb7a_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-50-51/wandb/run-20230719_035112-ffedeb7a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: Syncing run FSR_Trainable_ffedeb7a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ffedeb7a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb:                      mae 0.4164\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb:                     mape 0.10594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb:                     rmse 0.7267\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb:       time_since_restore 1.41982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb:         time_this_iter_s 1.41982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb:             time_total_s 1.41982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb:                timestamp 1689706267\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: 🚀 View run FSR_Trainable_ffedeb7a at: https://wandb.ai/seokjin/FSR-prediction/runs/ffedeb7a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405413)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035112-ffedeb7a/logs\n",
      "2023-07-19 03:51:25,540\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.908 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:51:25,541\tWARNING util.py:315 -- The `process_trial_result` operation took 1.910 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:51:25,542\tWARNING util.py:315 -- Processing trial results took 1.911 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:51:25,543\tWARNING util.py:315 -- The `process_trial_result` operation took 1.912 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_832961ff_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-51-06/wandb/run-20230719_035128-832961ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: Syncing run FSR_Trainable_832961ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/832961ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb:                      mae 0.36189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb:                     mape 0.10222\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb:                     rmse 0.69928\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb:       time_since_restore 2.70163\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb:         time_this_iter_s 1.27918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb:             time_total_s 2.70163\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb:                timestamp 1689706286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: 🚀 View run FSR_Trainable_832961ff at: https://wandb.ai/seokjin/FSR-prediction/runs/832961ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405647)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035128-832961ff/logs\n",
      "2023-07-19 03:51:42,097\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.164 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:51:42,100\tWARNING util.py:315 -- The `process_trial_result` operation took 2.168 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:51:42,102\tWARNING util.py:315 -- Processing trial results took 2.170 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:51:42,103\tWARNING util.py:315 -- The `process_trial_result` operation took 2.171 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_24c7aee5_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-51-22/wandb/run-20230719_035145-24c7aee5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: Syncing run FSR_Trainable_24c7aee5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/24c7aee5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb:                      mae ▁▁▄▅█▅▅▅▅▅▅▄▄▄▄▄▅▅▅▄▄▄▅▄▄▅▄▄▃▄▃▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb:                     mape ▂▁▄▇█▄▄▅▅▅▅▄▄▄▅▅▆▆▆▅▅▅▆▅▅▆▅▄▄▄▄▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb:                     rmse █▆▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb:         time_this_iter_s █▇▂▅▅▅▁▁▁▅▄▃▃▃▆▄▄▄▃▃▃▄▄▃▃▅▄▅▃▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb:                      mae 0.36136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb:                     mape 0.10415\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb:                     rmse 0.67201\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb:       time_since_restore 336.19344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb:         time_this_iter_s 9.8746\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb:             time_total_s 336.19344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb:                timestamp 1689706347\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: 🚀 View run FSR_Trainable_28425721 at: https://wandb.ai/seokjin/FSR-prediction/runs/28425721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=402594)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034650-28425721/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_034650-28425721/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb:                      mae █▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb:                     mape █▇▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb:                     rmse █▇▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb:         time_this_iter_s █▃▃▂▅▂▁▂▂▂▂▄▂▂▃▃▃▂▁▁▆▄▃▂▂▁▂▂▂▁▃▁▂▁▂▁▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035044-f5ff39b5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 03:52:41,496\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.919 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:52:41,500\tWARNING util.py:315 -- The `process_trial_result` operation took 1.924 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:52:41,502\tWARNING util.py:315 -- Processing trial results took 1.926 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:52:41,504\tWARNING util.py:315 -- The `process_trial_result` operation took 1.928 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb:                      mae █▅▄▄▄▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb:                     mape █▆▅▅▅▅▅▅▅▅▄▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb:                     rmse █▇▆▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb:         time_this_iter_s █▄▃▃▄▃▃▄▄▃▃▂▃▅▄▄▄▃▃▃▃▄▄▃▂▂▃▃▃▃▃▄▃▂▂▃▂▃▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb:                      mae 0.33139\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb:                     mape 0.09294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb:                     rmse 0.61311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb:       time_since_restore 122.90865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb:         time_this_iter_s 1.26381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb:             time_total_s 122.90865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb:                timestamp 1689706358\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: 🚀 View run FSR_Trainable_ee2bdf4e at: https://wandb.ai/seokjin/FSR-prediction/runs/ee2bdf4e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035031-ee2bdf4e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=404723)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_cfadcfb5_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-51-38/wandb/run-20230719_035244-cfadcfb5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: Syncing run FSR_Trainable_cfadcfb5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cfadcfb5\n",
      "2023-07-19 03:52:52,600\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.356 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:52:52,604\tWARNING util.py:315 -- The `process_trial_result` operation took 2.361 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:52:52,606\tWARNING util.py:315 -- Processing trial results took 2.363 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:52:52,608\tWARNING util.py:315 -- The `process_trial_result` operation took 2.365 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_d4909022_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-52-37/wandb/run-20230719_035255-d4909022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: Syncing run FSR_Trainable_d4909022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d4909022\n",
      "2023-07-19 03:53:03,412\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.846 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:53:03,415\tWARNING util.py:315 -- The `process_trial_result` operation took 1.850 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:53:03,417\tWARNING util.py:315 -- Processing trial results took 1.852 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:53:03,419\tWARNING util.py:315 -- The `process_trial_result` operation took 1.854 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_36edc5db_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-52-47/wandb/run-20230719_035306-36edc5db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: Syncing run FSR_Trainable_36edc5db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/36edc5db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb:                      mae █▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb:                     mape █▇▆▅▅▄▄▃▃▃▃▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb:                     rmse █▇▆▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb:         time_this_iter_s █▄▂▂▂▃▂▂▄▂▃▂▂▃▃▂▃▂▁▃▂▁▆▂▂▄▃▂▅▃▂▂▂▂▃▃▃▄▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb:                      mae 0.3376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb:                     mape 0.09721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb:                     rmse 0.63423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb:       time_since_restore 119.93478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb:         time_this_iter_s 1.15825\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb:             time_total_s 119.93478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb:                timestamp 1689706427\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: 🚀 View run FSR_Trainable_24c7aee5 at: https://wandb.ai/seokjin/FSR-prediction/runs/24c7aee5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=405880)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035145-24c7aee5/logs\n",
      "2023-07-19 03:54:01,454\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.928 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:54:01,457\tWARNING util.py:315 -- The `process_trial_result` operation took 1.931 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:54:01,459\tWARNING util.py:315 -- Processing trial results took 1.934 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:54:01,462\tWARNING util.py:315 -- The `process_trial_result` operation took 1.936 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_e3457318_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-52-59/wandb/run-20230719_035404-e3457318\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: Syncing run FSR_Trainable_e3457318\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e3457318\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb:                      mae █▆▆▆▅▅▄▄▄▄▄▄▄▄▄▄▄▄▃▃▄▃▃▃▄▃▂▄▂▂▂▂▂▂▂▂▂▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb:                     mape █▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▂▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb:                     rmse █▆▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb:         time_this_iter_s █▃▁▇▄▃▄▂▂▄▃▃▃▃▂▃▃▃▂▃▃▃▂▃▂▄▃▂▄▂▂▂▂▂▃▃▂▂▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb:                      mae 0.32982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb:                     mape 0.09311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb:                     rmse 0.61944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb:       time_since_restore 116.21363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb:         time_this_iter_s 1.19978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb:             time_total_s 116.21363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb:                timestamp 1689706483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: 🚀 View run FSR_Trainable_cfadcfb5 at: https://wandb.ai/seokjin/FSR-prediction/runs/cfadcfb5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406170)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035244-cfadcfb5/logs\n",
      "2023-07-19 03:54:58,114\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.803 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:54:58,117\tWARNING util.py:315 -- The `process_trial_result` operation took 1.806 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:54:58,118\tWARNING util.py:315 -- Processing trial results took 1.808 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:54:58,120\tWARNING util.py:315 -- The `process_trial_result` operation took 1.809 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_dfc34fb3_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-53-57/wandb/run-20230719_035500-dfc34fb3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: Syncing run FSR_Trainable_dfc34fb3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/dfc34fb3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb:                      mae ▄▁▁▃▄▃█▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb:                     mape ▃▁▆█▇▁█▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb:                     rmse █▇▅▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb:         time_this_iter_s █▇▂▃▆▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb:                timestamp ▁▃▄▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb:                      mae 0.35237\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb:                     mape 0.10425\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb:                     rmse 0.67598\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb:       time_since_restore 19.36754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb:         time_this_iter_s 2.16074\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb:             time_total_s 19.36754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb:                timestamp 1689706514\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: 🚀 View run FSR_Trainable_dfc34fb3 at: https://wandb.ai/seokjin/FSR-prediction/runs/dfc34fb3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407153)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035500-dfc34fb3/logs\n",
      "2023-07-19 03:55:30,025\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.855 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:55:30,028\tWARNING util.py:315 -- The `process_trial_result` operation took 1.857 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:55:30,029\tWARNING util.py:315 -- Processing trial results took 1.859 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:55:30,032\tWARNING util.py:315 -- The `process_trial_result` operation took 1.862 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_48e09294_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-54-53/wandb/run-20230719_035532-48e09294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: Syncing run FSR_Trainable_48e09294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/48e09294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb:                      mae 0.36192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb:                     mape 0.10351\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb:                     rmse 0.69991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb:       time_since_restore 4.84059\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb:         time_this_iter_s 2.1846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb:             time_total_s 4.84059\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb:                timestamp 1689706532\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: 🚀 View run FSR_Trainable_48e09294 at: https://wandb.ai/seokjin/FSR-prediction/runs/48e09294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407396)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035532-48e09294/logs\n",
      "2023-07-19 03:55:46,581\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.903 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:55:46,585\tWARNING util.py:315 -- The `process_trial_result` operation took 1.907 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:55:46,587\tWARNING util.py:315 -- Processing trial results took 1.909 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:55:46,588\tWARNING util.py:315 -- The `process_trial_result` operation took 1.910 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_7d33338d_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-55-25/wandb/run-20230719_035549-7d33338d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: Syncing run FSR_Trainable_7d33338d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7d33338d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb:                      mae 0.37229\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb:                     mape 0.10371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb:                     rmse 0.70599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb:       time_since_restore 1.86671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb:         time_this_iter_s 1.86671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb:             time_total_s 1.86671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb:                timestamp 1689706544\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: 🚀 View run FSR_Trainable_7d33338d at: https://wandb.ai/seokjin/FSR-prediction/runs/7d33338d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407630)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035549-7d33338d/logs\n",
      "2023-07-19 03:56:01,964\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.082 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:56:01,966\tWARNING util.py:315 -- The `process_trial_result` operation took 2.085 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:56:01,969\tWARNING util.py:315 -- Processing trial results took 2.088 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:56:01,972\tWARNING util.py:315 -- The `process_trial_result` operation took 2.092 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_90a4ec38_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-55-42/wandb/run-20230719_035604-90a4ec38\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: Syncing run FSR_Trainable_90a4ec38\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/90a4ec38\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb:                      mae 0.36182\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb:                     mape 0.10253\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb:                     rmse 0.69938\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb:       time_since_restore 3.30662\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb:         time_this_iter_s 1.52513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb:             time_total_s 3.30662\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb:                timestamp 1689706563\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: 🚀 View run FSR_Trainable_90a4ec38 at: https://wandb.ai/seokjin/FSR-prediction/runs/90a4ec38\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=407861)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035604-90a4ec38/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 03:56:17,917\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.574 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:56:17,920\tWARNING util.py:315 -- The `process_trial_result` operation took 1.578 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:56:17,923\tWARNING util.py:315 -- Processing trial results took 1.581 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:56:17,924\tWARNING util.py:315 -- The `process_trial_result` operation took 1.583 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb:                      mae ▇▆▇████▇▆▆▅▆▅▅▇▄▄▃▃▅▃▃▃▂▃▂▂▂▂▂▂▂▂▂▄▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb:                     mape █▇▇▇▆▆▆▅▅▅▅▅▄▄▅▄▄▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▄▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb:                     rmse █▇▆▆▅▅▅▅▄▄▄▄▄▃▄▃▃▂▂▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▃▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb:         time_this_iter_s █▂▁▃▅▂▂▂▂▁▂▂▁▃▃▂▂▁▁▁▂▂▂▂▁▁▄▂▃▂▁▁▃▂▁▃▂▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb:                      mae 0.32122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb:                     mape 0.08964\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb:                     rmse 0.60744\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb:       time_since_restore 196.79789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb:         time_this_iter_s 1.80368\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb:             time_total_s 196.79789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb:                timestamp 1689706573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: 🚀 View run FSR_Trainable_d4909022 at: https://wandb.ai/seokjin/FSR-prediction/runs/d4909022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406397)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035255-d4909022/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_e2404c92_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-55-58/wandb/run-20230719_035620-e2404c92\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: Syncing run FSR_Trainable_e2404c92\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e2404c92\n",
      "2023-07-19 03:56:31,809\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.813 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:56:31,811\tWARNING util.py:315 -- The `process_trial_result` operation took 1.816 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:56:31,814\tWARNING util.py:315 -- Processing trial results took 1.819 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:56:31,816\tWARNING util.py:315 -- The `process_trial_result` operation took 1.820 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_f763ff5d_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-56-14/wandb/run-20230719_035634-f763ff5d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Syncing run FSR_Trainable_f763ff5d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f763ff5d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb:                      mae █▆▆▇▇▇▆▆▆▆▄▅▃▃▃▃▅▄▄▃▂▂▃▃▂▂▂▂▂▂▁▁▁▂▃▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb:                     mape █▇▆▆▆▆▅▅▄▅▄▄▄▄▃▃▄▃▄▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb:                     rmse █▇▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▃▂▂▂▂▁▂▂▂▁▂▁▁▁▁▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb:         time_this_iter_s █▃▆▃▃▂▂▂▂▃▂▂▅▂▁▂▁▃▂▂▂▂▂▇▅▅▃▂▆▃▂▄▄▂▃▃▂▆▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb:                      mae 0.32479\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb:                     mape 0.09227\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb:                     rmse 0.61619\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb:       time_since_restore 201.07722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb:         time_this_iter_s 2.01705\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb:             time_total_s 201.07722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb:                timestamp 1689706591\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: 🚀 View run FSR_Trainable_36edc5db at: https://wandb.ai/seokjin/FSR-prediction/runs/36edc5db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406611)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035306-36edc5db/logs\n",
      "2023-07-19 03:56:47,251\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.810 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:56:47,253\tWARNING util.py:315 -- The `process_trial_result` operation took 1.813 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:56:47,255\tWARNING util.py:315 -- Processing trial results took 1.814 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:56:47,256\tWARNING util.py:315 -- The `process_trial_result` operation took 1.816 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_194e8d7e_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-56-27/wandb/run-20230719_035650-194e8d7e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: Syncing run FSR_Trainable_194e8d7e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/194e8d7e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb:                      mae █▇████▇▆▆▆▅▅▅▆▄▄▆▄▃▅▃▃▃▂▃▂▂▂▃▂▂▁▁▁▂▂▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb:                     mape █▇▇▇▆▆▅▅▄▅▄▄▄▄▄▃▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb:                     rmse █▇▆▅▅▅▄▄▄▄▄▄▃▄▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb:         time_this_iter_s █▂▂▂▂▁▂▁▁▂▁▂▂▁▅▃▂▅▂▂▂▄▄▂▂▅▃▁▇▅▃▂▆▄▂▅▃▃▇▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb:                      mae 0.31795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb:                     mape 0.09101\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb:                     rmse 0.60514\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb:       time_since_restore 159.59001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb:         time_this_iter_s 1.70923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb:             time_total_s 159.59001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb:                timestamp 1689706612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: 🚀 View run FSR_Trainable_e3457318 at: https://wandb.ai/seokjin/FSR-prediction/runs/e3457318\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=406884)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035404-e3457318/logs\n",
      "2023-07-19 03:57:10,717\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.690 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:57:10,721\tWARNING util.py:315 -- The `process_trial_result` operation took 1.694 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:57:10,723\tWARNING util.py:315 -- Processing trial results took 1.696 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:57:10,725\tWARNING util.py:315 -- The `process_trial_result` operation took 1.698 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_00ffbd6d_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-56-43/wandb/run-20230719_035710-00ffbd6d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: Syncing run FSR_Trainable_00ffbd6d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/00ffbd6d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb:                      mae 0.49666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb:                     mape 0.11691\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb:                     rmse 0.78265\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb:       time_since_restore 5.08404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb:         time_this_iter_s 5.08404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb:             time_total_s 5.08404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb:                timestamp 1689706629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: 🚀 View run FSR_Trainable_00ffbd6d at: https://wandb.ai/seokjin/FSR-prediction/runs/00ffbd6d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408791)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035710-00ffbd6d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 03:57:31,147\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.545 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:57:31,152\tWARNING util.py:315 -- The `process_trial_result` operation took 1.551 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:57:31,153\tWARNING util.py:315 -- Processing trial results took 1.552 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:57:31,165\tWARNING util.py:315 -- The `process_trial_result` operation took 1.564 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_597a6791_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-57-04/wandb/run-20230719_035730-597a6791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: Syncing run FSR_Trainable_597a6791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/597a6791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb:                      mae 1.21924\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb:                     mape 0.19254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb:                     rmse 1.58128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb:       time_since_restore 6.54601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb:         time_this_iter_s 6.54601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb:             time_total_s 6.54601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb:                timestamp 1689706649\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: 🚀 View run FSR_Trainable_597a6791 at: https://wandb.ai/seokjin/FSR-prediction/runs/597a6791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409030)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035730-597a6791/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb:                      mae ▂▁▇▂▄▄▆▆▅▅▆▅▄▆▆▅▅▇▆▆▆▆▅█▅▅▅▅▃▆▄▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb:                     mape ▅▃█▅▅▆██▇▆▇▇▆▆▆▅▄▆▄▅▅▅▃▆▃▄▃▃▁▄▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb:                     rmse █▇▇▅▅▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▃▂▂▁▂▁▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb:         time_this_iter_s ▇▃▁▃▂▄▃▄▃▇▄▃▂▁▃▄▁▁█▄▂▂▄▅▁▅▃▃▅▅▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035634-f763ff5d/logs\n",
      "2023-07-19 03:57:44,534\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.982 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:57:44,538\tWARNING util.py:315 -- The `process_trial_result` operation took 1.986 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:57:44,541\tWARNING util.py:315 -- Processing trial results took 1.989 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:57:44,543\tWARNING util.py:315 -- The `process_trial_result` operation took 1.991 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_2f435512_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-57-23/wandb/run-20230719_035747-2f435512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: Syncing run FSR_Trainable_2f435512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2f435512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb:                      mae 0.39566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb:                     mape 0.11058\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb:                     rmse 0.7182\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb:       time_since_restore 1.93869\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb:         time_this_iter_s 1.93869\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb:             time_total_s 1.93869\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb:                timestamp 1689706662\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: 🚀 View run FSR_Trainable_2f435512 at: https://wandb.ai/seokjin/FSR-prediction/runs/2f435512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035747-2f435512/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409275)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 03:57:57,145\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.069 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:57:57,148\tWARNING util.py:315 -- The `process_trial_result` operation took 2.072 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:57:57,149\tWARNING util.py:315 -- Processing trial results took 2.073 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:57:57,150\tWARNING util.py:315 -- The `process_trial_result` operation took 2.074 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_cd21359b_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-57-40/wandb/run-20230719_035757-cd21359b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: Syncing run FSR_Trainable_cd21359b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cd21359b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 03:58:08,200\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.904 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:58:08,202\tWARNING util.py:315 -- The `process_trial_result` operation took 1.908 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:58:08,204\tWARNING util.py:315 -- Processing trial results took 1.909 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:58:08,205\tWARNING util.py:315 -- The `process_trial_result` operation took 1.910 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb:                      mae 0.34816\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb:                     mape 0.10296\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb:                     rmse 0.69758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb:       time_since_restore 7.95351\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb:         time_this_iter_s 3.68666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb:             time_total_s 7.95351\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb:                timestamp 1689706680\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_1de51a04_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-57-50/wandb/run-20230719_035808-1de51a04\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Syncing run FSR_Trainable_1de51a04\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1de51a04\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: 🚀 View run FSR_Trainable_cd21359b at: https://wandb.ai/seokjin/FSR-prediction/runs/cd21359b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409497)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035757-cd21359b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035808-1de51a04/logs\n",
      "2023-07-19 03:58:16,237\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.855 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:58:16,241\tWARNING util.py:315 -- The `process_trial_result` operation took 1.860 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:58:16,243\tWARNING util.py:315 -- Processing trial results took 1.862 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:58:16,244\tWARNING util.py:315 -- The `process_trial_result` operation took 1.863 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_b912fe9e_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-58-01/wandb/run-20230719_035819-b912fe9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: Syncing run FSR_Trainable_b912fe9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b912fe9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb:                      mae █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb:                     mape █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb:                     rmse █▆▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb:         time_this_iter_s █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb:                      mae 0.35444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb:                     mape 0.10476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb:                     rmse 0.68791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb:       time_since_restore 5.9004\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb:         time_this_iter_s 1.19245\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb:             time_total_s 5.9004\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb:                timestamp 1689706700\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: 🚀 View run FSR_Trainable_b912fe9e at: https://wandb.ai/seokjin/FSR-prediction/runs/b912fe9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=409958)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035819-b912fe9e/logs\n",
      "2023-07-19 03:58:26,808\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.003 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:58:26,811\tWARNING util.py:315 -- The `process_trial_result` operation took 2.007 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:58:26,814\tWARNING util.py:315 -- Processing trial results took 2.010 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:58:26,815\tWARNING util.py:315 -- The `process_trial_result` operation took 2.011 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_b8de1926_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-58-12/wandb/run-20230719_035829-b8de1926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: Syncing run FSR_Trainable_b8de1926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b8de1926\n",
      "2023-07-19 03:58:37,458\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.733 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:58:37,461\tWARNING util.py:315 -- The `process_trial_result` operation took 1.737 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:58:37,464\tWARNING util.py:315 -- Processing trial results took 1.741 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:58:37,467\tWARNING util.py:315 -- The `process_trial_result` operation took 1.744 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_a728f25d_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-58-22/wandb/run-20230719_035840-a728f25d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: Syncing run FSR_Trainable_a728f25d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a728f25d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb:                      mae █▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▂▃▂▂▃▂▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb:                     mape █▇▇▆▅▅▅▅▄▄▄▃▃▃▃▃▃▃▄▄▄▄▄▃▄▃▃▃▃▃▃▃▃▂▂▂▂▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb:                     rmse █▆▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb:         time_this_iter_s █▄▄▃▃▄▃▄▅▇▄▄▃▂▆▄▃▃▄▄▅▂▂▁▁▅▂▂▅▄▂▆▃▂▄▂▅▄▆▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb:                      mae 0.32659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb:                     mape 0.09272\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb:                     rmse 0.61504\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb:       time_since_restore 123.85304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb:         time_this_iter_s 1.29886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb:             time_total_s 123.85304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb:                timestamp 1689706716\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: 🚀 View run FSR_Trainable_e2404c92 at: https://wandb.ai/seokjin/FSR-prediction/runs/e2404c92\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408095)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035620-e2404c92/logs\n",
      "2023-07-19 03:58:52,607\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.741 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:58:52,612\tWARNING util.py:315 -- The `process_trial_result` operation took 1.746 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:58:52,615\tWARNING util.py:315 -- Processing trial results took 1.749 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:58:52,616\tWARNING util.py:315 -- The `process_trial_result` operation took 1.751 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_9d94984e_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-58-34/wandb/run-20230719_035856-9d94984e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: Syncing run FSR_Trainable_9d94984e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9d94984e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb:                      mae ██▇▆▆▆▆▆▆▆▅▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▁█▄▃▂▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb:                     mape ██▇▆▅▅▅▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▂▁▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb:                     rmse █▇▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▃▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb:         time_this_iter_s █▃▂▂▄▃▄▃▄▃▄▅▄▃▁▁▃▂▂▅▃▃▆▃▃▄▃▅▃▅▃▄▂▂▅▄▃▂▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb:                      mae 0.32863\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb:                     mape 0.09282\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb:                     rmse 0.61336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb:       time_since_restore 126.29702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb:         time_this_iter_s 1.10968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb:             time_total_s 126.29702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb:                timestamp 1689706748\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: 🚀 View run FSR_Trainable_194e8d7e at: https://wandb.ai/seokjin/FSR-prediction/runs/194e8d7e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=408554)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035650-194e8d7e/logs\n",
      "2023-07-19 03:59:23,831\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.776 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:59:23,834\tWARNING util.py:315 -- The `process_trial_result` operation took 1.779 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:59:23,836\tWARNING util.py:315 -- Processing trial results took 1.781 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:59:23,838\tWARNING util.py:315 -- The `process_trial_result` operation took 1.784 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_74dd520f_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-58-49/wandb/run-20230719_035926-74dd520f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Syncing run FSR_Trainable_74dd520f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/74dd520f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb:                      mae ▄▁▁▃▃▃▃▄▅▅▆▆█▆█▆▅▆▆▄▅▆▄▃▃▃▃▃▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb:                     mape █▆▅▅▄▄▄▄▄▄▅▅▆▄▅▄▃▄▄▃▃▄▃▃▃▃▂▂▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb:                     rmse █▇▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb:         time_this_iter_s █▄▃▅▅▆▄▄▇▂▂▁▃▄▄▃▂▂▂▂▂▂▂▂▁▂▃▃▁▃▃█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb:                      mae 0.34198\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb:                     mape 0.09791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb:                     rmse 0.64214\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb:       time_since_restore 53.60147\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb:         time_this_iter_s 2.06358\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb:             time_total_s 53.60147\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb:                timestamp 1689706762\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: 🚀 View run FSR_Trainable_b8de1926 at: https://wandb.ai/seokjin/FSR-prediction/runs/b8de1926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410185)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035829-b8de1926/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035926-74dd520f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035926-74dd520f/logs\n",
      "2023-07-19 03:59:38,048\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.967 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:59:38,052\tWARNING util.py:315 -- The `process_trial_result` operation took 1.972 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:59:38,054\tWARNING util.py:315 -- Processing trial results took 1.974 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:59:38,055\tWARNING util.py:315 -- The `process_trial_result` operation took 1.975 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_d98df421_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-59-19/wandb/run-20230719_035940-d98df421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: Syncing run FSR_Trainable_d98df421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d98df421\n",
      "2023-07-19 03:59:48,202\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.873 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:59:48,204\tWARNING util.py:315 -- The `process_trial_result` operation took 1.876 s, which may be a performance bottleneck.\n",
      "2023-07-19 03:59:48,207\tWARNING util.py:315 -- Processing trial results took 1.879 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 03:59:48,210\tWARNING util.py:315 -- The `process_trial_result` operation took 1.882 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_3ea89d29_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-59-34/wandb/run-20230719_035951-3ea89d29\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: Syncing run FSR_Trainable_3ea89d29\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3ea89d29\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb:                      mae 0.44435\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb:                     mape 0.11415\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb:                     rmse 0.75368\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb:       time_since_restore 1.67394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb:         time_this_iter_s 1.67394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb:             time_total_s 1.67394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb:                timestamp 1689706786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: 🚀 View run FSR_Trainable_3ea89d29 at: https://wandb.ai/seokjin/FSR-prediction/runs/3ea89d29\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411348)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035951-3ea89d29/logs\n",
      "2023-07-19 04:00:03,248\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.052 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:00:03,253\tWARNING util.py:315 -- The `process_trial_result` operation took 2.058 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:00:03,255\tWARNING util.py:315 -- Processing trial results took 2.060 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:00:03,256\tWARNING util.py:315 -- The `process_trial_result` operation took 2.061 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_f602708e_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-59-44/wandb/run-20230719_040006-f602708e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: Syncing run FSR_Trainable_f602708e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f602708e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb:                      mae 0.42795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb:                     mape 0.11014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb:                     rmse 0.74355\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb:       time_since_restore 1.58073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb:         time_this_iter_s 1.58073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb:             time_total_s 1.58073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb:                timestamp 1689706801\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: 🚀 View run FSR_Trainable_f602708e at: https://wandb.ai/seokjin/FSR-prediction/runs/f602708e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411579)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040006-f602708e/logs\n",
      "2023-07-19 04:00:18,052\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.727 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:00:18,055\tWARNING util.py:315 -- The `process_trial_result` operation took 1.730 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:00:18,057\tWARNING util.py:315 -- Processing trial results took 1.732 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:00:18,058\tWARNING util.py:315 -- The `process_trial_result` operation took 1.734 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_ec364354_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_03-59-59/wandb/run-20230719_040021-ec364354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: Syncing run FSR_Trainable_ec364354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ec364354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb:                      mae 0.35759\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb:                     mape 0.10128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb:                     rmse 0.70156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb:       time_since_restore 1.94898\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb:         time_this_iter_s 1.94898\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb:             time_total_s 1.94898\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb:                timestamp 1689706816\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: 🚀 View run FSR_Trainable_ec364354 at: https://wandb.ai/seokjin/FSR-prediction/runs/ec364354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411810)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040021-ec364354/logs\n",
      "2023-07-19 04:00:33,220\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.713 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:00:33,222\tWARNING util.py:315 -- The `process_trial_result` operation took 1.716 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:00:33,223\tWARNING util.py:315 -- Processing trial results took 1.717 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:00:33,224\tWARNING util.py:315 -- The `process_trial_result` operation took 1.718 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_ed0248b3_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-00-14/wandb/run-20230719_040036-ed0248b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: Syncing run FSR_Trainable_ed0248b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ed0248b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb:                      mae 0.35852\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb:                     mape 0.10232\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb:                     rmse 0.69951\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb:       time_since_restore 4.25965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb:         time_this_iter_s 2.02172\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb:             time_total_s 4.25965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb:                timestamp 1689706835\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: 🚀 View run FSR_Trainable_ed0248b3 at: https://wandb.ai/seokjin/FSR-prediction/runs/ed0248b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412042)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040036-ed0248b3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 04:00:51,095\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.025 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:00:51,099\tWARNING util.py:315 -- The `process_trial_result` operation took 2.030 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:00:51,100\tWARNING util.py:315 -- Processing trial results took 2.031 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:00:51,102\tWARNING util.py:315 -- The `process_trial_result` operation took 2.033 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb:                      mae █▇▆▆▆▆▆▅▅▄▄▄▄▃▃▃▃▃▃▄▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb:                     mape █▇▅▅▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb:                     rmse █▇▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb:         time_this_iter_s █▅▃▂▄▄▄▃▃▂▃▃▃▃▆▄▂▁▁▄▃▆▃▅▃▄▄▃▃▃▃▄▃▃▆▅▃▃▂▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb:                      mae 0.32354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb:                     mape 0.08948\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb:                     rmse 0.60059\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb:       time_since_restore 121.59016\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb:         time_this_iter_s 1.416\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb:             time_total_s 121.59016\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb:                timestamp 1689706848\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: 🚀 View run FSR_Trainable_a728f25d at: https://wandb.ai/seokjin/FSR-prediction/runs/a728f25d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410408)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035840-a728f25d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_8ad86243_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-00-29/wandb/run-20230719_040053-8ad86243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: Syncing run FSR_Trainable_8ad86243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8ad86243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb:                      mae 0.36862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb:                     mape 0.10393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb:                     rmse 0.70178\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb:       time_since_restore 3.06443\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb:         time_this_iter_s 3.06443\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb:             time_total_s 3.06443\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb:                timestamp 1689706849\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: 🚀 View run FSR_Trainable_8ad86243 at: https://wandb.ai/seokjin/FSR-prediction/runs/8ad86243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412275)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040053-8ad86243/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 04:01:06,800\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.739 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:01:06,802\tWARNING util.py:315 -- The `process_trial_result` operation took 1.741 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:01:06,803\tWARNING util.py:315 -- Processing trial results took 1.743 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:01:06,805\tWARNING util.py:315 -- The `process_trial_result` operation took 1.744 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_24576498_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-00-46/wandb/run-20230719_040107-24576498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: Syncing run FSR_Trainable_24576498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/24576498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: / 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb:                      mae █▇▆▆▆▆▅▅▆▄▅▅▅▆▂▂▂▂▂▇▃▂▃▂▂▅▂▂▂▁▁▂▂▁▁▁▁▂▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb:                     mape █▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb:                     rmse █▇▆▅▅▅▅▄▄▄▃▃▃▄▂▂▂▂▂▄▂▁▂▂▂▃▂▁▁▁▁▁▁▁▁▁▁▁▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb:         time_this_iter_s █▅▅▄▃▂▂▄▃▄▄▃▁▂▆▃▃▄▅▃▂▆▆▅▄▅▄▄▂▃▆▄▄▂▆▂▂▁▅▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb:                      mae 0.31687\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb:                     mape 0.0861\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb:                     rmse 0.59279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb:       time_since_restore 120.83829\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb:         time_this_iter_s 1.43057\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb:             time_total_s 120.83829\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb:                timestamp 1689706864\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: 🚀 View run FSR_Trainable_9d94984e at: https://wandb.ai/seokjin/FSR-prediction/runs/9d94984e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=410641)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035856-9d94984e/logs\n",
      "2023-07-19 04:01:14,138\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.290 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:01:14,141\tWARNING util.py:315 -- The `process_trial_result` operation took 2.294 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:01:14,143\tWARNING util.py:315 -- Processing trial results took 2.296 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:01:14,155\tWARNING util.py:315 -- The `process_trial_result` operation took 2.309 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_e7085d33_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-00-59/wandb/run-20230719_040117-e7085d33\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: Syncing run FSR_Trainable_e7085d33\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e7085d33\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 04:01:31,083\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.159 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:01:31,087\tWARNING util.py:315 -- The `process_trial_result` operation took 2.164 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:01:31,088\tWARNING util.py:315 -- Processing trial results took 2.165 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:01:31,090\tWARNING util.py:315 -- The `process_trial_result` operation took 2.167 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_db1779a3_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-01-10/wandb/run-20230719_040130-db1779a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: Syncing run FSR_Trainable_db1779a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/db1779a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb:                      mae ▂▁▄▆██▃▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb:                     mape ▂▁▆███▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb:                     rmse █▄▂▂▂▂▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb:         time_this_iter_s ▆▁▃▅█▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb:                      mae 0.36406\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb:                     mape 0.09839\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb:                     rmse 0.67159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb:       time_since_restore 41.20241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb:         time_this_iter_s 4.78295\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb:             time_total_s 41.20241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb:                timestamp 1689706902\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: 🚀 View run FSR_Trainable_24576498 at: https://wandb.ai/seokjin/FSR-prediction/runs/24576498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412514)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040107-24576498/logs\n",
      "2023-07-19 04:01:58,803\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.991 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:01:58,806\tWARNING util.py:315 -- The `process_trial_result` operation took 1.994 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:01:58,808\tWARNING util.py:315 -- Processing trial results took 1.996 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:01:58,810\tWARNING util.py:315 -- The `process_trial_result` operation took 1.999 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_333e936e_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-01-22/wandb/run-20230719_040201-333e936e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: Syncing run FSR_Trainable_333e936e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/333e936e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb:                      mae ▆████▇▆▆▆▆▄▅▅▄▅▃▂▄▆▃▃▃▃▂▂▂▂▃▂▂▂▁▁▂▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb:                     mape ██▇▇▇▆▆▆▆▅▅▅▅▅▅▄▄▅▆▄▅▄▄▄▄▄▃▄▃▃▃▂▂▂▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb:                     rmse █▇▆▆▅▅▅▅▅▄▃▃▃▃▄▂▂▃▄▂▂▂▂▂▂▂▂▃▂▂▁▁▁▂▁▁▁▁▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb:         time_this_iter_s ▇▃▅▃▃▄▃▃▄▄▃▃▅▄▃▅▂▁▃▄▁█▅▇▄▄▃▄▂▆▄▃▃▂▃▃▃▃▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb:                      mae 0.31444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb:                     mape 0.08395\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb:                     rmse 0.61717\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb:       time_since_restore 168.45056\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb:         time_this_iter_s 1.70021\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb:             time_total_s 168.45056\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb:                timestamp 1689706962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: 🚀 View run FSR_Trainable_d98df421 at: https://wandb.ai/seokjin/FSR-prediction/runs/d98df421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=411135)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_035940-d98df421/logs\n",
      "2023-07-19 04:02:57,848\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.231 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:02:57,851\tWARNING util.py:315 -- The `process_trial_result` operation took 2.234 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:02:57,852\tWARNING util.py:315 -- Processing trial results took 2.235 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:02:57,853\tWARNING util.py:315 -- The `process_trial_result` operation took 2.237 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_4c44a518_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-01-54/wandb/run-20230719_040301-4c44a518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: Syncing run FSR_Trainable_4c44a518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4c44a518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb:                      mae █▇▇▇▇▇▆▆▆▆▆▆▅▅▅▄▄▄▄▄▃▄▄▃▄▃▃▃▃▄▂▂▂▂▁▁▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb:                     mape █▇▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb:                     rmse █▇▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb:         time_this_iter_s █▆▄▆▃▂▂▁▁▂▁▂▂▃▂▃▂▂▂▂▂▂▃▁▂▂▄▂▂▇▃▃▂▂▂▃▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb:                      mae 0.32092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb:                     mape 0.08872\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb:                     rmse 0.59978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb:       time_since_restore 130.447\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb:         time_this_iter_s 1.18915\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb:             time_total_s 130.447\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb:                timestamp 1689707010\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: 🚀 View run FSR_Trainable_e7085d33 at: https://wandb.ai/seokjin/FSR-prediction/runs/e7085d33\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412739)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040117-e7085d33/logs\n",
      "2023-07-19 04:03:45,614\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.172 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:03:45,618\tWARNING util.py:315 -- The `process_trial_result` operation took 2.177 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:03:45,620\tWARNING util.py:315 -- Processing trial results took 2.179 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:03:45,622\tWARNING util.py:315 -- The `process_trial_result` operation took 2.181 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_477fb7ee_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-02-53/wandb/run-20230719_040348-477fb7ee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Syncing run FSR_Trainable_477fb7ee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/477fb7ee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb:                      mae ██▇▇▇▆▅▅▄▄▃▃▅▃▄▃▄▃▂▂▂▁▂▁▄▂▃▁▃▁▁▁▂▂▁▁▁▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb:                     mape █▇▇▇▆▆▆▆▅▅▄▄▄▄▃▄▄▃▃▃▃▂▂▂▄▃▄▂▄▂▃▃▃▂▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb:                     rmse █▆▆▅▅▄▄▄▃▃▂▂▂▁▂▂▂▁▁▁▁▁▁▁▃▂▂▁▂▁▁▁▁▂▂▂▂▂▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb:         time_this_iter_s █▂▂▂▁▂▁▁▂▁▂▂▃▅▃▁▂▂▂▂▁▂▃▃▂▃▂▂▃▂▂▁▂▂▁▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb:                      mae 0.3165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb:                     mape 0.087\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb:                     rmse 0.61411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb:       time_since_restore 166.3673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb:         time_this_iter_s 1.63805\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb:             time_total_s 166.3673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb:                timestamp 1689707089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: 🚀 View run FSR_Trainable_333e936e at: https://wandb.ai/seokjin/FSR-prediction/runs/333e936e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413202)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040201-333e936e/logs\n",
      "2023-07-19 04:05:06,072\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.246 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:05:06,076\tWARNING util.py:315 -- The `process_trial_result` operation took 2.250 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:05:06,078\tWARNING util.py:315 -- Processing trial results took 2.252 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:05:06,080\tWARNING util.py:315 -- The `process_trial_result` operation took 2.254 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_0385c1d6_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-03-41/wandb/run-20230719_040508-0385c1d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: Syncing run FSR_Trainable_0385c1d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0385c1d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb:                      mae █▇▇▇▇▇▆▅▅█▅▄▄▄▄▃▃▃▄▄▇▃▃▃▃▄▄▃▂▂▃▁▃▂▂▂▃▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb:                     mape █▇▆▆▆▆▅▄▄▆▄▄▃▃▃▃▃▃▄▃▅▃▃▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb:                     rmse █▇▆▆▅▅▅▄▄▅▃▃▃▃▂▂▂▃▃▃▄▃▃▃▂▃▃▃▂▂▂▁▂▂▂▁▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb:         time_this_iter_s █▃▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▂▂▂▂▂▁▂▂▃▂▂▂▂▃▂▂▂▂▁▃▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb:                      mae 0.31348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb:                     mape 0.08611\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb:                     rmse 0.59032\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb:       time_since_restore 123.71226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb:         time_this_iter_s 1.26395\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb:             time_total_s 123.71226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb:                timestamp 1689707104\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: 🚀 View run FSR_Trainable_4c44a518 at: https://wandb.ai/seokjin/FSR-prediction/runs/4c44a518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413473)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040301-4c44a518/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb:                      mae █▂▁▄▄▆▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb:                     mape █▁▃▄▆▅▇▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb:                     rmse █▅▄▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb:         time_this_iter_s █▅▂▃▃▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb:                timestamp ▁▃▄▅▅▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb:                      mae 0.3669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb:                     mape 0.1039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb:                     rmse 0.66427\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb:       time_since_restore 12.93929\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb:         time_this_iter_s 1.39779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb:             time_total_s 12.93929\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb:                timestamp 1689707117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: 🚀 View run FSR_Trainable_0385c1d6 at: https://wandb.ai/seokjin/FSR-prediction/runs/0385c1d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040508-0385c1d6/logs\n",
      "2023-07-19 04:05:21,833\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.314 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:05:21,837\tWARNING util.py:315 -- The `process_trial_result` operation took 2.319 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:05:21,839\tWARNING util.py:315 -- Processing trial results took 2.320 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:05:21,840\tWARNING util.py:315 -- The `process_trial_result` operation took 2.322 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb:                      mae █▆▆▆▆▅▅▅▅▄▄▄▄▆▄▃▃▄▄▄▃▃▃▃▃▂▃▃▅▃▁▂▂▂▁▁▁▁▁▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb:                     mape █▆▆▆▆▅▅▅▅▄▄▄▄▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▁▂▂▁▁▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb:                     rmse █▇▆▆▆▅▅▄▄▄▃▃▃▄▃▂▂▃▃▃▂▃▂▂▂▂▂▂▄▂▁▂▂▂▁▁▁▁▁▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb:         time_this_iter_s █▃▂▃▂▁▂▁▃▂▁▂▁▁▂▂▂▁▁▂▂▂▂▂▁▁▃▂▂▁▄▁▂▃▂▂▂▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414023)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_ac5a6022_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-05-01/wandb/run-20230719_040524-ac5a6022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: Syncing run FSR_Trainable_ac5a6022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ac5a6022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ac5a6022\n",
      "2023-07-19 04:05:32,505\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.884 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:05:32,509\tWARNING util.py:315 -- The `process_trial_result` operation took 1.889 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:05:32,511\tWARNING util.py:315 -- Processing trial results took 1.891 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:05:32,512\tWARNING util.py:315 -- The `process_trial_result` operation took 1.893 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=413735)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_d030706f_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-05-17/wandb/run-20230719_040534-d030706f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: Syncing run FSR_Trainable_d030706f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d030706f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb:                      mae 0.38569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb:                     mape 0.10608\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb:                     rmse 0.7125\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb:       time_since_restore 2.73864\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb:         time_this_iter_s 2.73864\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb:             time_total_s 2.73864\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb:                timestamp 1689707130\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: 🚀 View run FSR_Trainable_d030706f at: https://wandb.ai/seokjin/FSR-prediction/runs/d030706f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414495)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040534-d030706f/logs\n",
      "2023-07-19 04:05:41,740\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.999 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:05:41,745\tWARNING util.py:315 -- The `process_trial_result` operation took 2.004 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:05:41,756\tWARNING util.py:315 -- Processing trial results took 2.015 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:05:41,757\tWARNING util.py:315 -- The `process_trial_result` operation took 2.017 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_def25e2f_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-05-27/wandb/run-20230719_040544-def25e2f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: Syncing run FSR_Trainable_def25e2f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/def25e2f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb:                      mae 0.38096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb:                     mape 0.11694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb:                     rmse 0.70161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb:       time_since_restore 1.52596\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb:         time_this_iter_s 1.52596\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb:             time_total_s 1.52596\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb:                timestamp 1689707139\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: 🚀 View run FSR_Trainable_def25e2f at: https://wandb.ai/seokjin/FSR-prediction/runs/def25e2f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040544-def25e2f/logs\n",
      "2023-07-19 04:05:53,899\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.926 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:05:53,903\tWARNING util.py:315 -- The `process_trial_result` operation took 1.931 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:05:53,905\tWARNING util.py:315 -- Processing trial results took 1.933 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:05:53,906\tWARNING util.py:315 -- The `process_trial_result` operation took 1.934 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_8ab3a6de_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-05-38/wandb/run-20230719_040557-8ab3a6de\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Syncing run FSR_Trainable_8ab3a6de\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8ab3a6de\n",
      "2023-07-19 04:06:07,705\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.147 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:06:07,708\tWARNING util.py:315 -- The `process_trial_result` operation took 2.151 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:06:07,710\tWARNING util.py:315 -- Processing trial results took 2.152 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:06:07,711\tWARNING util.py:315 -- The `process_trial_result` operation took 2.154 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_0066a45c_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-05-50/wandb/run-20230719_040610-0066a45c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: Syncing run FSR_Trainable_0066a45c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0066a45c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb:                      mae ▇█▇▆▆▆▄▄▃▃▃▄▃▂▃▂▂▂▂▂▂▁▁▁▁▁▂▁▁▂▁▁▂▁▁▁▂▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb:                     mape ██▆▅▄▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▂▁▁▂▁▁▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb:                     rmse █▇▆▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb:         time_this_iter_s █▁▅▄▂▃▂▁▄▃▇▅▃▄▂▃▃▄▃▄▃▆▃▃▃▃▃▃▄▄▄▂▃▃▄▃▃▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb:                      mae 0.34221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb:                     mape 0.09158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb:                     rmse 0.62022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb:       time_since_restore 148.19782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb:         time_this_iter_s 1.40416\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb:             time_total_s 148.19782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb:                timestamp 1689707278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: 🚀 View run FSR_Trainable_ac5a6022 at: https://wandb.ai/seokjin/FSR-prediction/runs/ac5a6022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414262)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040524-ac5a6022/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb:                      mae █▆▅▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▃▃▂▃▂▂▂▁▁▁▁▁▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb:                     mape █▇▅▅▄▄▄▃▄▃▃▃▃▃▃▂▃▃▃▂▃▃▃▃▃▂▂▃▂▂▂▃▂▂▁▁▁▁▅▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb:                     rmse █▇▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb:         time_this_iter_s █▃▂▂▄▃▂▂▁▂▃▂▂▂▃▂▂▂▂▂▁▂▃▂▃▃▂▂▁▃▂▁▂▃▂▃▇▁▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040557-8ab3a6de/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040557-8ab3a6de/logs\n",
      "2023-07-19 04:08:12,074\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.812 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:08:12,077\tWARNING util.py:315 -- The `process_trial_result` operation took 1.816 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:08:12,079\tWARNING util.py:315 -- Processing trial results took 1.817 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:08:12,079\tWARNING util.py:315 -- The `process_trial_result` operation took 1.818 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=414948)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_4ad05cbc_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-06-03/wandb/run-20230719_040815-4ad05cbc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: Syncing run FSR_Trainable_4ad05cbc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4ad05cbc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb:                      mae ▇▇████▇▇▆▆▆▆▆▅▅▆▅▄▄▆▄▃▄▄▃▃▂▂▂▂▂▃▃▂▄▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb:                     mape ████▇▆▆▅▅▅▅▅▅▅▄▅▄▄▄▅▃▃▄▃▃▃▂▂▂▂▂▃▃▃▄▂▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb:                     rmse █▆▆▆▅▅▅▅▄▄▄▄▄▄▃▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb:         time_this_iter_s █▅▃▂▄▃▃▃▂▂▂▃▃▄▃▃▃▃▃▂▂▂▂▃▃▃▂▃▂▂▂▂▄▂▂▂▂▂▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb:                      mae 0.31775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb:                     mape 0.09087\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb:                     rmse 0.59877\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb:       time_since_restore 123.16494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb:         time_this_iter_s 1.41631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb:             time_total_s 123.16494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb:                timestamp 1689707293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: 🚀 View run FSR_Trainable_0066a45c at: https://wandb.ai/seokjin/FSR-prediction/runs/0066a45c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040610-0066a45c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415164)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 04:08:33,898\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 5.079 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:08:33,902\tWARNING util.py:315 -- The `process_trial_result` operation took 5.083 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:08:33,904\tWARNING util.py:315 -- Processing trial results took 5.086 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:08:33,906\tWARNING util.py:315 -- The `process_trial_result` operation took 5.087 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_8094e3b1_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-08-08/wandb/run-20230719_040837-8094e3b1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: Syncing run FSR_Trainable_8094e3b1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8094e3b1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 04:08:48,266\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.178 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:08:48,271\tWARNING util.py:315 -- The `process_trial_result` operation took 2.184 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:08:48,274\tWARNING util.py:315 -- Processing trial results took 2.187 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:08:48,276\tWARNING util.py:315 -- The `process_trial_result` operation took 2.189 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb:                      mae █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb:                     mape █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb:                     rmse █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb:         time_this_iter_s █▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb:                      mae 0.34556\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb:                     mape 0.10234\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb:                     rmse 0.67583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb:       time_since_restore 12.33993\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb:         time_this_iter_s 2.22664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb:             time_total_s 12.33993\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb:                timestamp 1689707321\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: 🚀 View run FSR_Trainable_8094e3b1 at: https://wandb.ai/seokjin/FSR-prediction/runs/8094e3b1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415738)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040837-8094e3b1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_ae002d89_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-08-23/wandb/run-20230719_040850-ae002d89\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: Syncing run FSR_Trainable_ae002d89\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ae002d89\n",
      "2023-07-19 04:08:59,702\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.956 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:08:59,704\tWARNING util.py:315 -- The `process_trial_result` operation took 1.960 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:08:59,707\tWARNING util.py:315 -- Processing trial results took 1.962 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:08:59,719\tWARNING util.py:315 -- The `process_trial_result` operation took 1.974 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_71b9e672_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-08-43/wandb/run-20230719_040902-71b9e672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: Syncing run FSR_Trainable_71b9e672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/71b9e672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb:                      mae █▆▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▃▄▃▂▃▂▁▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb:                     mape █▇▆▆▅▅▅▄▄▃▃▃▃▂▂▂▂▃▂▂▂▃▂▂▂▁▂▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb:                     rmse █▇▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb:         time_this_iter_s ▃▂▂▁▂▄▁▃▆█▇▂▄▄▂▂▃▃▁▂▂▂▂▂▃▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb:                      mae 0.33997\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb:                     mape 0.09559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb:                     rmse 0.63593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb:       time_since_restore 51.46476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb:         time_this_iter_s 1.28676\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb:             time_total_s 51.46476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb:                timestamp 1689707348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: 🚀 View run FSR_Trainable_4ad05cbc at: https://wandb.ai/seokjin/FSR-prediction/runs/4ad05cbc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415508)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040815-4ad05cbc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb:                      mae ▇█▇▇▇▇▆▆▇▅▄▄▄▄▃▄▃▂▂▃▃▃▂▂▂▃▃▂▃▁▁▁▂▂▁▂▂▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb:                     mape ▇█▇▆▆▆▆▅▆▆▅▅▅▄▄▃▄▃▂▂▂▃▂▂▂▃▃▁▂▁▂▂▃▃▁▃▃▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb:                     rmse █▇▆▆▆▆▅▅▅▄▄▃▃▃▃▃▃▂▂▂▂▃▂▂▂▃▃▃▂▁▁▂▂▂▁▂▂▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb:         time_this_iter_s █▂▂▃▂▂▂▃▃▂▂▂▂▂▂▂▂▂▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▆▇▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb:                      mae 0.29341\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb:                     mape 0.08053\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb:                     rmse 0.5719\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb:       time_since_restore 455.66377\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb:         time_this_iter_s 4.98491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb:             time_total_s 455.66377\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb:                timestamp 1689707354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: 🚀 View run FSR_Trainable_db1779a3 at: https://wandb.ai/seokjin/FSR-prediction/runs/db1779a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=412958)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040130-db1779a3/logs\n",
      "2023-07-19 04:09:27,484\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.422 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:09:27,489\tWARNING util.py:315 -- The `process_trial_result` operation took 2.427 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:09:27,490\tWARNING util.py:315 -- Processing trial results took 2.428 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:09:27,491\tWARNING util.py:315 -- The `process_trial_result` operation took 2.430 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_73205455_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-08-55/wandb/run-20230719_040928-73205455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: Syncing run FSR_Trainable_73205455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/73205455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb:                      mae 0.34804\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb:                     mape 0.10117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb:                     rmse 0.70003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb:       time_since_restore 4.25772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb:         time_this_iter_s 4.25772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb:             time_total_s 4.25772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb:                timestamp 1689707365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: 🚀 View run FSR_Trainable_73205455 at: https://wandb.ai/seokjin/FSR-prediction/runs/73205455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416437)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040928-73205455/logs\n",
      "2023-07-19 04:09:39,017\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.891 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:09:39,018\tWARNING util.py:315 -- The `process_trial_result` operation took 1.893 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:09:39,020\tWARNING util.py:315 -- Processing trial results took 1.895 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:09:39,021\tWARNING util.py:315 -- The `process_trial_result` operation took 1.896 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_725f2efb_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-09-20/wandb/run-20230719_040939-725f2efb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: Syncing run FSR_Trainable_725f2efb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/725f2efb\n",
      "2023-07-19 04:09:50,586\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.067 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:09:50,588\tWARNING util.py:315 -- The `process_trial_result` operation took 2.069 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:09:50,590\tWARNING util.py:315 -- Processing trial results took 2.071 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:09:50,592\tWARNING util.py:315 -- The `process_trial_result` operation took 2.073 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_2efed0e1_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-09-32/wandb/run-20230719_040951-2efed0e1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: Syncing run FSR_Trainable_2efed0e1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2efed0e1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb:                      mae ▃▁▃█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb:                     mape ▁▂▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb:                     rmse █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb:         time_this_iter_s ▇▂█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb:                timestamp ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb:                      mae 0.34129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb:                     mape 0.10095\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb:                     rmse 0.67605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb:       time_since_restore 18.20523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb:         time_this_iter_s 4.06018\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb:             time_total_s 18.20523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb:                timestamp 1689707392\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: 🚀 View run FSR_Trainable_725f2efb at: https://wandb.ai/seokjin/FSR-prediction/runs/725f2efb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416654)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040939-725f2efb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 04:10:13,509\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.770 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:10:13,512\tWARNING util.py:315 -- The `process_trial_result` operation took 2.775 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:10:13,518\tWARNING util.py:315 -- Processing trial results took 2.781 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:10:13,520\tWARNING util.py:315 -- The `process_trial_result` operation took 2.783 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_6dca7ca5_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-09-42/wandb/run-20230719_041013-6dca7ca5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: Syncing run FSR_Trainable_6dca7ca5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6dca7ca5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb:                      mae █▇▇▇▆▅▅▅▅▄▄▄▅▄▃▃▃▄▄▃▃▃▂▂▂▂▃▂▁▂▂▂▃▁▁▁▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb:                     mape ███▇▆▆▆▆▆▅▅▅▅▅▅▅▄▅▄▄▄▄▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb:                     rmse █▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▂▁▁▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb:         time_this_iter_s █▃▅▄▃▄▂▄▃▁▃▁▄▂▂▂▃▄▄▃▄▄▃▃▃▃▃▃▂▂▂▂▂▃▃▂▃▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb:                      mae 0.31689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb:                     mape 0.08524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb:                     rmse 0.60117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb:       time_since_restore 169.97196\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb:         time_this_iter_s 1.65667\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb:             time_total_s 169.97196\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb:                timestamp 1689707507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: 🚀 View run FSR_Trainable_ae002d89 at: https://wandb.ai/seokjin/FSR-prediction/runs/ae002d89\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=415962)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040850-ae002d89/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb:                      mae █▇▇▆▆▆▅▆▅▅▅▅▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb:                     mape ███▇▆▆▆▆▆▆▆▆▆▅▅▅▄▄▄▄▄▄▃▃▄▃▃▂▂▃▂▂▂▂▁▁▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb:                     rmse █▇▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb:         time_this_iter_s █▅▅▅▄▅▅▁▅▄▆▃▅▃▂▇▇▆▅▆▄▄▅▄▅▃▄▄▅▅▅▆▃▃▄▄▄▅▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb:                      mae 0.30581\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb:                     mape 0.08573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb:                     rmse 0.58847\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb:       time_since_restore 172.24061\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb:         time_this_iter_s 1.75564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb:             time_total_s 172.24061\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb:                timestamp 1689707520\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: 🚀 View run FSR_Trainable_71b9e672 at: https://wandb.ai/seokjin/FSR-prediction/runs/71b9e672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416187)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040902-71b9e672/logs\n",
      "2023-07-19 04:12:05,194\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.167 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:12:05,197\tWARNING util.py:315 -- The `process_trial_result` operation took 2.171 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:12:05,199\tWARNING util.py:315 -- Processing trial results took 2.173 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:12:05,201\tWARNING util.py:315 -- The `process_trial_result` operation took 2.175 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_e9ddb955_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-10-05/wandb/run-20230719_041207-e9ddb955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: Syncing run FSR_Trainable_e9ddb955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e9ddb955\n",
      "2023-07-19 04:12:19,472\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.976 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:12:19,479\tWARNING util.py:315 -- The `process_trial_result` operation took 2.983 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:12:19,481\tWARNING util.py:315 -- Processing trial results took 2.986 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:12:19,485\tWARNING util.py:315 -- The `process_trial_result` operation took 2.990 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_f451ac61_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-11-59/wandb/run-20230719_041221-f451ac61\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: Syncing run FSR_Trainable_f451ac61\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f451ac61\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb:                      mae ▇▆▇▆▆▆▇▇▅▆▆▄▅▄▄▃▃▃▃▃▃▃█▂▂▂▁▁▁▁▁▁▃▂▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb:                     mape █▇▇▆▆▆▆▆▅▅▅▅▅▄▅▄▄▄▄▄▄▃▅▃▃▃▂▂▂▂▂▂▃▃▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb:                     rmse █▇▆▅▅▅▅▅▄▄▄▃▄▃▃▃▂▂▂▃▂▂▅▂▂▂▁▁▁▁▁▁▃▂▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb:         time_this_iter_s ▇▂██▄▁▆▃▂▂▂▃▁▃▃▂▃▃▂▂▂▃▃▂▂▂▃▂▃▃▂▂▄▂▂▂▃▂▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb:                      mae 0.30233\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb:                     mape 0.08126\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb:                     rmse 0.58645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb:       time_since_restore 279.77235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb:         time_this_iter_s 2.61312\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb:             time_total_s 279.77235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb:                timestamp 1689707810\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: 🚀 View run FSR_Trainable_e9ddb955 at: https://wandb.ai/seokjin/FSR-prediction/runs/e9ddb955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417413)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_041207-e9ddb955/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb:                      mae ▇█▇█▇▇▆▆▅▆▄▄▄▄▄▅▄▃▃▃▃▅▃▂▂▂▂▂▂▁▂▂▂▁▂▁▁▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb:                     mape ██▇▇▆▆▆▆▅▆▅▅▅▄▄▅▅▄▄▃▃▄▂▂▂▂▂▃▂▂▁▃▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb:                     rmse █▇▆▆▅▅▅▄▄▄▄▄▃▃▃▄▃▃▃▂▂▃▂▂▂▂▁▂▁▁▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb:         time_this_iter_s ▅█▃▃▃▂▂▃▂▂▂▃▂▂▂▂▂▂▂▁▁▂▄▁▅▃▂▂▃▃▂▂▂▂▂▁▂▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb:                      mae 0.30065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb:                     mape 0.08216\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb:                     rmse 0.5741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb:       time_since_restore 280.05136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb:         time_this_iter_s 2.63517\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb:             time_total_s 280.05136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb:                timestamp 1689707823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: 🚀 View run FSR_Trainable_f451ac61 at: https://wandb.ai/seokjin/FSR-prediction/runs/f451ac61\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417637)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_041221-f451ac61/logs\n",
      "2023-07-19 04:17:10,211\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.562 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:17:10,213\tWARNING util.py:315 -- The `process_trial_result` operation took 2.566 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:17:10,215\tWARNING util.py:315 -- Processing trial results took 2.567 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:17:10,216\tWARNING util.py:315 -- The `process_trial_result` operation took 2.568 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_e9eabb15_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-12-13/wandb/run-20230719_041712-e9eabb15\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: Syncing run FSR_Trainable_e9eabb15\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e9eabb15\n",
      "2023-07-19 04:17:22,614\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.156 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:17:22,616\tWARNING util.py:315 -- The `process_trial_result` operation took 2.159 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:17:22,619\tWARNING util.py:315 -- Processing trial results took 2.162 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:17:22,620\tWARNING util.py:315 -- The `process_trial_result` operation took 2.163 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_84bae381_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-17-04/wandb/run-20230719_041724-84bae381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: Syncing run FSR_Trainable_84bae381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/84bae381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb:                      mae ▃▂▁▅▄▄▄▄▄▅▄▆▄▆▂█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb:                     mape █▆▅▇▅▄▄▄▄▃▃▄▂▄▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb:                     rmse █▇▆▄▄▃▃▃▃▂▂▂▂▂▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb:         time_this_iter_s █▅▂▃▂▃▂▃▁▂▇▃▂▆▃▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb:                timestamp ▁▂▂▃▃▄▄▄▅▅▆▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb:                      mae 0.35484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb:                     mape 0.09713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb:                     rmse 0.64735\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb:       time_since_restore 45.84926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb:         time_this_iter_s 3.10129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb:             time_total_s 45.84926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb:                timestamp 1689707886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: 🚀 View run FSR_Trainable_84bae381 at: https://wandb.ai/seokjin/FSR-prediction/runs/84bae381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418269)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_041724-84bae381/logs\n",
      "2023-07-19 04:18:25,535\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.629 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:18:25,539\tWARNING util.py:315 -- The `process_trial_result` operation took 2.633 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:18:25,543\tWARNING util.py:315 -- Processing trial results took 2.638 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:18:25,545\tWARNING util.py:315 -- The `process_trial_result` operation took 2.640 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_a802a962_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-17-17/wandb/run-20230719_041828-a802a962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: Syncing run FSR_Trainable_a802a962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a802a962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb:                      mae █▇██▇▇▆█▆▆▆▆▅▅▅▄▄▄▃▃▃▂▂▃▃▂▅▃▂▂▂▂▂▃▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb:                     mape █▇▇▆▆▆▆▆▅▅▅▅▅▅▅▅▅▄▄▄▃▃▃▃▂▂▃▃▂▂▂▂▂▃▂▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb:                     rmse █▇▆▆▅▅▅▆▅▄▅▅▄▄▄▃▃▃▃▃▃▂▂▃▃▃▅▃▂▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb:         time_this_iter_s ▇▁▅▅▁▁▁▂▂▂▄▅▇▅▃▃▃▃▂▃▃▂▃▃▆▇▄▃▃▄▂▂▂▄▄▄▃▆█▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb:                      mae 0.28133\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb:                     mape 0.07555\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb:                     rmse 0.55166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb:       time_since_restore 510.82171\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb:         time_this_iter_s 5.27446\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb:             time_total_s 510.82171\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb:                timestamp 1689707907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: 🚀 View run FSR_Trainable_2efed0e1 at: https://wandb.ai/seokjin/FSR-prediction/runs/2efed0e1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=416876)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_040951-2efed0e1/logs\n",
      "2023-07-19 04:18:46,816\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.935 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:18:46,820\tWARNING util.py:315 -- The `process_trial_result` operation took 2.941 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:18:46,824\tWARNING util.py:315 -- Processing trial results took 2.944 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:18:46,826\tWARNING util.py:315 -- The `process_trial_result` operation took 2.946 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_57517084_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-18-19/wandb/run-20230719_041848-57517084\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Syncing run FSR_Trainable_57517084\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/57517084\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb:                      mae ▇███▇▇▇▆▆▅▆▆▅▆▅▄▄▄▄▄▅▄▄▂▂▃▂▂▂▂▃▃▁▁▂▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb:                     mape ███▇▇▇▇▆▆▆▆▇▅▇▅▅▅▅▅▅▆▄▄▃▃▂▃▂▂▂▂▂▁▂▁▁▁▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb:                     rmse █▇▆▆▅▅▅▄▄▄▄▄▃▄▄▄▃▃▃▃▄▃▃▂▂▂▂▁▁▂▂▂▁▁▂▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb:         time_this_iter_s ▇▆▁▁▂▂▁▁▃▅▇█▃▄▃▂▂▃▃▃▃▃▆▆▃▂▃▃▃▂▃▃▄▅▃▆▇▅▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb:                      mae 0.28852\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb:                     mape 0.07721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb:                     rmse 0.56973\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb:       time_since_restore 514.74451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb:         time_this_iter_s 5.22693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb:             time_total_s 514.74451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb:                timestamp 1689707934\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: 🚀 View run FSR_Trainable_6dca7ca5 at: https://wandb.ai/seokjin/FSR-prediction/runs/6dca7ca5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=417113)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_041013-6dca7ca5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_041013-6dca7ca5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb:                      mae ▂▁▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb:                     mape █▃▁▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb:                     rmse █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb:         time_this_iter_s █▃▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "2023-07-19 04:19:10,607\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.404 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:19:10,610\tWARNING util.py:315 -- The `process_trial_result` operation took 2.409 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:19:10,611\tWARNING util.py:315 -- Processing trial results took 2.410 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:19:10,613\tWARNING util.py:315 -- The `process_trial_result` operation took 2.412 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_ca3746b5_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-18-40/wandb/run-20230719_041912-ca3746b5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Syncing run FSR_Trainable_ca3746b5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ca3746b5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_0133600f_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-19-05/wandb/run-20230719_041926-0133600f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: Syncing run FSR_Trainable_0133600f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0133600f\n",
      "2023-07-19 04:19:28,969\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.699 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:19:28,974\tWARNING util.py:315 -- The `process_trial_result` operation took 2.705 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:19:28,975\tWARNING util.py:315 -- Processing trial results took 2.707 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:19:28,977\tWARNING util.py:315 -- The `process_trial_result` operation took 2.708 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb:                      mae 1.83596\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb:                     mape 0.28374\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb:                     rmse 2.12064\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb:       time_since_restore 8.65977\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb:         time_this_iter_s 8.65977\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb:             time_total_s 8.65977\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb:                timestamp 1689707966\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: 🚀 View run FSR_Trainable_0133600f at: https://wandb.ai/seokjin/FSR-prediction/runs/0133600f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419230)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_041926-0133600f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb:                      mae ▂▁▃▆▆▅█▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb:                     mape ▅▃▃██▆█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb:                     rmse █▅▃▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb:         time_this_iter_s ▆▃▁█▅▅▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419016)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_2b850322_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-19-17/wandb/run-20230719_041949-2b850322\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: Syncing run FSR_Trainable_2b850322\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2b850322\n",
      "2023-07-19 04:19:51,210\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.337 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:19:51,213\tWARNING util.py:315 -- The `process_trial_result` operation took 2.341 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:19:51,214\tWARNING util.py:315 -- Processing trial results took 2.343 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:19:51,215\tWARNING util.py:315 -- The `process_trial_result` operation took 2.344 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb:                      mae 1.70749\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb:                     mape 0.26907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb:                     rmse 1.91813\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb:       time_since_restore 8.28284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb:         time_this_iter_s 8.28284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb:             time_total_s 8.28284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb:                timestamp 1689707988\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: 🚀 View run FSR_Trainable_2b850322 at: https://wandb.ai/seokjin/FSR-prediction/runs/2b850322\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419478)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_041949-2b850322/logs\n",
      "2023-07-19 04:20:01,503\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.089 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:20:01,506\tWARNING util.py:315 -- The `process_trial_result` operation took 2.094 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:20:01,509\tWARNING util.py:315 -- Processing trial results took 2.096 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:20:01,510\tWARNING util.py:315 -- The `process_trial_result` operation took 2.097 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_942275ec_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-19-40/wandb/run-20230719_042001-942275ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: Syncing run FSR_Trainable_942275ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/942275ec\n",
      "2023-07-19 04:20:14,092\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.410 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:20:14,094\tWARNING util.py:315 -- The `process_trial_result` operation took 2.414 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:20:14,096\tWARNING util.py:315 -- Processing trial results took 2.415 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:20:14,097\tWARNING util.py:315 -- The `process_trial_result` operation took 2.416 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_9288852c_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-19-53/wandb/run-20230719_042014-9288852c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: Syncing run FSR_Trainable_9288852c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9288852c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb:                      mae 0.34229\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb:                     mape 0.09947\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb:                     rmse 0.68861\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb:       time_since_restore 11.37147\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb:         time_this_iter_s 5.18402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb:             time_total_s 11.37147\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb:                timestamp 1689708019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: 🚀 View run FSR_Trainable_9288852c at: https://wandb.ai/seokjin/FSR-prediction/runs/9288852c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419919)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_042014-9288852c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 04:20:41,204\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.713 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:20:41,207\tWARNING util.py:315 -- The `process_trial_result` operation took 2.718 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:20:41,209\tWARNING util.py:315 -- Processing trial results took 2.720 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:20:41,211\tWARNING util.py:315 -- The `process_trial_result` operation took 2.722 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_0d66a628_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-20-05/wandb/run-20230719_042041-0d66a628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: Syncing run FSR_Trainable_0d66a628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0d66a628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb:                      mae 0.34375\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb:                     mape 0.10036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb:                     rmse 0.69272\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb:       time_since_restore 12.58361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb:         time_this_iter_s 5.97127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb:             time_total_s 12.58361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb:                timestamp 1689708047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: 🚀 View run FSR_Trainable_0d66a628 at: https://wandb.ai/seokjin/FSR-prediction/runs/0d66a628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420153)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_042041-0d66a628/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 04:21:16,338\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.875 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:21:16,343\tWARNING util.py:315 -- The `process_trial_result` operation took 2.881 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:21:16,346\tWARNING util.py:315 -- Processing trial results took 2.884 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:21:16,348\tWARNING util.py:315 -- The `process_trial_result` operation took 2.885 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_551b247c_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-20-32/wandb/run-20230719_042116-551b247c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: Syncing run FSR_Trainable_551b247c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/551b247c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb:                      mae ████▆█▆▇▅▄▄▄▃▃▆▃▃▃▃▂▂▂▂▄▃▂▁▂▂▂▂▁▂▁▁▆▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb:                     mape █▇▇▇▆▆▅▆▅▅▄▅▄▄▄▃▃▄▃▂▂▂▂▂▂▂▁▁▁▁▂▁▂▁▁▄▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb:                     rmse █▇▆▆▅▅▅▅▃▃▃▃▃▂▄▂▂▂▁▁▁▁▁▃▃▂▁▂▂▂▂▁▂▂▁▃▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb:         time_this_iter_s ▆▄▄▃▃▃▃▄▅▃▅▅▃▄▃▁▄▃▄▃▁▄▃▄▅▄▃▃▄▄█▆▅▄▃▃▃▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb:                      mae 0.29302\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb:                     mape 0.08106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb:                     rmse 0.58114\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb:       time_since_restore 287.92209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb:         time_this_iter_s 2.71421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb:             time_total_s 287.92209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb:                timestamp 1689708129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: 🚀 View run FSR_Trainable_e9eabb15 at: https://wandb.ai/seokjin/FSR-prediction/runs/e9eabb15\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418046)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_041712-e9eabb15/logs\n",
      "2023-07-19 04:22:30,817\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.719 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:22:30,822\tWARNING util.py:315 -- The `process_trial_result` operation took 2.724 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:22:30,823\tWARNING util.py:315 -- Processing trial results took 2.726 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:22:30,824\tWARNING util.py:315 -- The `process_trial_result` operation took 2.727 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_b531ab6b_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-21-06/wandb/run-20230719_042234-b531ab6b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: Syncing run FSR_Trainable_b531ab6b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b531ab6b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb:                      mae █▇████▇▇▇█▅▆▅▅▅▅▅▆▄▄▄▃▃▂▂▄▃▄▃▃▃▂▂▂▁▁▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb:                     mape █▇▇▇▆▆▆▆▆▇▅▅▅▅▅▅▅▅▅▄▄▄▃▃▃▄▄▄▄▄▃▃▃▂▂▁▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb:                     rmse █▇▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▄▃▃▃▂▂▂▂▃▂▃▂▂▂▂▂▂▁▁▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb:         time_this_iter_s ▅▅▃▃▃▁▄▃▄▃▁▄▄▃▄▃▃▆▃█▇▆▄▃▃▃▃▃▃▃▃▄▃▃▄▃▃▃▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb:                      mae 0.29893\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb:                     mape 0.08099\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb:                     rmse 0.57742\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb:       time_since_restore 295.17509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb:         time_this_iter_s 3.10946\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb:             time_total_s 295.17509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb:                timestamp 1689708210\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: 🚀 View run FSR_Trainable_a802a962 at: https://wandb.ai/seokjin/FSR-prediction/runs/a802a962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=418538)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_041828-a802a962/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 04:23:52,647\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.632 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:23:52,650\tWARNING util.py:315 -- The `process_trial_result` operation took 2.636 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:23:52,652\tWARNING util.py:315 -- Processing trial results took 2.637 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:23:52,654\tWARNING util.py:315 -- The `process_trial_result` operation took 2.640 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_797a94cd_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-22-24/wandb/run-20230719_042352-797a94cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: Syncing run FSR_Trainable_797a94cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/797a94cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb:                      mae ▇██▇▇▆▇█▆▆▅▄▅▄▄▂▂▂▃▃▄▅▃▂▂▂▄▂▁▁▁▁▂▂▂▅▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb:                     mape ██▇▇▆▆▇▆▆▆▅▅▅▄▄▃▃▃▄▂▃▃▃▂▂▁▃▁▁▂▁▂▂▂▁▁▂▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb:                     rmse █▆▆▅▅▄▄▅▄▃▃▂▃▂▃▂▁▂▂▂▄▃▂▁▂▁▃▂▁▁▁▂▂▂▂▄▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb:         time_this_iter_s █▄▃▄▂▃▄▃▅▁▆▄▃▃▄▃▂▂▃▃▃▄▃▂▄▃▂▄▃▄▃▂▃▂▂▃▂▃▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb:                      mae 0.28977\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb:                     mape 0.0775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb:                     rmse 0.56175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb:       time_since_restore 302.4514\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb:         time_this_iter_s 2.96484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb:             time_total_s 302.4514\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb:                timestamp 1689708456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: 🚀 View run FSR_Trainable_b531ab6b at: https://wandb.ai/seokjin/FSR-prediction/runs/b531ab6b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420662)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_042234-b531ab6b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 04:27:58,522\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.514 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:27:58,526\tWARNING util.py:315 -- The `process_trial_result` operation took 2.518 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:27:58,527\tWARNING util.py:315 -- Processing trial results took 2.520 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:27:58,530\tWARNING util.py:315 -- The `process_trial_result` operation took 2.522 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_03-37-48/FSR_Trainable_ca336f36_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_Simple_2023-07-19_04-23-43/wandb/run-20230719_042758-ca336f36\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: Syncing run FSR_Trainable_ca336f36\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ca336f36\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb:                      mae ▆▆▆▆▆█▅▅▅▅▄▃▃▃▃▃▂▃▃▂▂▃▂▂▁▁▂▂▃▄▃▂▁▁▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb:                     mape ██▇▇▇█▇▇▆▇▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▂▃▂▃▃▃▃▂▂▃▂▃▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb:                     rmse █▆▆▅▅█▅▅▅▅▄▃▃▃▂▂▂▂▃▂▂▃▂▁▁▁▂▃▃▄▃▂▁▁▂▂▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb:         time_this_iter_s ▇▄▁▄█▅▂▂▂▂▄▃▂▂▄▁▂▂▁▂▂▂▂▃▂▂▃▃▂▂▂▂▂▂▂▂▂▂▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb:                      mae 0.28548\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb:                     mape 0.07861\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb:                     rmse 0.56522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb:       time_since_restore 549.91669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb:         time_this_iter_s 5.45722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb:             time_total_s 549.91669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb:                timestamp 1689708561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: 🚀 View run FSR_Trainable_942275ec at: https://wandb.ai/seokjin/FSR-prediction/runs/942275ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=419697)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_042001-942275ec/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb:                      mae ▇█▇▇▇█▆▇▆▅▆▅▄▃▃▄▃▂▂▂▃▂▂▂▂▂▃▂▂▂▂▁▂▂▂▂▁▁▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb:                     mape ██▇▇▇▇▆▇▇▆▆▆▅▄▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▃▁▂▃▂▂▂▂▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb:                     rmse █▇▆▆▆▆▅▅▅▄▅▄▃▃▃▃▃▂▂▂▃▂▂▂▂▂▃▂▂▂▂▁▂▂▂▂▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb:         time_this_iter_s █▃▃▃▃▄▃▄▃▃▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb:                      mae 0.27815\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb:                     mape 0.07089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb:                     rmse 0.55517\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb:       time_since_restore 533.07135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb:         time_this_iter_s 4.6049\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb:             time_total_s 533.07135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb:                timestamp 1689708612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: 🚀 View run FSR_Trainable_551b247c at: https://wandb.ai/seokjin/FSR-prediction/runs/551b247c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420393)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_042116-551b247c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb:                      mae ▆█▇▇█▆▇▇▆▆▅▄▄▄▄▅▅▂▂▃▃▃▂▂▃▂▂▃▄▁▁▁▃▂▃▂▂▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb:                     mape ██▇▇▇▆▆▇▆▆▆▆▅▄▄▄▄▃▂▂▄▂▂▂▂▂▂▃▂▂▂▁▄▂▂▂▂▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb:                     rmse █▆▆▅▅▅▄▅▄▄▄▃▃▃▃▄▃▂▂▂▃▂▂▂▂▂▂▃▃▁▁▁▃▂▂▂▂▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb:         time_this_iter_s █▆▆▆▅▆▆▆▆▆▆▅▆▆▆▆▆▆▅▆▆▆▆▆▅▄▄▄▃▂▂▂▂▂▂▁▂▃▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb:                      mae 0.2862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb:                     mape 0.07176\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb:                     rmse 0.58646\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb:       time_since_restore 483.11193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb:         time_this_iter_s 3.53882\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb:             time_total_s 483.11193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb:                timestamp 1689708716\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: 🚀 View run FSR_Trainable_797a94cd at: https://wandb.ai/seokjin/FSR-prediction/runs/797a94cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=420934)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_042352-797a94cd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb:                      mae ▇████▇██▇▇▆▆▆▆▆▄▅▄▄▃▃▅▃▃▃▂▃▄▃▂▂▃▂▂▂▂▂▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb:                     mape ████▇▇▇▆▇▆▆▇▆▆▆▅▅▄▄▃▃▄▃▃▂▂▃▂▃▂▁▂▁▂▁▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb:                     rmse █▇▆▆▆▆▆▆▅▆▅▅▄▄▄▃▄▃▃▃▃▄▃▂▂▂▂▃▂▂▁▂▂▂▂▂▂▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb:         time_this_iter_s █▆▆▆▆▆▆▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▁▁▁▃▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb:                      mae 0.27566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb:                     mape 0.0739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb:                     rmse 0.54282\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb:       time_since_restore 349.57881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb:         time_this_iter_s 2.46339\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb:             time_total_s 349.57881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb:                timestamp 1689708826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: 🚀 View run FSR_Trainable_ca336f36 at: https://wandb.ai/seokjin/FSR-prediction/runs/ca336f36\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=421308)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_042758-ca336f36/logs\n",
      "2023-07-19 04:33:51,090\tINFO tune.py:1111 -- Total run time: 3358.07 seconds (3353.39 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
