{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2\n",
    "\n",
    "Index_X = FSR_for_force\n",
    "\n",
    "Index_y = force\n",
    "\n",
    "Data = Splited by Time\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-07-19_01-25-28/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-07-19_01-25-28\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "155.939"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.LSTM'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': ['FSR_for_force'],\n",
    "        'index_y': ['force'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_time'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-19 01:25:28,561] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 01:25:30,727\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-07-19 01:25:32,871\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-19 02:20:16</td></tr>\n",
       "<tr><td>Running for: </td><td>00:54:43.53        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.2/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=98<br>Bracket: Iter 64.000: -179.233328888701 | Iter 32.000: -190.29699992366096 | Iter 16.000: -202.41923095049918 | Iter 8.000: -215.10861371069444 | Iter 4.000: -225.69818023390067 | Iter 2.000: -245.22880052751873 | Iter 1.000: -292.54638239691724<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 2<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_cb2a0ab8</td><td style=\"text-align: right;\">           1</td><td>/home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_cb2a0ab8_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-44-07/error.txt</td></tr>\n",
       "<tr><td>FSR_Trainable_84b2dd83</td><td style=\"text-align: right;\">           1</td><td>/home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_84b2dd83_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-48-48/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>index_X          </th><th>index_y  </th><th>model         </th><th style=\"text-align: right;\">    model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">  model_args/num_layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">       mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_b33015b0</td><td>TERMINATED</td><td>172.26.215.93:343759</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.70258e-05</td><td>sklearn.preproc_c330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        30.639  </td><td style=\"text-align: right;\">501.491</td><td style=\"text-align: right;\">263.186 </td><td style=\"text-align: right;\">2.70533e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_dec1d3bc</td><td>TERMINATED</td><td>172.26.215.93:343830</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000118354</td><td>sklearn.preproc_c330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       282.555  </td><td style=\"text-align: right;\">473.674</td><td style=\"text-align: right;\">333.961 </td><td style=\"text-align: right;\">6.69987e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_2e07ed6e</td><td>TERMINATED</td><td>172.26.215.93:344000</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0173801  </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       273.919  </td><td style=\"text-align: right;\">455.851</td><td style=\"text-align: right;\">250.002 </td><td style=\"text-align: right;\">6.89387e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_eb7c8397</td><td>TERMINATED</td><td>172.26.215.93:344171</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00930899 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       513.796  </td><td style=\"text-align: right;\">340.535</td><td style=\"text-align: right;\">163.032 </td><td style=\"text-align: right;\">1.43527e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_d35e4a82</td><td>TERMINATED</td><td>172.26.215.93:344517</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.19314e-05</td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       295.63   </td><td style=\"text-align: right;\">453.537</td><td style=\"text-align: right;\">265.363 </td><td style=\"text-align: right;\">8.98206e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_710c8739</td><td>TERMINATED</td><td>172.26.215.93:344985</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000494913</td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       260.039  </td><td style=\"text-align: right;\">211.253</td><td style=\"text-align: right;\">109.532 </td><td style=\"text-align: right;\">1.31731e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_06afbe31</td><td>TERMINATED</td><td>172.26.215.93:345171</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00340188 </td><td>sklearn.preproc_c330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       141.056  </td><td style=\"text-align: right;\">203.836</td><td style=\"text-align: right;\">115.884 </td><td style=\"text-align: right;\">1.27579e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_8cad7d00</td><td>TERMINATED</td><td>172.26.215.93:345462</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00106563 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.23119</td><td style=\"text-align: right;\">484.579</td><td style=\"text-align: right;\">248.847 </td><td style=\"text-align: right;\">2.87471e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_c4a94404</td><td>TERMINATED</td><td>172.26.215.93:345661</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        3.51704e-05</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.05959</td><td style=\"text-align: right;\">477.799</td><td style=\"text-align: right;\">238.344 </td><td style=\"text-align: right;\">3.28731e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_796ead13</td><td>TERMINATED</td><td>172.26.215.93:345927</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        1.43257e-05</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.66094</td><td style=\"text-align: right;\">531.855</td><td style=\"text-align: right;\">241.084 </td><td style=\"text-align: right;\">5.66079e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_5e4396e7</td><td>TERMINATED</td><td>172.26.215.93:346126</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.030035   </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       174.035  </td><td style=\"text-align: right;\">180.984</td><td style=\"text-align: right;\"> 89.0623</td><td style=\"text-align: right;\">7.16183e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_bc99afd4</td><td>TERMINATED</td><td>172.26.215.93:346414</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0147541  </td><td>sklearn.preproc_c330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       171.525  </td><td style=\"text-align: right;\">258.487</td><td style=\"text-align: right;\">134.242 </td><td style=\"text-align: right;\">9.33075e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_0ccf5958</td><td>TERMINATED</td><td>172.26.215.93:346682</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000488327</td><td>sklearn.preproc_c330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.08401</td><td style=\"text-align: right;\">527.37 </td><td style=\"text-align: right;\">376.17  </td><td style=\"text-align: right;\">6.72232e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_4ca4593b</td><td>TERMINATED</td><td>172.26.215.93:346875</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        9.06283e-05</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.3664 </td><td style=\"text-align: right;\">474.693</td><td style=\"text-align: right;\">234.772 </td><td style=\"text-align: right;\">1.56131e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_9ef56328</td><td>TERMINATED</td><td>172.26.215.93:347091</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.089611   </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        52.8879 </td><td style=\"text-align: right;\">391.732</td><td style=\"text-align: right;\">201.686 </td><td style=\"text-align: right;\">3.72418e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_52c95b51</td><td>TERMINATED</td><td>172.26.215.93:347315</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00312858 </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         4.89085</td><td style=\"text-align: right;\">451.839</td><td style=\"text-align: right;\">257.581 </td><td style=\"text-align: right;\">8.35459e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_159a7bec</td><td>TERMINATED</td><td>172.26.215.93:347532</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0686951  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        26.6769 </td><td style=\"text-align: right;\">276.579</td><td style=\"text-align: right;\">145.546 </td><td style=\"text-align: right;\">2.69837e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_2af6a57e</td><td>TERMINATED</td><td>172.26.215.93:347774</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0950132  </td><td>sklearn.preproc_c330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        18.9533 </td><td style=\"text-align: right;\">373.101</td><td style=\"text-align: right;\">203.902 </td><td style=\"text-align: right;\">2.77894e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_05cee5c2</td><td>TERMINATED</td><td>172.26.215.93:348047</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0342296  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       120.023  </td><td style=\"text-align: right;\">217.888</td><td style=\"text-align: right;\">112.135 </td><td style=\"text-align: right;\">2.05035e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_595d4aaf</td><td>TERMINATED</td><td>172.26.215.93:348229</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.010441   </td><td>sklearn.preproc_c330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       187.021  </td><td style=\"text-align: right;\">262.972</td><td style=\"text-align: right;\">145.901 </td><td style=\"text-align: right;\">1.31201e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_133473b2</td><td>TERMINATED</td><td>172.26.215.93:348449</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0159106  </td><td>sklearn.preproc_c330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       187.226  </td><td style=\"text-align: right;\">250.125</td><td style=\"text-align: right;\">126.535 </td><td style=\"text-align: right;\">9.23241e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_f98865c4</td><td>TERMINATED</td><td>172.26.215.93:348688</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0140931  </td><td>sklearn.preproc_c330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       188.494  </td><td style=\"text-align: right;\">245.861</td><td style=\"text-align: right;\">127.595 </td><td style=\"text-align: right;\">1.0132e+17 </td></tr>\n",
       "<tr><td>FSR_Trainable_dd639a48</td><td>TERMINATED</td><td>172.26.215.93:349010</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00357473 </td><td>sklearn.preproc_c330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       189.922  </td><td style=\"text-align: right;\">221.363</td><td style=\"text-align: right;\">109.029 </td><td style=\"text-align: right;\">6.35057e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_47653478</td><td>TERMINATED</td><td>172.26.215.93:349274</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00542949 </td><td>sklearn.preproc_c330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       195.864  </td><td style=\"text-align: right;\">237.401</td><td style=\"text-align: right;\">120.281 </td><td style=\"text-align: right;\">7.61501e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_4d7cb0db</td><td>TERMINATED</td><td>172.26.215.93:349450</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00347273 </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       343.259  </td><td style=\"text-align: right;\">198.833</td><td style=\"text-align: right;\"> 96.109 </td><td style=\"text-align: right;\">1.03282e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_d67aa423</td><td>TERMINATED</td><td>172.26.215.93:349699</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00309151 </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       341.028  </td><td style=\"text-align: right;\">178.925</td><td style=\"text-align: right;\"> 90.0725</td><td style=\"text-align: right;\">1.23389e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_794b11fd</td><td>TERMINATED</td><td>172.26.215.93:350000</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00388653 </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.98695</td><td style=\"text-align: right;\">449.287</td><td style=\"text-align: right;\">268.241 </td><td style=\"text-align: right;\">7.48432e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_a88c717d</td><td>TERMINATED</td><td>172.26.215.93:350195</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00072543 </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.33508</td><td style=\"text-align: right;\">466.984</td><td style=\"text-align: right;\">264.993 </td><td style=\"text-align: right;\">1.05679e+09</td></tr>\n",
       "<tr><td>FSR_Trainable_0591c3b9</td><td>TERMINATED</td><td>172.26.215.93:350420</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000582007</td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.81586</td><td style=\"text-align: right;\">448.992</td><td style=\"text-align: right;\">264.765 </td><td style=\"text-align: right;\">7.96461e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_c5e5f9a8</td><td>TERMINATED</td><td>172.26.215.93:350654</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00132137 </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.54195</td><td style=\"text-align: right;\">452.924</td><td style=\"text-align: right;\">267.825 </td><td style=\"text-align: right;\">9.06504e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_af803565</td><td>TERMINATED</td><td>172.26.215.93:350904</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00199685 </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        14.5529 </td><td style=\"text-align: right;\">452.845</td><td style=\"text-align: right;\">259.551 </td><td style=\"text-align: right;\">8.68819e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_def4ce95</td><td>TERMINATED</td><td>172.26.215.93:351122</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000319865</td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.84261</td><td style=\"text-align: right;\">450.624</td><td style=\"text-align: right;\">269.202 </td><td style=\"text-align: right;\">8.57689e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_efbf9c28</td><td>TERMINATED</td><td>172.26.215.93:351308</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0368777  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.45116</td><td style=\"text-align: right;\">453.223</td><td style=\"text-align: right;\">259.833 </td><td style=\"text-align: right;\">8.01536e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_7050bf07</td><td>TERMINATED</td><td>172.26.215.93:351530</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00181421 </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         8.29589</td><td style=\"text-align: right;\">448.838</td><td style=\"text-align: right;\">250.613 </td><td style=\"text-align: right;\">9.27102e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_f89fef40</td><td>TERMINATED</td><td>172.26.215.93:351752</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0317132  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       124.918  </td><td style=\"text-align: right;\">240.291</td><td style=\"text-align: right;\">122.386 </td><td style=\"text-align: right;\">2.90385e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_234d8753</td><td>TERMINATED</td><td>172.26.215.93:352322</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0294957  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       122.974  </td><td style=\"text-align: right;\">207.855</td><td style=\"text-align: right;\">108.814 </td><td style=\"text-align: right;\">2.48437e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_53b41998</td><td>TERMINATED</td><td>172.26.215.93:352525</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0057085  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.86565</td><td style=\"text-align: right;\">388.611</td><td style=\"text-align: right;\">210.7   </td><td style=\"text-align: right;\">5.90428e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_0b61e291</td><td>TERMINATED</td><td>172.26.215.93:352736</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00643489 </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       173.645  </td><td style=\"text-align: right;\">244.408</td><td style=\"text-align: right;\">126.145 </td><td style=\"text-align: right;\">1.80165e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_4e70bdaa</td><td>TERMINATED</td><td>172.26.215.93:352962</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00626125 </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        22.5654 </td><td style=\"text-align: right;\">283.891</td><td style=\"text-align: right;\">143.892 </td><td style=\"text-align: right;\">3.35404e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_9f95f4c5</td><td>TERMINATED</td><td>172.26.215.93:353179</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00635371 </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       268.712  </td><td style=\"text-align: right;\">170.304</td><td style=\"text-align: right;\"> 83.9111</td><td style=\"text-align: right;\">1.16435e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_4d352246</td><td>TERMINATED</td><td>172.26.215.93:353455</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00636323 </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        21.8508 </td><td style=\"text-align: right;\">273.815</td><td style=\"text-align: right;\">133.96  </td><td style=\"text-align: right;\">1.58199e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_7b3e9d66</td><td>TERMINATED</td><td>172.26.215.93:353693</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00809681 </td><td>sklearn.preproc_c330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        11.368  </td><td style=\"text-align: right;\">287.255</td><td style=\"text-align: right;\">151.23  </td><td style=\"text-align: right;\">1.21945e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_8390d4ff</td><td>TERMINATED</td><td>172.26.215.93:353926</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00223271 </td><td>sklearn.preproc_c330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         6.07636</td><td style=\"text-align: right;\">330.767</td><td style=\"text-align: right;\">179.106 </td><td style=\"text-align: right;\">1.88735e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_057012f0</td><td>TERMINATED</td><td>172.26.215.93:354161</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00140593 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.43193</td><td style=\"text-align: right;\">466.443</td><td style=\"text-align: right;\">237.199 </td><td style=\"text-align: right;\">2.48437e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_92788f4d</td><td>TERMINATED</td><td>172.26.215.93:354336</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0222284  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       126.423  </td><td style=\"text-align: right;\">206.599</td><td style=\"text-align: right;\">107.132 </td><td style=\"text-align: right;\">2.50714e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_d8ffcb88</td><td>TERMINATED</td><td>172.26.215.93:354861</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0219122  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       123.9    </td><td style=\"text-align: right;\">209.466</td><td style=\"text-align: right;\">109.011 </td><td style=\"text-align: right;\">2.46453e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_a80f6aca</td><td>TERMINATED</td><td>172.26.215.93:355135</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0217651  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       127.564  </td><td style=\"text-align: right;\">192.257</td><td style=\"text-align: right;\"> 98.2016</td><td style=\"text-align: right;\">1.82857e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_572c33b0</td><td>TERMINATED</td><td>172.26.215.93:355318</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0098458  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       121.766  </td><td style=\"text-align: right;\">167.055</td><td style=\"text-align: right;\"> 82.1057</td><td style=\"text-align: right;\">9.9673e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_b32d4df7</td><td>TERMINATED</td><td>172.26.215.93:355600</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0114271  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       119.334  </td><td style=\"text-align: right;\">166.866</td><td style=\"text-align: right;\"> 80.8387</td><td style=\"text-align: right;\">8.43849e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_75ff7e79</td><td>TERMINATED</td><td>172.26.215.93:355840</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00291892 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.96965</td><td style=\"text-align: right;\">420.086</td><td style=\"text-align: right;\">214.39  </td><td style=\"text-align: right;\">3.51494e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_819776d6</td><td>TERMINATED</td><td>172.26.215.93:356053</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00993689 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.04713</td><td style=\"text-align: right;\">414.321</td><td style=\"text-align: right;\">208.37  </td><td style=\"text-align: right;\">3.97175e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_42d35536</td><td>TERMINATED</td><td>172.26.215.93:356302</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00907838 </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.95997</td><td style=\"text-align: right;\">451.027</td><td style=\"text-align: right;\">262.124 </td><td style=\"text-align: right;\">8.49492e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_4514255c</td><td>TERMINATED</td><td>172.26.215.93:356532</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00416418 </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.50775</td><td style=\"text-align: right;\">467.971</td><td style=\"text-align: right;\">283.331 </td><td style=\"text-align: right;\">1.08519e+09</td></tr>\n",
       "<tr><td>FSR_Trainable_bea649a3</td><td>TERMINATED</td><td>172.26.215.93:356730</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00396631 </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.77739</td><td style=\"text-align: right;\">451.8  </td><td style=\"text-align: right;\">267.301 </td><td style=\"text-align: right;\">1.04791e+09</td></tr>\n",
       "<tr><td>FSR_Trainable_5caf7559</td><td>TERMINATED</td><td>172.26.215.93:356941</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0126894  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         7.49799</td><td style=\"text-align: right;\">295.657</td><td style=\"text-align: right;\">146.754 </td><td style=\"text-align: right;\">2.38266e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_2f9ef68e</td><td>TERMINATED</td><td>172.26.215.93:357180</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0142886  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       257.678  </td><td style=\"text-align: right;\">170.258</td><td style=\"text-align: right;\"> 80.8426</td><td style=\"text-align: right;\">8.61775e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_89a6d069</td><td>TERMINATED</td><td>172.26.215.93:357402</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0594926  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       256.633  </td><td style=\"text-align: right;\">173.173</td><td style=\"text-align: right;\"> 79.8601</td><td style=\"text-align: right;\">6.79571e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_c4df8bd3</td><td>TERMINATED</td><td>172.26.215.93:357622</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0181438  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       260.518  </td><td style=\"text-align: right;\">168.331</td><td style=\"text-align: right;\"> 79.9344</td><td style=\"text-align: right;\">7.68582e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_2f82b8ea</td><td>TERMINATED</td><td>172.26.215.93:357837</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0503381  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       119.288  </td><td style=\"text-align: right;\">169.899</td><td style=\"text-align: right;\"> 78.3005</td><td style=\"text-align: right;\">7.30485e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_6e322367</td><td>TERMINATED</td><td>172.26.215.93:358183</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0194214  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       122.829  </td><td style=\"text-align: right;\">170.412</td><td style=\"text-align: right;\"> 79.8558</td><td style=\"text-align: right;\">8.2844e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_2fbf2b26</td><td>TERMINATED</td><td>172.26.215.93:358466</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0601877  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       122.943  </td><td style=\"text-align: right;\">170.663</td><td style=\"text-align: right;\"> 82.0923</td><td style=\"text-align: right;\">8.50057e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e8b85816</td><td>TERMINATED</td><td>172.26.215.93:358659</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0521157  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       189.38   </td><td style=\"text-align: right;\">156.218</td><td style=\"text-align: right;\"> 71.845 </td><td style=\"text-align: right;\">6.17423e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ce47611f</td><td>TERMINATED</td><td>172.26.215.93:358886</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0445137  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       191.234  </td><td style=\"text-align: right;\">158.401</td><td style=\"text-align: right;\"> 75.3166</td><td style=\"text-align: right;\">7.79718e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_3d287886</td><td>TERMINATED</td><td>172.26.215.93:359105</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0149017  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       115.75   </td><td style=\"text-align: right;\">179.728</td><td style=\"text-align: right;\"> 88.8005</td><td style=\"text-align: right;\">8.78878e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ef893e7c</td><td>TERMINATED</td><td>172.26.215.93:359447</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0127637  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       125.548  </td><td style=\"text-align: right;\">179.246</td><td style=\"text-align: right;\"> 89.6039</td><td style=\"text-align: right;\">1.02428e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_a1ffe1ec</td><td>TERMINATED</td><td>172.26.215.93:359691</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0128767  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       129.13   </td><td style=\"text-align: right;\">169.095</td><td style=\"text-align: right;\"> 81.6175</td><td style=\"text-align: right;\">1.04727e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_f21b1363</td><td>TERMINATED</td><td>172.26.215.93:359950</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0114047  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       127.215  </td><td style=\"text-align: right;\">166.915</td><td style=\"text-align: right;\"> 81.3586</td><td style=\"text-align: right;\">8.78573e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_7d5ead86</td><td>TERMINATED</td><td>172.26.215.93:360147</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0439223  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       193.503  </td><td style=\"text-align: right;\">155.939</td><td style=\"text-align: right;\"> 73.5956</td><td style=\"text-align: right;\">6.47584e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f84633a9</td><td>TERMINATED</td><td>172.26.215.93:360439</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0450474  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       198.602  </td><td style=\"text-align: right;\">158.31 </td><td style=\"text-align: right;\"> 73.6977</td><td style=\"text-align: right;\">6.98467e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e923645d</td><td>TERMINATED</td><td>172.26.215.93:360681</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0376057  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       123.056  </td><td style=\"text-align: right;\">198.008</td><td style=\"text-align: right;\"> 91.2879</td><td style=\"text-align: right;\">7.54566e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_44cd64c6</td><td>TERMINATED</td><td>172.26.215.93:360931</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0462541  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       127.585  </td><td style=\"text-align: right;\">161.331</td><td style=\"text-align: right;\"> 76.4072</td><td style=\"text-align: right;\">8.04255e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_538374ed</td><td>TERMINATED</td><td>172.26.215.93:361204</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0446221  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       141.381  </td><td style=\"text-align: right;\">158.662</td><td style=\"text-align: right;\"> 76.1629</td><td style=\"text-align: right;\">8.15457e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_8b20cd80</td><td>TERMINATED</td><td>172.26.215.93:361383</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.089911   </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.82567</td><td style=\"text-align: right;\">396.984</td><td style=\"text-align: right;\">196.486 </td><td style=\"text-align: right;\">3.3156e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_e0056a8e</td><td>TERMINATED</td><td>172.26.215.93:361624</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0477314  </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.84628</td><td style=\"text-align: right;\">382.262</td><td style=\"text-align: right;\">185.317 </td><td style=\"text-align: right;\">2.80808e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_e39d3cf9</td><td>TERMINATED</td><td>172.26.215.93:361852</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0466486  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       192.828  </td><td style=\"text-align: right;\">159.023</td><td style=\"text-align: right;\"> 73.2193</td><td style=\"text-align: right;\">6.2965e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_6bd0b62f</td><td>TERMINATED</td><td>172.26.215.93:362116</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0277166  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       121.453  </td><td style=\"text-align: right;\">162.937</td><td style=\"text-align: right;\"> 79.0248</td><td style=\"text-align: right;\">7.83896e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_68679ffc</td><td>TERMINATED</td><td>172.26.215.93:362316</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0734187  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       123.92   </td><td style=\"text-align: right;\">171.255</td><td style=\"text-align: right;\"> 79.0088</td><td style=\"text-align: right;\">7.9892e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_a78b778a</td><td>TERMINATED</td><td>172.26.215.93:362628</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0409421  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       124.314  </td><td style=\"text-align: right;\">163.66 </td><td style=\"text-align: right;\"> 79.5245</td><td style=\"text-align: right;\">8.13018e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_5beffbcc</td><td>TERMINATED</td><td>172.26.215.93:362873</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0763299  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">        87.4615 </td><td style=\"text-align: right;\">184.94 </td><td style=\"text-align: right;\"> 87.0241</td><td style=\"text-align: right;\">8.10518e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ff689005</td><td>TERMINATED</td><td>172.26.215.93:363084</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0741541  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       136.098  </td><td style=\"text-align: right;\">172.491</td><td style=\"text-align: right;\"> 81.5366</td><td style=\"text-align: right;\">8.60808e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_295bd539</td><td>TERMINATED</td><td>172.26.215.93:363346</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.04306    </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       134.805  </td><td style=\"text-align: right;\">161.615</td><td style=\"text-align: right;\"> 77.2046</td><td style=\"text-align: right;\">8.08892e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e9a462e4</td><td>TERMINATED</td><td>172.26.215.93:363605</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0412687  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       136.642  </td><td style=\"text-align: right;\">164.123</td><td style=\"text-align: right;\"> 78.2414</td><td style=\"text-align: right;\">7.79399e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_41021d62</td><td>TERMINATED</td><td>172.26.215.93:363790</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0262742  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       133.245  </td><td style=\"text-align: right;\">170.141</td><td style=\"text-align: right;\"> 81.8337</td><td style=\"text-align: right;\">7.23046e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_edd691bd</td><td>TERMINATED</td><td>172.26.215.93:364094</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0284057  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       133.235  </td><td style=\"text-align: right;\">169.306</td><td style=\"text-align: right;\"> 81.3387</td><td style=\"text-align: right;\">9.56008e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_dc2ac365</td><td>TERMINATED</td><td>172.26.215.93:364348</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0991538  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.46065</td><td style=\"text-align: right;\">259.479</td><td style=\"text-align: right;\">129.104 </td><td style=\"text-align: right;\">1.60872e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_9ae8645b</td><td>TERMINATED</td><td>172.26.215.93:364555</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0302585  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       136.177  </td><td style=\"text-align: right;\">167.323</td><td style=\"text-align: right;\"> 80.3759</td><td style=\"text-align: right;\">8.83486e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e2d1e51f</td><td>TERMINATED</td><td>172.26.215.93:364824</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0284163  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       135.631  </td><td style=\"text-align: right;\">173.049</td><td style=\"text-align: right;\"> 82.2989</td><td style=\"text-align: right;\">9.13945e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_a57b2b7b</td><td>TERMINATED</td><td>172.26.215.93:365003</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0290351  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       134.991  </td><td style=\"text-align: right;\">166.306</td><td style=\"text-align: right;\"> 80.3191</td><td style=\"text-align: right;\">8.18013e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_47019065</td><td>TERMINATED</td><td>172.26.215.93:365311</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0507964  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       134.373  </td><td style=\"text-align: right;\">169.716</td><td style=\"text-align: right;\"> 80.1933</td><td style=\"text-align: right;\">7.4554e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_bfb7ebd0</td><td>TERMINATED</td><td>172.26.215.93:365574</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0517066  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.19567</td><td style=\"text-align: right;\">235.432</td><td style=\"text-align: right;\">117.611 </td><td style=\"text-align: right;\">1.07047e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_bbcb84e4</td><td>TERMINATED</td><td>172.26.215.93:365803</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0519952  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.91471</td><td style=\"text-align: right;\">293.605</td><td style=\"text-align: right;\">150.292 </td><td style=\"text-align: right;\">1.51074e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_429b5b9e</td><td>TERMINATED</td><td>172.26.215.93:366020</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0449884  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.43231</td><td style=\"text-align: right;\">473.38 </td><td style=\"text-align: right;\">238.998 </td><td style=\"text-align: right;\">5.40953e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_1c7b460c</td><td>TERMINATED</td><td>172.26.215.93:366226</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0399617  </td><td>sklearn.preproc_c330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.35202</td><td style=\"text-align: right;\">359.019</td><td style=\"text-align: right;\">189.847 </td><td style=\"text-align: right;\">2.20409e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_44e54786</td><td>TERMINATED</td><td>172.26.215.93:366449</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0373441  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       123.955  </td><td style=\"text-align: right;\">163.393</td><td style=\"text-align: right;\"> 79.0055</td><td style=\"text-align: right;\">7.73091e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_adefbb03</td><td>TERMINATED</td><td>172.26.215.93:366692</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0615652  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       122.859  </td><td style=\"text-align: right;\">175.32 </td><td style=\"text-align: right;\"> 80.0233</td><td style=\"text-align: right;\">7.38032e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f6ea6cea</td><td>TERMINATED</td><td>172.26.215.93:366911</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0638011  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       122.528  </td><td style=\"text-align: right;\">172.488</td><td style=\"text-align: right;\"> 79.9604</td><td style=\"text-align: right;\">7.54811e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_183fe6c6</td><td>TERMINATED</td><td>172.26.215.93:367180</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0360713  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       114.206  </td><td style=\"text-align: right;\">159.161</td><td style=\"text-align: right;\"> 77.3712</td><td style=\"text-align: right;\">6.99547e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e7a576c4</td><td>TERMINATED</td><td>172.26.215.93:367479</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0370999  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        71.2717 </td><td style=\"text-align: right;\">175.111</td><td style=\"text-align: right;\"> 81.6435</td><td style=\"text-align: right;\">9.49782e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_cb2a0ab8</td><td>ERROR     </td><td>172.26.215.93:351992</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0319684  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">       117.144  </td><td style=\"text-align: right;\">304.628</td><td style=\"text-align: right;\">161.445 </td><td style=\"text-align: right;\">3.93432e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_84b2dd83</td><td>ERROR     </td><td>172.26.215.93:354554</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c750</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0245875  </td><td>sklearn.preproc_5fb0</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">       120.227  </td><td style=\"text-align: right;\">355.607</td><td style=\"text-align: right;\">187.262 </td><td style=\"text-align: right;\">5.62977e+08</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 01:25:32,921\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">       mape</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_057012f0</td><td>2023-07-19_01-48-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">237.199 </td><td style=\"text-align: right;\">2.48437e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">354161</td><td style=\"text-align: right;\">466.443</td><td style=\"text-align: right;\">             4.43193</td><td style=\"text-align: right;\">          4.43193 </td><td style=\"text-align: right;\">       4.43193</td><td style=\"text-align: right;\"> 1689698922</td><td style=\"text-align: right;\">                   1</td><td>057012f0  </td></tr>\n",
       "<tr><td>FSR_Trainable_0591c3b9</td><td>2023-07-19_01-42-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">264.765 </td><td style=\"text-align: right;\">7.96461e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">350420</td><td style=\"text-align: right;\">448.992</td><td style=\"text-align: right;\">             2.81586</td><td style=\"text-align: right;\">          2.81586 </td><td style=\"text-align: right;\">       2.81586</td><td style=\"text-align: right;\"> 1689698564</td><td style=\"text-align: right;\">                   1</td><td>0591c3b9  </td></tr>\n",
       "<tr><td>FSR_Trainable_05cee5c2</td><td>2023-07-19_01-38-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">112.135 </td><td style=\"text-align: right;\">2.05035e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">348047</td><td style=\"text-align: right;\">217.888</td><td style=\"text-align: right;\">           120.023  </td><td style=\"text-align: right;\">          1.15786 </td><td style=\"text-align: right;\">     120.023  </td><td style=\"text-align: right;\"> 1689698313</td><td style=\"text-align: right;\">                 100</td><td>05cee5c2  </td></tr>\n",
       "<tr><td>FSR_Trainable_06afbe31</td><td>2023-07-19_01-33-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">115.884 </td><td style=\"text-align: right;\">1.27579e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">345171</td><td style=\"text-align: right;\">203.836</td><td style=\"text-align: right;\">           141.056  </td><td style=\"text-align: right;\">          1.18067 </td><td style=\"text-align: right;\">     141.056  </td><td style=\"text-align: right;\"> 1689698008</td><td style=\"text-align: right;\">                 100</td><td>06afbe31  </td></tr>\n",
       "<tr><td>FSR_Trainable_0b61e291</td><td>2023-07-19_01-49-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">126.145 </td><td style=\"text-align: right;\">1.80165e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">352736</td><td style=\"text-align: right;\">244.408</td><td style=\"text-align: right;\">           173.645  </td><td style=\"text-align: right;\">          2.77315 </td><td style=\"text-align: right;\">     173.645  </td><td style=\"text-align: right;\"> 1689698983</td><td style=\"text-align: right;\">                  64</td><td>0b61e291  </td></tr>\n",
       "<tr><td>FSR_Trainable_0ccf5958</td><td>2023-07-19_01-35-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">376.17  </td><td style=\"text-align: right;\">6.72232e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">346682</td><td style=\"text-align: right;\">527.37 </td><td style=\"text-align: right;\">             1.08401</td><td style=\"text-align: right;\">          1.08401 </td><td style=\"text-align: right;\">       1.08401</td><td style=\"text-align: right;\"> 1689698103</td><td style=\"text-align: right;\">                   1</td><td>0ccf5958  </td></tr>\n",
       "<tr><td>FSR_Trainable_133473b2</td><td>2023-07-19_01-39-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">126.535 </td><td style=\"text-align: right;\">9.23241e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">348449</td><td style=\"text-align: right;\">250.125</td><td style=\"text-align: right;\">           187.226  </td><td style=\"text-align: right;\">          1.82873 </td><td style=\"text-align: right;\">     187.226  </td><td style=\"text-align: right;\"> 1689698398</td><td style=\"text-align: right;\">                 100</td><td>133473b2  </td></tr>\n",
       "<tr><td>FSR_Trainable_159a7bec</td><td>2023-07-19_01-36-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">145.546 </td><td style=\"text-align: right;\">2.69837e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">347532</td><td style=\"text-align: right;\">276.579</td><td style=\"text-align: right;\">            26.6769 </td><td style=\"text-align: right;\">          0.781137</td><td style=\"text-align: right;\">      26.6769 </td><td style=\"text-align: right;\"> 1689698177</td><td style=\"text-align: right;\">                  32</td><td>159a7bec  </td></tr>\n",
       "<tr><td>FSR_Trainable_183fe6c6</td><td>2023-07-19_02-19-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 77.3712</td><td style=\"text-align: right;\">6.99547e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">367180</td><td style=\"text-align: right;\">159.161</td><td style=\"text-align: right;\">           114.206  </td><td style=\"text-align: right;\">          0.804114</td><td style=\"text-align: right;\">     114.206  </td><td style=\"text-align: right;\"> 1689700768</td><td style=\"text-align: right;\">                 100</td><td>183fe6c6  </td></tr>\n",
       "<tr><td>FSR_Trainable_1c7b460c</td><td>2023-07-19_02-16-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">189.847 </td><td style=\"text-align: right;\">2.20409e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">366226</td><td style=\"text-align: right;\">359.019</td><td style=\"text-align: right;\">             6.35202</td><td style=\"text-align: right;\">          6.35202 </td><td style=\"text-align: right;\">       6.35202</td><td style=\"text-align: right;\"> 1689700594</td><td style=\"text-align: right;\">                   1</td><td>1c7b460c  </td></tr>\n",
       "<tr><td>FSR_Trainable_234d8753</td><td>2023-07-19_01-48-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">108.814 </td><td style=\"text-align: right;\">2.48437e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">352322</td><td style=\"text-align: right;\">207.855</td><td style=\"text-align: right;\">           122.974  </td><td style=\"text-align: right;\">          1.2333  </td><td style=\"text-align: right;\">     122.974  </td><td style=\"text-align: right;\"> 1689698909</td><td style=\"text-align: right;\">                 100</td><td>234d8753  </td></tr>\n",
       "<tr><td>FSR_Trainable_295bd539</td><td>2023-07-19_02-12-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 77.2046</td><td style=\"text-align: right;\">8.08892e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">363346</td><td style=\"text-align: right;\">161.615</td><td style=\"text-align: right;\">           134.805  </td><td style=\"text-align: right;\">          1.30817 </td><td style=\"text-align: right;\">     134.805  </td><td style=\"text-align: right;\"> 1689700360</td><td style=\"text-align: right;\">                 100</td><td>295bd539  </td></tr>\n",
       "<tr><td>FSR_Trainable_2af6a57e</td><td>2023-07-19_01-36-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">203.902 </td><td style=\"text-align: right;\">2.77894e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">347774</td><td style=\"text-align: right;\">373.101</td><td style=\"text-align: right;\">            18.9533 </td><td style=\"text-align: right;\">          1.15188 </td><td style=\"text-align: right;\">      18.9533 </td><td style=\"text-align: right;\"> 1689698179</td><td style=\"text-align: right;\">                  16</td><td>2af6a57e  </td></tr>\n",
       "<tr><td>FSR_Trainable_2e07ed6e</td><td>2023-07-19_01-30-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">250.002 </td><td style=\"text-align: right;\">6.89387e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\">455.851</td><td style=\"text-align: right;\">           273.919  </td><td style=\"text-align: right;\">         19.7656  </td><td style=\"text-align: right;\">     273.919  </td><td style=\"text-align: right;\"> 1689697831</td><td style=\"text-align: right;\">                  16</td><td>2e07ed6e  </td></tr>\n",
       "<tr><td>FSR_Trainable_2f82b8ea</td><td>2023-07-19_01-56-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 78.3005</td><td style=\"text-align: right;\">7.30485e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">357837</td><td style=\"text-align: right;\">169.899</td><td style=\"text-align: right;\">           119.288  </td><td style=\"text-align: right;\">          1.16376 </td><td style=\"text-align: right;\">     119.288  </td><td style=\"text-align: right;\"> 1689699384</td><td style=\"text-align: right;\">                 100</td><td>2f82b8ea  </td></tr>\n",
       "<tr><td>FSR_Trainable_2f9ef68e</td><td>2023-07-19_01-58-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 80.8426</td><td style=\"text-align: right;\">8.61775e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">357180</td><td style=\"text-align: right;\">170.258</td><td style=\"text-align: right;\">           257.678  </td><td style=\"text-align: right;\">          2.39615 </td><td style=\"text-align: right;\">     257.678  </td><td style=\"text-align: right;\"> 1689699492</td><td style=\"text-align: right;\">                 100</td><td>2f9ef68e  </td></tr>\n",
       "<tr><td>FSR_Trainable_2fbf2b26</td><td>2023-07-19_02-00-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 82.0923</td><td style=\"text-align: right;\">8.50057e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">358466</td><td style=\"text-align: right;\">170.663</td><td style=\"text-align: right;\">           122.943  </td><td style=\"text-align: right;\">          1.19469 </td><td style=\"text-align: right;\">     122.943  </td><td style=\"text-align: right;\"> 1689699637</td><td style=\"text-align: right;\">                 100</td><td>2fbf2b26  </td></tr>\n",
       "<tr><td>FSR_Trainable_3d287886</td><td>2023-07-19_02-01-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 88.8005</td><td style=\"text-align: right;\">8.78878e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">359105</td><td style=\"text-align: right;\">179.728</td><td style=\"text-align: right;\">           115.75   </td><td style=\"text-align: right;\">          1.07645 </td><td style=\"text-align: right;\">     115.75   </td><td style=\"text-align: right;\"> 1689699663</td><td style=\"text-align: right;\">                 100</td><td>3d287886  </td></tr>\n",
       "<tr><td>FSR_Trainable_41021d62</td><td>2023-07-19_02-13-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 81.8337</td><td style=\"text-align: right;\">7.23046e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">363790</td><td style=\"text-align: right;\">170.141</td><td style=\"text-align: right;\">           133.245  </td><td style=\"text-align: right;\">          1.16093 </td><td style=\"text-align: right;\">     133.245  </td><td style=\"text-align: right;\"> 1689700418</td><td style=\"text-align: right;\">                 100</td><td>41021d62  </td></tr>\n",
       "<tr><td>FSR_Trainable_429b5b9e</td><td>2023-07-19_02-16-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">238.998 </td><td style=\"text-align: right;\">5.40953e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">366020</td><td style=\"text-align: right;\">473.38 </td><td style=\"text-align: right;\">             6.43231</td><td style=\"text-align: right;\">          6.43231 </td><td style=\"text-align: right;\">       6.43231</td><td style=\"text-align: right;\"> 1689700584</td><td style=\"text-align: right;\">                   1</td><td>429b5b9e  </td></tr>\n",
       "<tr><td>FSR_Trainable_42d35536</td><td>2023-07-19_01-53-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">262.124 </td><td style=\"text-align: right;\">8.49492e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">356302</td><td style=\"text-align: right;\">451.027</td><td style=\"text-align: right;\">             8.95997</td><td style=\"text-align: right;\">          8.95997 </td><td style=\"text-align: right;\">       8.95997</td><td style=\"text-align: right;\"> 1689699184</td><td style=\"text-align: right;\">                   1</td><td>42d35536  </td></tr>\n",
       "<tr><td>FSR_Trainable_44cd64c6</td><td>2023-07-19_02-06-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 76.4072</td><td style=\"text-align: right;\">8.04255e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">360931</td><td style=\"text-align: right;\">161.331</td><td style=\"text-align: right;\">           127.585  </td><td style=\"text-align: right;\">          1.07207 </td><td style=\"text-align: right;\">     127.585  </td><td style=\"text-align: right;\"> 1689700014</td><td style=\"text-align: right;\">                 100</td><td>44cd64c6  </td></tr>\n",
       "<tr><td>FSR_Trainable_44e54786</td><td>2023-07-19_02-18-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 79.0055</td><td style=\"text-align: right;\">7.73091e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">366449</td><td style=\"text-align: right;\">163.393</td><td style=\"text-align: right;\">           123.955  </td><td style=\"text-align: right;\">          1.10199 </td><td style=\"text-align: right;\">     123.955  </td><td style=\"text-align: right;\"> 1689700730</td><td style=\"text-align: right;\">                 100</td><td>44e54786  </td></tr>\n",
       "<tr><td>FSR_Trainable_4514255c</td><td>2023-07-19_01-53-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">283.331 </td><td style=\"text-align: right;\">1.08519e+09</td><td>172.26.215.93</td><td style=\"text-align: right;\">356532</td><td style=\"text-align: right;\">467.971</td><td style=\"text-align: right;\">             1.50775</td><td style=\"text-align: right;\">          1.50775 </td><td style=\"text-align: right;\">       1.50775</td><td style=\"text-align: right;\"> 1689699197</td><td style=\"text-align: right;\">                   1</td><td>4514255c  </td></tr>\n",
       "<tr><td>FSR_Trainable_47019065</td><td>2023-07-19_02-17-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 80.1933</td><td style=\"text-align: right;\">7.4554e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">365311</td><td style=\"text-align: right;\">169.716</td><td style=\"text-align: right;\">           134.373  </td><td style=\"text-align: right;\">          1.26147 </td><td style=\"text-align: right;\">     134.373  </td><td style=\"text-align: right;\"> 1689700636</td><td style=\"text-align: right;\">                 100</td><td>47019065  </td></tr>\n",
       "<tr><td>FSR_Trainable_47653478</td><td>2023-07-19_01-43-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">120.281 </td><td style=\"text-align: right;\">7.61501e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">349274</td><td style=\"text-align: right;\">237.401</td><td style=\"text-align: right;\">           195.864  </td><td style=\"text-align: right;\">          2.09305 </td><td style=\"text-align: right;\">     195.864  </td><td style=\"text-align: right;\"> 1689698606</td><td style=\"text-align: right;\">                 100</td><td>47653478  </td></tr>\n",
       "<tr><td>FSR_Trainable_4ca4593b</td><td>2023-07-19_01-35-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">234.772 </td><td style=\"text-align: right;\">1.56131e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">346875</td><td style=\"text-align: right;\">474.693</td><td style=\"text-align: right;\">             6.3664 </td><td style=\"text-align: right;\">          6.3664  </td><td style=\"text-align: right;\">       6.3664 </td><td style=\"text-align: right;\"> 1689698122</td><td style=\"text-align: right;\">                   1</td><td>4ca4593b  </td></tr>\n",
       "<tr><td>FSR_Trainable_4d352246</td><td>2023-07-19_01-47-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">133.96  </td><td style=\"text-align: right;\">1.58199e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">353455</td><td style=\"text-align: right;\">273.815</td><td style=\"text-align: right;\">            21.8508 </td><td style=\"text-align: right;\">          2.6246  </td><td style=\"text-align: right;\">      21.8508 </td><td style=\"text-align: right;\"> 1689698865</td><td style=\"text-align: right;\">                   8</td><td>4d352246  </td></tr>\n",
       "<tr><td>FSR_Trainable_4d7cb0db</td><td>2023-07-19_01-46-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 96.109 </td><td style=\"text-align: right;\">1.03282e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">349450</td><td style=\"text-align: right;\">198.833</td><td style=\"text-align: right;\">           343.259  </td><td style=\"text-align: right;\">          3.27944 </td><td style=\"text-align: right;\">     343.259  </td><td style=\"text-align: right;\"> 1689698764</td><td style=\"text-align: right;\">                 100</td><td>4d7cb0db  </td></tr>\n",
       "<tr><td>FSR_Trainable_4e70bdaa</td><td>2023-07-19_01-47-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">143.892 </td><td style=\"text-align: right;\">3.35404e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">352962</td><td style=\"text-align: right;\">283.891</td><td style=\"text-align: right;\">            22.5654 </td><td style=\"text-align: right;\">          2.66961 </td><td style=\"text-align: right;\">      22.5654 </td><td style=\"text-align: right;\"> 1689698831</td><td style=\"text-align: right;\">                   8</td><td>4e70bdaa  </td></tr>\n",
       "<tr><td>FSR_Trainable_52c95b51</td><td>2023-07-19_01-35-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">257.581 </td><td style=\"text-align: right;\">8.35459e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">347315</td><td style=\"text-align: right;\">451.839</td><td style=\"text-align: right;\">             4.89085</td><td style=\"text-align: right;\">          2.03821 </td><td style=\"text-align: right;\">       4.89085</td><td style=\"text-align: right;\"> 1689698144</td><td style=\"text-align: right;\">                   2</td><td>52c95b51  </td></tr>\n",
       "<tr><td>FSR_Trainable_538374ed</td><td>2023-07-19_02-08-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 76.1629</td><td style=\"text-align: right;\">8.15457e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">361204</td><td style=\"text-align: right;\">158.662</td><td style=\"text-align: right;\">           141.381  </td><td style=\"text-align: right;\">          1.30661 </td><td style=\"text-align: right;\">     141.381  </td><td style=\"text-align: right;\"> 1689700112</td><td style=\"text-align: right;\">                 100</td><td>538374ed  </td></tr>\n",
       "<tr><td>FSR_Trainable_53b41998</td><td>2023-07-19_01-46-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">210.7   </td><td style=\"text-align: right;\">5.90428e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">352525</td><td style=\"text-align: right;\">388.611</td><td style=\"text-align: right;\">             7.86565</td><td style=\"text-align: right;\">          3.61871 </td><td style=\"text-align: right;\">       7.86565</td><td style=\"text-align: right;\"> 1689698796</td><td style=\"text-align: right;\">                   2</td><td>53b41998  </td></tr>\n",
       "<tr><td>FSR_Trainable_572c33b0</td><td>2023-07-19_01-53-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 82.1057</td><td style=\"text-align: right;\">9.9673e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">355318</td><td style=\"text-align: right;\">167.055</td><td style=\"text-align: right;\">           121.766  </td><td style=\"text-align: right;\">          1.11574 </td><td style=\"text-align: right;\">     121.766  </td><td style=\"text-align: right;\"> 1689699215</td><td style=\"text-align: right;\">                 100</td><td>572c33b0  </td></tr>\n",
       "<tr><td>FSR_Trainable_595d4aaf</td><td>2023-07-19_01-39-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">145.901 </td><td style=\"text-align: right;\">1.31201e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">348229</td><td style=\"text-align: right;\">262.972</td><td style=\"text-align: right;\">           187.021  </td><td style=\"text-align: right;\">          1.8371  </td><td style=\"text-align: right;\">     187.021  </td><td style=\"text-align: right;\"> 1689698388</td><td style=\"text-align: right;\">                 100</td><td>595d4aaf  </td></tr>\n",
       "<tr><td>FSR_Trainable_5beffbcc</td><td>2023-07-19_02-10-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 87.0241</td><td style=\"text-align: right;\">8.10518e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">362873</td><td style=\"text-align: right;\">184.94 </td><td style=\"text-align: right;\">            87.4615 </td><td style=\"text-align: right;\">          1.22644 </td><td style=\"text-align: right;\">      87.4615 </td><td style=\"text-align: right;\"> 1689700253</td><td style=\"text-align: right;\">                  64</td><td>5beffbcc  </td></tr>\n",
       "<tr><td>FSR_Trainable_5caf7559</td><td>2023-07-19_01-53-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">146.754 </td><td style=\"text-align: right;\">2.38266e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">356941</td><td style=\"text-align: right;\">295.657</td><td style=\"text-align: right;\">             7.49799</td><td style=\"text-align: right;\">          1.72403 </td><td style=\"text-align: right;\">       7.49799</td><td style=\"text-align: right;\"> 1689699228</td><td style=\"text-align: right;\">                   4</td><td>5caf7559  </td></tr>\n",
       "<tr><td>FSR_Trainable_5e4396e7</td><td>2023-07-19_01-35-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 89.0623</td><td style=\"text-align: right;\">7.16183e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">346126</td><td style=\"text-align: right;\">180.984</td><td style=\"text-align: right;\">           174.035  </td><td style=\"text-align: right;\">          1.55153 </td><td style=\"text-align: right;\">     174.035  </td><td style=\"text-align: right;\"> 1689698137</td><td style=\"text-align: right;\">                 100</td><td>5e4396e7  </td></tr>\n",
       "<tr><td>FSR_Trainable_68679ffc</td><td>2023-07-19_02-09-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 79.0088</td><td style=\"text-align: right;\">7.9892e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">362316</td><td style=\"text-align: right;\">171.255</td><td style=\"text-align: right;\">           123.92   </td><td style=\"text-align: right;\">          1.42865 </td><td style=\"text-align: right;\">     123.92   </td><td style=\"text-align: right;\"> 1689700164</td><td style=\"text-align: right;\">                 100</td><td>68679ffc  </td></tr>\n",
       "<tr><td>FSR_Trainable_6bd0b62f</td><td>2023-07-19_02-09-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 79.0248</td><td style=\"text-align: right;\">7.83896e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">362116</td><td style=\"text-align: right;\">162.937</td><td style=\"text-align: right;\">           121.453  </td><td style=\"text-align: right;\">          1.08682 </td><td style=\"text-align: right;\">     121.453  </td><td style=\"text-align: right;\"> 1689700148</td><td style=\"text-align: right;\">                 100</td><td>6bd0b62f  </td></tr>\n",
       "<tr><td>FSR_Trainable_6e322367</td><td>2023-07-19_01-58-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 79.8558</td><td style=\"text-align: right;\">8.2844e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">358183</td><td style=\"text-align: right;\">170.412</td><td style=\"text-align: right;\">           122.829  </td><td style=\"text-align: right;\">          1.12352 </td><td style=\"text-align: right;\">     122.829  </td><td style=\"text-align: right;\"> 1689699526</td><td style=\"text-align: right;\">                 100</td><td>6e322367  </td></tr>\n",
       "<tr><td>FSR_Trainable_7050bf07</td><td>2023-07-19_01-44-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">250.613 </td><td style=\"text-align: right;\">9.27102e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">351530</td><td style=\"text-align: right;\">448.838</td><td style=\"text-align: right;\">             8.29589</td><td style=\"text-align: right;\">          3.69194 </td><td style=\"text-align: right;\">       8.29589</td><td style=\"text-align: right;\"> 1689698646</td><td style=\"text-align: right;\">                   2</td><td>7050bf07  </td></tr>\n",
       "<tr><td>FSR_Trainable_710c8739</td><td>2023-07-19_01-35-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">109.532 </td><td style=\"text-align: right;\">1.31731e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">344985</td><td style=\"text-align: right;\">211.253</td><td style=\"text-align: right;\">           260.039  </td><td style=\"text-align: right;\">          2.77415 </td><td style=\"text-align: right;\">     260.039  </td><td style=\"text-align: right;\"> 1689698117</td><td style=\"text-align: right;\">                 100</td><td>710c8739  </td></tr>\n",
       "<tr><td>FSR_Trainable_75ff7e79</td><td>2023-07-19_01-52-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">214.39  </td><td style=\"text-align: right;\">3.51494e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">355840</td><td style=\"text-align: right;\">420.086</td><td style=\"text-align: right;\">             3.96965</td><td style=\"text-align: right;\">          3.96965 </td><td style=\"text-align: right;\">       3.96965</td><td style=\"text-align: right;\"> 1689699141</td><td style=\"text-align: right;\">                   1</td><td>75ff7e79  </td></tr>\n",
       "<tr><td>FSR_Trainable_794b11fd</td><td>2023-07-19_01-42-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">268.241 </td><td style=\"text-align: right;\">7.48432e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">350000</td><td style=\"text-align: right;\">449.287</td><td style=\"text-align: right;\">             2.98695</td><td style=\"text-align: right;\">          2.98695 </td><td style=\"text-align: right;\">       2.98695</td><td style=\"text-align: right;\"> 1689698534</td><td style=\"text-align: right;\">                   1</td><td>794b11fd  </td></tr>\n",
       "<tr><td>FSR_Trainable_796ead13</td><td>2023-07-19_01-32-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">241.084 </td><td style=\"text-align: right;\">5.66079e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">345927</td><td style=\"text-align: right;\">531.855</td><td style=\"text-align: right;\">             1.66094</td><td style=\"text-align: right;\">          1.66094 </td><td style=\"text-align: right;\">       1.66094</td><td style=\"text-align: right;\"> 1689697941</td><td style=\"text-align: right;\">                   1</td><td>796ead13  </td></tr>\n",
       "<tr><td>FSR_Trainable_7b3e9d66</td><td>2023-07-19_01-48-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">151.23  </td><td style=\"text-align: right;\">1.21945e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">353693</td><td style=\"text-align: right;\">287.255</td><td style=\"text-align: right;\">            11.368  </td><td style=\"text-align: right;\">          2.54754 </td><td style=\"text-align: right;\">      11.368  </td><td style=\"text-align: right;\"> 1689698889</td><td style=\"text-align: right;\">                   4</td><td>7b3e9d66  </td></tr>\n",
       "<tr><td>FSR_Trainable_7d5ead86</td><td>2023-07-19_02-05-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 73.5956</td><td style=\"text-align: right;\">6.47584e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">360147</td><td style=\"text-align: right;\">155.939</td><td style=\"text-align: right;\">           193.503  </td><td style=\"text-align: right;\">          1.86021 </td><td style=\"text-align: right;\">     193.503  </td><td style=\"text-align: right;\"> 1689699947</td><td style=\"text-align: right;\">                 100</td><td>7d5ead86  </td></tr>\n",
       "<tr><td>FSR_Trainable_819776d6</td><td>2023-07-19_01-52-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">208.37  </td><td style=\"text-align: right;\">3.97175e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">356053</td><td style=\"text-align: right;\">414.321</td><td style=\"text-align: right;\">             9.04713</td><td style=\"text-align: right;\">          9.04713 </td><td style=\"text-align: right;\">       9.04713</td><td style=\"text-align: right;\"> 1689699163</td><td style=\"text-align: right;\">                   1</td><td>819776d6  </td></tr>\n",
       "<tr><td>FSR_Trainable_8390d4ff</td><td>2023-07-19_01-48-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">179.106 </td><td style=\"text-align: right;\">1.88735e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">353926</td><td style=\"text-align: right;\">330.767</td><td style=\"text-align: right;\">             6.07636</td><td style=\"text-align: right;\">          2.876   </td><td style=\"text-align: right;\">       6.07636</td><td style=\"text-align: right;\"> 1689698908</td><td style=\"text-align: right;\">                   2</td><td>8390d4ff  </td></tr>\n",
       "<tr><td>FSR_Trainable_84b2dd83</td><td>2023-07-19_01-51-04</td><td>False </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        96</td><td style=\"text-align: right;\">187.262 </td><td style=\"text-align: right;\">5.62977e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">354554</td><td style=\"text-align: right;\">355.607</td><td style=\"text-align: right;\">           120.227  </td><td style=\"text-align: right;\">          1.28162 </td><td style=\"text-align: right;\">     120.227  </td><td style=\"text-align: right;\"> 1689699064</td><td style=\"text-align: right;\">                  96</td><td>84b2dd83  </td></tr>\n",
       "<tr><td>FSR_Trainable_89a6d069</td><td>2023-07-19_01-58-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 79.8601</td><td style=\"text-align: right;\">6.79571e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">357402</td><td style=\"text-align: right;\">173.173</td><td style=\"text-align: right;\">           256.633  </td><td style=\"text-align: right;\">          2.39333 </td><td style=\"text-align: right;\">     256.633  </td><td style=\"text-align: right;\"> 1689699503</td><td style=\"text-align: right;\">                 100</td><td>89a6d069  </td></tr>\n",
       "<tr><td>FSR_Trainable_8b20cd80</td><td>2023-07-19_02-06-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">196.486 </td><td style=\"text-align: right;\">3.3156e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">361383</td><td style=\"text-align: right;\">396.984</td><td style=\"text-align: right;\">             1.82567</td><td style=\"text-align: right;\">          1.82567 </td><td style=\"text-align: right;\">       1.82567</td><td style=\"text-align: right;\"> 1689699970</td><td style=\"text-align: right;\">                   1</td><td>8b20cd80  </td></tr>\n",
       "<tr><td>FSR_Trainable_8cad7d00</td><td>2023-07-19_01-31-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">248.847 </td><td style=\"text-align: right;\">2.87471e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">345462</td><td style=\"text-align: right;\">484.579</td><td style=\"text-align: right;\">             2.23119</td><td style=\"text-align: right;\">          2.23119 </td><td style=\"text-align: right;\">       2.23119</td><td style=\"text-align: right;\"> 1689697903</td><td style=\"text-align: right;\">                   1</td><td>8cad7d00  </td></tr>\n",
       "<tr><td>FSR_Trainable_92788f4d</td><td>2023-07-19_01-51-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">107.132 </td><td style=\"text-align: right;\">2.50714e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">354336</td><td style=\"text-align: right;\">206.599</td><td style=\"text-align: right;\">           126.423  </td><td style=\"text-align: right;\">          1.24417 </td><td style=\"text-align: right;\">     126.423  </td><td style=\"text-align: right;\"> 1689699061</td><td style=\"text-align: right;\">                 100</td><td>92788f4d  </td></tr>\n",
       "<tr><td>FSR_Trainable_9ae8645b</td><td>2023-07-19_02-15-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 80.3759</td><td style=\"text-align: right;\">8.83486e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">364555</td><td style=\"text-align: right;\">167.323</td><td style=\"text-align: right;\">           136.177  </td><td style=\"text-align: right;\">          1.39073 </td><td style=\"text-align: right;\">     136.177  </td><td style=\"text-align: right;\"> 1689700534</td><td style=\"text-align: right;\">                 100</td><td>9ae8645b  </td></tr>\n",
       "<tr><td>FSR_Trainable_9ef56328</td><td>2023-07-19_01-36-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">201.686 </td><td style=\"text-align: right;\">3.72418e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">347091</td><td style=\"text-align: right;\">391.732</td><td style=\"text-align: right;\">            52.8879 </td><td style=\"text-align: right;\">          1.98599 </td><td style=\"text-align: right;\">      52.8879 </td><td style=\"text-align: right;\"> 1689698187</td><td style=\"text-align: right;\">                  32</td><td>9ef56328  </td></tr>\n",
       "<tr><td>FSR_Trainable_9f95f4c5</td><td>2023-07-19_01-51-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 83.9111</td><td style=\"text-align: right;\">1.16435e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">353179</td><td style=\"text-align: right;\">170.304</td><td style=\"text-align: right;\">           268.712  </td><td style=\"text-align: right;\">          2.53237 </td><td style=\"text-align: right;\">     268.712  </td><td style=\"text-align: right;\"> 1689699101</td><td style=\"text-align: right;\">                 100</td><td>9f95f4c5  </td></tr>\n",
       "<tr><td>FSR_Trainable_a1ffe1ec</td><td>2023-07-19_02-03-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 81.6175</td><td style=\"text-align: right;\">1.04727e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">359691</td><td style=\"text-align: right;\">169.095</td><td style=\"text-align: right;\">           129.13   </td><td style=\"text-align: right;\">          1.2557  </td><td style=\"text-align: right;\">     129.13   </td><td style=\"text-align: right;\"> 1689699814</td><td style=\"text-align: right;\">                 100</td><td>a1ffe1ec  </td></tr>\n",
       "<tr><td>FSR_Trainable_a57b2b7b</td><td>2023-07-19_02-16-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 80.3191</td><td style=\"text-align: right;\">8.18013e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">365003</td><td style=\"text-align: right;\">166.306</td><td style=\"text-align: right;\">           134.991  </td><td style=\"text-align: right;\">          1.12108 </td><td style=\"text-align: right;\">     134.991  </td><td style=\"text-align: right;\"> 1689700577</td><td style=\"text-align: right;\">                 100</td><td>a57b2b7b  </td></tr>\n",
       "<tr><td>FSR_Trainable_a78b778a</td><td>2023-07-19_02-10-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 79.5245</td><td style=\"text-align: right;\">8.13018e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">362628</td><td style=\"text-align: right;\">163.66 </td><td style=\"text-align: right;\">           124.314  </td><td style=\"text-align: right;\">          1.23115 </td><td style=\"text-align: right;\">     124.314  </td><td style=\"text-align: right;\"> 1689700258</td><td style=\"text-align: right;\">                 100</td><td>a78b778a  </td></tr>\n",
       "<tr><td>FSR_Trainable_a80f6aca</td><td>2023-07-19_01-53-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 98.2016</td><td style=\"text-align: right;\">1.82857e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">355135</td><td style=\"text-align: right;\">192.257</td><td style=\"text-align: right;\">           127.564  </td><td style=\"text-align: right;\">          1.19598 </td><td style=\"text-align: right;\">     127.564  </td><td style=\"text-align: right;\"> 1689699209</td><td style=\"text-align: right;\">                 100</td><td>a80f6aca  </td></tr>\n",
       "<tr><td>FSR_Trainable_a88c717d</td><td>2023-07-19_01-42-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">264.993 </td><td style=\"text-align: right;\">1.05679e+09</td><td>172.26.215.93</td><td style=\"text-align: right;\">350195</td><td style=\"text-align: right;\">466.984</td><td style=\"text-align: right;\">             2.33508</td><td style=\"text-align: right;\">          2.33508 </td><td style=\"text-align: right;\">       2.33508</td><td style=\"text-align: right;\"> 1689698549</td><td style=\"text-align: right;\">                   1</td><td>a88c717d  </td></tr>\n",
       "<tr><td>FSR_Trainable_adefbb03</td><td>2023-07-19_02-18-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 80.0233</td><td style=\"text-align: right;\">7.38032e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">366692</td><td style=\"text-align: right;\">175.32 </td><td style=\"text-align: right;\">           122.859  </td><td style=\"text-align: right;\">          1.18751 </td><td style=\"text-align: right;\">     122.859  </td><td style=\"text-align: right;\"> 1689700739</td><td style=\"text-align: right;\">                 100</td><td>adefbb03  </td></tr>\n",
       "<tr><td>FSR_Trainable_af803565</td><td>2023-07-19_01-43-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">259.551 </td><td style=\"text-align: right;\">8.68819e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">350904</td><td style=\"text-align: right;\">452.845</td><td style=\"text-align: right;\">            14.5529 </td><td style=\"text-align: right;\">         14.5529  </td><td style=\"text-align: right;\">      14.5529 </td><td style=\"text-align: right;\"> 1689698613</td><td style=\"text-align: right;\">                   1</td><td>af803565  </td></tr>\n",
       "<tr><td>FSR_Trainable_b32d4df7</td><td>2023-07-19_01-54-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 80.8387</td><td style=\"text-align: right;\">8.43849e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">355600</td><td style=\"text-align: right;\">166.866</td><td style=\"text-align: right;\">           119.334  </td><td style=\"text-align: right;\">          1.29288 </td><td style=\"text-align: right;\">     119.334  </td><td style=\"text-align: right;\"> 1689699246</td><td style=\"text-align: right;\">                 100</td><td>b32d4df7  </td></tr>\n",
       "<tr><td>FSR_Trainable_b33015b0</td><td>2023-07-19_01-26-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">263.186 </td><td style=\"text-align: right;\">2.70533e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">343759</td><td style=\"text-align: right;\">501.491</td><td style=\"text-align: right;\">            30.639  </td><td style=\"text-align: right;\">          4.79731 </td><td style=\"text-align: right;\">      30.639  </td><td style=\"text-align: right;\"> 1689697571</td><td style=\"text-align: right;\">                   8</td><td>b33015b0  </td></tr>\n",
       "<tr><td>FSR_Trainable_bbcb84e4</td><td>2023-07-19_02-16-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">150.292 </td><td style=\"text-align: right;\">1.51074e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">365803</td><td style=\"text-align: right;\">293.605</td><td style=\"text-align: right;\">             2.91471</td><td style=\"text-align: right;\">          1.36757 </td><td style=\"text-align: right;\">       2.91471</td><td style=\"text-align: right;\"> 1689700568</td><td style=\"text-align: right;\">                   2</td><td>bbcb84e4  </td></tr>\n",
       "<tr><td>FSR_Trainable_bc99afd4</td><td>2023-07-19_01-36-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">134.242 </td><td style=\"text-align: right;\">9.33075e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">346414</td><td style=\"text-align: right;\">258.487</td><td style=\"text-align: right;\">           171.525  </td><td style=\"text-align: right;\">          1.69651 </td><td style=\"text-align: right;\">     171.525  </td><td style=\"text-align: right;\"> 1689698204</td><td style=\"text-align: right;\">                 100</td><td>bc99afd4  </td></tr>\n",
       "<tr><td>FSR_Trainable_bea649a3</td><td>2023-07-19_01-53-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">267.301 </td><td style=\"text-align: right;\">1.04791e+09</td><td>172.26.215.93</td><td style=\"text-align: right;\">356730</td><td style=\"text-align: right;\">451.8  </td><td style=\"text-align: right;\">             1.77739</td><td style=\"text-align: right;\">          1.77739 </td><td style=\"text-align: right;\">       1.77739</td><td style=\"text-align: right;\"> 1689699212</td><td style=\"text-align: right;\">                   1</td><td>bea649a3  </td></tr>\n",
       "<tr><td>FSR_Trainable_bfb7ebd0</td><td>2023-07-19_02-15-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">117.611 </td><td style=\"text-align: right;\">1.07047e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">365574</td><td style=\"text-align: right;\">235.432</td><td style=\"text-align: right;\">             5.19567</td><td style=\"text-align: right;\">          1.0504  </td><td style=\"text-align: right;\">       5.19567</td><td style=\"text-align: right;\"> 1689700552</td><td style=\"text-align: right;\">                   4</td><td>bfb7ebd0  </td></tr>\n",
       "<tr><td>FSR_Trainable_c4a94404</td><td>2023-07-19_01-32-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">238.344 </td><td style=\"text-align: right;\">3.28731e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">345661</td><td style=\"text-align: right;\">477.799</td><td style=\"text-align: right;\">             6.05959</td><td style=\"text-align: right;\">          6.05959 </td><td style=\"text-align: right;\">       6.05959</td><td style=\"text-align: right;\"> 1689697927</td><td style=\"text-align: right;\">                   1</td><td>c4a94404  </td></tr>\n",
       "<tr><td>FSR_Trainable_c4df8bd3</td><td>2023-07-19_01-58-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 79.9344</td><td style=\"text-align: right;\">7.68582e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">357622</td><td style=\"text-align: right;\">168.331</td><td style=\"text-align: right;\">           260.518  </td><td style=\"text-align: right;\">          2.98765 </td><td style=\"text-align: right;\">     260.518  </td><td style=\"text-align: right;\"> 1689699519</td><td style=\"text-align: right;\">                 100</td><td>c4df8bd3  </td></tr>\n",
       "<tr><td>FSR_Trainable_c5e5f9a8</td><td>2023-07-19_01-43-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">267.825 </td><td style=\"text-align: right;\">9.06504e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">350654</td><td style=\"text-align: right;\">452.924</td><td style=\"text-align: right;\">             8.54195</td><td style=\"text-align: right;\">          8.54195 </td><td style=\"text-align: right;\">       8.54195</td><td style=\"text-align: right;\"> 1689698586</td><td style=\"text-align: right;\">                   1</td><td>c5e5f9a8  </td></tr>\n",
       "<tr><td>FSR_Trainable_cb2a0ab8</td><td>2023-07-19_01-46-23</td><td>False </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        94</td><td style=\"text-align: right;\">161.445 </td><td style=\"text-align: right;\">3.93432e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">351992</td><td style=\"text-align: right;\">304.628</td><td style=\"text-align: right;\">           117.144  </td><td style=\"text-align: right;\">          1.10345 </td><td style=\"text-align: right;\">     117.144  </td><td style=\"text-align: right;\"> 1689698783</td><td style=\"text-align: right;\">                  94</td><td>cb2a0ab8  </td></tr>\n",
       "<tr><td>FSR_Trainable_ce47611f</td><td>2023-07-19_02-02-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 75.3166</td><td style=\"text-align: right;\">7.79718e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">358886</td><td style=\"text-align: right;\">158.401</td><td style=\"text-align: right;\">           191.234  </td><td style=\"text-align: right;\">          1.94717 </td><td style=\"text-align: right;\">     191.234  </td><td style=\"text-align: right;\"> 1689699730</td><td style=\"text-align: right;\">                 100</td><td>ce47611f  </td></tr>\n",
       "<tr><td>FSR_Trainable_d35e4a82</td><td>2023-07-19_01-31-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">265.363 </td><td style=\"text-align: right;\">8.98206e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">344517</td><td style=\"text-align: right;\">453.537</td><td style=\"text-align: right;\">           295.63   </td><td style=\"text-align: right;\">          2.36634 </td><td style=\"text-align: right;\">     295.63   </td><td style=\"text-align: right;\"> 1689697888</td><td style=\"text-align: right;\">                 100</td><td>d35e4a82  </td></tr>\n",
       "<tr><td>FSR_Trainable_d67aa423</td><td>2023-07-19_01-46-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 90.0725</td><td style=\"text-align: right;\">1.23389e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">349699</td><td style=\"text-align: right;\">178.925</td><td style=\"text-align: right;\">           341.028  </td><td style=\"text-align: right;\">          3.56893 </td><td style=\"text-align: right;\">     341.028  </td><td style=\"text-align: right;\"> 1689698776</td><td style=\"text-align: right;\">                 100</td><td>d67aa423  </td></tr>\n",
       "<tr><td>FSR_Trainable_d8ffcb88</td><td>2023-07-19_01-52-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">109.011 </td><td style=\"text-align: right;\">2.46453e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">354861</td><td style=\"text-align: right;\">209.466</td><td style=\"text-align: right;\">           123.9    </td><td style=\"text-align: right;\">          1.2487  </td><td style=\"text-align: right;\">     123.9    </td><td style=\"text-align: right;\"> 1689699126</td><td style=\"text-align: right;\">                 100</td><td>d8ffcb88  </td></tr>\n",
       "<tr><td>FSR_Trainable_dc2ac365</td><td>2023-07-19_02-12-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">129.104 </td><td style=\"text-align: right;\">1.60872e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">364348</td><td style=\"text-align: right;\">259.479</td><td style=\"text-align: right;\">             3.46065</td><td style=\"text-align: right;\">          1.53472 </td><td style=\"text-align: right;\">       3.46065</td><td style=\"text-align: right;\"> 1689700377</td><td style=\"text-align: right;\">                   2</td><td>dc2ac365  </td></tr>\n",
       "<tr><td>FSR_Trainable_dd639a48</td><td>2023-07-19_01-42-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">109.029 </td><td style=\"text-align: right;\">6.35057e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">349010</td><td style=\"text-align: right;\">221.363</td><td style=\"text-align: right;\">           189.922  </td><td style=\"text-align: right;\">          1.85793 </td><td style=\"text-align: right;\">     189.922  </td><td style=\"text-align: right;\"> 1689698521</td><td style=\"text-align: right;\">                 100</td><td>dd639a48  </td></tr>\n",
       "<tr><td>FSR_Trainable_dec1d3bc</td><td>2023-07-19_01-30-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">333.961 </td><td style=\"text-align: right;\">6.69987e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">343830</td><td style=\"text-align: right;\">473.674</td><td style=\"text-align: right;\">           282.555  </td><td style=\"text-align: right;\">          2.23798 </td><td style=\"text-align: right;\">     282.555  </td><td style=\"text-align: right;\"> 1689697837</td><td style=\"text-align: right;\">                 100</td><td>dec1d3bc  </td></tr>\n",
       "<tr><td>FSR_Trainable_def4ce95</td><td>2023-07-19_01-43-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">269.202 </td><td style=\"text-align: right;\">8.57689e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">351122</td><td style=\"text-align: right;\">450.624</td><td style=\"text-align: right;\">             5.84261</td><td style=\"text-align: right;\">          5.84261 </td><td style=\"text-align: right;\">       5.84261</td><td style=\"text-align: right;\"> 1689698622</td><td style=\"text-align: right;\">                   1</td><td>def4ce95  </td></tr>\n",
       "<tr><td>FSR_Trainable_e0056a8e</td><td>2023-07-19_02-06-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">185.317 </td><td style=\"text-align: right;\">2.80808e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">361624</td><td style=\"text-align: right;\">382.262</td><td style=\"text-align: right;\">             1.84628</td><td style=\"text-align: right;\">          1.84628 </td><td style=\"text-align: right;\">       1.84628</td><td style=\"text-align: right;\"> 1689699985</td><td style=\"text-align: right;\">                   1</td><td>e0056a8e  </td></tr>\n",
       "<tr><td>FSR_Trainable_e2d1e51f</td><td>2023-07-19_02-16-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 82.2989</td><td style=\"text-align: right;\">9.13945e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">364824</td><td style=\"text-align: right;\">173.049</td><td style=\"text-align: right;\">           135.631  </td><td style=\"text-align: right;\">          1.73869 </td><td style=\"text-align: right;\">     135.631  </td><td style=\"text-align: right;\"> 1689700569</td><td style=\"text-align: right;\">                 100</td><td>e2d1e51f  </td></tr>\n",
       "<tr><td>FSR_Trainable_e39d3cf9</td><td>2023-07-19_02-10-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 73.2193</td><td style=\"text-align: right;\">6.2965e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">361852</td><td style=\"text-align: right;\">159.023</td><td style=\"text-align: right;\">           192.828  </td><td style=\"text-align: right;\">          1.87564 </td><td style=\"text-align: right;\">     192.828  </td><td style=\"text-align: right;\"> 1689700203</td><td style=\"text-align: right;\">                 100</td><td>e39d3cf9  </td></tr>\n",
       "<tr><td>FSR_Trainable_e7a576c4</td><td>2023-07-19_02-20-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 81.6435</td><td style=\"text-align: right;\">9.49782e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">367479</td><td style=\"text-align: right;\">175.111</td><td style=\"text-align: right;\">            71.2717 </td><td style=\"text-align: right;\">          0.741234</td><td style=\"text-align: right;\">      71.2717 </td><td style=\"text-align: right;\"> 1689700816</td><td style=\"text-align: right;\">                 100</td><td>e7a576c4  </td></tr>\n",
       "<tr><td>FSR_Trainable_e8b85816</td><td>2023-07-19_02-01-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 71.845 </td><td style=\"text-align: right;\">6.17423e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">358659</td><td style=\"text-align: right;\">156.218</td><td style=\"text-align: right;\">           189.38   </td><td style=\"text-align: right;\">          2.03257 </td><td style=\"text-align: right;\">     189.38   </td><td style=\"text-align: right;\"> 1689699717</td><td style=\"text-align: right;\">                 100</td><td>e8b85816  </td></tr>\n",
       "<tr><td>FSR_Trainable_e923645d</td><td>2023-07-19_02-05-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 91.2879</td><td style=\"text-align: right;\">7.54566e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">360681</td><td style=\"text-align: right;\">198.008</td><td style=\"text-align: right;\">           123.056  </td><td style=\"text-align: right;\">          2.00626 </td><td style=\"text-align: right;\">     123.056  </td><td style=\"text-align: right;\"> 1689699953</td><td style=\"text-align: right;\">                  64</td><td>e923645d  </td></tr>\n",
       "<tr><td>FSR_Trainable_e9a462e4</td><td>2023-07-19_02-13-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 78.2414</td><td style=\"text-align: right;\">7.79399e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">363605</td><td style=\"text-align: right;\">164.123</td><td style=\"text-align: right;\">           136.642  </td><td style=\"text-align: right;\">          1.39478 </td><td style=\"text-align: right;\">     136.642  </td><td style=\"text-align: right;\"> 1689700411</td><td style=\"text-align: right;\">                 100</td><td>e9a462e4  </td></tr>\n",
       "<tr><td>FSR_Trainable_eb7c8397</td><td>2023-07-19_01-34-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">163.032 </td><td style=\"text-align: right;\">1.43527e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">344171</td><td style=\"text-align: right;\">340.535</td><td style=\"text-align: right;\">           513.796  </td><td style=\"text-align: right;\">          4.63433 </td><td style=\"text-align: right;\">     513.796  </td><td style=\"text-align: right;\"> 1689698092</td><td style=\"text-align: right;\">                 100</td><td>eb7c8397  </td></tr>\n",
       "<tr><td>FSR_Trainable_edd691bd</td><td>2023-07-19_02-14-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 81.3387</td><td style=\"text-align: right;\">9.56008e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">364094</td><td style=\"text-align: right;\">169.306</td><td style=\"text-align: right;\">           133.235  </td><td style=\"text-align: right;\">          1.35666 </td><td style=\"text-align: right;\">     133.235  </td><td style=\"text-align: right;\"> 1689700477</td><td style=\"text-align: right;\">                 100</td><td>edd691bd  </td></tr>\n",
       "<tr><td>FSR_Trainable_ef893e7c</td><td>2023-07-19_02-03-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 89.6039</td><td style=\"text-align: right;\">1.02428e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">359447</td><td style=\"text-align: right;\">179.246</td><td style=\"text-align: right;\">           125.548  </td><td style=\"text-align: right;\">          1.08898 </td><td style=\"text-align: right;\">     125.548  </td><td style=\"text-align: right;\"> 1689699784</td><td style=\"text-align: right;\">                 100</td><td>ef893e7c  </td></tr>\n",
       "<tr><td>FSR_Trainable_efbf9c28</td><td>2023-07-19_01-43-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">259.833 </td><td style=\"text-align: right;\">8.01536e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">351308</td><td style=\"text-align: right;\">453.223</td><td style=\"text-align: right;\">             4.45116</td><td style=\"text-align: right;\">          4.45116 </td><td style=\"text-align: right;\">       4.45116</td><td style=\"text-align: right;\"> 1689698631</td><td style=\"text-align: right;\">                   1</td><td>efbf9c28  </td></tr>\n",
       "<tr><td>FSR_Trainable_f21b1363</td><td>2023-07-19_02-04-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 81.3586</td><td style=\"text-align: right;\">8.78573e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">359950</td><td style=\"text-align: right;\">166.915</td><td style=\"text-align: right;\">           127.215  </td><td style=\"text-align: right;\">          1.27994 </td><td style=\"text-align: right;\">     127.215  </td><td style=\"text-align: right;\"> 1689699865</td><td style=\"text-align: right;\">                 100</td><td>f21b1363  </td></tr>\n",
       "<tr><td>FSR_Trainable_f6ea6cea</td><td>2023-07-19_02-19-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 79.9604</td><td style=\"text-align: right;\">7.54811e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">366911</td><td style=\"text-align: right;\">172.488</td><td style=\"text-align: right;\">           122.528  </td><td style=\"text-align: right;\">          0.990587</td><td style=\"text-align: right;\">     122.528  </td><td style=\"text-align: right;\"> 1689700753</td><td style=\"text-align: right;\">                 100</td><td>f6ea6cea  </td></tr>\n",
       "<tr><td>FSR_Trainable_f84633a9</td><td>2023-07-19_02-06-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 73.6977</td><td style=\"text-align: right;\">6.98467e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">360439</td><td style=\"text-align: right;\">158.31 </td><td style=\"text-align: right;\">           198.602  </td><td style=\"text-align: right;\">          2.49095 </td><td style=\"text-align: right;\">     198.602  </td><td style=\"text-align: right;\"> 1689700007</td><td style=\"text-align: right;\">                 100</td><td>f84633a9  </td></tr>\n",
       "<tr><td>FSR_Trainable_f89fef40</td><td>2023-07-19_01-46-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">122.386 </td><td style=\"text-align: right;\">2.90385e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">351752</td><td style=\"text-align: right;\">240.291</td><td style=\"text-align: right;\">           124.918  </td><td style=\"text-align: right;\">          1.12672 </td><td style=\"text-align: right;\">     124.918  </td><td style=\"text-align: right;\"> 1689698777</td><td style=\"text-align: right;\">                 100</td><td>f89fef40  </td></tr>\n",
       "<tr><td>FSR_Trainable_f98865c4</td><td>2023-07-19_01-40-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">127.595 </td><td style=\"text-align: right;\">1.0132e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">348688</td><td style=\"text-align: right;\">245.861</td><td style=\"text-align: right;\">           188.494  </td><td style=\"text-align: right;\">          2.31298 </td><td style=\"text-align: right;\">     188.494  </td><td style=\"text-align: right;\"> 1689698413</td><td style=\"text-align: right;\">                 100</td><td>f98865c4  </td></tr>\n",
       "<tr><td>FSR_Trainable_ff689005</td><td>2023-07-19_02-12-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 81.5366</td><td style=\"text-align: right;\">8.60808e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">363084</td><td style=\"text-align: right;\">172.491</td><td style=\"text-align: right;\">           136.098  </td><td style=\"text-align: right;\">          1.39727 </td><td style=\"text-align: right;\">     136.098  </td><td style=\"text-align: right;\"> 1689700321</td><td style=\"text-align: right;\">                 100</td><td>ff689005  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_b33015b0_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-19_01-25-32/wandb/run-20230719_012544-b33015b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: Syncing run FSR_Trainable_b33015b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b33015b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_dec1d3bc_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-19_01-25-38/wandb/run-20230719_012553-dec1d3bc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: Syncing run FSR_Trainable_dec1d3bc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/dec1d3bc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_2e07ed6e_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-19_01-25-46/wandb/run-20230719_012603-2e07ed6e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: Syncing run FSR_Trainable_2e07ed6e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2e07ed6e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_eb7c8397_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-19_01-25-55/wandb/run-20230719_012614-eb7c8397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb: Syncing run FSR_Trainable_eb7c8397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/eb7c8397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:                      mae 263.18578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:                     mape 2.7053296276563466e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:                     rmse 501.49121\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:       time_since_restore 30.63905\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:         time_this_iter_s 4.79731\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:             time_total_s 30.63905\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:                timestamp 1689697571\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb:  View run FSR_Trainable_b33015b0 at: https://wandb.ai/seokjin/FSR-prediction/runs/b33015b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343829)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_012544-b33015b0/logs\n",
      "2023-07-19 01:26:32,153\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.673 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:26:32,159\tWARNING util.py:315 -- The `process_trial_result` operation took 2.680 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:26:32,163\tWARNING util.py:315 -- Processing trial results took 2.684 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:26:32,166\tWARNING util.py:315 -- The `process_trial_result` operation took 2.687 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_d35e4a82_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-19_01-26-06/wandb/run-20230719_012635-d35e4a82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: Syncing run FSR_Trainable_d35e4a82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d35e4a82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:                      mae 250.00167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:                     mape 6.893870917254005e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:                     rmse 455.85116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:       time_since_restore 273.91882\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:         time_this_iter_s 19.76563\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:             time_total_s 273.91882\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:                timestamp 1689697831\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb:  View run FSR_Trainable_2e07ed6e at: https://wandb.ai/seokjin/FSR-prediction/runs/2e07ed6e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344170)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_012603-2e07ed6e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:                      mae 333.96061\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:                     mape 6.69986781370278e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:                     rmse 473.67409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:       time_since_restore 282.55475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:         time_this_iter_s 2.23798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:             time_total_s 282.55475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:                timestamp 1689697837\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb:  View run FSR_Trainable_dec1d3bc at: https://wandb.ai/seokjin/FSR-prediction/runs/dec1d3bc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=343999)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_012553-dec1d3bc/logs\n",
      "2023-07-19 01:30:51,636\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.885 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:30:51,639\tWARNING util.py:315 -- The `process_trial_result` operation took 2.889 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:30:51,641\tWARNING util.py:315 -- Processing trial results took 2.891 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:30:51,643\tWARNING util.py:315 -- The `process_trial_result` operation took 2.893 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_710c8739_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-19_01-26-26/wandb/run-20230719_013053-710c8739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: Syncing run FSR_Trainable_710c8739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/710c8739\n",
      "2023-07-19 01:31:01,393\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.958 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:31:01,398\tWARNING util.py:315 -- The `process_trial_result` operation took 1.964 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:31:01,399\tWARNING util.py:315 -- Processing trial results took 1.966 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:31:01,401\tWARNING util.py:315 -- The `process_trial_result` operation took 1.967 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_06afbe31_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-19_01-30-45/wandb/run-20230719_013104-06afbe31\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: Syncing run FSR_Trainable_06afbe31\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/06afbe31\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:                      mae 265.36283\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:                     mape 898205617.39288\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:                     rmse 453.53722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:       time_since_restore 295.63018\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:         time_this_iter_s 2.36634\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:             time_total_s 295.63018\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:                timestamp 1689697888\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb:  View run FSR_Trainable_d35e4a82 at: https://wandb.ai/seokjin/FSR-prediction/runs/d35e4a82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344573)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_012635-d35e4a82/logs\n",
      "2023-07-19 01:31:46,015\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.603 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:31:46,020\tWARNING util.py:315 -- The `process_trial_result` operation took 2.610 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:31:46,023\tWARNING util.py:315 -- Processing trial results took 2.612 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:31:46,030\tWARNING util.py:315 -- The `process_trial_result` operation took 2.619 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_8cad7d00_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-19_01-30-57/wandb/run-20230719_013149-8cad7d00\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: Syncing run FSR_Trainable_8cad7d00\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8cad7d00\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:                      mae 248.8469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:                     mape 2874708155664526.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:                     rmse 484.57889\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:       time_since_restore 2.23119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:         time_this_iter_s 2.23119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:             time_total_s 2.23119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:                timestamp 1689697903\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb:  View run FSR_Trainable_8cad7d00 at: https://wandb.ai/seokjin/FSR-prediction/runs/8cad7d00\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345518)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_013149-8cad7d00/logs\n",
      "2023-07-19 01:32:08,934\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.755 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:32:08,939\tWARNING util.py:315 -- The `process_trial_result` operation took 1.761 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:32:08,941\tWARNING util.py:315 -- Processing trial results took 1.763 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:32:08,942\tWARNING util.py:315 -- The `process_trial_result` operation took 1.764 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_c4a94404_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-19_01-31-41/wandb/run-20230719_013209-c4a94404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb: Syncing run FSR_Trainable_c4a94404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4a94404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:                      mae 238.34423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:                     mape 3287307626226243.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:                     rmse 477.79919\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:       time_since_restore 6.05959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:         time_this_iter_s 6.05959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:             time_total_s 6.05959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:                timestamp 1689697927\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb:  View run FSR_Trainable_c4a94404 at: https://wandb.ai/seokjin/FSR-prediction/runs/c4a94404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345755)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_013209-c4a94404/logs\n",
      "2023-07-19 01:32:23,429\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.573 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:32:23,434\tWARNING util.py:315 -- The `process_trial_result` operation took 1.578 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:32:23,436\tWARNING util.py:315 -- Processing trial results took 1.581 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:32:23,438\tWARNING util.py:315 -- The `process_trial_result` operation took 1.582 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_796ead13_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-32-01/wandb/run-20230719_013226-796ead13\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb: Syncing run FSR_Trainable_796ead13\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/796ead13\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)03 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:                      mae 241.08427\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:                     mape 5660792867174945.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:                     rmse 531.85455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:       time_since_restore 1.66094\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:         time_this_iter_s 1.66094\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:             time_total_s 1.66094\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:                timestamp 1689697941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb:  View run FSR_Trainable_796ead13 at: https://wandb.ai/seokjin/FSR-prediction/runs/796ead13\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345987)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_013226-796ead13/logs\n",
      "2023-07-19 01:32:39,447\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.820 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:32:39,451\tWARNING util.py:315 -- The `process_trial_result` operation took 1.825 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:32:39,454\tWARNING util.py:315 -- Processing trial results took 1.828 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:32:39,456\tWARNING util.py:315 -- The `process_trial_result` operation took 1.830 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_5e4396e7_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-32-20/wandb/run-20230719_013243-5e4396e7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: Syncing run FSR_Trainable_5e4396e7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5e4396e7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:                      mae 115.88449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:                     mape 1.275793799545967e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:                     rmse 203.83583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:       time_since_restore 141.05553\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:         time_this_iter_s 1.18067\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:             time_total_s 141.05553\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:                timestamp 1689698008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb:  View run FSR_Trainable_06afbe31 at: https://wandb.ai/seokjin/FSR-prediction/runs/06afbe31\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345268)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_013104-06afbe31/logs\n",
      "2023-07-19 01:33:42,482\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.086 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:33:42,486\tWARNING util.py:315 -- The `process_trial_result` operation took 2.091 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:33:42,489\tWARNING util.py:315 -- Processing trial results took 2.094 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:33:42,491\tWARNING util.py:315 -- The `process_trial_result` operation took 2.096 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_bc99afd4_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-32-35/wandb/run-20230719_013344-bc99afd4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb: Syncing run FSR_Trainable_bc99afd4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/bc99afd4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:                      mae 163.03184\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:                     mape 1.4352658705854012e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:                     rmse 340.53503\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:       time_since_restore 513.79606\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:         time_this_iter_s 4.63433\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:             time_total_s 513.79606\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:                timestamp 1689698092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb:  View run FSR_Trainable_eb7c8397 at: https://wandb.ai/seokjin/FSR-prediction/runs/eb7c8397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=344337)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_012614-eb7c8397/logs\n",
      "2023-07-19 01:35:05,266\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.198 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:35:05,270\tWARNING util.py:315 -- The `process_trial_result` operation took 2.203 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:35:05,271\tWARNING util.py:315 -- Processing trial results took 2.204 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:35:05,273\tWARNING util.py:315 -- The `process_trial_result` operation took 2.206 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_0ccf5958_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-33-38/wandb/run-20230719_013508-0ccf5958\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: Syncing run FSR_Trainable_0ccf5958\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0ccf5958\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:                      mae 376.17006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:                     mape 6.722315460900253e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:                     rmse 527.37034\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:       time_since_restore 1.08401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:         time_this_iter_s 1.08401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:             time_total_s 1.08401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:                timestamp 1689698103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb:  View run FSR_Trainable_0ccf5958 at: https://wandb.ai/seokjin/FSR-prediction/runs/0ccf5958\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_013508-0ccf5958/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:                      mae 109.5317\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:                     mape 131730806.22297\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:                     rmse 211.25261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:       time_since_restore 260.03863\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:         time_this_iter_s 2.77415\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:             time_total_s 260.03863\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:                timestamp 1689698117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb:  View run FSR_Trainable_710c8739 at: https://wandb.ai/seokjin/FSR-prediction/runs/710c8739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=345056)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_013053-710c8739/logs\n",
      "2023-07-19 01:35:23,952\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.835 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:35:23,957\tWARNING util.py:315 -- The `process_trial_result` operation took 1.841 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:35:23,959\tWARNING util.py:315 -- Processing trial results took 1.843 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:35:23,961\tWARNING util.py:315 -- The `process_trial_result` operation took 1.845 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_4ca4593b_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-35-02/wandb/run-20230719_013522-4ca4593b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Syncing run FSR_Trainable_4ca4593b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4ca4593b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb:       training_iteration \n",
      "2023-07-19 01:35:32,056\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.120 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:35:32,059\tWARNING util.py:315 -- The `process_trial_result` operation took 2.124 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:35:32,062\tWARNING util.py:315 -- Processing trial results took 2.127 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:35:32,065\tWARNING util.py:315 -- The `process_trial_result` operation took 2.131 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_9ef56328_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-35-15/wandb/run-20230719_013534-9ef56328\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb: Syncing run FSR_Trainable_9ef56328\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9ef56328\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:                      mae 89.06235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:                     mape 71618277.97004\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:                     rmse 180.98378\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:       time_since_restore 174.03453\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:         time_this_iter_s 1.55153\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:             time_total_s 174.03453\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:                timestamp 1689698137\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb:  View run FSR_Trainable_5e4396e7 at: https://wandb.ai/seokjin/FSR-prediction/runs/5e4396e7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346213)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_013243-5e4396e7/logs\n",
      "2023-07-19 01:35:42,436\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.940 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:35:42,440\tWARNING util.py:315 -- The `process_trial_result` operation took 1.945 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:35:42,442\tWARNING util.py:315 -- Processing trial results took 1.947 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:35:42,443\tWARNING util.py:315 -- The `process_trial_result` operation took 1.948 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_52c95b51_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-35-27/wandb/run-20230719_013544-52c95b51\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb: Syncing run FSR_Trainable_52c95b51\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/52c95b51\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:                      mae 257.58135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:                     mape 835458597.6383\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:                     rmse 451.83921\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:       time_since_restore 4.89085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:         time_this_iter_s 2.03821\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:             time_total_s 4.89085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:                timestamp 1689698144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb:  View run FSR_Trainable_52c95b51 at: https://wandb.ai/seokjin/FSR-prediction/runs/52c95b51\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347415)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_013544-52c95b51/logs\n",
      "2023-07-19 01:35:50,384\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.772 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:35:50,386\tWARNING util.py:315 -- The `process_trial_result` operation took 1.775 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:35:50,388\tWARNING util.py:315 -- Processing trial results took 1.777 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:35:50,389\tWARNING util.py:315 -- The `process_trial_result` operation took 1.778 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_159a7bec_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-35-37/wandb/run-20230719_013553-159a7bec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb: Syncing run FSR_Trainable_159a7bec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/159a7bec\n",
      "2023-07-19 01:36:01,756\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.593 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:36:01,758\tWARNING util.py:315 -- The `process_trial_result` operation took 1.596 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:36:01,761\tWARNING util.py:315 -- Processing trial results took 1.598 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:36:01,762\tWARNING util.py:315 -- The `process_trial_result` operation took 1.600 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_2af6a57e_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-35-47/wandb/run-20230719_013604-2af6a57e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Syncing run FSR_Trainable_2af6a57e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2af6a57e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:                      mae 145.54551\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:                     mape 269836862.38259\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:                     rmse 276.57922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:       time_since_restore 26.67689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:         time_this_iter_s 0.78114\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:             time_total_s 26.67689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:                timestamp 1689698177\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb:  View run FSR_Trainable_159a7bec at: https://wandb.ai/seokjin/FSR-prediction/runs/159a7bec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347641)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_013553-159a7bec/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: \\ 0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: | 0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: | 0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb:       training_iteration \n",
      "2023-07-19 01:36:29,210\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.813 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:36:29,214\tWARNING util.py:315 -- The `process_trial_result` operation took 1.818 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:36:29,216\tWARNING util.py:315 -- Processing trial results took 1.821 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:36:29,219\tWARNING util.py:315 -- The `process_trial_result` operation took 1.823 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347861)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_05cee5c2_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-35-58/wandb/run-20230719_013632-05cee5c2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb: Syncing run FSR_Trainable_05cee5c2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/05cee5c2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:                      mae 201.68584\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:                     mape 372417722.01872\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:                     rmse 391.73243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:       time_since_restore 52.88786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:         time_this_iter_s 1.98599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:             time_total_s 52.88786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:                timestamp 1689698187\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb:  View run FSR_Trainable_9ef56328 at: https://wandb.ai/seokjin/FSR-prediction/runs/9ef56328\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_013534-9ef56328/logs\n",
      "2023-07-19 01:36:39,100\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.862 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:36:39,104\tWARNING util.py:315 -- The `process_trial_result` operation took 1.866 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:36:39,105\tWARNING util.py:315 -- Processing trial results took 1.868 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:36:39,107\tWARNING util.py:315 -- The `process_trial_result` operation took 1.870 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=347194)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_595d4aaf_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-36-25/wandb/run-20230719_013641-595d4aaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: Syncing run FSR_Trainable_595d4aaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/595d4aaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 01:36:48,863\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.812 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:36:48,866\tWARNING util.py:315 -- The `process_trial_result` operation took 1.815 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:36:48,869\tWARNING util.py:315 -- Processing trial results took 1.818 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:36:48,871\tWARNING util.py:315 -- The `process_trial_result` operation took 1.820 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:                      mae 134.24207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:                     mape 9.330746203149643e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:                     rmse 258.48668\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:       time_since_restore 171.52457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:         time_this_iter_s 1.69651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:             time_total_s 171.52457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:                timestamp 1689698204\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb:  View run FSR_Trainable_bc99afd4 at: https://wandb.ai/seokjin/FSR-prediction/runs/bc99afd4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=346470)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_013344-bc99afd4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_133473b2_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-36-34/wandb/run-20230719_013651-133473b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb: Syncing run FSR_Trainable_133473b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/133473b2\n",
      "2023-07-19 01:37:01,796\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.562 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:37:01,798\tWARNING util.py:315 -- The `process_trial_result` operation took 1.565 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:37:01,800\tWARNING util.py:315 -- Processing trial results took 1.567 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:37:01,802\tWARNING util.py:315 -- The `process_trial_result` operation took 1.569 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_f98865c4_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-36-44/wandb/run-20230719_013703-f98865c4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb: Syncing run FSR_Trainable_f98865c4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f98865c4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:                      mae 112.13537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:                     mape 205034858.69395\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:                     rmse 217.88846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:       time_since_restore 120.0227\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:         time_this_iter_s 1.15786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:             time_total_s 120.0227\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:                timestamp 1689698313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb:  View run FSR_Trainable_05cee5c2 at: https://wandb.ai/seokjin/FSR-prediction/runs/05cee5c2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348113)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_013632-05cee5c2/logs\n",
      "2023-07-19 01:38:48,556\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.523 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:38:48,560\tWARNING util.py:315 -- The `process_trial_result` operation took 1.527 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:38:48,561\tWARNING util.py:315 -- Processing trial results took 1.529 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:38:48,563\tWARNING util.py:315 -- The `process_trial_result` operation took 1.530 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_dd639a48_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-36-57/wandb/run-20230719_013850-dd639a48\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb: Syncing run FSR_Trainable_dd639a48\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/dd639a48\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:                      mae 145.90059\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:                     mape 1.3120115629040418e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:                     rmse 262.9718\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:       time_since_restore 187.02117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:         time_this_iter_s 1.8371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:             time_total_s 187.02117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:                timestamp 1689698388\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb:  View run FSR_Trainable_595d4aaf at: https://wandb.ai/seokjin/FSR-prediction/runs/595d4aaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348336)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_013641-595d4aaf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:                      mae 126.53506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:                     mape 9.232409643895926e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:                     rmse 250.12521\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:       time_since_restore 187.22588\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:         time_this_iter_s 1.82873\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:             time_total_s 187.22588\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:                timestamp 1689698398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb:  View run FSR_Trainable_133473b2 at: https://wandb.ai/seokjin/FSR-prediction/runs/133473b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348546)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_013651-133473b2/logs\n",
      "2023-07-19 01:40:04,944\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.010 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:40:04,947\tWARNING util.py:315 -- The `process_trial_result` operation took 2.013 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:40:04,949\tWARNING util.py:315 -- Processing trial results took 2.016 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:40:04,952\tWARNING util.py:315 -- The `process_trial_result` operation took 2.018 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_47653478_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-38-44/wandb/run-20230719_014007-47653478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb: Syncing run FSR_Trainable_47653478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/47653478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 01:40:16,483\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.569 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:40:16,485\tWARNING util.py:315 -- The `process_trial_result` operation took 1.573 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:40:16,488\tWARNING util.py:315 -- Processing trial results took 1.575 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:40:16,489\tWARNING util.py:315 -- The `process_trial_result` operation took 1.577 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:                      mae 127.59508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:                     mape 1.0132000910255672e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:                     rmse 245.86078\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:       time_since_restore 188.49363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:         time_this_iter_s 2.31298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:             time_total_s 188.49363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:                timestamp 1689698413\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb:  View run FSR_Trainable_f98865c4 at: https://wandb.ai/seokjin/FSR-prediction/runs/f98865c4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=348774)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_013703-f98865c4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_4d7cb0db_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-40-00/wandb/run-20230719_014017-4d7cb0db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb: Syncing run FSR_Trainable_4d7cb0db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4d7cb0db\n",
      "2023-07-19 01:40:31,081\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.939 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:40:31,085\tWARNING util.py:315 -- The `process_trial_result` operation took 1.944 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:40:31,086\tWARNING util.py:315 -- Processing trial results took 1.944 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:40:31,087\tWARNING util.py:315 -- The `process_trial_result` operation took 1.946 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_d67aa423_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-40-10/wandb/run-20230719_014031-d67aa423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: Syncing run FSR_Trainable_d67aa423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d67aa423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)03 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:                      mae 109.02852\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:                     mape 6.350572809996863e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:                     rmse 221.36254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:       time_since_restore 189.92206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:         time_this_iter_s 1.85793\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:             time_total_s 189.92206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:                timestamp 1689698521\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb:  View run FSR_Trainable_dd639a48 at: https://wandb.ai/seokjin/FSR-prediction/runs/dd639a48\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349065)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_013850-dd639a48/logs\n",
      "2023-07-19 01:42:16,693\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.146 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:42:16,697\tWARNING util.py:315 -- The `process_trial_result` operation took 2.152 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:42:16,702\tWARNING util.py:315 -- Processing trial results took 2.157 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:42:16,703\tWARNING util.py:315 -- The `process_trial_result` operation took 2.158 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_794b11fd_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-40-24/wandb/run-20230719_014218-794b11fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: Syncing run FSR_Trainable_794b11fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/794b11fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:                      mae 268.24144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:                     mape 748432368.16757\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:                     rmse 449.28699\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:       time_since_restore 2.98695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:         time_this_iter_s 2.98695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:             time_total_s 2.98695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:                timestamp 1689698534\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb:  View run FSR_Trainable_794b11fd at: https://wandb.ai/seokjin/FSR-prediction/runs/794b11fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350053)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014218-794b11fd/logs\n",
      "2023-07-19 01:42:31,345\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.983 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:42:31,349\tWARNING util.py:315 -- The `process_trial_result` operation took 1.987 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:42:31,351\tWARNING util.py:315 -- Processing trial results took 1.990 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:42:31,354\tWARNING util.py:315 -- The `process_trial_result` operation took 1.993 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_a88c717d_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-42-11/wandb/run-20230719_014233-a88c717d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: Syncing run FSR_Trainable_a88c717d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a88c717d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:                      mae 264.99274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:                     mape 1056787178.85627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:                     rmse 466.98381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:       time_since_restore 2.33508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:         time_this_iter_s 2.33508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:             time_total_s 2.33508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:                timestamp 1689698549\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb:  View run FSR_Trainable_a88c717d at: https://wandb.ai/seokjin/FSR-prediction/runs/a88c717d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350278)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014233-a88c717d/logs\n",
      "2023-07-19 01:42:46,400\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.687 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:42:46,404\tWARNING util.py:315 -- The `process_trial_result` operation took 1.692 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:42:46,407\tWARNING util.py:315 -- Processing trial results took 1.694 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:42:46,408\tWARNING util.py:315 -- The `process_trial_result` operation took 1.696 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_0591c3b9_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-42-27/wandb/run-20230719_014248-0591c3b9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb: Syncing run FSR_Trainable_0591c3b9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0591c3b9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)03 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:                      mae 264.76489\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:                     mape 796460666.48891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:                     rmse 448.99193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:       time_since_restore 2.81586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:         time_this_iter_s 2.81586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:             time_total_s 2.81586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:                timestamp 1689698564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb:  View run FSR_Trainable_0591c3b9 at: https://wandb.ai/seokjin/FSR-prediction/runs/0591c3b9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350503)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014248-0591c3b9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_c5e5f9a8_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-42-41/wandb/run-20230719_014304-c5e5f9a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: Syncing run FSR_Trainable_c5e5f9a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c5e5f9a8\n",
      "2023-07-19 01:43:08,093\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.078 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:43:08,096\tWARNING util.py:315 -- The `process_trial_result` operation took 2.082 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:43:08,099\tWARNING util.py:315 -- Processing trial results took 2.085 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:43:08,101\tWARNING util.py:315 -- The `process_trial_result` operation took 2.087 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:                      mae 267.82456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:                     mape 906504057.94494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:                     rmse 452.92372\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:       time_since_restore 8.54195\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:         time_this_iter_s 8.54195\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:             time_total_s 8.54195\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:                timestamp 1689698586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb:  View run FSR_Trainable_c5e5f9a8 at: https://wandb.ai/seokjin/FSR-prediction/runs/c5e5f9a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350728)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014304-c5e5f9a8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_af803565_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-42-57/wandb/run-20230719_014325-af803565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb: Syncing run FSR_Trainable_af803565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/af803565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:                      mae 120.28057\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:                     mape 7.61500716875851e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:                     rmse 237.40122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:       time_since_restore 195.8644\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:         time_this_iter_s 2.09305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:             time_total_s 195.8644\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:                timestamp 1689698606\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb:  View run FSR_Trainable_47653478 at: https://wandb.ai/seokjin/FSR-prediction/runs/47653478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349334)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014007-47653478/logs\n",
      "2023-07-19 01:43:35,168\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.066 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:43:35,171\tWARNING util.py:315 -- The `process_trial_result` operation took 2.069 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:43:35,172\tWARNING util.py:315 -- Processing trial results took 2.071 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:43:35,176\tWARNING util.py:315 -- The `process_trial_result` operation took 2.075 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:                      mae 259.55055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:                     mape 868819418.40674\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:                     rmse 452.84543\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:       time_since_restore 14.55286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:         time_this_iter_s 14.55286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:             time_total_s 14.55286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:                timestamp 1689698613\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb:  View run FSR_Trainable_af803565 at: https://wandb.ai/seokjin/FSR-prediction/runs/af803565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=350959)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014325-af803565/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 01:43:44,373\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.724 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:43:44,377\tWARNING util.py:315 -- The `process_trial_result` operation took 1.728 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:43:44,380\tWARNING util.py:315 -- Processing trial results took 1.731 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:43:44,382\tWARNING util.py:315 -- The `process_trial_result` operation took 1.733 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_def4ce95_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-43-18/wandb/run-20230719_014343-def4ce95\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: Syncing run FSR_Trainable_def4ce95\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/def4ce95\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:                      mae 269.20206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:                     mape 857688950.82336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:                     rmse 450.62371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:       time_since_restore 5.84261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:         time_this_iter_s 5.84261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:             time_total_s 5.84261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:                timestamp 1689698622\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb:  View run FSR_Trainable_def4ce95 at: https://wandb.ai/seokjin/FSR-prediction/runs/def4ce95\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351190)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014343-def4ce95/logs\n",
      "2023-07-19 01:43:52,940\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.752 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:43:52,943\tWARNING util.py:315 -- The `process_trial_result` operation took 1.756 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:43:52,946\tWARNING util.py:315 -- Processing trial results took 1.759 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:43:52,948\tWARNING util.py:315 -- The `process_trial_result` operation took 1.761 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_efbf9c28_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-43-36/wandb/run-20230719_014353-efbf9c28\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: Syncing run FSR_Trainable_efbf9c28\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/efbf9c28\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:                      mae 259.83281\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:                     mape 801536203.05258\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:                     rmse 453.22321\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:       time_since_restore 4.45116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:         time_this_iter_s 4.45116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:             time_total_s 4.45116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:                timestamp 1689698631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb:  View run FSR_Trainable_efbf9c28 at: https://wandb.ai/seokjin/FSR-prediction/runs/efbf9c28\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351412)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014353-efbf9c28/logs\n",
      "2023-07-19 01:44:03,181\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.278 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:44:03,184\tWARNING util.py:315 -- The `process_trial_result` operation took 2.283 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:44:03,190\tWARNING util.py:315 -- Processing trial results took 2.288 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:44:03,191\tWARNING util.py:315 -- The `process_trial_result` operation took 2.290 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_7050bf07_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-43-46/wandb/run-20230719_014403-7050bf07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb: Syncing run FSR_Trainable_7050bf07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7050bf07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 01:44:10,700\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.086 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:44:10,703\tWARNING util.py:315 -- The `process_trial_result` operation took 2.089 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:44:10,705\tWARNING util.py:315 -- Processing trial results took 2.091 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:44:10,707\tWARNING util.py:315 -- The `process_trial_result` operation took 2.094 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:                      mae 250.61323\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:                     mape 927101813.33221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:                     rmse 448.83756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:       time_since_restore 8.29589\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:         time_this_iter_s 3.69194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:             time_total_s 8.29589\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:                timestamp 1689698646\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb:  View run FSR_Trainable_7050bf07 at: https://wandb.ai/seokjin/FSR-prediction/runs/7050bf07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351635)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014403-7050bf07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_f89fef40_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-43-56/wandb/run-20230719_014413-f89fef40\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb: Syncing run FSR_Trainable_f89fef40\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f89fef40\n",
      "2023-07-19 01:44:23,686\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.184 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:44:23,692\tWARNING util.py:315 -- The `process_trial_result` operation took 2.190 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:44:23,696\tWARNING util.py:315 -- Processing trial results took 2.194 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:44:23,699\tWARNING util.py:315 -- The `process_trial_result` operation took 2.198 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_cb2a0ab8_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-44-07/wandb/run-20230719_014426-cb2a0ab8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb: Syncing run FSR_Trainable_cb2a0ab8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/cb2a0ab8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:                      mae 96.10905\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:                     mape 103281773.20261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:                     rmse 198.83303\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:       time_since_restore 343.25867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:         time_this_iter_s 3.27944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:             time_total_s 343.25867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:                timestamp 1689698764\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb:  View run FSR_Trainable_4d7cb0db at: https://wandb.ai/seokjin/FSR-prediction/runs/4d7cb0db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349553)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014017-4d7cb0db/logs\n",
      "2023-07-19 01:46:18,212\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.685 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:46:18,215\tWARNING util.py:315 -- The `process_trial_result` operation took 1.689 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:46:18,217\tWARNING util.py:315 -- Processing trial results took 1.691 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:46:18,220\tWARNING util.py:315 -- The `process_trial_result` operation took 1.694 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_234d8753_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-44-19/wandb/run-20230719_014621-234d8753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Syncing run FSR_Trainable_234d8753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/234d8753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:                      mae 90.07249\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:                     mape 123388506.62449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:                     rmse 178.92482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:       time_since_restore 341.0285\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:         time_this_iter_s 3.56893\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:             time_total_s 341.0285\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:                timestamp 1689698776\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb:  View run FSR_Trainable_d67aa423 at: https://wandb.ai/seokjin/FSR-prediction/runs/d67aa423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=349780)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014031-d67aa423/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb:                timestamp \n",
      "2023-07-19 01:46:24,061\tERROR tune_controller.py:873 -- Trial task failed for trial FSR_Trainable_cb2a0ab8\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 2540, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::ResourceTrainable.train()\u001b[39m (pid=351992, ip=172.26.215.93, actor_id=61f3386e984e0935cadf64f101000000, repr=<ray.tune.trainable.util.FSR_Trainable object at 0x7f2a789fb4f0>)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 386, in train\n",
      "    result = self.step()\n",
      "  File \"/home/seokj/workspace/FSR-prediction/fsr_trainable.py\", line 73, in step\n",
      "    mae.append(sklearn.metrics.mean_absolute_error(y, pred))\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 196, in mean_absolute_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014426-cb2a0ab8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=351853)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014426-cb2a0ab8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb: iterations_since_restore 94\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:                      mae 161.44475\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:                     mape 393432230.3499\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:                     rmse 304.62754\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:       time_since_restore 117.14393\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:         time_this_iter_s 1.10345\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:             time_total_s 117.14393\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:                timestamp 1689698783\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:       training_iteration 94\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb:  View run FSR_Trainable_cb2a0ab8 at: https://wandb.ai/seokjin/FSR-prediction/runs/cb2a0ab8\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352079)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014426-cb2a0ab8/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2023-07-19 01:46:32,860\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.637 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:46:32,863\tWARNING util.py:315 -- The `process_trial_result` operation took 1.641 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:46:32,865\tWARNING util.py:315 -- Processing trial results took 1.642 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:46:32,866\tWARNING util.py:315 -- The `process_trial_result` operation took 1.643 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_53b41998_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-46-14/wandb/run-20230719_014633-53b41998\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: Syncing run FSR_Trainable_53b41998\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/53b41998\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:                      mae 210.69954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:                     mape 590428049.07643\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:                     rmse 388.61052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:       time_since_restore 7.86565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:         time_this_iter_s 3.61871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:             time_total_s 7.86565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:                timestamp 1689698796\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb:  View run FSR_Trainable_53b41998 at: https://wandb.ai/seokjin/FSR-prediction/runs/53b41998\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352622)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014633-53b41998/logs\n",
      "2023-07-19 01:46:40,755\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.902 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:46:40,757\tWARNING util.py:315 -- The `process_trial_result` operation took 1.905 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:46:40,759\tWARNING util.py:315 -- Processing trial results took 1.907 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:46:40,760\tWARNING util.py:315 -- The `process_trial_result` operation took 1.909 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_0b61e291_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-46-27/wandb/run-20230719_014642-0b61e291\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: Syncing run FSR_Trainable_0b61e291\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0b61e291\n",
      "2023-07-19 01:46:50,210\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.878 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:46:50,213\tWARNING util.py:315 -- The `process_trial_result` operation took 1.882 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:46:50,216\tWARNING util.py:315 -- Processing trial results took 1.886 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:46:50,218\tWARNING util.py:315 -- The `process_trial_result` operation took 1.887 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_4e70bdaa_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-46-35/wandb/run-20230719_014651-4e70bdaa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb: Syncing run FSR_Trainable_4e70bdaa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4e70bdaa\n",
      "2023-07-19 01:47:00,243\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.614 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:47:00,247\tWARNING util.py:315 -- The `process_trial_result` operation took 1.619 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:47:00,249\tWARNING util.py:315 -- Processing trial results took 1.621 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:47:00,250\tWARNING util.py:315 -- The `process_trial_result` operation took 1.622 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_9f95f4c5_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-46-45/wandb/run-20230719_014701-9f95f4c5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: Syncing run FSR_Trainable_9f95f4c5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9f95f4c5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:                      mae 143.89211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:                     mape 335403642.92807\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:                     rmse 283.89128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:       time_since_restore 22.56538\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:         time_this_iter_s 2.66961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:             time_total_s 22.56538\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:                timestamp 1689698831\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb:  View run FSR_Trainable_4e70bdaa at: https://wandb.ai/seokjin/FSR-prediction/runs/4e70bdaa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353066)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014651-4e70bdaa/logs\n",
      "2023-07-19 01:47:27,177\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.871 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:47:27,179\tWARNING util.py:315 -- The `process_trial_result` operation took 1.876 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:47:27,183\tWARNING util.py:315 -- Processing trial results took 1.879 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:47:27,185\tWARNING util.py:315 -- The `process_trial_result` operation took 1.881 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_4d352246_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-46-55/wandb/run-20230719_014728-4d352246\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb: Syncing run FSR_Trainable_4d352246\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4d352246\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:                      mae 133.95962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:                     mape 158199408.46334\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:                     rmse 273.81503\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:       time_since_restore 21.85078\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:         time_this_iter_s 2.6246\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:             time_total_s 21.85078\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:                timestamp 1689698865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb:  View run FSR_Trainable_4d352246 at: https://wandb.ai/seokjin/FSR-prediction/runs/4d352246\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353510)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014728-4d352246/logs\n",
      "2023-07-19 01:48:01,869\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.573 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:48:01,871\tWARNING util.py:315 -- The `process_trial_result` operation took 1.576 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:48:01,872\tWARNING util.py:315 -- Processing trial results took 1.577 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:48:01,874\tWARNING util.py:315 -- The `process_trial_result` operation took 1.579 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_7b3e9d66_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-47-22/wandb/run-20230719_014803-7b3e9d66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb: Syncing run FSR_Trainable_7b3e9d66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7b3e9d66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:                      mae 151.23002\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:                     mape 1.219452073910982e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:                     rmse 287.25483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:       time_since_restore 11.36798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:         time_this_iter_s 2.54754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:             time_total_s 11.36798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:                timestamp 1689698889\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb:  View run FSR_Trainable_7b3e9d66 at: https://wandb.ai/seokjin/FSR-prediction/runs/7b3e9d66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353749)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014803-7b3e9d66/logs\n",
      "2023-07-19 01:48:25,983\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.914 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:48:25,988\tWARNING util.py:315 -- The `process_trial_result` operation took 1.920 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:48:25,990\tWARNING util.py:315 -- Processing trial results took 1.922 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:48:25,991\tWARNING util.py:315 -- The `process_trial_result` operation took 1.923 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_8390d4ff_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-47-56/wandb/run-20230719_014827-8390d4ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: Syncing run FSR_Trainable_8390d4ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8390d4ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)02 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:                      mae 179.10586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:                     mape 1.8873489387849405e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:                     rmse 330.76701\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:       time_since_restore 6.07636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:         time_this_iter_s 2.876\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:             time_total_s 6.07636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:                timestamp 1689698908\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb:  View run FSR_Trainable_8390d4ff at: https://wandb.ai/seokjin/FSR-prediction/runs/8390d4ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353980)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014827-8390d4ff/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014621-234d8753/logs\n",
      "2023-07-19 01:48:44,936\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.180 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:48:44,940\tWARNING util.py:315 -- The `process_trial_result` operation took 2.185 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:48:44,941\tWARNING util.py:315 -- Processing trial results took 2.186 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:48:44,942\tWARNING util.py:315 -- The `process_trial_result` operation took 2.187 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_057012f0_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-48-20/wandb/run-20230719_014845-057012f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: Syncing run FSR_Trainable_057012f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/057012f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 01:48:52,095\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.775 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:48:52,099\tWARNING util.py:315 -- The `process_trial_result` operation took 1.779 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:48:52,100\tWARNING util.py:315 -- Processing trial results took 1.780 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:48:52,107\tWARNING util.py:315 -- The `process_trial_result` operation took 1.786 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:                      mae 237.19893\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:                     mape 2484368973672729.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:                     rmse 466.44262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:       time_since_restore 4.43193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:         time_this_iter_s 4.43193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:             time_total_s 4.43193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:                timestamp 1689698922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb:  View run FSR_Trainable_057012f0 at: https://wandb.ai/seokjin/FSR-prediction/runs/057012f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354222)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014845-057012f0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_92788f4d_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-48-38/wandb/run-20230719_014855-92788f4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: Syncing run FSR_Trainable_92788f4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/92788f4d\n",
      "2023-07-19 01:49:02,581\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.679 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:49:02,584\tWARNING util.py:315 -- The `process_trial_result` operation took 1.683 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:49:02,586\tWARNING util.py:315 -- Processing trial results took 1.686 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:49:02,588\tWARNING util.py:315 -- The `process_trial_result` operation took 1.687 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_84b2dd83_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-48-48/wandb/run-20230719_014905-84b2dd83\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Syncing run FSR_Trainable_84b2dd83\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/84b2dd83\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:                      mae 126.14474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:                     mape 180165184.73393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:                     rmse 244.40774\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:       time_since_restore 173.64456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:         time_this_iter_s 2.77315\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:             time_total_s 173.64456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:                timestamp 1689698983\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb:  View run FSR_Trainable_0b61e291 at: https://wandb.ai/seokjin/FSR-prediction/runs/0b61e291\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=352841)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014642-0b61e291/logs\n",
      "2023-07-19 01:49:58,569\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.735 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:49:58,571\tWARNING util.py:315 -- The `process_trial_result` operation took 1.738 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:49:58,573\tWARNING util.py:315 -- Processing trial results took 1.740 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:49:58,576\tWARNING util.py:315 -- The `process_trial_result` operation took 1.743 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_d8ffcb88_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-48-59/wandb/run-20230719_015002-d8ffcb88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: Syncing run FSR_Trainable_d8ffcb88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d8ffcb88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 01:51:05,354\tERROR tune_controller.py:873 -- Trial task failed for trial FSR_Trainable_84b2dd83\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 2540, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::ResourceTrainable.train()\u001b[39m (pid=354554, ip=172.26.215.93, actor_id=c39013cbed36f21e3f06de8d01000000, repr=<ray.tune.trainable.util.FSR_Trainable object at 0x7f0c9ddff550>)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 386, in train\n",
      "    result = self.step()\n",
      "  File \"/home/seokj/workspace/FSR-prediction/fsr_trainable.py\", line 73, in step\n",
      "    mae.append(sklearn.metrics.mean_absolute_error(y, pred))\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 196, in mean_absolute_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:                      mae 107.13235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:                     mape 250714382.94487\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:                     rmse 206.59857\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:       time_since_restore 126.42337\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:         time_this_iter_s 1.24417\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:             time_total_s 126.42337\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:                timestamp 1689699061\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb:  View run FSR_Trainable_92788f4d at: https://wandb.ai/seokjin/FSR-prediction/runs/92788f4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354436)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014855-92788f4d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014905-84b2dd83/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014905-84b2dd83/logs\n",
      "2023-07-19 01:51:14,565\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.879 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:51:14,569\tWARNING util.py:315 -- The `process_trial_result` operation took 1.885 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:51:14,571\tWARNING util.py:315 -- Processing trial results took 1.886 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:51:14,572\tWARNING util.py:315 -- The `process_trial_result` operation took 1.888 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354660)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_a80f6aca_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-49-55/wandb/run-20230719_015117-a80f6aca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: Syncing run FSR_Trainable_a80f6aca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a80f6aca\n",
      "2023-07-19 01:51:24,930\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.644 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:51:24,932\tWARNING util.py:315 -- The `process_trial_result` operation took 1.647 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:51:24,936\tWARNING util.py:315 -- Processing trial results took 1.652 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:51:24,938\tWARNING util.py:315 -- The `process_trial_result` operation took 1.653 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_572c33b0_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-51-11/wandb/run-20230719_015127-572c33b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb: Syncing run FSR_Trainable_572c33b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/572c33b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:                      mae 83.91115\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:                     mape 116435259.31407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:                     rmse 170.30362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:       time_since_restore 268.7123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:         time_this_iter_s 2.53237\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:             time_total_s 268.7123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:                timestamp 1689699101\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb:  View run FSR_Trainable_9f95f4c5 at: https://wandb.ai/seokjin/FSR-prediction/runs/9f95f4c5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=353277)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_014701-9f95f4c5/logs\n",
      "2023-07-19 01:51:55,297\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.749 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:51:55,299\tWARNING util.py:315 -- The `process_trial_result` operation took 1.752 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:51:55,301\tWARNING util.py:315 -- Processing trial results took 1.754 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:51:55,302\tWARNING util.py:315 -- The `process_trial_result` operation took 1.755 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_b32d4df7_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-51-21/wandb/run-20230719_015158-b32d4df7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb: Syncing run FSR_Trainable_b32d4df7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b32d4df7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:                      mae 109.01084\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:                     mape 246453412.82329\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:                     rmse 209.46628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:       time_since_restore 123.89987\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:         time_this_iter_s 1.2487\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:             time_total_s 123.89987\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:                timestamp 1689699126\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb:  View run FSR_Trainable_d8ffcb88 at: https://wandb.ai/seokjin/FSR-prediction/runs/d8ffcb88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=354917)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015002-d8ffcb88/logs\n",
      "2023-07-19 01:52:23,377\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.548 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:52:23,381\tWARNING util.py:315 -- The `process_trial_result` operation took 1.552 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:52:23,382\tWARNING util.py:315 -- Processing trial results took 1.554 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:52:23,393\tWARNING util.py:315 -- The `process_trial_result` operation took 1.565 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_75ff7e79_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-51-51/wandb/run-20230719_015224-75ff7e79\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: Syncing run FSR_Trainable_75ff7e79\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/75ff7e79\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:                      mae 214.39004\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:                     mape 3.5149441344981508e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:                     rmse 420.08565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:       time_since_restore 3.96965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:         time_this_iter_s 3.96965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:             time_total_s 3.96965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:                timestamp 1689699141\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb:  View run FSR_Trainable_75ff7e79 at: https://wandb.ai/seokjin/FSR-prediction/runs/75ff7e79\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355895)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015224-75ff7e79/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_819776d6_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-52-17/wandb/run-20230719_015241-819776d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb: Syncing run FSR_Trainable_819776d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/819776d6\n",
      "2023-07-19 01:52:44,571\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.501 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:52:44,574\tWARNING util.py:315 -- The `process_trial_result` operation took 1.505 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:52:44,576\tWARNING util.py:315 -- Processing trial results took 1.506 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:52:44,579\tWARNING util.py:315 -- The `process_trial_result` operation took 1.509 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:                      mae 208.37023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:                     mape 3.971747663979806e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:                     rmse 414.32127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:       time_since_restore 9.04713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:         time_this_iter_s 9.04713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:             time_total_s 9.04713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:                timestamp 1689699163\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb:  View run FSR_Trainable_819776d6 at: https://wandb.ai/seokjin/FSR-prediction/runs/819776d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356127)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015241-819776d6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_42d35536_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-52-34/wandb/run-20230719_015302-42d35536\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb: Syncing run FSR_Trainable_42d35536\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/42d35536\n",
      "2023-07-19 01:53:05,814\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.674 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:53:05,817\tWARNING util.py:315 -- The `process_trial_result` operation took 1.677 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:53:05,818\tWARNING util.py:315 -- Processing trial results took 1.678 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:53:05,819\tWARNING util.py:315 -- The `process_trial_result` operation took 1.679 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:                      mae 262.12362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:                     mape 849492213.60183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:                     rmse 451.02674\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:       time_since_restore 8.95997\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:         time_this_iter_s 8.95997\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:             time_total_s 8.95997\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:                timestamp 1689699184\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb:  View run FSR_Trainable_42d35536 at: https://wandb.ai/seokjin/FSR-prediction/runs/42d35536\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356360)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015302-42d35536/logs\n",
      "2023-07-19 01:53:19,183\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.807 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:53:19,185\tWARNING util.py:315 -- The `process_trial_result` operation took 1.810 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:53:19,186\tWARNING util.py:315 -- Processing trial results took 1.812 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:53:19,189\tWARNING util.py:315 -- The `process_trial_result` operation took 1.814 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_4514255c_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-52-55/wandb/run-20230719_015322-4514255c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: Syncing run FSR_Trainable_4514255c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4514255c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:                      mae 283.33098\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:                     mape 1085194936.47381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:                     rmse 467.97141\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:       time_since_restore 1.50775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:         time_this_iter_s 1.50775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:             time_total_s 1.50775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:                timestamp 1689699197\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb:  View run FSR_Trainable_4514255c at: https://wandb.ai/seokjin/FSR-prediction/runs/4514255c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356593)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015322-4514255c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 01:53:34,219\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.032 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:53:34,225\tWARNING util.py:315 -- The `process_trial_result` operation took 2.038 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:53:34,226\tWARNING util.py:315 -- Processing trial results took 2.040 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:53:34,229\tWARNING util.py:315 -- The `process_trial_result` operation took 2.043 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: / 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:                      mae 98.20161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:                     mape 182857024.69902\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:                     rmse 192.25663\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:       time_since_restore 127.56389\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:         time_this_iter_s 1.19598\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:             time_total_s 127.56389\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:                timestamp 1689699209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb:  View run FSR_Trainable_a80f6aca at: https://wandb.ai/seokjin/FSR-prediction/runs/a80f6aca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355204)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015117-a80f6aca/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_bea649a3_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-53-15/wandb/run-20230719_015336-bea649a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb: Syncing run FSR_Trainable_bea649a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/bea649a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015336-bea649a3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015336-bea649a3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015336-bea649a3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355416)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015336-bea649a3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb: iterations_since_restore 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:                      mae 267.30125\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:                     mape 1047906359.52212\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:                     rmse 451.80002\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:       time_since_restore 1.77739\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:         time_this_iter_s 1.77739\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:             time_total_s 1.77739\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:                timestamp 1689699212\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:       training_iteration 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb:  View run FSR_Trainable_bea649a3 at: https://wandb.ai/seokjin/FSR-prediction/runs/bea649a3\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015336-bea649a3/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015336-bea649a3/logs\n",
      "2023-07-19 01:53:43,713\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.046 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:53:43,717\tWARNING util.py:315 -- The `process_trial_result` operation took 2.050 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:53:43,718\tWARNING util.py:315 -- Processing trial results took 2.052 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:53:43,719\tWARNING util.py:315 -- The `process_trial_result` operation took 2.053 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=356823)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_5caf7559_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-53-30/wandb/run-20230719_015345-5caf7559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb: Syncing run FSR_Trainable_5caf7559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5caf7559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:                      mae 146.75365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:                     mape 238266310.0605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:                     rmse 295.6571\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:       time_since_restore 7.49799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:         time_this_iter_s 1.72403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:             time_total_s 7.49799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:                timestamp 1689699228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb:  View run FSR_Trainable_5caf7559 at: https://wandb.ai/seokjin/FSR-prediction/runs/5caf7559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357060)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015345-5caf7559/logs\n",
      "2023-07-19 01:53:53,499\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.855 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:53:53,501\tWARNING util.py:315 -- The `process_trial_result` operation took 1.859 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:53:53,512\tWARNING util.py:315 -- Processing trial results took 1.869 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:53:53,514\tWARNING util.py:315 -- The `process_trial_result` operation took 1.872 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_2f9ef68e_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-53-39/wandb/run-20230719_015354-2f9ef68e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: Syncing run FSR_Trainable_2f9ef68e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2f9ef68e\n",
      "2023-07-19 01:54:03,392\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.304 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:54:03,394\tWARNING util.py:315 -- The `process_trial_result` operation took 2.307 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:54:03,397\tWARNING util.py:315 -- Processing trial results took 2.310 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:54:03,400\tWARNING util.py:315 -- The `process_trial_result` operation took 2.313 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_89a6d069_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-53-48/wandb/run-20230719_015405-89a6d069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: Syncing run FSR_Trainable_89a6d069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/89a6d069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:                      mae 80.83872\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:                     mape 84384858.48468\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:                     rmse 166.86572\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:       time_since_restore 119.33367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:         time_this_iter_s 1.29288\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:             time_total_s 119.33367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:                timestamp 1689699246\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb:  View run FSR_Trainable_b32d4df7 at: https://wandb.ai/seokjin/FSR-prediction/runs/b32d4df7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=355656)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015158-b32d4df7/logs\n",
      "2023-07-19 01:54:14,416\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.465 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:54:14,419\tWARNING util.py:315 -- The `process_trial_result` operation took 2.469 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:54:14,422\tWARNING util.py:315 -- Processing trial results took 2.473 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:54:14,425\tWARNING util.py:315 -- The `process_trial_result` operation took 2.475 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_c4df8bd3_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-53-58/wandb/run-20230719_015415-c4df8bd3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: Syncing run FSR_Trainable_c4df8bd3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4df8bd3\n",
      "2023-07-19 01:54:23,695\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.968 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:54:23,699\tWARNING util.py:315 -- The `process_trial_result` operation took 1.973 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:54:23,701\tWARNING util.py:315 -- Processing trial results took 1.975 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:54:23,703\tWARNING util.py:315 -- The `process_trial_result` operation took 1.977 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_2f82b8ea_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-54-08/wandb/run-20230719_015426-2f82b8ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb: Syncing run FSR_Trainable_2f82b8ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2f82b8ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:                      mae 78.30049\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:                     mape 73048546.09273\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:                     rmse 169.8992\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:       time_since_restore 119.28833\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:         time_this_iter_s 1.16376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:             time_total_s 119.28833\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:                timestamp 1689699384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb:  View run FSR_Trainable_2f82b8ea at: https://wandb.ai/seokjin/FSR-prediction/runs/2f82b8ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015426-2f82b8ea/logs\n",
      "2023-07-19 01:56:38,883\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.135 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:56:38,886\tWARNING util.py:315 -- The `process_trial_result` operation took 2.139 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:56:38,888\tWARNING util.py:315 -- Processing trial results took 2.141 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:56:38,890\tWARNING util.py:315 -- The `process_trial_result` operation took 2.143 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_6e322367_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-54-19/wandb/run-20230719_015642-6e322367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: Syncing run FSR_Trainable_6e322367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/6e322367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:                      mae 80.84263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:                     mape 86177460.53825\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:                     rmse 170.25807\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:       time_since_restore 257.67795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:         time_this_iter_s 2.39615\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:             time_total_s 257.67795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:                timestamp 1689699492\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb:  View run FSR_Trainable_2f9ef68e at: https://wandb.ai/seokjin/FSR-prediction/runs/2f9ef68e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357282)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015354-2f9ef68e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 01:58:27,843\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.573 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:58:27,846\tWARNING util.py:315 -- The `process_trial_result` operation took 1.576 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:58:27,848\tWARNING util.py:315 -- Processing trial results took 1.579 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:58:27,852\tWARNING util.py:315 -- The `process_trial_result` operation took 1.583 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:                      mae 79.86012\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:                     mape 67957147.98471\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:                     rmse 173.17313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:       time_since_restore 256.6329\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:         time_this_iter_s 2.39333\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:             time_total_s 256.6329\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:                timestamp 1689699503\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb:  View run FSR_Trainable_89a6d069 at: https://wandb.ai/seokjin/FSR-prediction/runs/89a6d069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357506)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015405-89a6d069/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_2fbf2b26_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-56-35/wandb/run-20230719_015831-2fbf2b26\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: Syncing run FSR_Trainable_2fbf2b26\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2fbf2b26\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 01:58:41,432\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.720 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:58:41,435\tWARNING util.py:315 -- The `process_trial_result` operation took 1.724 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:58:41,438\tWARNING util.py:315 -- Processing trial results took 1.726 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:58:41,439\tWARNING util.py:315 -- The `process_trial_result` operation took 1.727 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:                      mae 79.93439\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:                     mape 76858178.85305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:                     rmse 168.33137\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:       time_since_restore 260.51836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:         time_this_iter_s 2.98765\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:             time_total_s 260.51836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:                timestamp 1689699519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb:  View run FSR_Trainable_c4df8bd3 at: https://wandb.ai/seokjin/FSR-prediction/runs/c4df8bd3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=357722)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015415-c4df8bd3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_e8b85816_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-58-24/wandb/run-20230719_015843-e8b85816\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: Syncing run FSR_Trainable_e8b85816\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e8b85816\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:                      mae 79.85575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:                     mape 82844036.37089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:                     rmse 170.41189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:       time_since_restore 122.82924\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:         time_this_iter_s 1.12352\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:             time_total_s 122.82924\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:                timestamp 1689699526\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb:  View run FSR_Trainable_6e322367 at: https://wandb.ai/seokjin/FSR-prediction/runs/6e322367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358237)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015642-6e322367/logs\n",
      "2023-07-19 01:58:54,557\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.040 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:58:54,561\tWARNING util.py:315 -- The `process_trial_result` operation took 2.046 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:58:54,563\tWARNING util.py:315 -- Processing trial results took 2.048 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:58:54,565\tWARNING util.py:315 -- The `process_trial_result` operation took 2.049 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_ce47611f_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-58-37/wandb/run-20230719_015857-ce47611f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: Syncing run FSR_Trainable_ce47611f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ce47611f\n",
      "2023-07-19 01:59:04,197\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.553 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:59:04,199\tWARNING util.py:315 -- The `process_trial_result` operation took 1.556 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:59:04,201\tWARNING util.py:315 -- Processing trial results took 1.558 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:59:04,201\tWARNING util.py:315 -- The `process_trial_result` operation took 1.559 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...204)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_3d287886_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-58-50/wandb/run-20230719_015907-3d287886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: Syncing run FSR_Trainable_3d287886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/3d287886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: / 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:                      mae 82.09227\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:                     mape 85005699.14268\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:                     rmse 170.66349\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:       time_since_restore 122.94277\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:         time_this_iter_s 1.19469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:             time_total_s 122.94277\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:                timestamp 1689699637\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb:  View run FSR_Trainable_2fbf2b26 at: https://wandb.ai/seokjin/FSR-prediction/runs/2fbf2b26\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358527)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015831-2fbf2b26/logs\n",
      "2023-07-19 02:00:52,086\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.896 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:00:52,088\tWARNING util.py:315 -- The `process_trial_result` operation took 1.899 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:00:52,092\tWARNING util.py:315 -- Processing trial results took 1.903 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:00:52,093\tWARNING util.py:315 -- The `process_trial_result` operation took 1.904 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_ef893e7c_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-59-01/wandb/run-20230719_020055-ef893e7c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: Syncing run FSR_Trainable_ef893e7c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ef893e7c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:                      mae 88.80051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:                     mape 87887764.24229\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:                     rmse 179.72753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:       time_since_restore 115.75049\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:         time_this_iter_s 1.07645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:             time_total_s 115.75049\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:                timestamp 1689699663\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb:  View run FSR_Trainable_3d287886 at: https://wandb.ai/seokjin/FSR-prediction/runs/3d287886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359204)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015907-3d287886/logs\n",
      "2023-07-19 02:01:17,988\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.042 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:01:17,993\tWARNING util.py:315 -- The `process_trial_result` operation took 2.047 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:01:17,994\tWARNING util.py:315 -- Processing trial results took 2.049 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:01:17,996\tWARNING util.py:315 -- The `process_trial_result` operation took 2.050 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_a1ffe1ec_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-00-48/wandb/run-20230719_020121-a1ffe1ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: Syncing run FSR_Trainable_a1ffe1ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a1ffe1ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:                      mae 71.84498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:                     mape 61742286.87534\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:                     rmse 156.218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:       time_since_restore 189.3802\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:         time_this_iter_s 2.03257\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:             time_total_s 189.3802\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:                timestamp 1689699717\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb:  View run FSR_Trainable_e8b85816 at: https://wandb.ai/seokjin/FSR-prediction/runs/e8b85816\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358747)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015843-e8b85816/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:02:14,070\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.462 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:02:14,073\tWARNING util.py:315 -- The `process_trial_result` operation took 2.466 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:02:14,074\tWARNING util.py:315 -- Processing trial results took 2.467 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:02:14,075\tWARNING util.py:315 -- The `process_trial_result` operation took 2.468 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:                      mae 75.31665\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:                     mape 77971774.10152\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:                     rmse 158.40129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:       time_since_restore 191.23401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:         time_this_iter_s 1.94717\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:             time_total_s 191.23401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:                timestamp 1689699730\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb:  View run FSR_Trainable_ce47611f at: https://wandb.ai/seokjin/FSR-prediction/runs/ce47611f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=358986)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_015857-ce47611f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_f21b1363_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-01-14/wandb/run-20230719_020218-f21b1363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: Syncing run FSR_Trainable_f21b1363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f21b1363\n",
      "2023-07-19 02:02:30,253\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.035 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:02:30,255\tWARNING util.py:315 -- The `process_trial_result` operation took 2.037 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:02:30,257\tWARNING util.py:315 -- Processing trial results took 2.039 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:02:30,258\tWARNING util.py:315 -- The `process_trial_result` operation took 2.041 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_7d5ead86_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-02-09/wandb/run-20230719_020232-7d5ead86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: Syncing run FSR_Trainable_7d5ead86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7d5ead86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:                      mae 89.60395\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:                     mape 102428460.28676\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:                     rmse 179.24612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:       time_since_restore 125.54814\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:         time_this_iter_s 1.08898\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:             time_total_s 125.54814\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:                timestamp 1689699784\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb:  View run FSR_Trainable_ef893e7c at: https://wandb.ai/seokjin/FSR-prediction/runs/ef893e7c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359507)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_020055-ef893e7c/logs\n",
      "2023-07-19 02:03:19,964\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.909 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:03:19,967\tWARNING util.py:315 -- The `process_trial_result` operation took 1.913 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:03:19,971\tWARNING util.py:315 -- Processing trial results took 1.916 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:03:19,972\tWARNING util.py:315 -- The `process_trial_result` operation took 1.917 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...497)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_f84633a9_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-02-24/wandb/run-20230719_020322-f84633a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb: Syncing run FSR_Trainable_f84633a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f84633a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:                      mae 81.61747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:                     mape 104727261.9846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:                     rmse 169.09484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:       time_since_restore 129.13023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:         time_this_iter_s 1.2557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:             time_total_s 129.13023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:                timestamp 1689699814\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb:  View run FSR_Trainable_a1ffe1ec at: https://wandb.ai/seokjin/FSR-prediction/runs/a1ffe1ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=359747)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_020121-a1ffe1ec/logs\n",
      "2023-07-19 02:03:49,697\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.922 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:03:49,699\tWARNING util.py:315 -- The `process_trial_result` operation took 1.924 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:03:49,699\tWARNING util.py:315 -- Processing trial results took 1.925 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:03:49,700\tWARNING util.py:315 -- The `process_trial_result` operation took 1.926 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_e923645d_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-03-15/wandb/run-20230719_020352-e923645d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: Syncing run FSR_Trainable_e923645d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e923645d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:                      mae 81.35855\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:                     mape 87857308.239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:                     rmse 166.91521\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:       time_since_restore 127.21453\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:         time_this_iter_s 1.27994\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:             time_total_s 127.21453\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:                timestamp 1689699865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb:  View run FSR_Trainable_f21b1363 at: https://wandb.ai/seokjin/FSR-prediction/runs/f21b1363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360007)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_020218-f21b1363/logs\n",
      "2023-07-19 02:04:40,797\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.099 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:04:40,800\tWARNING util.py:315 -- The `process_trial_result` operation took 2.102 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:04:40,800\tWARNING util.py:315 -- Processing trial results took 2.103 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:04:40,801\tWARNING util.py:315 -- The `process_trial_result` operation took 2.104 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_44cd64c6_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-03-45/wandb/run-20230719_020444-44cd64c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: Syncing run FSR_Trainable_44cd64c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/44cd64c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:                      mae 73.59558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:                     mape 64758382.41251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:                     rmse 155.93922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:       time_since_restore 193.50256\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:         time_this_iter_s 1.86021\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:             time_total_s 193.50256\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:                timestamp 1689699947\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb:  View run FSR_Trainable_7d5ead86 at: https://wandb.ai/seokjin/FSR-prediction/runs/7d5ead86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360239)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_020232-7d5ead86/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:                      mae 91.28788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:                     mape 75456610.75455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:                     rmse 198.00825\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:       time_since_restore 123.05571\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:         time_this_iter_s 2.00626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:             time_total_s 123.05571\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:                timestamp 1689699953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb:  View run FSR_Trainable_e923645d at: https://wandb.ai/seokjin/FSR-prediction/runs/e923645d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360734)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_020352-e923645d/logs\n",
      "2023-07-19 02:06:01,733\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.080 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:06:01,735\tWARNING util.py:315 -- The `process_trial_result` operation took 2.083 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:06:01,739\tWARNING util.py:315 -- Processing trial results took 2.087 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:06:01,741\tWARNING util.py:315 -- The `process_trial_result` operation took 2.089 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_538374ed_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-04-36/wandb/run-20230719_020604-538374ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb: Syncing run FSR_Trainable_538374ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/538374ed\n",
      "2023-07-19 02:06:12,240\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.858 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:06:12,245\tWARNING util.py:315 -- The `process_trial_result` operation took 1.863 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:06:12,246\tWARNING util.py:315 -- Processing trial results took 1.864 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:06:12,249\tWARNING util.py:315 -- The `process_trial_result` operation took 1.868 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_8b20cd80_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-05-57/wandb/run-20230719_020615-8b20cd80\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: Syncing run FSR_Trainable_8b20cd80\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8b20cd80\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:                      mae 196.48622\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:                     mape 3.3156041650020504e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:                     rmse 396.98378\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:       time_since_restore 1.82567\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:         time_this_iter_s 1.82567\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:             time_total_s 1.82567\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:                timestamp 1689699970\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb:  View run FSR_Trainable_8b20cd80 at: https://wandb.ai/seokjin/FSR-prediction/runs/8b20cd80\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361484)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_020615-8b20cd80/logs\n",
      "2023-07-19 02:06:27,676\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.999 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:06:27,679\tWARNING util.py:315 -- The `process_trial_result` operation took 2.002 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:06:27,681\tWARNING util.py:315 -- Processing trial results took 2.005 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:06:27,683\tWARNING util.py:315 -- The `process_trial_result` operation took 2.006 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_e0056a8e_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-06-08/wandb/run-20230719_020631-e0056a8e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: Syncing run FSR_Trainable_e0056a8e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e0056a8e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:                      mae 185.31747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:                     mape 2.808082111575197e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:                     rmse 382.26222\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:       time_since_restore 1.84628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:         time_this_iter_s 1.84628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:             time_total_s 1.84628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:                timestamp 1689699985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb:  View run FSR_Trainable_e0056a8e at: https://wandb.ai/seokjin/FSR-prediction/runs/e0056a8e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361713)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_020631-e0056a8e/logs\n",
      "2023-07-19 02:06:44,497\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.850 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:06:44,499\tWARNING util.py:315 -- The `process_trial_result` operation took 1.852 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:06:44,501\tWARNING util.py:315 -- Processing trial results took 1.854 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:06:44,502\tWARNING util.py:315 -- The `process_trial_result` operation took 1.855 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_e39d3cf9_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-06-23/wandb/run-20230719_020647-e39d3cf9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb: Syncing run FSR_Trainable_e39d3cf9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e39d3cf9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:                      mae 73.69774\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:                     mape 69846726.17993\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:                     rmse 158.30986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:       time_since_restore 198.60167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:         time_this_iter_s 2.49095\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:             time_total_s 198.60167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:                timestamp 1689700007\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb:  View run FSR_Trainable_f84633a9 at: https://wandb.ai/seokjin/FSR-prediction/runs/f84633a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360497)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_020322-f84633a9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:                      mae 76.4072\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:                     mape 80425457.90742\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:                     rmse 161.33111\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:       time_since_restore 127.58547\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:         time_this_iter_s 1.07207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:             time_total_s 127.58547\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:                timestamp 1689700014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb:  View run FSR_Trainable_44cd64c6 at: https://wandb.ai/seokjin/FSR-prediction/runs/44cd64c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=360987)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_020444-44cd64c6/logs\n",
      "2023-07-19 02:07:02,296\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.755 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:07:02,299\tWARNING util.py:315 -- The `process_trial_result` operation took 1.758 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:07:02,300\tWARNING util.py:315 -- Processing trial results took 1.760 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:07:02,303\tWARNING util.py:315 -- The `process_trial_result` operation took 1.763 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_6bd0b62f_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-06-40/wandb/run-20230719_020705-6bd0b62f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: Syncing run FSR_Trainable_6bd0b62f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/6bd0b62f\n",
      "2023-07-19 02:07:15,178\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.834 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:07:15,180\tWARNING util.py:315 -- The `process_trial_result` operation took 1.838 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:07:15,184\tWARNING util.py:315 -- Processing trial results took 1.842 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:07:15,186\tWARNING util.py:315 -- The `process_trial_result` operation took 1.843 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_68679ffc_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-06-58/wandb/run-20230719_020718-68679ffc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: Syncing run FSR_Trainable_68679ffc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/68679ffc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:                      mae 76.16294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:                     mape 81545683.80705\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:                     rmse 158.66199\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:       time_since_restore 141.38111\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:         time_this_iter_s 1.30661\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:             time_total_s 141.38111\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:                timestamp 1689700112\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb:  View run FSR_Trainable_538374ed at: https://wandb.ai/seokjin/FSR-prediction/runs/538374ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361267)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_020604-538374ed/logs\n",
      "2023-07-19 02:08:47,232\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.925 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:08:47,235\tWARNING util.py:315 -- The `process_trial_result` operation took 1.929 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:08:47,237\tWARNING util.py:315 -- Processing trial results took 1.931 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:08:47,239\tWARNING util.py:315 -- The `process_trial_result` operation took 1.933 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_a78b778a_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-07-11/wandb/run-20230719_020850-a78b778a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: Syncing run FSR_Trainable_a78b778a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a78b778a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:                      mae 79.02477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:                     mape 78389571.52862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:                     rmse 162.93734\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:       time_since_restore 121.45343\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:         time_this_iter_s 1.08682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:             time_total_s 121.45343\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:                timestamp 1689700148\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb:  View run FSR_Trainable_6bd0b62f at: https://wandb.ai/seokjin/FSR-prediction/runs/6bd0b62f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362180)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_020705-6bd0b62f/logs\n",
      "2023-07-19 02:09:23,112\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.766 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:09:23,114\tWARNING util.py:315 -- The `process_trial_result` operation took 1.770 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:09:23,115\tWARNING util.py:315 -- Processing trial results took 1.770 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:09:23,116\tWARNING util.py:315 -- The `process_trial_result` operation took 1.771 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_5beffbcc_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-08-43/wandb/run-20230719_020926-5beffbcc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: Syncing run FSR_Trainable_5beffbcc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5beffbcc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:                      mae 79.00875\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:                     mape 79891978.76446\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:                     rmse 171.25486\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:       time_since_restore 123.91985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:         time_this_iter_s 1.42865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:             time_total_s 123.91985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:                timestamp 1689700164\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb:  View run FSR_Trainable_68679ffc at: https://wandb.ai/seokjin/FSR-prediction/runs/68679ffc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362398)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_020718-68679ffc/logs\n",
      "2023-07-19 02:09:39,928\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.970 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:09:39,929\tWARNING util.py:315 -- The `process_trial_result` operation took 1.973 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:09:39,932\tWARNING util.py:315 -- Processing trial results took 1.975 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:09:39,933\tWARNING util.py:315 -- The `process_trial_result` operation took 1.977 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_ff689005_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-09-19/wandb/run-20230719_020942-ff689005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb: Syncing run FSR_Trainable_ff689005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ff689005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-07-19 02:10:18,529\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.966 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:10:18,531\tWARNING util.py:315 -- The `process_trial_result` operation took 1.969 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:10:18,534\tWARNING util.py:315 -- Processing trial results took 1.972 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:10:18,536\tWARNING util.py:315 -- The `process_trial_result` operation took 1.974 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:                      mae 73.2193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:                     mape 62965012.33035\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:                     rmse 159.02302\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:       time_since_restore 192.82828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:         time_this_iter_s 1.87564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:             time_total_s 192.82828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:                timestamp 1689700203\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb:  View run FSR_Trainable_e39d3cf9 at: https://wandb.ai/seokjin/FSR-prediction/runs/e39d3cf9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=361942)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_020647-e39d3cf9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_295bd539_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-09-35/wandb/run-20230719_021021-295bd539\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb: Syncing run FSR_Trainable_295bd539\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/295bd539\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:                      mae 87.02411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:                     mape 81051813.6968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:                     rmse 184.94009\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:       time_since_restore 87.46153\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:         time_this_iter_s 1.22644\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:             time_total_s 87.46153\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:                timestamp 1689700253\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb:  View run FSR_Trainable_5beffbcc at: https://wandb.ai/seokjin/FSR-prediction/runs/5beffbcc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362930)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_020926-5beffbcc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb:  View run FSR_Trainable_a78b778a at: https://wandb.ai/seokjin/FSR-prediction/runs/a78b778a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=362686)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_020850-a78b778a/logs\n",
      "2023-07-19 02:11:07,899\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.156 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:11:07,902\tWARNING util.py:315 -- The `process_trial_result` operation took 2.160 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:11:07,904\tWARNING util.py:315 -- Processing trial results took 2.162 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:11:07,906\tWARNING util.py:315 -- The `process_trial_result` operation took 2.164 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_e9a462e4_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-10-14/wandb/run-20230719_021111-e9a462e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: Syncing run FSR_Trainable_e9a462e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e9a462e4\n",
      "2023-07-19 02:11:18,755\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.693 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:11:18,757\tWARNING util.py:315 -- The `process_trial_result` operation took 1.696 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:11:18,759\tWARNING util.py:315 -- Processing trial results took 1.697 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:11:18,759\tWARNING util.py:315 -- The `process_trial_result` operation took 1.698 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...889)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_41021d62_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-11-04/wandb/run-20230719_021122-41021d62\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Syncing run FSR_Trainable_41021d62\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/41021d62\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:                      mae 81.53657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:                     mape 86080790.47086\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:                     rmse 172.49123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:       time_since_restore 136.09827\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:         time_this_iter_s 1.39727\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:             time_total_s 136.09827\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:                timestamp 1689700321\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb:  View run FSR_Trainable_ff689005 at: https://wandb.ai/seokjin/FSR-prediction/runs/ff689005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363159)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_020942-ff689005/logs\n",
      "2023-07-19 02:12:16,520\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.970 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:12:16,523\tWARNING util.py:315 -- The `process_trial_result` operation took 1.974 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:12:16,526\tWARNING util.py:315 -- Processing trial results took 1.977 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:12:16,527\tWARNING util.py:315 -- The `process_trial_result` operation took 1.978 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_edd691bd_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-11-15/wandb/run-20230719_021219-edd691bd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: Syncing run FSR_Trainable_edd691bd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/edd691bd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:                      mae 77.20464\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:                     mape 80889210.09292\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:                     rmse 161.61502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:       time_since_restore 134.80463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:         time_this_iter_s 1.30817\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:             time_total_s 134.80463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:                timestamp 1689700360\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb:  View run FSR_Trainable_295bd539 at: https://wandb.ai/seokjin/FSR-prediction/runs/295bd539\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363398)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021021-295bd539/logs\n",
      "2023-07-19 02:12:55,478\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.852 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:12:55,480\tWARNING util.py:315 -- The `process_trial_result` operation took 1.855 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:12:55,483\tWARNING util.py:315 -- Processing trial results took 1.858 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:12:55,484\tWARNING util.py:315 -- The `process_trial_result` operation took 1.859 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_dc2ac365_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-12-12/wandb/run-20230719_021258-dc2ac365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: Syncing run FSR_Trainable_dc2ac365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/dc2ac365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:                      mae 129.10362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:                     mape 160872075.68589\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:                     rmse 259.47916\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:       time_since_restore 3.46065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:         time_this_iter_s 1.53472\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:             time_total_s 3.46065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:                timestamp 1689700377\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb:  View run FSR_Trainable_dc2ac365 at: https://wandb.ai/seokjin/FSR-prediction/runs/dc2ac365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364402)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021258-dc2ac365/logs\n",
      "2023-07-19 02:13:11,490\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.528 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:13:11,492\tWARNING util.py:315 -- The `process_trial_result` operation took 1.531 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:13:11,493\tWARNING util.py:315 -- Processing trial results took 1.532 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:13:11,494\tWARNING util.py:315 -- The `process_trial_result` operation took 1.533 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_9ae8645b_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-12-51/wandb/run-20230719_021314-9ae8645b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: Syncing run FSR_Trainable_9ae8645b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9ae8645b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:                      mae 78.24137\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:                     mape 77939875.62113\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:                     rmse 164.12299\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:       time_since_restore 136.64188\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:         time_this_iter_s 1.39478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:             time_total_s 136.64188\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:                timestamp 1689700411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb:  View run FSR_Trainable_e9a462e4 at: https://wandb.ai/seokjin/FSR-prediction/runs/e9a462e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363676)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021111-e9a462e4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021122-41021d62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021122-41021d62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021122-41021d62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021122-41021d62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021122-41021d62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021122-41021d62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021122-41021d62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021122-41021d62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021122-41021d62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021122-41021d62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021122-41021d62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021122-41021d62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021122-41021d62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021122-41021d62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021122-41021d62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021122-41021d62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021122-41021d62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=363889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021122-41021d62/logs\n",
      "2023-07-19 02:13:45,817\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.891 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:13:45,820\tWARNING util.py:315 -- The `process_trial_result` operation took 1.894 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:13:45,823\tWARNING util.py:315 -- Processing trial results took 1.897 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:13:45,825\tWARNING util.py:315 -- The `process_trial_result` operation took 1.900 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_e2d1e51f_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-13-08/wandb/run-20230719_021349-e2d1e51f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: Syncing run FSR_Trainable_e2d1e51f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e2d1e51f\n",
      "2023-07-19 02:13:56,164\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.531 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:13:56,167\tWARNING util.py:315 -- The `process_trial_result` operation took 1.534 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:13:56,168\tWARNING util.py:315 -- Processing trial results took 1.536 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:13:56,170\tWARNING util.py:315 -- The `process_trial_result` operation took 1.537 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_a57b2b7b_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-13-42/wandb/run-20230719_021359-a57b2b7b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb: Syncing run FSR_Trainable_a57b2b7b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a57b2b7b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:                      mae 81.33874\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:                     mape 95600805.35498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:                     rmse 169.30624\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:       time_since_restore 133.2354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:         time_this_iter_s 1.35666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:             time_total_s 133.2354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:                timestamp 1689700477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb:  View run FSR_Trainable_edd691bd at: https://wandb.ai/seokjin/FSR-prediction/runs/edd691bd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364153)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021219-edd691bd/logs\n",
      "2023-07-19 02:14:52,378\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.688 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:14:52,379\tWARNING util.py:315 -- The `process_trial_result` operation took 1.690 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:14:52,381\tWARNING util.py:315 -- Processing trial results took 1.692 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:14:52,382\tWARNING util.py:315 -- The `process_trial_result` operation took 1.693 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_47019065_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-13-52/wandb/run-20230719_021455-47019065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb: Syncing run FSR_Trainable_47019065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/47019065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:                      mae 80.37586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:                     mape 88348635.04656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:                     rmse 167.32299\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:       time_since_restore 136.17748\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:         time_this_iter_s 1.39073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:             time_total_s 136.17748\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:                timestamp 1689700534\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb:  View run FSR_Trainable_9ae8645b at: https://wandb.ai/seokjin/FSR-prediction/runs/9ae8645b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364634)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021314-9ae8645b/logs\n",
      "2023-07-19 02:15:48,892\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.813 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:15:48,894\tWARNING util.py:315 -- The `process_trial_result` operation took 1.817 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:15:48,895\tWARNING util.py:315 -- Processing trial results took 1.818 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:15:48,896\tWARNING util.py:315 -- The `process_trial_result` operation took 1.819 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_bfb7ebd0_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-14-48/wandb/run-20230719_021552-bfb7ebd0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: Syncing run FSR_Trainable_bfb7ebd0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/bfb7ebd0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:                      mae 117.61119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:                     mape 107046711.50804\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:                     rmse 235.43213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:       time_since_restore 5.19567\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:         time_this_iter_s 1.0504\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:             time_total_s 5.19567\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:                timestamp 1689700552\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb:  View run FSR_Trainable_bfb7ebd0 at: https://wandb.ai/seokjin/FSR-prediction/runs/bfb7ebd0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365632)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021552-bfb7ebd0/logs\n",
      "2023-07-19 02:16:07,074\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.927 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:16:07,077\tWARNING util.py:315 -- The `process_trial_result` operation took 1.931 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:16:07,079\tWARNING util.py:315 -- Processing trial results took 1.932 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:16:07,081\tWARNING util.py:315 -- The `process_trial_result` operation took 1.934 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_bbcb84e4_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-15-45/wandb/run-20230719_021610-bbcb84e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: Syncing run FSR_Trainable_bbcb84e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/bbcb84e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:                      mae 82.29889\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:                     mape 91394546.32605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:                     rmse 173.04886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:       time_since_restore 135.63052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:         time_this_iter_s 1.73869\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:             time_total_s 135.63052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:                timestamp 1689700569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb:  View run FSR_Trainable_e2d1e51f at: https://wandb.ai/seokjin/FSR-prediction/runs/e2d1e51f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=364889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021349-e2d1e51f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021610-bbcb84e4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365864)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:                      mae 80.31905\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:                     mape 81801309.59045\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:                     rmse 166.30585\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:       time_since_restore 134.99057\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:         time_this_iter_s 1.12108\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:             time_total_s 134.99057\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:                timestamp 1689700577\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb:  View run FSR_Trainable_a57b2b7b at: https://wandb.ai/seokjin/FSR-prediction/runs/a57b2b7b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021359-a57b2b7b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365108)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_429b5b9e_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-16-03/wandb/run-20230719_021625-429b5b9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb: Syncing run FSR_Trainable_429b5b9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/429b5b9e\n",
      "2023-07-19 02:16:26,520\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.759 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:16:26,524\tWARNING util.py:315 -- The `process_trial_result` operation took 1.764 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:16:26,526\tWARNING util.py:315 -- Processing trial results took 1.765 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:16:26,528\tWARNING util.py:315 -- The `process_trial_result` operation took 1.768 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:                      mae 238.99774\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:                     mape 540953067.60991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:                     rmse 473.37989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:       time_since_restore 6.43231\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:         time_this_iter_s 6.43231\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:             time_total_s 6.43231\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:                timestamp 1689700584\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb:  View run FSR_Trainable_429b5b9e at: https://wandb.ai/seokjin/FSR-prediction/runs/429b5b9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021625-429b5b9e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366111)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 02:16:36,010\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.581 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:16:36,013\tWARNING util.py:315 -- The `process_trial_result` operation took 1.584 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:16:36,014\tWARNING util.py:315 -- Processing trial results took 1.585 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:16:36,015\tWARNING util.py:315 -- The `process_trial_result` operation took 1.586 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_1c7b460c_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-16-18/wandb/run-20230719_021635-1c7b460c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: Syncing run FSR_Trainable_1c7b460c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/1c7b460c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 02:16:40,678\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.612 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:16:40,680\tWARNING util.py:315 -- The `process_trial_result` operation took 1.615 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:16:40,681\tWARNING util.py:315 -- Processing trial results took 1.616 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:16:40,682\tWARNING util.py:315 -- The `process_trial_result` operation took 1.617 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:                      mae 189.84677\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:                     mape 2.204088615138479e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:                     rmse 359.01937\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:       time_since_restore 6.35202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:         time_this_iter_s 6.35202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:             time_total_s 6.35202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:                timestamp 1689700594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb:  View run FSR_Trainable_1c7b460c at: https://wandb.ai/seokjin/FSR-prediction/runs/1c7b460c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366334)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021635-1c7b460c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_44e54786_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-16-28/wandb/run-20230719_021643-44e54786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: Syncing run FSR_Trainable_44e54786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/44e54786\n",
      "2023-07-19 02:16:52,041\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.828 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:16:52,043\tWARNING util.py:315 -- The `process_trial_result` operation took 1.831 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:16:52,045\tWARNING util.py:315 -- Processing trial results took 1.832 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:16:52,047\tWARNING util.py:315 -- The `process_trial_result` operation took 1.835 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_adefbb03_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-16-37/wandb/run-20230719_021655-adefbb03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: Syncing run FSR_Trainable_adefbb03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/adefbb03\n",
      "2023-07-19 02:17:05,315\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.818 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:17:05,317\tWARNING util.py:315 -- The `process_trial_result` operation took 1.821 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:17:05,319\tWARNING util.py:315 -- Processing trial results took 1.823 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:17:05,321\tWARNING util.py:315 -- The `process_trial_result` operation took 1.825 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_f6ea6cea_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-16-48/wandb/run-20230719_021708-f6ea6cea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: Syncing run FSR_Trainable_f6ea6cea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f6ea6cea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:                      mae 80.19333\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:                     mape 74553975.15662\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:                     rmse 169.7161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:       time_since_restore 134.37284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:         time_this_iter_s 1.26147\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:             time_total_s 134.37284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:                timestamp 1689700636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb:  View run FSR_Trainable_47019065 at: https://wandb.ai/seokjin/FSR-prediction/runs/47019065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=365369)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021455-47019065/logs\n",
      "2023-07-19 02:17:31,319\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.922 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:17:31,321\tWARNING util.py:315 -- The `process_trial_result` operation took 1.925 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:17:31,325\tWARNING util.py:315 -- Processing trial results took 1.928 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:17:31,326\tWARNING util.py:315 -- The `process_trial_result` operation took 1.929 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_183fe6c6_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_02-17-01/wandb/run-20230719_021734-183fe6c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb: Syncing run FSR_Trainable_183fe6c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/183fe6c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:                      mae 79.00548\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:                     mape 77309120.76437\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:                     rmse 163.39273\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:       time_since_restore 123.95469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:         time_this_iter_s 1.10199\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:             time_total_s 123.95469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:                timestamp 1689700730\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb:  View run FSR_Trainable_44e54786 at: https://wandb.ai/seokjin/FSR-prediction/runs/44e54786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366557)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021643-44e54786/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:                      mae 80.02327\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:                     mape 73803218.62237\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:                     rmse 175.31956\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:       time_since_restore 122.85915\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:         time_this_iter_s 1.18751\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:             time_total_s 122.85915\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:                timestamp 1689700739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb:  View run FSR_Trainable_adefbb03 at: https://wandb.ai/seokjin/FSR-prediction/runs/adefbb03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366780)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021655-adefbb03/logs\n",
      "2023-07-19 02:19:04,807\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.738 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:19:04,810\tWARNING util.py:315 -- The `process_trial_result` operation took 1.741 s, which may be a performance bottleneck.\n",
      "2023-07-19 02:19:04,812\tWARNING util.py:315 -- Processing trial results took 1.744 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 02:19:04,813\tWARNING util.py:315 -- The `process_trial_result` operation took 1.745 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_01-25-28/FSR_Trainable_e7a576c4_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y_2023-07-19_02-17-27/wandb/run-20230719_021908-e7a576c4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb: Syncing run FSR_Trainable_e7a576c4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e7a576c4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:                      mae 79.96037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:                     mape 75481148.54235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:                     rmse 172.48794\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:       time_since_restore 122.52787\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:         time_this_iter_s 0.99059\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:             time_total_s 122.52787\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:                timestamp 1689700753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb:  View run FSR_Trainable_f6ea6cea at: https://wandb.ai/seokjin/FSR-prediction/runs/f6ea6cea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=366995)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021708-f6ea6cea/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:                      mae 77.37119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:                     mape 69954709.2222\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:                     rmse 159.16142\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:       time_since_restore 114.20564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:         time_this_iter_s 0.80411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:             time_total_s 114.20564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:                timestamp 1689700768\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb:  View run FSR_Trainable_183fe6c6 at: https://wandb.ai/seokjin/FSR-prediction/runs/183fe6c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367238)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021734-183fe6c6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:                      mae 81.64348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:                     mape 94978215.07803\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:                     rmse 175.11147\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:       time_since_restore 71.27167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:         time_this_iter_s 0.74123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:             time_total_s 71.27167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:                timestamp 1689700816\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb:  View run FSR_Trainable_e7a576c4 at: https://wandb.ai/seokjin/FSR-prediction/runs/e7a576c4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=367541)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_021908-e7a576c4/logs\n",
      "2023-07-19 02:20:19,943\tERROR tune.py:1107 -- Trials did not complete: [FSR_Trainable_cb2a0ab8, FSR_Trainable_84b2dd83]\n",
      "2023-07-19 02:20:19,944\tINFO tune.py:1111 -- Total run time: 3287.07 seconds (3283.50 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
