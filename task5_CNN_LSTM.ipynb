{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task5\n",
    "\n",
    "Index_X = FSR_for_force\n",
    "\n",
    "Index_y = force\n",
    "\n",
    "Data = Splited by Subject\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-07-19_10-04-48/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-07-19_10-04-48\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "175.383"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.CNN_LSTM'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': 'FSR_for_force',\n",
    "        'index_y': 'force',\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_subject'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-19 10:04:48,890] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 10:04:51,029\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-07-19 10:04:52,728\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-19 10:35:08</td></tr>\n",
       "<tr><td>Running for: </td><td>00:30:15.85        </td></tr>\n",
       "<tr><td>Memory:      </td><td>2.8/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=99<br>Bracket: Iter 64.000: -191.9623848978731 | Iter 32.000: -194.48198864438044 | Iter 16.000: -197.65726742848295 | Iter 8.000: -200.68990883743425 | Iter 4.000: -207.3780407858046 | Iter 2.000: -214.4321161892909 | Iter 1.000: -248.66393078808338<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_2a147f32</td><td style=\"text-align: right;\">           1</td><td>/home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_2a147f32_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_10-08-30/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>index_X      </th><th>index_y  </th><th>model             </th><th style=\"text-align: right;\">    model_args/cnn_hidde\n",
       "n_size</th><th style=\"text-align: right;\">  model_args/cnn_num_l\n",
       "ayer</th><th style=\"text-align: right;\">    model_args/lstm_hidd\n",
       "en_size</th><th style=\"text-align: right;\">  model_args/lstm_num_\n",
       "layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">       mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_db65304b</td><td>TERMINATED</td><td>172.26.215.93:537469</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000221625</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      241.031   </td><td style=\"text-align: right;\">257.788</td><td style=\"text-align: right;\">144.967 </td><td style=\"text-align: right;\">1.62247e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_186ff38b</td><td>TERMINATED</td><td>172.26.215.93:537540</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000127632</td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      178.887   </td><td style=\"text-align: right;\">326.009</td><td style=\"text-align: right;\">171.809 </td><td style=\"text-align: right;\">4.50653e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_c3e72fed</td><td>TERMINATED</td><td>172.26.215.93:537710</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        8.26611e-05</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">       35.4499  </td><td style=\"text-align: right;\">389.662</td><td style=\"text-align: right;\">247.088 </td><td style=\"text-align: right;\">5.08347e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_4056ce2c</td><td>TERMINATED</td><td>172.26.215.93:537882</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        6.08538e-05</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      650.223   </td><td style=\"text-align: right;\">212.218</td><td style=\"text-align: right;\">111.207 </td><td style=\"text-align: right;\">9.81429e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_af6b29bf</td><td>TERMINATED</td><td>172.26.215.93:538236</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000978686</td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       13.2321  </td><td style=\"text-align: right;\">406.025</td><td style=\"text-align: right;\">247.348 </td><td style=\"text-align: right;\">9.31386e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_618d1c52</td><td>TERMINATED</td><td>172.26.215.93:538467</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0492304  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">      114.02    </td><td style=\"text-align: right;\">321.436</td><td style=\"text-align: right;\">164.729 </td><td style=\"text-align: right;\">3.68025e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_2276ca4f</td><td>TERMINATED</td><td>172.26.215.93:538739</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000458711</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.84447 </td><td style=\"text-align: right;\">392.402</td><td style=\"text-align: right;\">226.789 </td><td style=\"text-align: right;\">3.81784e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_acf03874</td><td>TERMINATED</td><td>172.26.215.93:538917</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000214351</td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      290.75    </td><td style=\"text-align: right;\">216.842</td><td style=\"text-align: right;\">112.216 </td><td style=\"text-align: right;\">3.36078e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e37afb38</td><td>TERMINATED</td><td>172.26.215.93:539413</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        3.84266e-05</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.041   </td><td style=\"text-align: right;\">519.754</td><td style=\"text-align: right;\">396.188 </td><td style=\"text-align: right;\">7.88015e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_fd45abb5</td><td>TERMINATED</td><td>172.26.215.93:539609</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0173061  </td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        9.45854 </td><td style=\"text-align: right;\">458.032</td><td style=\"text-align: right;\">269.437 </td><td style=\"text-align: right;\">1.23542e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_5b747ab2</td><td>TERMINATED</td><td>172.26.215.93:539820</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.69643e-05</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.81722 </td><td style=\"text-align: right;\">493.651</td><td style=\"text-align: right;\">299.515 </td><td style=\"text-align: right;\">4.42442e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_f1970aaf</td><td>TERMINATED</td><td>172.26.215.93:540059</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0255391  </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.27955 </td><td style=\"text-align: right;\">401.767</td><td style=\"text-align: right;\">228.796 </td><td style=\"text-align: right;\">3.05767e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_ca6decca</td><td>TERMINATED</td><td>172.26.215.93:540285</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000271265</td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      141.162   </td><td style=\"text-align: right;\">236.575</td><td style=\"text-align: right;\">140.46  </td><td style=\"text-align: right;\">5.19805e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f30d8e8d</td><td>TERMINATED</td><td>172.26.215.93:540491</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00275174 </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       26.103   </td><td style=\"text-align: right;\">397.384</td><td style=\"text-align: right;\">192.104 </td><td style=\"text-align: right;\">3.53442e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_16cc40a3</td><td>TERMINATED</td><td>172.26.215.93:540774</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00431179 </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       20.5926  </td><td style=\"text-align: right;\">356.6  </td><td style=\"text-align: right;\">179.496 </td><td style=\"text-align: right;\">2.95206e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_dc44db54</td><td>TERMINATED</td><td>172.26.215.93:541008</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0573892  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      263.41    </td><td style=\"text-align: right;\">217.824</td><td style=\"text-align: right;\">109.749 </td><td style=\"text-align: right;\">2.69677e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ad33fb7e</td><td>TERMINATED</td><td>172.26.215.93:541271</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.06656    </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       98.9685  </td><td style=\"text-align: right;\">261.539</td><td style=\"text-align: right;\">136.73  </td><td style=\"text-align: right;\">3.64016e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_faff7bd9</td><td>TERMINATED</td><td>172.26.215.93:541519</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000381469</td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.18166 </td><td style=\"text-align: right;\">402.189</td><td style=\"text-align: right;\">250.158 </td><td style=\"text-align: right;\">1.57174e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_84b8b236</td><td>TERMINATED</td><td>172.26.215.93:541725</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000383309</td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        4.97973 </td><td style=\"text-align: right;\">370.986</td><td style=\"text-align: right;\">218.075 </td><td style=\"text-align: right;\">7.43414e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f28e1912</td><td>TERMINATED</td><td>172.26.215.93:541983</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        6.59026e-05</td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.16482 </td><td style=\"text-align: right;\">401.374</td><td style=\"text-align: right;\">239.821 </td><td style=\"text-align: right;\">1.3292e+08 </td></tr>\n",
       "<tr><td>FSR_Trainable_a1801155</td><td>TERMINATED</td><td>172.26.215.93:542183</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        5.90159e-05</td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.88369 </td><td style=\"text-align: right;\">385.398</td><td style=\"text-align: right;\">242.034 </td><td style=\"text-align: right;\">1.37212e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_72e0dfe8</td><td>TERMINATED</td><td>172.26.215.93:542392</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        1.26322e-05</td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.99163 </td><td style=\"text-align: right;\">390.832</td><td style=\"text-align: right;\">187.741 </td><td style=\"text-align: right;\">4.97031e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_12da65ac</td><td>TERMINATED</td><td>172.26.215.93:542608</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00116263 </td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.66373 </td><td style=\"text-align: right;\">411.259</td><td style=\"text-align: right;\">214.855 </td><td style=\"text-align: right;\">3.43225e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_ab9d6e57</td><td>TERMINATED</td><td>172.26.215.93:542837</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00162818 </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       68.338   </td><td style=\"text-align: right;\">323.046</td><td style=\"text-align: right;\">173.062 </td><td style=\"text-align: right;\">4.25166e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_95e32c40</td><td>TERMINATED</td><td>172.26.215.93:543060</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000176784</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.83332 </td><td style=\"text-align: right;\">895.691</td><td style=\"text-align: right;\">563.519 </td><td style=\"text-align: right;\">9.61292e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_1eeb4d5f</td><td>TERMINATED</td><td>172.26.215.93:543311</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000126794</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.87408 </td><td style=\"text-align: right;\">619.731</td><td style=\"text-align: right;\">474.35  </td><td style=\"text-align: right;\">1.14037e+18</td></tr>\n",
       "<tr><td>FSR_Trainable_bc84585a</td><td>TERMINATED</td><td>172.26.215.93:543536</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000190056</td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        4.12492 </td><td style=\"text-align: right;\">389.466</td><td style=\"text-align: right;\">247.442 </td><td style=\"text-align: right;\">1.6776e+08 </td></tr>\n",
       "<tr><td>FSR_Trainable_5db9a04e</td><td>TERMINATED</td><td>172.26.215.93:543767</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000257361</td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        3.38838 </td><td style=\"text-align: right;\">396.155</td><td style=\"text-align: right;\">250.248 </td><td style=\"text-align: right;\">1.71001e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_2284a968</td><td>TERMINATED</td><td>172.26.215.93:543964</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00899906 </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      100.875   </td><td style=\"text-align: right;\">212.433</td><td style=\"text-align: right;\">109.827 </td><td style=\"text-align: right;\">3.62733e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_7cafa4c8</td><td>TERMINATED</td><td>172.26.215.93:544199</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00759635 </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      102.747   </td><td style=\"text-align: right;\">197.945</td><td style=\"text-align: right;\">101.968 </td><td style=\"text-align: right;\">3.06934e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_28b2c667</td><td>TERMINATED</td><td>172.26.215.93:544430</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00770443 </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      102.32    </td><td style=\"text-align: right;\">191.678</td><td style=\"text-align: right;\"> 98.9347</td><td style=\"text-align: right;\">2.79369e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ebea30a5</td><td>TERMINATED</td><td>172.26.215.93:544654</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00771679 </td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        4.48729 </td><td style=\"text-align: right;\">354.34 </td><td style=\"text-align: right;\">186.48  </td><td style=\"text-align: right;\">3.83573e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_57d3b397</td><td>TERMINATED</td><td>172.26.215.93:544914</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00747424 </td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        4.07643 </td><td style=\"text-align: right;\">359.107</td><td style=\"text-align: right;\">179.878 </td><td style=\"text-align: right;\">1.78247e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_c5a39582</td><td>TERMINATED</td><td>172.26.215.93:545150</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000613333</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       53.553   </td><td style=\"text-align: right;\">344.572</td><td style=\"text-align: right;\">187.463 </td><td style=\"text-align: right;\">1.99828e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_ad0eb2b4</td><td>TERMINATED</td><td>172.26.215.93:545415</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.96066e-05</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        8.08494 </td><td style=\"text-align: right;\">547.256</td><td style=\"text-align: right;\">276.943 </td><td style=\"text-align: right;\">2.495e+17  </td></tr>\n",
       "<tr><td>FSR_Trainable_d4185b3f</td><td>TERMINATED</td><td>172.26.215.93:545593</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0704233  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      102.429   </td><td style=\"text-align: right;\">200.769</td><td style=\"text-align: right;\"> 98.3582</td><td style=\"text-align: right;\">3.13264e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_4e830b65</td><td>TERMINATED</td><td>172.26.215.93:545855</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000112325</td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.44092 </td><td style=\"text-align: right;\">376.937</td><td style=\"text-align: right;\">241.081 </td><td style=\"text-align: right;\">1.38277e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_9ac72315</td><td>TERMINATED</td><td>172.26.215.93:546074</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00937305 </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       82.8115  </td><td style=\"text-align: right;\">199.641</td><td style=\"text-align: right;\">104.696 </td><td style=\"text-align: right;\">3.41913e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_d2a94033</td><td>TERMINATED</td><td>172.26.215.93:546298</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0144974  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       60.377   </td><td style=\"text-align: right;\">190.908</td><td style=\"text-align: right;\"> 99.3856</td><td style=\"text-align: right;\">3.1541e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_0b303d44</td><td>TERMINATED</td><td>172.26.215.93:546511</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.012984   </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       58.9112  </td><td style=\"text-align: right;\">196.397</td><td style=\"text-align: right;\">103.087 </td><td style=\"text-align: right;\">2.91012e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_4209070a</td><td>TERMINATED</td><td>172.26.215.93:546837</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0151026  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       59.5102  </td><td style=\"text-align: right;\">186.057</td><td style=\"text-align: right;\"> 98.003 </td><td style=\"text-align: right;\">2.69218e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_24560de9</td><td>TERMINATED</td><td>172.26.215.93:547037</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0259796  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       60.498   </td><td style=\"text-align: right;\">195.382</td><td style=\"text-align: right;\"> 98.1257</td><td style=\"text-align: right;\">2.53076e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_8312c849</td><td>TERMINATED</td><td>172.26.215.93:547278</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0333472  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       62.4141  </td><td style=\"text-align: right;\">194.052</td><td style=\"text-align: right;\">100.666 </td><td style=\"text-align: right;\">2.49539e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_2813b9b2</td><td>TERMINATED</td><td>172.26.215.93:547490</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0324573  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       56.4834  </td><td style=\"text-align: right;\">190.517</td><td style=\"text-align: right;\"> 97.9328</td><td style=\"text-align: right;\">2.36606e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_4f59511a</td><td>TERMINATED</td><td>172.26.215.93:547801</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0238227  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       37.2457  </td><td style=\"text-align: right;\">200.876</td><td style=\"text-align: right;\">106.177 </td><td style=\"text-align: right;\">2.80984e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_d06a69bc</td><td>TERMINATED</td><td>172.26.215.93:547999</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.033511   </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       56.8371  </td><td style=\"text-align: right;\">192.678</td><td style=\"text-align: right;\"> 99.0702</td><td style=\"text-align: right;\">2.69448e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f97d6a66</td><td>TERMINATED</td><td>172.26.215.93:548229</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.033461   </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       55.1545  </td><td style=\"text-align: right;\">193.509</td><td style=\"text-align: right;\">100.463 </td><td style=\"text-align: right;\">2.74006e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_d645a338</td><td>TERMINATED</td><td>172.26.215.93:548465</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0346308  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       54.5252  </td><td style=\"text-align: right;\">196.948</td><td style=\"text-align: right;\">103.233 </td><td style=\"text-align: right;\">2.72388e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_c94a92ec</td><td>TERMINATED</td><td>172.26.215.93:548742</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0354084  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       55.2094  </td><td style=\"text-align: right;\">189.855</td><td style=\"text-align: right;\">100.385 </td><td style=\"text-align: right;\">3.65303e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_65daf275</td><td>TERMINATED</td><td>172.26.215.93:549004</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0376838  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.87505 </td><td style=\"text-align: right;\">231.137</td><td style=\"text-align: right;\">121.662 </td><td style=\"text-align: right;\">3.89028e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_73ceb852</td><td>TERMINATED</td><td>172.26.215.93:549209</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0148716  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       29.2962  </td><td style=\"text-align: right;\">202.898</td><td style=\"text-align: right;\">107.364 </td><td style=\"text-align: right;\">2.82788e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_72d6fcf5</td><td>TERMINATED</td><td>172.26.215.93:549452</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0835006  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       57.3248  </td><td style=\"text-align: right;\">199.708</td><td style=\"text-align: right;\"> 98.1076</td><td style=\"text-align: right;\">2.64786e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_1042a77b</td><td>TERMINATED</td><td>172.26.215.93:549666</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0149003  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        8.19376 </td><td style=\"text-align: right;\">211.094</td><td style=\"text-align: right;\">109.18  </td><td style=\"text-align: right;\">3.40848e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_cfb64bff</td><td>TERMINATED</td><td>172.26.215.93:549887</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0957157  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.03192 </td><td style=\"text-align: right;\">315.964</td><td style=\"text-align: right;\">173.777 </td><td style=\"text-align: right;\">7.74528e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ec866daa</td><td>TERMINATED</td><td>172.26.215.93:550130</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0999582  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       58.9622  </td><td style=\"text-align: right;\">190.4  </td><td style=\"text-align: right;\"> 97.5377</td><td style=\"text-align: right;\">3.66568e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_4d2b310e</td><td>TERMINATED</td><td>172.26.215.93:550360</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0214958  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        9.72917 </td><td style=\"text-align: right;\">202.672</td><td style=\"text-align: right;\">105.542 </td><td style=\"text-align: right;\">3.43212e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_fb905107</td><td>TERMINATED</td><td>172.26.215.93:550578</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0187785  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       59.5719  </td><td style=\"text-align: right;\">190.191</td><td style=\"text-align: right;\">101.308 </td><td style=\"text-align: right;\">2.58875e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_3e4279dd</td><td>TERMINATED</td><td>172.26.215.93:550800</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0500407  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       79.1864  </td><td style=\"text-align: right;\">199.524</td><td style=\"text-align: right;\"> 98.5974</td><td style=\"text-align: right;\">2.55824e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_b6bd78e9</td><td>TERMINATED</td><td>172.26.215.93:551035</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0530309  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        4.19851 </td><td style=\"text-align: right;\">214.825</td><td style=\"text-align: right;\">111.763 </td><td style=\"text-align: right;\">4.22588e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_168fed47</td><td>TERMINATED</td><td>172.26.215.93:551293</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0497455  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       26.8056  </td><td style=\"text-align: right;\">201.004</td><td style=\"text-align: right;\">101.774 </td><td style=\"text-align: right;\">3.27244e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_734d5dd2</td><td>TERMINATED</td><td>172.26.215.93:551544</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0475404  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        9.84836 </td><td style=\"text-align: right;\">201.48 </td><td style=\"text-align: right;\">105.11  </td><td style=\"text-align: right;\">3.36917e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ece22011</td><td>TERMINATED</td><td>172.26.215.93:551794</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.050616   </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       70.7411  </td><td style=\"text-align: right;\">194.365</td><td style=\"text-align: right;\"> 98.2275</td><td style=\"text-align: right;\">2.78598e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_44149a06</td><td>TERMINATED</td><td>172.26.215.93:551995</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0975489  </td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.37917 </td><td style=\"text-align: right;\">379.68 </td><td style=\"text-align: right;\">216.356 </td><td style=\"text-align: right;\">1.01196e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_d676d4d4</td><td>TERMINATED</td><td>172.26.215.93:552194</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.097734   </td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.32756 </td><td style=\"text-align: right;\">388.656</td><td style=\"text-align: right;\">236.028 </td><td style=\"text-align: right;\">1.15214e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_7cf331a6</td><td>TERMINATED</td><td>172.26.215.93:552413</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0204779  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.93141 </td><td style=\"text-align: right;\">215.875</td><td style=\"text-align: right;\">119.081 </td><td style=\"text-align: right;\">4.16079e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ffed4354</td><td>TERMINATED</td><td>172.26.215.93:552670</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0180365  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        5.02855 </td><td style=\"text-align: right;\">207.852</td><td style=\"text-align: right;\">112.106 </td><td style=\"text-align: right;\">4.53327e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f04af01c</td><td>TERMINATED</td><td>172.26.215.93:552901</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0122161  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.62756 </td><td style=\"text-align: right;\">214.134</td><td style=\"text-align: right;\">114.638 </td><td style=\"text-align: right;\">3.57957e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_a64dc20d</td><td>TERMINATED</td><td>172.26.215.93:553129</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0125724  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.12864 </td><td style=\"text-align: right;\">330.074</td><td style=\"text-align: right;\">184.173 </td><td style=\"text-align: right;\">5.63948e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_8e36da61</td><td>TERMINATED</td><td>172.26.215.93:553353</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0663424  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        4.32085 </td><td style=\"text-align: right;\">330.176</td><td style=\"text-align: right;\">169.635 </td><td style=\"text-align: right;\">4.68594e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_276c1961</td><td>TERMINATED</td><td>172.26.215.93:553554</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00450026 </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.34939 </td><td style=\"text-align: right;\">215.278</td><td style=\"text-align: right;\">119.328 </td><td style=\"text-align: right;\">4.25074e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_52f6ac72</td><td>TERMINATED</td><td>172.26.215.93:553793</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00493369 </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.02115 </td><td style=\"text-align: right;\">332.993</td><td style=\"text-align: right;\">184.62  </td><td style=\"text-align: right;\">4.92046e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_90e6ac13</td><td>TERMINATED</td><td>172.26.215.93:554030</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0267472  </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.91954 </td><td style=\"text-align: right;\">340.925</td><td style=\"text-align: right;\">181.505 </td><td style=\"text-align: right;\">2.42912e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_e1fcce58</td><td>TERMINATED</td><td>172.26.215.93:554122</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0235097  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       15.1207  </td><td style=\"text-align: right;\">201.214</td><td style=\"text-align: right;\">107.33  </td><td style=\"text-align: right;\">3.30044e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_1870f879</td><td>TERMINATED</td><td>172.26.215.93:554431</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0184759  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.48483 </td><td style=\"text-align: right;\">304.912</td><td style=\"text-align: right;\">151.77  </td><td style=\"text-align: right;\">4.078e+07  </td></tr>\n",
       "<tr><td>FSR_Trainable_999096be</td><td>TERMINATED</td><td>172.26.215.93:554524</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0172626  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.56969 </td><td style=\"text-align: right;\">231.838</td><td style=\"text-align: right;\">125.879 </td><td style=\"text-align: right;\">4.97958e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_b9aeaa9e</td><td>TERMINATED</td><td>172.26.215.93:554833</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0284351  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       53.6649  </td><td style=\"text-align: right;\">185.47 </td><td style=\"text-align: right;\"> 96.9951</td><td style=\"text-align: right;\">2.41156e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_d41a892c</td><td>TERMINATED</td><td>172.26.215.93:555076</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0367303  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.926104</td><td style=\"text-align: right;\">395.027</td><td style=\"text-align: right;\">240.539 </td><td style=\"text-align: right;\">1.73325e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_bdc4ad54</td><td>TERMINATED</td><td>172.26.215.93:555288</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0366987  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.0249  </td><td style=\"text-align: right;\">377.942</td><td style=\"text-align: right;\">228.634 </td><td style=\"text-align: right;\">1.50996e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_9e9a8c9b</td><td>TERMINATED</td><td>172.26.215.93:555514</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.029337   </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">      143.268   </td><td style=\"text-align: right;\">202.349</td><td style=\"text-align: right;\"> 99.7174</td><td style=\"text-align: right;\">2.78058e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e9a1363c</td><td>TERMINATED</td><td>172.26.215.93:555723</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0714562  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       11.3695  </td><td style=\"text-align: right;\">380.693</td><td style=\"text-align: right;\">209.042 </td><td style=\"text-align: right;\">9.64289e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_4f7583e2</td><td>TERMINATED</td><td>172.26.215.93:555937</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0659644  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">       16.5479  </td><td style=\"text-align: right;\">265.237</td><td style=\"text-align: right;\">145.382 </td><td style=\"text-align: right;\">7.08725e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_d6458403</td><td>TERMINATED</td><td>172.26.215.93:556165</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0301858  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">      144.439   </td><td style=\"text-align: right;\">201.88 </td><td style=\"text-align: right;\">100.444 </td><td style=\"text-align: right;\">2.76594e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_8b356e81</td><td>TERMINATED</td><td>172.26.215.93:556414</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0286594  </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       68.6516  </td><td style=\"text-align: right;\">184.291</td><td style=\"text-align: right;\"> 96.434 </td><td style=\"text-align: right;\">8.11255e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_ef63f3e9</td><td>TERMINATED</td><td>172.26.215.93:556636</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0264319  </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.49535 </td><td style=\"text-align: right;\">221.074</td><td style=\"text-align: right;\">123.254 </td><td style=\"text-align: right;\">1.82625e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_87828ea3</td><td>TERMINATED</td><td>172.26.215.93:556862</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0104637  </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       16.1139  </td><td style=\"text-align: right;\">204.517</td><td style=\"text-align: right;\">112.815 </td><td style=\"text-align: right;\">1.16823e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_04550823</td><td>TERMINATED</td><td>172.26.215.93:557137</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0271144  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        5.73553 </td><td style=\"text-align: right;\">205.804</td><td style=\"text-align: right;\">112.421 </td><td style=\"text-align: right;\">5.72221e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_966fc53a</td><td>TERMINATED</td><td>172.26.215.93:557367</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0271632  </td><td>sklearn.preproc_85d0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.78873 </td><td style=\"text-align: right;\">220.485</td><td style=\"text-align: right;\">119.905 </td><td style=\"text-align: right;\">4.97062e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_cd929861</td><td>TERMINATED</td><td>172.26.215.93:557565</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0159232  </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       67.7896  </td><td style=\"text-align: right;\">181.073</td><td style=\"text-align: right;\">101.759 </td><td style=\"text-align: right;\">1.05188e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_7b92b6a6</td><td>TERMINATED</td><td>172.26.215.93:557793</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0152991  </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       82.1378  </td><td style=\"text-align: right;\">180.626</td><td style=\"text-align: right;\"> 93.8838</td><td style=\"text-align: right;\">7.54612e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_133b3320</td><td>TERMINATED</td><td>172.26.215.93:558055</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0437922  </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       82.415   </td><td style=\"text-align: right;\">180.338</td><td style=\"text-align: right;\"> 95.3481</td><td style=\"text-align: right;\">8.83109e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_cb86380a</td><td>TERMINATED</td><td>172.26.215.93:558302</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0420545  </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       66.0912  </td><td style=\"text-align: right;\">175.383</td><td style=\"text-align: right;\"> 96.792 </td><td style=\"text-align: right;\">9.5271e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_87af99ee</td><td>TERMINATED</td><td>172.26.215.93:558550</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0414334  </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.13525 </td><td style=\"text-align: right;\">393.635</td><td style=\"text-align: right;\">254.59  </td><td style=\"text-align: right;\">3.89191e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_c3f9a74e</td><td>TERMINATED</td><td>172.26.215.93:558753</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0427438  </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.17752 </td><td style=\"text-align: right;\">561.136</td><td style=\"text-align: right;\">333.089 </td><td style=\"text-align: right;\">4.85131e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_77c94b7f</td><td>TERMINATED</td><td>172.26.215.93:558977</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0420738  </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       66.5405  </td><td style=\"text-align: right;\">178.304</td><td style=\"text-align: right;\"> 94.2382</td><td style=\"text-align: right;\">8.80787e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_4ea2105a</td><td>TERMINATED</td><td>172.26.215.93:559215</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.01627    </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       65.2368  </td><td style=\"text-align: right;\">181.616</td><td style=\"text-align: right;\"> 98.1698</td><td style=\"text-align: right;\">9.2489e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_96c49e29</td><td>TERMINATED</td><td>172.26.215.93:559437</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0213937  </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       67.0539  </td><td style=\"text-align: right;\">181.832</td><td style=\"text-align: right;\"> 97.818 </td><td style=\"text-align: right;\">9.27332e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_93b20787</td><td>TERMINATED</td><td>172.26.215.93:559674</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0195855  </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       65.1158  </td><td style=\"text-align: right;\">181.761</td><td style=\"text-align: right;\"> 95.834 </td><td style=\"text-align: right;\">8.53051e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_861548a0</td><td>TERMINATED</td><td>172.26.215.93:559984</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.021328   </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       50.4827  </td><td style=\"text-align: right;\">186.339</td><td style=\"text-align: right;\">102.707 </td><td style=\"text-align: right;\">1.09353e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_bca622bf</td><td>TERMINATED</td><td>172.26.215.93:560186</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0226378  </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       50.0987  </td><td style=\"text-align: right;\">183.533</td><td style=\"text-align: right;\">100.851 </td><td style=\"text-align: right;\">1.03194e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_2a147f32</td><td>ERROR     </td><td>172.26.215.93:539140</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8750</td><td>FSR_for_force</td><td>force    </td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.08405    </td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">           </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 10:04:52,782\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th>iterations_since_restore  </th><th>mae               </th><th>mape                  </th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th>rmse              </th><th>time_since_restore  </th><th>time_this_iter_s   </th><th>time_total_s      </th><th style=\"text-align: right;\">  timestamp</th><th>training_iteration  </th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_04550823</td><td>2023-07-19_10-30-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td>8                         </td><td>112.42096682566239</td><td>57222060.29654999     </td><td>172.26.215.93</td><td style=\"text-align: right;\">557137</td><td>205.80363268537778</td><td>5.735527276992798   </td><td>0.7180960178375244 </td><td>5.735527276992798 </td><td style=\"text-align: right;\"> 1689730209</td><td>8                   </td><td>04550823  </td></tr>\n",
       "<tr><td>FSR_Trainable_0b303d44</td><td>2023-07-19_10-20-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>103.08707356825435</td><td>29101165.046163544    </td><td>172.26.215.93</td><td style=\"text-align: right;\">546511</td><td>196.39673227109924</td><td>58.91115355491638   </td><td>0.5763323307037354 </td><td>58.91115355491638 </td><td style=\"text-align: right;\"> 1689729603</td><td>100                 </td><td>0b303d44  </td></tr>\n",
       "<tr><td>FSR_Trainable_1042a77b</td><td>2023-07-19_10-23-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td>8                         </td><td>109.17992069609224</td><td>34084808.14799351     </td><td>172.26.215.93</td><td style=\"text-align: right;\">549666</td><td>211.09390634586273</td><td>8.193757772445679   </td><td>0.905691385269165  </td><td>8.193757772445679 </td><td style=\"text-align: right;\"> 1689729819</td><td>8                   </td><td>1042a77b  </td></tr>\n",
       "<tr><td>FSR_Trainable_12da65ac</td><td>2023-07-19_10-14-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>214.85546754434617</td><td>3.432250433803145e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">542608</td><td>411.25935363882394</td><td>2.6637344360351562  </td><td>2.6637344360351562 </td><td>2.6637344360351562</td><td style=\"text-align: right;\"> 1689729290</td><td>1                   </td><td>12da65ac  </td></tr>\n",
       "<tr><td>FSR_Trainable_133b3320</td><td>2023-07-19_10-32-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>95.34809289261013 </td><td>8.831093019106966e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">558055</td><td>180.33788810137406</td><td>82.4149911403656    </td><td>0.753211259841919  </td><td>82.4149911403656  </td><td style=\"text-align: right;\"> 1689730361</td><td>100                 </td><td>133b3320  </td></tr>\n",
       "<tr><td>FSR_Trainable_168fed47</td><td>2023-07-19_10-25-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td>32                        </td><td>101.77359846724376</td><td>32724386.176956315    </td><td>172.26.215.93</td><td style=\"text-align: right;\">551293</td><td>201.00438020373676</td><td>26.805556297302246  </td><td>0.8039631843566895 </td><td>26.805556297302246</td><td style=\"text-align: right;\"> 1689729927</td><td>32                  </td><td>168fed47  </td></tr>\n",
       "<tr><td>FSR_Trainable_16cc40a3</td><td>2023-07-19_10-11-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td>8                         </td><td>179.49576098770262</td><td>29520617.40309351     </td><td>172.26.215.93</td><td style=\"text-align: right;\">540774</td><td>356.6000698410469 </td><td>20.59261178970337   </td><td>2.3784596920013428 </td><td>20.59261178970337 </td><td style=\"text-align: right;\"> 1689729063</td><td>8                   </td><td>16cc40a3  </td></tr>\n",
       "<tr><td>FSR_Trainable_186ff38b</td><td>2023-07-19_10-08-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>171.808662212664  </td><td>45065250.718636945    </td><td>172.26.215.93</td><td style=\"text-align: right;\">537540</td><td>326.0092273410227 </td><td>178.88699316978455  </td><td>1.6999669075012207 </td><td>178.88699316978455</td><td style=\"text-align: right;\"> 1689728889</td><td>100                 </td><td>186ff38b  </td></tr>\n",
       "<tr><td>FSR_Trainable_1870f879</td><td>2023-07-19_10-27-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>151.76969178341685</td><td>40779955.736919306    </td><td>172.26.215.93</td><td style=\"text-align: right;\">554431</td><td>304.9119314092871 </td><td>1.4848284721374512  </td><td>0.732330322265625  </td><td>1.4848284721374512</td><td style=\"text-align: right;\"> 1689730050</td><td>2                   </td><td>1870f879  </td></tr>\n",
       "<tr><td>FSR_Trainable_1eeb4d5f</td><td>2023-07-19_10-15-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>474.34990077719425</td><td>1.140369999208248e+18 </td><td>172.26.215.93</td><td style=\"text-align: right;\">543311</td><td>619.7314686624255 </td><td>2.874079704284668   </td><td>2.874079704284668  </td><td>2.874079704284668 </td><td style=\"text-align: right;\"> 1689729328</td><td>1                   </td><td>1eeb4d5f  </td></tr>\n",
       "<tr><td>FSR_Trainable_2276ca4f</td><td>2023-07-19_10-08-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>226.7893392343505 </td><td>3.817836072827627e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">538739</td><td>392.4015437407102 </td><td>2.844472646713257   </td><td>2.844472646713257  </td><td>2.844472646713257 </td><td style=\"text-align: right;\"> 1689728902</td><td>1                   </td><td>2276ca4f  </td></tr>\n",
       "<tr><td>FSR_Trainable_2284a968</td><td>2023-07-19_10-17-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>109.82652222356846</td><td>36273341.20517168     </td><td>172.26.215.93</td><td style=\"text-align: right;\">543964</td><td>212.43288467571196</td><td>100.87517786026001  </td><td>0.9518430233001709 </td><td>100.87517786026001</td><td style=\"text-align: right;\"> 1689729477</td><td>100                 </td><td>2284a968  </td></tr>\n",
       "<tr><td>FSR_Trainable_24560de9</td><td>2023-07-19_10-21-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>98.12571290205746 </td><td>25307568.857926488    </td><td>172.26.215.93</td><td style=\"text-align: right;\">547037</td><td>195.38150959762186</td><td>60.497994899749756  </td><td>0.6247842311859131 </td><td>60.497994899749756</td><td style=\"text-align: right;\"> 1689729684</td><td>100                 </td><td>24560de9  </td></tr>\n",
       "<tr><td>FSR_Trainable_276c1961</td><td>2023-07-19_10-26-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td>4                         </td><td>119.32834378190346</td><td>42507392.61570869     </td><td>172.26.215.93</td><td style=\"text-align: right;\">553554</td><td>215.2777726256745 </td><td>3.3493869304656982  </td><td>0.7479078769683838 </td><td>3.3493869304656982</td><td style=\"text-align: right;\"> 1689730018</td><td>4                   </td><td>276c1961  </td></tr>\n",
       "<tr><td>FSR_Trainable_2813b9b2</td><td>2023-07-19_10-21-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>97.93281443840392 </td><td>23660579.599757947    </td><td>172.26.215.93</td><td style=\"text-align: right;\">547490</td><td>190.51730499553483</td><td>56.48344302177429   </td><td>0.6287283897399902 </td><td>56.48344302177429 </td><td style=\"text-align: right;\"> 1689729705</td><td>100                 </td><td>2813b9b2  </td></tr>\n",
       "<tr><td>FSR_Trainable_28b2c667</td><td>2023-07-19_10-18-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>98.9347101544868  </td><td>27936878.798468046    </td><td>172.26.215.93</td><td style=\"text-align: right;\">544430</td><td>191.67759572568377</td><td>102.32003545761108  </td><td>0.8693184852600098 </td><td>102.32003545761108</td><td style=\"text-align: right;\"> 1689729499</td><td>100                 </td><td>28b2c667  </td></tr>\n",
       "<tr><td>FSR_Trainable_2a147f32</td><td>2023-07-19_10-08-40</td><td>      </td><td>DESKTOP-0P789CI</td><td>                          </td><td>                  </td><td>                      </td><td>172.26.215.93</td><td style=\"text-align: right;\">539140</td><td>                  </td><td>                    </td><td>                   </td><td>                  </td><td style=\"text-align: right;\"> 1689728920</td><td>                    </td><td>2a147f32  </td></tr>\n",
       "<tr><td>FSR_Trainable_3e4279dd</td><td>2023-07-19_10-26-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>98.5973651098234  </td><td>25582441.93948777     </td><td>172.26.215.93</td><td style=\"text-align: right;\">550800</td><td>199.52399101062082</td><td>79.18638634681702   </td><td>0.878878116607666  </td><td>79.18638634681702 </td><td style=\"text-align: right;\"> 1689729960</td><td>100                 </td><td>3e4279dd  </td></tr>\n",
       "<tr><td>FSR_Trainable_4056ce2c</td><td>2023-07-19_10-16-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>111.20679326571226</td><td>9.81428976663767e+16  </td><td>172.26.215.93</td><td style=\"text-align: right;\">537882</td><td>212.2175198674792 </td><td>650.2225041389465   </td><td>6.055992841720581  </td><td>650.2225041389465 </td><td style=\"text-align: right;\"> 1689729381</td><td>100                 </td><td>4056ce2c  </td></tr>\n",
       "<tr><td>FSR_Trainable_4209070a</td><td>2023-07-19_10-21-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>98.00296614959063 </td><td>26921818.279484067    </td><td>172.26.215.93</td><td style=\"text-align: right;\">546837</td><td>186.05713134570263</td><td>59.51016712188721   </td><td>0.6288337707519531 </td><td>59.51016712188721 </td><td style=\"text-align: right;\"> 1689729675</td><td>100                 </td><td>4209070a  </td></tr>\n",
       "<tr><td>FSR_Trainable_44149a06</td><td>2023-07-19_10-25-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>216.3562883569517 </td><td>1.011957113646146e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">551995</td><td>379.68046131784416</td><td>2.379171133041382   </td><td>2.379171133041382  </td><td>2.379171133041382 </td><td style=\"text-align: right;\"> 1689729947</td><td>1                   </td><td>44149a06  </td></tr>\n",
       "<tr><td>FSR_Trainable_4d2b310e</td><td>2023-07-19_10-24-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td>16                        </td><td>105.54245023132306</td><td>34321161.764620036    </td><td>172.26.215.93</td><td style=\"text-align: right;\">550360</td><td>202.6722093748445 </td><td>9.72917127609253    </td><td>0.5551877021789551 </td><td>9.72917127609253  </td><td style=\"text-align: right;\"> 1689729854</td><td>16                  </td><td>4d2b310e  </td></tr>\n",
       "<tr><td>FSR_Trainable_4e830b65</td><td>2023-07-19_10-18-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>241.08084541099447</td><td>138276987.78184292    </td><td>172.26.215.93</td><td style=\"text-align: right;\">545855</td><td>376.9369371190525 </td><td>2.4409186840057373  </td><td>1.043233871459961  </td><td>2.4409186840057373</td><td style=\"text-align: right;\"> 1689729512</td><td>2                   </td><td>4e830b65  </td></tr>\n",
       "<tr><td>FSR_Trainable_4ea2105a</td><td>2023-07-19_10-33-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>98.16975180643853 </td><td>9.248900701903326e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">559215</td><td>181.61613833756053</td><td>65.23678994178772   </td><td>0.6214380264282227 </td><td>65.23678994178772 </td><td style=\"text-align: right;\"> 1689730438</td><td>100                 </td><td>4ea2105a  </td></tr>\n",
       "<tr><td>FSR_Trainable_4f59511a</td><td>2023-07-19_10-22-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td>64                        </td><td>106.17672399833842</td><td>28098376.69714593     </td><td>172.26.215.93</td><td style=\"text-align: right;\">547801</td><td>200.87597364368042</td><td>37.24571394920349   </td><td>0.5018751621246338 </td><td>37.24571394920349 </td><td style=\"text-align: right;\"> 1689729732</td><td>64                  </td><td>4f59511a  </td></tr>\n",
       "<tr><td>FSR_Trainable_4f7583e2</td><td>2023-07-19_10-28-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>145.38191655260627</td><td>70872539.36319755     </td><td>172.26.215.93</td><td style=\"text-align: right;\">555937</td><td>265.2373255459471 </td><td>16.54788637161255   </td><td>7.827141761779785  </td><td>16.54788637161255 </td><td style=\"text-align: right;\"> 1689730130</td><td>2                   </td><td>4f7583e2  </td></tr>\n",
       "<tr><td>FSR_Trainable_52f6ac72</td><td>2023-07-19_10-27-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>184.62031950352463</td><td>49204586.04398694     </td><td>172.26.215.93</td><td style=\"text-align: right;\">553793</td><td>332.99256139405446</td><td>1.0211458206176758  </td><td>1.0211458206176758 </td><td>1.0211458206176758</td><td style=\"text-align: right;\"> 1689730024</td><td>1                   </td><td>52f6ac72  </td></tr>\n",
       "<tr><td>FSR_Trainable_57d3b397</td><td>2023-07-19_10-17-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>179.87808885088185</td><td>1.7824747884608254e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">544914</td><td>359.10742388615506</td><td>4.076427936553955   </td><td>1.9576654434204102 </td><td>4.076427936553955 </td><td style=\"text-align: right;\"> 1689729423</td><td>2                   </td><td>57d3b397  </td></tr>\n",
       "<tr><td>FSR_Trainable_5b747ab2</td><td>2023-07-19_10-09-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>299.51540051718723</td><td>4.424418246552514e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">539820</td><td>493.6512709470086 </td><td>1.817220687866211   </td><td>1.817220687866211  </td><td>1.817220687866211 </td><td style=\"text-align: right;\"> 1689728970</td><td>1                   </td><td>5b747ab2  </td></tr>\n",
       "<tr><td>FSR_Trainable_5db9a04e</td><td>2023-07-19_10-15-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>250.247925234967  </td><td>171001364.064234      </td><td>172.26.215.93</td><td style=\"text-align: right;\">543767</td><td>396.155074358035  </td><td>3.388380765914917   </td><td>3.388380765914917  </td><td>3.388380765914917 </td><td style=\"text-align: right;\"> 1689729359</td><td>1                   </td><td>5db9a04e  </td></tr>\n",
       "<tr><td>FSR_Trainable_618d1c52</td><td>2023-07-19_10-08-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td>16                        </td><td>164.72936265010043</td><td>36802533.06883169     </td><td>172.26.215.93</td><td style=\"text-align: right;\">538467</td><td>321.4360250887644 </td><td>114.01995182037354  </td><td>6.530397891998291  </td><td>114.01995182037354</td><td style=\"text-align: right;\"> 1689728900</td><td>16                  </td><td>618d1c52  </td></tr>\n",
       "<tr><td>FSR_Trainable_65daf275</td><td>2023-07-19_10-23-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td>4                         </td><td>121.66207220392047</td><td>38902766.88478038     </td><td>172.26.215.93</td><td style=\"text-align: right;\">549004</td><td>231.13661307023054</td><td>2.8750503063201904  </td><td>0.5978469848632812 </td><td>2.8750503063201904</td><td style=\"text-align: right;\"> 1689729780</td><td>4                   </td><td>65daf275  </td></tr>\n",
       "<tr><td>FSR_Trainable_72d6fcf5</td><td>2023-07-19_10-24-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td>64                        </td><td>98.10760134052056 </td><td>26478590.697066773    </td><td>172.26.215.93</td><td style=\"text-align: right;\">549452</td><td>199.70762039078699</td><td>57.32475757598877   </td><td>0.902634859085083  </td><td>57.32475757598877 </td><td style=\"text-align: right;\"> 1689729866</td><td>64                  </td><td>72d6fcf5  </td></tr>\n",
       "<tr><td>FSR_Trainable_72e0dfe8</td><td>2023-07-19_10-14-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>187.74148662972064</td><td>4970305812091363.0    </td><td>172.26.215.93</td><td style=\"text-align: right;\">542392</td><td>390.83150209044965</td><td>1.991626262664795   </td><td>1.991626262664795  </td><td>1.991626262664795 </td><td style=\"text-align: right;\"> 1689729280</td><td>1                   </td><td>72e0dfe8  </td></tr>\n",
       "<tr><td>FSR_Trainable_734d5dd2</td><td>2023-07-19_10-25-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td>16                        </td><td>105.11017005669132</td><td>33691731.10938858     </td><td>172.26.215.93</td><td style=\"text-align: right;\">551544</td><td>201.4799826475644 </td><td>9.848355054855347   </td><td>0.5786306858062744 </td><td>9.848355054855347 </td><td style=\"text-align: right;\"> 1689729927</td><td>16                  </td><td>734d5dd2  </td></tr>\n",
       "<tr><td>FSR_Trainable_73ceb852</td><td>2023-07-19_10-23-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td>32                        </td><td>107.36435226811302</td><td>28278755.930757362    </td><td>172.26.215.93</td><td style=\"text-align: right;\">549209</td><td>202.89833712468226</td><td>29.296181440353394  </td><td>0.7942047119140625 </td><td>29.296181440353394</td><td style=\"text-align: right;\"> 1689729821</td><td>32                  </td><td>73ceb852  </td></tr>\n",
       "<tr><td>FSR_Trainable_77c94b7f</td><td>2023-07-19_10-33-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>94.23823231447669 </td><td>8.807867112255685e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">558977</td><td>178.30428460068973</td><td>66.54045796394348   </td><td>0.6762053966522217 </td><td>66.54045796394348 </td><td style=\"text-align: right;\"> 1689730429</td><td>100                 </td><td>77c94b7f  </td></tr>\n",
       "<tr><td>FSR_Trainable_7b92b6a6</td><td>2023-07-19_10-32-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>93.88375297322374 </td><td>7.54612112647505e+16  </td><td>172.26.215.93</td><td style=\"text-align: right;\">557793</td><td>180.62564771762806</td><td>82.13783860206604   </td><td>1.0536270141601562 </td><td>82.13783860206604 </td><td style=\"text-align: right;\"> 1689730340</td><td>100                 </td><td>7b92b6a6  </td></tr>\n",
       "<tr><td>FSR_Trainable_7cafa4c8</td><td>2023-07-19_10-18-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>101.96752395741501</td><td>30693421.882550657    </td><td>172.26.215.93</td><td style=\"text-align: right;\">544199</td><td>197.94483496698604</td><td>102.74670624732971  </td><td>1.3551535606384277 </td><td>102.74670624732971</td><td style=\"text-align: right;\"> 1689729489</td><td>100                 </td><td>7cafa4c8  </td></tr>\n",
       "<tr><td>FSR_Trainable_7cf331a6</td><td>2023-07-19_10-26-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td>4                         </td><td>119.0814825237784 </td><td>41607935.10340511     </td><td>172.26.215.93</td><td style=\"text-align: right;\">552413</td><td>215.87524292349022</td><td>2.9314072132110596  </td><td>0.6472682952880859 </td><td>2.9314072132110596</td><td style=\"text-align: right;\"> 1689729969</td><td>4                   </td><td>7cf331a6  </td></tr>\n",
       "<tr><td>FSR_Trainable_8312c849</td><td>2023-07-19_10-21-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>100.66645396223848</td><td>24953935.037216704    </td><td>172.26.215.93</td><td style=\"text-align: right;\">547278</td><td>194.0523225692769 </td><td>62.414053201675415  </td><td>0.6011011600494385 </td><td>62.414053201675415</td><td style=\"text-align: right;\"> 1689729697</td><td>100                 </td><td>8312c849  </td></tr>\n",
       "<tr><td>FSR_Trainable_84b8b236</td><td>2023-07-19_10-14-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>218.07527804395238</td><td>74341412.56420691     </td><td>172.26.215.93</td><td style=\"text-align: right;\">541725</td><td>370.98636921434206</td><td>4.979733228683472   </td><td>2.269744396209717  </td><td>4.979733228683472 </td><td style=\"text-align: right;\"> 1689729245</td><td>2                   </td><td>84b8b236  </td></tr>\n",
       "<tr><td>FSR_Trainable_861548a0</td><td>2023-07-19_10-34-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>102.7065061913604 </td><td>1.0935293803517069e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">559984</td><td>186.33856139594002</td><td>50.48274874687195   </td><td>0.5282192230224609 </td><td>50.48274874687195 </td><td style=\"text-align: right;\"> 1689730497</td><td>100                 </td><td>861548a0  </td></tr>\n",
       "<tr><td>FSR_Trainable_87828ea3</td><td>2023-07-19_10-29-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td>16                        </td><td>112.81533369137203</td><td>1.1682250327067294e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">556862</td><td>204.5166391287411 </td><td>16.113897562026978  </td><td>0.9607279300689697 </td><td>16.113897562026978</td><td style=\"text-align: right;\"> 1689730189</td><td>16                  </td><td>87828ea3  </td></tr>\n",
       "<tr><td>FSR_Trainable_87af99ee</td><td>2023-07-19_10-32-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>254.58987019351986</td><td>3.8919061031778963e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">558550</td><td>393.635232813119  </td><td>1.1352512836456299  </td><td>1.1352512836456299 </td><td>1.1352512836456299</td><td style=\"text-align: right;\"> 1689730325</td><td>1                   </td><td>87af99ee  </td></tr>\n",
       "<tr><td>FSR_Trainable_8b356e81</td><td>2023-07-19_10-30-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>96.4339716665742  </td><td>8.11254599995656e+16  </td><td>172.26.215.93</td><td style=\"text-align: right;\">556414</td><td>184.29146761748638</td><td>68.65157556533813   </td><td>0.6349997520446777 </td><td>68.65157556533813 </td><td style=\"text-align: right;\"> 1689730221</td><td>100                 </td><td>8b356e81  </td></tr>\n",
       "<tr><td>FSR_Trainable_8e36da61</td><td>2023-07-19_10-26-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>169.63459712698884</td><td>46859399.75212125     </td><td>172.26.215.93</td><td style=\"text-align: right;\">553353</td><td>330.1762527954045 </td><td>4.3208465576171875  </td><td>4.3208465576171875 </td><td>4.3208465576171875</td><td style=\"text-align: right;\"> 1689730008</td><td>1                   </td><td>8e36da61  </td></tr>\n",
       "<tr><td>FSR_Trainable_90e6ac13</td><td>2023-07-19_10-27-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>181.50451119668386</td><td>2.429115119599393e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">554030</td><td>340.92452188760075</td><td>1.9195423126220703  </td><td>1.9195423126220703 </td><td>1.9195423126220703</td><td style=\"text-align: right;\"> 1689730033</td><td>1                   </td><td>90e6ac13  </td></tr>\n",
       "<tr><td>FSR_Trainable_93b20787</td><td>2023-07-19_10-34-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>95.8339795953267  </td><td>8.530514445850264e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">559674</td><td>181.76118808556726</td><td>65.1157660484314    </td><td>0.5754952430725098 </td><td>65.1157660484314  </td><td style=\"text-align: right;\"> 1689730466</td><td>100                 </td><td>93b20787  </td></tr>\n",
       "<tr><td>FSR_Trainable_95e32c40</td><td>2023-07-19_10-15-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>563.5191654611307 </td><td>9.612920438883105e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">543060</td><td>895.6909564471789 </td><td>2.8333165645599365  </td><td>2.8333165645599365 </td><td>2.8333165645599365</td><td style=\"text-align: right;\"> 1689729311</td><td>1                   </td><td>95e32c40  </td></tr>\n",
       "<tr><td>FSR_Trainable_966fc53a</td><td>2023-07-19_10-30-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>119.90531236581937</td><td>49706188.43063173     </td><td>172.26.215.93</td><td style=\"text-align: right;\">557367</td><td>220.48522950997415</td><td>1.788729190826416   </td><td>0.8714885711669922 </td><td>1.788729190826416 </td><td style=\"text-align: right;\"> 1689730225</td><td>2                   </td><td>966fc53a  </td></tr>\n",
       "<tr><td>FSR_Trainable_96c49e29</td><td>2023-07-19_10-34-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>97.81800058841252 </td><td>9.273316517614691e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">559437</td><td>181.83237314904463</td><td>67.05391955375671   </td><td>0.5923833847045898 </td><td>67.05391955375671 </td><td style=\"text-align: right;\"> 1689730453</td><td>100                 </td><td>96c49e29  </td></tr>\n",
       "<tr><td>FSR_Trainable_999096be</td><td>2023-07-19_10-27-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>125.87870501442139</td><td>49795790.98976325     </td><td>172.26.215.93</td><td style=\"text-align: right;\">554524</td><td>231.83773297381927</td><td>1.5696923732757568  </td><td>0.7537379264831543 </td><td>1.5696923732757568</td><td style=\"text-align: right;\"> 1689730058</td><td>2                   </td><td>999096be  </td></tr>\n",
       "<tr><td>FSR_Trainable_9ac72315</td><td>2023-07-19_10-20-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>104.69611658603831</td><td>34191346.24210336     </td><td>172.26.215.93</td><td style=\"text-align: right;\">546074</td><td>199.6412470915377 </td><td>82.811514377594     </td><td>1.267223596572876  </td><td>82.811514377594   </td><td style=\"text-align: right;\"> 1689729607</td><td>100                 </td><td>9ac72315  </td></tr>\n",
       "<tr><td>FSR_Trainable_9e9a8c9b</td><td>2023-07-19_10-30-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td>64                        </td><td>99.71736219719452 </td><td>27805847.277370322    </td><td>172.26.215.93</td><td style=\"text-align: right;\">555514</td><td>202.3490890949725 </td><td>143.2679271697998   </td><td>2.664316415786743  </td><td>143.2679271697998 </td><td style=\"text-align: right;\"> 1689730253</td><td>64                  </td><td>9e9a8c9b  </td></tr>\n",
       "<tr><td>FSR_Trainable_a1801155</td><td>2023-07-19_10-14-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>242.03441586881894</td><td>137211871.8333112     </td><td>172.26.215.93</td><td style=\"text-align: right;\">542183</td><td>385.39832928808704</td><td>1.8836925029754639  </td><td>1.8836925029754639 </td><td>1.8836925029754639</td><td style=\"text-align: right;\"> 1689729270</td><td>1                   </td><td>a1801155  </td></tr>\n",
       "<tr><td>FSR_Trainable_a64dc20d</td><td>2023-07-19_10-26-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>184.17252530656816</td><td>56394783.94525849     </td><td>172.26.215.93</td><td style=\"text-align: right;\">553129</td><td>330.07443582048484</td><td>1.1286425590515137  </td><td>1.1286425590515137 </td><td>1.1286425590515137</td><td style=\"text-align: right;\"> 1689729996</td><td>1                   </td><td>a64dc20d  </td></tr>\n",
       "<tr><td>FSR_Trainable_ab9d6e57</td><td>2023-07-19_10-16-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td>16                        </td><td>173.06158319255744</td><td>42516613.19405546     </td><td>172.26.215.93</td><td style=\"text-align: right;\">542837</td><td>323.0460268328224 </td><td>68.33799242973328   </td><td>4.476243019104004  </td><td>68.33799242973328 </td><td style=\"text-align: right;\"> 1689729370</td><td>16                  </td><td>ab9d6e57  </td></tr>\n",
       "<tr><td>FSR_Trainable_acf03874</td><td>2023-07-19_10-13-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>112.21628172460002</td><td>33607818.623939805    </td><td>172.26.215.93</td><td style=\"text-align: right;\">538917</td><td>216.84214019097317</td><td>290.75022745132446  </td><td>3.1538033485412598 </td><td>290.75022745132446</td><td style=\"text-align: right;\"> 1689729210</td><td>100                 </td><td>acf03874  </td></tr>\n",
       "<tr><td>FSR_Trainable_ad0eb2b4</td><td>2023-07-19_10-18-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>276.9429603456491 </td><td>2.494997584556088e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">545415</td><td>547.2564203858958 </td><td>8.084943056106567   </td><td>8.084943056106567  </td><td>8.084943056106567 </td><td style=\"text-align: right;\"> 1689729496</td><td>1                   </td><td>ad0eb2b4  </td></tr>\n",
       "<tr><td>FSR_Trainable_ad33fb7e</td><td>2023-07-19_10-14-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td>32                        </td><td>136.73012534537042</td><td>36401615.974978395    </td><td>172.26.215.93</td><td style=\"text-align: right;\">541271</td><td>261.5388369768273 </td><td>98.96848464012146   </td><td>2.6987619400024414 </td><td>98.96848464012146 </td><td style=\"text-align: right;\"> 1689729256</td><td>32                  </td><td>ad33fb7e  </td></tr>\n",
       "<tr><td>FSR_Trainable_af6b29bf</td><td>2023-07-19_10-06-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>247.3483543047867 </td><td>9.313856347405637e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">538236</td><td>406.02483373768536</td><td>13.232107639312744  </td><td>13.232107639312744 </td><td>13.232107639312744</td><td style=\"text-align: right;\"> 1689728772</td><td>1                   </td><td>af6b29bf  </td></tr>\n",
       "<tr><td>FSR_Trainable_b6bd78e9</td><td>2023-07-19_10-24-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td>4                         </td><td>111.76307060453297</td><td>42258788.67063353     </td><td>172.26.215.93</td><td style=\"text-align: right;\">551035</td><td>214.8250412129358 </td><td>4.198513746261597   </td><td>0.7953090667724609 </td><td>4.198513746261597 </td><td style=\"text-align: right;\"> 1689729885</td><td>4                   </td><td>b6bd78e9  </td></tr>\n",
       "<tr><td>FSR_Trainable_b9aeaa9e</td><td>2023-07-19_10-28-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>96.99510315781848 </td><td>24115573.48424427     </td><td>172.26.215.93</td><td style=\"text-align: right;\">554833</td><td>185.46970920856884</td><td>53.664902687072754  </td><td>0.8663887977600098 </td><td>53.664902687072754</td><td style=\"text-align: right;\"> 1689730131</td><td>100                 </td><td>b9aeaa9e  </td></tr>\n",
       "<tr><td>FSR_Trainable_bc84585a</td><td>2023-07-19_10-15-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>247.44169751667354</td><td>167760340.72796872    </td><td>172.26.215.93</td><td style=\"text-align: right;\">543536</td><td>389.4663859948083 </td><td>4.124923944473267   </td><td>4.124923944473267  </td><td>4.124923944473267 </td><td style=\"text-align: right;\"> 1689729344</td><td>1                   </td><td>bc84585a  </td></tr>\n",
       "<tr><td>FSR_Trainable_bca622bf</td><td>2023-07-19_10-35-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>100.85105711251683</td><td>1.0319394583994859e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">560186</td><td>183.53258640865948</td><td>50.098684787750244  </td><td>0.3883657455444336 </td><td>50.098684787750244</td><td style=\"text-align: right;\"> 1689730508</td><td>100                 </td><td>bca622bf  </td></tr>\n",
       "<tr><td>FSR_Trainable_bdc4ad54</td><td>2023-07-19_10-28-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>228.6343728623086 </td><td>150996004.43553907    </td><td>172.26.215.93</td><td style=\"text-align: right;\">555288</td><td>377.9416369259737 </td><td>1.024902105331421   </td><td>1.024902105331421  </td><td>1.024902105331421 </td><td style=\"text-align: right;\"> 1689730083</td><td>1                   </td><td>bdc4ad54  </td></tr>\n",
       "<tr><td>FSR_Trainable_c3e72fed</td><td>2023-07-19_10-05-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td>4                         </td><td>247.08830097936854</td><td>5.0834703138216e+17   </td><td>172.26.215.93</td><td style=\"text-align: right;\">537710</td><td>389.66214896180594</td><td>35.44988512992859   </td><td>8.617376565933228  </td><td>35.44988512992859 </td><td style=\"text-align: right;\"> 1689728749</td><td>4                   </td><td>c3e72fed  </td></tr>\n",
       "<tr><td>FSR_Trainable_c3f9a74e</td><td>2023-07-19_10-32-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>333.08880077995536</td><td>4.8513081576527974e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">558753</td><td>561.1362575440088 </td><td>1.1775236129760742  </td><td>1.1775236129760742 </td><td>1.1775236129760742</td><td style=\"text-align: right;\"> 1689730340</td><td>1                   </td><td>c3f9a74e  </td></tr>\n",
       "<tr><td>FSR_Trainable_c5a39582</td><td>2023-07-19_10-18-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td>8                         </td><td>187.46293136242403</td><td>1.998279141509369e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">545150</td><td>344.5722187342439 </td><td>53.55302023887634   </td><td>6.669182538986206  </td><td>53.55302023887634 </td><td style=\"text-align: right;\"> 1689729489</td><td>8                   </td><td>c5a39582  </td></tr>\n",
       "<tr><td>FSR_Trainable_c94a92ec</td><td>2023-07-19_10-23-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>100.38528803750799</td><td>36530270.016729794    </td><td>172.26.215.93</td><td style=\"text-align: right;\">548742</td><td>189.85471814343276</td><td>55.20944356918335   </td><td>0.5358092784881592 </td><td>55.20944356918335 </td><td style=\"text-align: right;\"> 1689729807</td><td>100                 </td><td>c94a92ec  </td></tr>\n",
       "<tr><td>FSR_Trainable_ca6decca</td><td>2023-07-19_10-12-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>140.45988588041618</td><td>51980505.06212879     </td><td>172.26.215.93</td><td style=\"text-align: right;\">540285</td><td>236.5752358429129 </td><td>141.16171312332153  </td><td>1.3197689056396484 </td><td>141.16171312332153</td><td style=\"text-align: right;\"> 1689729140</td><td>100                 </td><td>ca6decca  </td></tr>\n",
       "<tr><td>FSR_Trainable_cb86380a</td><td>2023-07-19_10-32-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>96.79201911259646 </td><td>9.527100925184651e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">558302</td><td>175.3834886867086 </td><td>66.0912218093872    </td><td>0.8604233264923096 </td><td>66.0912218093872  </td><td style=\"text-align: right;\"> 1689730378</td><td>100                 </td><td>cb86380a  </td></tr>\n",
       "<tr><td>FSR_Trainable_cd929861</td><td>2023-07-19_10-31-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>101.75859231028508</td><td>1.0518768999351819e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">557565</td><td>181.07293272891   </td><td>67.78956413269043   </td><td>0.659794807434082  </td><td>67.78956413269043 </td><td style=\"text-align: right;\"> 1689730312</td><td>100                 </td><td>cd929861  </td></tr>\n",
       "<tr><td>FSR_Trainable_cfb64bff</td><td>2023-07-19_10-23-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>173.77701954684676</td><td>77452831.11715063     </td><td>172.26.215.93</td><td style=\"text-align: right;\">549887</td><td>315.9643940698454 </td><td>2.031919479370117   </td><td>0.9308228492736816 </td><td>2.031919479370117 </td><td style=\"text-align: right;\"> 1689729825</td><td>2                   </td><td>cfb64bff  </td></tr>\n",
       "<tr><td>FSR_Trainable_d06a69bc</td><td>2023-07-19_10-22-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>99.07015863924806 </td><td>26944770.732660923    </td><td>172.26.215.93</td><td style=\"text-align: right;\">547999</td><td>192.67811779119387</td><td>56.83710527420044   </td><td>0.48557424545288086</td><td>56.83710527420044 </td><td style=\"text-align: right;\"> 1689729764</td><td>100                 </td><td>d06a69bc  </td></tr>\n",
       "<tr><td>FSR_Trainable_d2a94033</td><td>2023-07-19_10-19-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>99.38560809760155 </td><td>31540959.778121945    </td><td>172.26.215.93</td><td style=\"text-align: right;\">546298</td><td>190.9084431428233 </td><td>60.37699794769287   </td><td>0.6281077861785889 </td><td>60.37699794769287 </td><td style=\"text-align: right;\"> 1689729595</td><td>100                 </td><td>d2a94033  </td></tr>\n",
       "<tr><td>FSR_Trainable_d4185b3f</td><td>2023-07-19_10-20-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>98.3582123794821  </td><td>31326384.27006814     </td><td>172.26.215.93</td><td style=\"text-align: right;\">545593</td><td>200.76864613031756</td><td>102.42877125740051  </td><td>0.9543874263763428 </td><td>102.42877125740051</td><td style=\"text-align: right;\"> 1689729613</td><td>100                 </td><td>d4185b3f  </td></tr>\n",
       "<tr><td>FSR_Trainable_d41a892c</td><td>2023-07-19_10-27-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>240.5390106645937 </td><td>173325347.25080377    </td><td>172.26.215.93</td><td style=\"text-align: right;\">555076</td><td>395.027309860576  </td><td>0.9261035919189453  </td><td>0.9261035919189453 </td><td>0.9261035919189453</td><td style=\"text-align: right;\"> 1689730073</td><td>1                   </td><td>d41a892c  </td></tr>\n",
       "<tr><td>FSR_Trainable_d6458403</td><td>2023-07-19_10-31-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td>64                        </td><td>100.44395350579322</td><td>27659420.177413546    </td><td>172.26.215.93</td><td style=\"text-align: right;\">556165</td><td>201.88033918888263</td><td>144.43858313560486  </td><td>2.1810624599456787 </td><td>144.43858313560486</td><td style=\"text-align: right;\"> 1689730286</td><td>64                  </td><td>d6458403  </td></tr>\n",
       "<tr><td>FSR_Trainable_d645a338</td><td>2023-07-19_10-23-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>103.23272304847848</td><td>27238829.491935       </td><td>172.26.215.93</td><td style=\"text-align: right;\">548465</td><td>196.94795723348915</td><td>54.52517557144165   </td><td>0.41233134269714355</td><td>54.52517557144165 </td><td style=\"text-align: right;\"> 1689729784</td><td>100                 </td><td>d645a338  </td></tr>\n",
       "<tr><td>FSR_Trainable_d676d4d4</td><td>2023-07-19_10-25-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>236.02790419039104</td><td>1.1521440199819182e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">552194</td><td>388.6561733876323 </td><td>2.327558994293213   </td><td>2.327558994293213  </td><td>2.327558994293213 </td><td style=\"text-align: right;\"> 1689729957</td><td>1                   </td><td>d676d4d4  </td></tr>\n",
       "<tr><td>FSR_Trainable_db65304b</td><td>2023-07-19_10-09-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>144.96663696093356</td><td>1.6224741899500333e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">537469</td><td>257.7877997253684 </td><td>241.03101301193237  </td><td>2.6609349250793457 </td><td>241.03101301193237</td><td style=\"text-align: right;\"> 1689728951</td><td>100                 </td><td>db65304b  </td></tr>\n",
       "<tr><td>FSR_Trainable_dc44db54</td><td>2023-07-19_10-15-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>109.74945981514054</td><td>26967716.727600176    </td><td>172.26.215.93</td><td style=\"text-align: right;\">541008</td><td>217.8239241239863 </td><td>263.40997838974     </td><td>2.3032708168029785 </td><td>263.40997838974   </td><td style=\"text-align: right;\"> 1689729349</td><td>100                 </td><td>dc44db54  </td></tr>\n",
       "<tr><td>FSR_Trainable_e1fcce58</td><td>2023-07-19_10-27-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td>32                        </td><td>107.32988614827157</td><td>33004386.67613723     </td><td>172.26.215.93</td><td style=\"text-align: right;\">554122</td><td>201.21411906209917</td><td>15.120689630508423  </td><td>0.4941892623901367 </td><td>15.120689630508423</td><td style=\"text-align: right;\"> 1689730059</td><td>32                  </td><td>e1fcce58  </td></tr>\n",
       "<tr><td>FSR_Trainable_e37afb38</td><td>2023-07-19_10-09-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>396.18832204482374</td><td>7.880145127973243e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">539413</td><td>519.7536891421853 </td><td>2.040999174118042   </td><td>2.040999174118042  </td><td>2.040999174118042 </td><td style=\"text-align: right;\"> 1689728943</td><td>1                   </td><td>e37afb38  </td></tr>\n",
       "<tr><td>FSR_Trainable_e9a1363c</td><td>2023-07-19_10-28-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>209.0423047729062 </td><td>96428940.36687605     </td><td>172.26.215.93</td><td style=\"text-align: right;\">555723</td><td>380.69292041979264</td><td>11.369533061981201  </td><td>11.369533061981201 </td><td>11.369533061981201</td><td style=\"text-align: right;\"> 1689730112</td><td>1                   </td><td>e9a1363c  </td></tr>\n",
       "<tr><td>FSR_Trainable_ebea30a5</td><td>2023-07-19_10-16-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>186.48046331092252</td><td>3.8357331567430584e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">544654</td><td>354.3399974707165 </td><td>4.487289905548096   </td><td>2.184497594833374  </td><td>4.487289905548096 </td><td style=\"text-align: right;\"> 1689729406</td><td>2                   </td><td>ebea30a5  </td></tr>\n",
       "<tr><td>FSR_Trainable_ec866daa</td><td>2023-07-19_10-25-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>97.53769006538353 </td><td>36656787.064994164    </td><td>172.26.215.93</td><td style=\"text-align: right;\">550130</td><td>190.39980136822325</td><td>58.962193727493286  </td><td>0.5745108127593994 </td><td>58.962193727493286</td><td style=\"text-align: right;\"> 1689729904</td><td>100                 </td><td>ec866daa  </td></tr>\n",
       "<tr><td>FSR_Trainable_ece22011</td><td>2023-07-19_10-27-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>98.22750488190972 </td><td>27859794.210991237    </td><td>172.26.215.93</td><td style=\"text-align: right;\">551794</td><td>194.36493084644235</td><td>70.74110388755798   </td><td>0.5392451286315918 </td><td>70.74110388755798 </td><td style=\"text-align: right;\"> 1689730022</td><td>100                 </td><td>ece22011  </td></tr>\n",
       "<tr><td>FSR_Trainable_ef63f3e9</td><td>2023-07-19_10-29-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td>2                         </td><td>123.25377269364424</td><td>1.8262465120120326e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">556636</td><td>221.07351232120274</td><td>2.4953479766845703  </td><td>1.178013801574707  </td><td>2.4953479766845703</td><td style=\"text-align: right;\"> 1689730158</td><td>2                   </td><td>ef63f3e9  </td></tr>\n",
       "<tr><td>FSR_Trainable_f04af01c</td><td>2023-07-19_10-26-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td>4                         </td><td>114.63824185083021</td><td>35795689.37103368     </td><td>172.26.215.93</td><td style=\"text-align: right;\">552901</td><td>214.13437208229453</td><td>2.6275599002838135  </td><td>0.5725564956665039 </td><td>2.6275599002838135</td><td style=\"text-align: right;\"> 1689729989</td><td>4                   </td><td>f04af01c  </td></tr>\n",
       "<tr><td>FSR_Trainable_f1970aaf</td><td>2023-07-19_10-09-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>228.79561435230156</td><td>3.057671768349057e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">540059</td><td>401.76746155671077</td><td>1.279554843902588   </td><td>1.279554843902588  </td><td>1.279554843902588 </td><td style=\"text-align: right;\"> 1689728981</td><td>1                   </td><td>f1970aaf  </td></tr>\n",
       "<tr><td>FSR_Trainable_f28e1912</td><td>2023-07-19_10-14-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>239.82066025858887</td><td>132919714.19318853    </td><td>172.26.215.93</td><td style=\"text-align: right;\">541983</td><td>401.37415899532755</td><td>2.1648242473602295  </td><td>2.1648242473602295 </td><td>2.1648242473602295</td><td style=\"text-align: right;\"> 1689729258</td><td>1                   </td><td>f28e1912  </td></tr>\n",
       "<tr><td>FSR_Trainable_f30d8e8d</td><td>2023-07-19_10-10-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td>8                         </td><td>192.1042428160857 </td><td>35344150.74998893     </td><td>172.26.215.93</td><td style=\"text-align: right;\">540491</td><td>397.38393454002676</td><td>26.103044271469116  </td><td>3.1197283267974854 </td><td>26.103044271469116</td><td style=\"text-align: right;\"> 1689729030</td><td>8                   </td><td>f30d8e8d  </td></tr>\n",
       "<tr><td>FSR_Trainable_f97d6a66</td><td>2023-07-19_10-22-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>100.46337620287811</td><td>27400571.513471216    </td><td>172.26.215.93</td><td style=\"text-align: right;\">548229</td><td>193.50919614773645</td><td>55.15450310707092   </td><td>0.6648826599121094 </td><td>55.15450310707092 </td><td style=\"text-align: right;\"> 1689729772</td><td>100                 </td><td>f97d6a66  </td></tr>\n",
       "<tr><td>FSR_Trainable_faff7bd9</td><td>2023-07-19_10-13-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>250.15762524569894</td><td>157174044.77184474    </td><td>172.26.215.93</td><td style=\"text-align: right;\">541519</td><td>402.18940959664576</td><td>3.1816599369049072  </td><td>3.1816599369049072 </td><td>3.1816599369049072</td><td style=\"text-align: right;\"> 1689729225</td><td>1                   </td><td>faff7bd9  </td></tr>\n",
       "<tr><td>FSR_Trainable_fb905107</td><td>2023-07-19_10-25-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td>100                       </td><td>101.30758307903359</td><td>25887474.68245019     </td><td>172.26.215.93</td><td style=\"text-align: right;\">550578</td><td>190.19069137233342</td><td>59.57190203666687   </td><td>0.5030398368835449 </td><td>59.57190203666687 </td><td style=\"text-align: right;\"> 1689729926</td><td>100                 </td><td>fb905107  </td></tr>\n",
       "<tr><td>FSR_Trainable_fd45abb5</td><td>2023-07-19_10-09-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td>1                         </td><td>269.43671232327483</td><td>1.235419192199669e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">539609</td><td>458.0316824795491 </td><td>9.458542585372925   </td><td>9.458542585372925  </td><td>9.458542585372925 </td><td style=\"text-align: right;\"> 1689728966</td><td>1                   </td><td>fd45abb5  </td></tr>\n",
       "<tr><td>FSR_Trainable_ffed4354</td><td>2023-07-19_10-26-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td>8                         </td><td>112.10623134945133</td><td>45332726.33567205     </td><td>172.26.215.93</td><td style=\"text-align: right;\">552670</td><td>207.85239989970734</td><td>5.028549671173096   </td><td>0.5575706958770752 </td><td>5.028549671173096 </td><td style=\"text-align: right;\"> 1689729982</td><td>8                   </td><td>ffed4354  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_db65304b_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_10-04-52/wandb/run-20230719_100502-db65304b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Syncing run FSR_Trainable_db65304b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/db65304b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_186ff38b_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_10-04-57/wandb/run-20230719_100510-186ff38b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: Syncing run FSR_Trainable_186ff38b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/186ff38b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_c3e72fed_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_10-05-04/wandb/run-20230719_100518-c3e72fed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb: Syncing run FSR_Trainable_c3e72fed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c3e72fed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_4056ce2c_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_10-05-11/wandb/run-20230719_100527-4056ce2c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: Syncing run FSR_Trainable_4056ce2c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4056ce2c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:                      mae 247.0883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:                     mape 5.0834703138216e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:                     rmse 389.66215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:       time_since_restore 35.44989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:         time_this_iter_s 8.61738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:             time_total_s 35.44989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:                timestamp 1689728749\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb:  View run FSR_Trainable_c3e72fed at: https://wandb.ai/seokjin/FSR-prediction/runs/c3e72fed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537881)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_100518-c3e72fed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_af6b29bf_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_10-05-20/wandb/run-20230719_100606-af6b29bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb: Syncing run FSR_Trainable_af6b29bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/af6b29bf\n",
      "2023-07-19 10:06:14,760\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.824 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:06:14,763\tWARNING util.py:315 -- The `process_trial_result` operation took 1.827 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:06:14,765\tWARNING util.py:315 -- Processing trial results took 1.829 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:06:14,767\tWARNING util.py:315 -- The `process_trial_result` operation took 1.831 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:                      mae 247.34835\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:                     mape 9.313856347405637e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:                     rmse 406.02483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:       time_since_restore 13.23211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:         time_this_iter_s 13.23211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:             time_total_s 13.23211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:                timestamp 1689728772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb:  View run FSR_Trainable_af6b29bf at: https://wandb.ai/seokjin/FSR-prediction/runs/af6b29bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538290)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_100606-af6b29bf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_618d1c52_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_10-05-59/wandb/run-20230719_100631-618d1c52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb: Syncing run FSR_Trainable_618d1c52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/618d1c52\n",
      "2023-07-19 10:06:34,736\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.677 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:06:34,737\tWARNING util.py:315 -- The `process_trial_result` operation took 1.679 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:06:34,739\tWARNING util.py:315 -- Processing trial results took 1.681 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:06:34,742\tWARNING util.py:315 -- The `process_trial_result` operation took 1.683 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:                      mae 171.80866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:                     mape 45065250.71864\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:                     rmse 326.00923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:       time_since_restore 178.88699\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:         time_this_iter_s 1.69997\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:             time_total_s 178.88699\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:                timestamp 1689728889\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb:  View run FSR_Trainable_186ff38b at: https://wandb.ai/seokjin/FSR-prediction/runs/186ff38b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537709)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_100510-186ff38b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 10:08:24,697\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.121 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:08:24,702\tWARNING util.py:315 -- The `process_trial_result` operation took 2.127 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:08:24,704\tWARNING util.py:315 -- Processing trial results took 2.129 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:08:24,709\tWARNING util.py:315 -- The `process_trial_result` operation took 2.134 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:                      mae 164.72936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:                     mape 36802533.06883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:                     rmse 321.43603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:       time_since_restore 114.01995\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:         time_this_iter_s 6.5304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:             time_total_s 114.01995\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:                timestamp 1689728900\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb:  View run FSR_Trainable_618d1c52 at: https://wandb.ai/seokjin/FSR-prediction/runs/618d1c52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538520)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_100631-618d1c52/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_2276ca4f_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_10-06-24/wandb/run-20230719_100826-2276ca4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: Syncing run FSR_Trainable_2276ca4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2276ca4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:                      mae 226.78934\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:                     mape 3.817836072827627e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:                     rmse 392.40154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:       time_since_restore 2.84447\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:         time_this_iter_s 2.84447\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:             time_total_s 2.84447\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:                timestamp 1689728902\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb:  View run FSR_Trainable_2276ca4f at: https://wandb.ai/seokjin/FSR-prediction/runs/2276ca4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538795)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_100826-2276ca4f/logs\n",
      "2023-07-19 10:08:36,189\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.106 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:08:36,194\tWARNING util.py:315 -- The `process_trial_result` operation took 2.110 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:08:36,197\tWARNING util.py:315 -- Processing trial results took 2.114 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:08:36,201\tWARNING util.py:315 -- The `process_trial_result` operation took 2.117 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_acf03874_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_10-08-19/wandb/run-20230719_100837-acf03874\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: Syncing run FSR_Trainable_acf03874\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/acf03874\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539241)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539241)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539241)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539241)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539241)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_2a147f32_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_10-08-30/wandb/run-20230719_100848-2a147f32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539241)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539241)\u001b[0m wandb: Syncing run FSR_Trainable_2a147f32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539241)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539241)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2a147f32\n",
      "2023-07-19 10:08:49,986\tERROR tune_controller.py:873 -- Trial task failed for trial FSR_Trainable_2a147f32\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 2540, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::ResourceTrainable.train()\u001b[39m (pid=539140, ip=172.26.215.93, actor_id=9972338233560f6a40fe24ad01000000, repr=<ray.tune.trainable.util.FSR_Trainable object at 0x7f6cce7b3640>)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 386, in train\n",
      "    result = self.step()\n",
      "  File \"/home/seokj/workspace/FSR-prediction/fsr_trainable.py\", line 73, in step\n",
      "    mae.append(sklearn.metrics.mean_absolute_error(y, pred))\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 196, in mean_absolute_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/metrics/_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539241)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539241)\u001b[0m wandb: - 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539241)\u001b[0m wandb: \\ 0.002 MB of 0.007 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539241)\u001b[0m wandb:  View run FSR_Trainable_2a147f32 at: https://wandb.ai/seokjin/FSR-prediction/runs/2a147f32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539241)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539241)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_100848-2a147f32/logs\n",
      "2023-07-19 10:09:06,149\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.196 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:09:06,153\tWARNING util.py:315 -- The `process_trial_result` operation took 2.201 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:09:06,156\tWARNING util.py:315 -- Processing trial results took 2.204 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:09:06,158\tWARNING util.py:315 -- The `process_trial_result` operation took 2.206 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_e37afb38_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-08-40/wandb/run-20230719_100908-e37afb38\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: Syncing run FSR_Trainable_e37afb38\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e37afb38\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:                      mae 396.18832\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:                     mape 7.880145127973243e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:                     rmse 519.75369\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:       time_since_restore 2.041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:         time_this_iter_s 2.041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:             time_total_s 2.041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:                timestamp 1689728943\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb:  View run FSR_Trainable_e37afb38 at: https://wandb.ai/seokjin/FSR-prediction/runs/e37afb38\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539467)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_100908-e37afb38/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_100502-db65304b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=537539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_fd45abb5_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-09-01/wandb/run-20230719_100924-fd45abb5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: Syncing run FSR_Trainable_fd45abb5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd45abb5\n",
      "2023-07-19 10:09:28,651\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.828 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:09:28,656\tWARNING util.py:315 -- The `process_trial_result` operation took 1.833 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:09:28,659\tWARNING util.py:315 -- Processing trial results took 1.837 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:09:28,661\tWARNING util.py:315 -- The `process_trial_result` operation took 1.839 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 10:09:32,801\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.209 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:09:32,807\tWARNING util.py:315 -- The `process_trial_result` operation took 2.216 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:09:32,809\tWARNING util.py:315 -- Processing trial results took 2.218 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:09:32,812\tWARNING util.py:315 -- The `process_trial_result` operation took 2.221 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:                      mae 269.43671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:                     mape 1.235419192199669e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:                     rmse 458.03168\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:       time_since_restore 9.45854\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:         time_this_iter_s 9.45854\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:             time_total_s 9.45854\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:                timestamp 1689728966\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb:  View run FSR_Trainable_fd45abb5 at: https://wandb.ai/seokjin/FSR-prediction/runs/fd45abb5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539706)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_100924-fd45abb5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_5b747ab2_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-09-17/wandb/run-20230719_100935-5b747ab2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: Syncing run FSR_Trainable_5b747ab2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5b747ab2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:                      mae 299.5154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:                     mape 4.424418246552514e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:                     rmse 493.65127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:       time_since_restore 1.81722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:         time_this_iter_s 1.81722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:             time_total_s 1.81722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:                timestamp 1689728970\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb:  View run FSR_Trainable_5b747ab2 at: https://wandb.ai/seokjin/FSR-prediction/runs/5b747ab2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539917)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_100935-5b747ab2/logs\n",
      "2023-07-19 10:09:43,962\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.265 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:09:43,966\tWARNING util.py:315 -- The `process_trial_result` operation took 2.270 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:09:43,968\tWARNING util.py:315 -- Processing trial results took 2.272 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:09:43,970\tWARNING util.py:315 -- The `process_trial_result` operation took 2.274 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_f1970aaf_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-09-28/wandb/run-20230719_100946-f1970aaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: Syncing run FSR_Trainable_f1970aaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f1970aaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:                      mae 228.79561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:                     mape 3.057671768349057e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:                     rmse 401.76746\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:       time_since_restore 1.27955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:         time_this_iter_s 1.27955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:             time_total_s 1.27955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:                timestamp 1689728981\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb:  View run FSR_Trainable_f1970aaf at: https://wandb.ai/seokjin/FSR-prediction/runs/f1970aaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540148)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_100946-f1970aaf/logs\n",
      "2023-07-19 10:09:55,924\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.077 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:09:55,927\tWARNING util.py:315 -- The `process_trial_result` operation took 2.082 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:09:55,929\tWARNING util.py:315 -- Processing trial results took 2.084 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:09:55,931\tWARNING util.py:315 -- The `process_trial_result` operation took 2.086 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_ca6decca_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-09-40/wandb/run-20230719_100958-ca6decca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb: Syncing run FSR_Trainable_ca6decca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ca6decca\n",
      "2023-07-19 10:10:08,647\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.849 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:10:08,651\tWARNING util.py:315 -- The `process_trial_result` operation took 1.853 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:10:08,653\tWARNING util.py:315 -- Processing trial results took 1.855 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:10:08,655\tWARNING util.py:315 -- The `process_trial_result` operation took 1.857 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_f30d8e8d_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-09-51/wandb/run-20230719_101009-f30d8e8d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: Syncing run FSR_Trainable_f30d8e8d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f30d8e8d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:                      mae 192.10424\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:                     mape 35344150.74999\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:                     rmse 397.38393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:       time_since_restore 26.10304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:         time_this_iter_s 3.11973\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:             time_total_s 26.10304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:                timestamp 1689729030\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb:  View run FSR_Trainable_f30d8e8d at: https://wandb.ai/seokjin/FSR-prediction/runs/f30d8e8d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540589)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101009-f30d8e8d/logs\n",
      "2023-07-19 10:10:46,193\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.034 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:10:46,199\tWARNING util.py:315 -- The `process_trial_result` operation took 2.040 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:10:46,201\tWARNING util.py:315 -- Processing trial results took 2.042 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:10:46,203\tWARNING util.py:315 -- The `process_trial_result` operation took 2.044 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_16cc40a3_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-10-02/wandb/run-20230719_101047-16cc40a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: Syncing run FSR_Trainable_16cc40a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/16cc40a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:                      mae 179.49576\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:                     mape 29520617.40309\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:                     rmse 356.60007\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:       time_since_restore 20.59261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:         time_this_iter_s 2.37846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:             time_total_s 20.59261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:                timestamp 1689729063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb:  View run FSR_Trainable_16cc40a3 at: https://wandb.ai/seokjin/FSR-prediction/runs/16cc40a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540827)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101047-16cc40a3/logs\n",
      "2023-07-19 10:11:20,745\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.298 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:11:20,750\tWARNING util.py:315 -- The `process_trial_result` operation took 2.305 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:11:20,753\tWARNING util.py:315 -- Processing trial results took 2.308 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:11:20,754\tWARNING util.py:315 -- The `process_trial_result` operation took 2.309 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_dc44db54_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-10-41/wandb/run-20230719_101121-dc44db54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Syncing run FSR_Trainable_dc44db54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/dc44db54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:                      mae 140.45989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:                     mape 51980505.06213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:                     rmse 236.57524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:       time_since_restore 141.16171\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:         time_this_iter_s 1.31977\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:             time_total_s 141.16171\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:                timestamp 1689729140\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb:  View run FSR_Trainable_ca6decca at: https://wandb.ai/seokjin/FSR-prediction/runs/ca6decca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=540373)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_100958-ca6decca/logs\n",
      "2023-07-19 10:12:39,136\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.563 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:12:39,140\tWARNING util.py:315 -- The `process_trial_result` operation took 2.568 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:12:39,141\tWARNING util.py:315 -- Processing trial results took 2.570 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:12:39,146\tWARNING util.py:315 -- The `process_trial_result` operation took 2.575 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_ad33fb7e_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-11-14/wandb/run-20230719_101239-ad33fb7e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb: Syncing run FSR_Trainable_ad33fb7e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ad33fb7e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 10:13:47,433\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.246 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:13:47,438\tWARNING util.py:315 -- The `process_trial_result` operation took 2.252 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:13:47,441\tWARNING util.py:315 -- Processing trial results took 2.255 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:13:47,443\tWARNING util.py:315 -- The `process_trial_result` operation took 2.257 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_faff7bd9_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-12-31/wandb/run-20230719_101349-faff7bd9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Syncing run FSR_Trainable_faff7bd9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/faff7bd9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:                      mae 112.21628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:                     mape 33607818.62394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:                     rmse 216.84214\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:       time_since_restore 290.75023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:         time_this_iter_s 3.1538\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:             time_total_s 290.75023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:                timestamp 1689729210\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb:  View run FSR_Trainable_acf03874 at: https://wandb.ai/seokjin/FSR-prediction/runs/acf03874\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=539024)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_100837-acf03874/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101349-faff7bd9/logs\n",
      "2023-07-19 10:14:03,155\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.354 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:14:03,162\tWARNING util.py:315 -- The `process_trial_result` operation took 2.361 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:14:03,164\tWARNING util.py:315 -- Processing trial results took 2.363 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:14:03,166\tWARNING util.py:315 -- The `process_trial_result` operation took 2.365 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541578)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_84b8b236_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-13-42/wandb/run-20230719_101405-84b8b236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb: Syncing run FSR_Trainable_84b8b236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/84b8b236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:                      mae 218.07528\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:                     mape 74341412.56421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:                     rmse 370.98637\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:       time_since_restore 4.97973\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:         time_this_iter_s 2.26974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:             time_total_s 4.97973\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:                timestamp 1689729245\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb:  View run FSR_Trainable_84b8b236 at: https://wandb.ai/seokjin/FSR-prediction/runs/84b8b236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541815)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101405-84b8b236/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 10:14:20,329\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.209 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:14:20,334\tWARNING util.py:315 -- The `process_trial_result` operation took 2.214 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:14:20,336\tWARNING util.py:315 -- Processing trial results took 2.216 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:14:20,341\tWARNING util.py:315 -- The `process_trial_result` operation took 2.221 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:                      mae 136.73013\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:                     mape 36401615.97498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:                     rmse 261.53884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:       time_since_restore 98.96848\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:         time_this_iter_s 2.69876\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:             time_total_s 98.96848\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:                timestamp 1689729256\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb:  View run FSR_Trainable_ad33fb7e at: https://wandb.ai/seokjin/FSR-prediction/runs/ad33fb7e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541326)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101239-ad33fb7e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_f28e1912_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-13-58/wandb/run-20230719_101423-f28e1912\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb: Syncing run FSR_Trainable_f28e1912\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f28e1912\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:                      mae 239.82066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:                     mape 132919714.19319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:                     rmse 401.37416\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:       time_since_restore 2.16482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:         time_this_iter_s 2.16482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:             time_total_s 2.16482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:                timestamp 1689729258\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb:  View run FSR_Trainable_f28e1912 at: https://wandb.ai/seokjin/FSR-prediction/runs/f28e1912\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542043)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101423-f28e1912/logs\n",
      "2023-07-19 10:14:32,410\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.291 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:14:32,415\tWARNING util.py:315 -- The `process_trial_result` operation took 2.297 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:14:32,418\tWARNING util.py:315 -- Processing trial results took 2.299 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:14:32,421\tWARNING util.py:315 -- The `process_trial_result` operation took 2.302 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_a1801155_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-14-16/wandb/run-20230719_101435-a1801155\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb: Syncing run FSR_Trainable_a1801155\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a1801155\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:                      mae 242.03442\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:                     mape 137211871.83331\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:                     rmse 385.39833\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:       time_since_restore 1.88369\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:         time_this_iter_s 1.88369\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:             time_total_s 1.88369\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:                timestamp 1689729270\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb:  View run FSR_Trainable_a1801155 at: https://wandb.ai/seokjin/FSR-prediction/runs/a1801155\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542274)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101435-a1801155/logs\n",
      "2023-07-19 10:14:42,532\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.300 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:14:42,538\tWARNING util.py:315 -- The `process_trial_result` operation took 2.306 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:14:42,541\tWARNING util.py:315 -- Processing trial results took 2.310 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:14:42,544\tWARNING util.py:315 -- The `process_trial_result` operation took 2.313 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_72e0dfe8_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-14-28/wandb/run-20230719_101444-72e0dfe8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb: Syncing run FSR_Trainable_72e0dfe8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/72e0dfe8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:                      mae 187.74149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:                     mape 4970305812091363.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:                     rmse 390.8315\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:       time_since_restore 1.99163\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:         time_this_iter_s 1.99163\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:             time_total_s 1.99163\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:                timestamp 1689729280\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb:  View run FSR_Trainable_72e0dfe8 at: https://wandb.ai/seokjin/FSR-prediction/runs/72e0dfe8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542493)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101444-72e0dfe8/logs\n",
      "2023-07-19 10:14:53,344\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.570 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:14:53,350\tWARNING util.py:315 -- The `process_trial_result` operation took 2.577 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:14:53,353\tWARNING util.py:315 -- Processing trial results took 2.580 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:14:53,356\tWARNING util.py:315 -- The `process_trial_result` operation took 2.583 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_12da65ac_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-14-38/wandb/run-20230719_101455-12da65ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: Syncing run FSR_Trainable_12da65ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/12da65ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:                      mae 214.85547\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:                     mape 3.432250433803145e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:                     rmse 411.25935\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:       time_since_restore 2.66373\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:         time_this_iter_s 2.66373\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:             time_total_s 2.66373\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:                timestamp 1689729290\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb:  View run FSR_Trainable_12da65ac at: https://wandb.ai/seokjin/FSR-prediction/runs/12da65ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542718)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101455-12da65ac/logs\n",
      "2023-07-19 10:15:03,175\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.187 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:15:03,178\tWARNING util.py:315 -- The `process_trial_result` operation took 2.194 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:15:03,181\tWARNING util.py:315 -- Processing trial results took 2.197 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:15:03,183\tWARNING util.py:315 -- The `process_trial_result` operation took 2.200 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_ab9d6e57_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-14-48/wandb/run-20230719_101505-ab9d6e57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: Syncing run FSR_Trainable_ab9d6e57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ab9d6e57\n",
      "2023-07-19 10:15:14,045\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.238 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:15:14,050\tWARNING util.py:315 -- The `process_trial_result` operation took 2.244 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:15:14,053\tWARNING util.py:315 -- Processing trial results took 2.247 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:15:14,056\tWARNING util.py:315 -- The `process_trial_result` operation took 2.249 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_95e32c40_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-14-58/wandb/run-20230719_101515-95e32c40\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb: Syncing run FSR_Trainable_95e32c40\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/95e32c40\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:                      mae 563.51917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:                     mape 9.612920438883105e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:                     rmse 895.69096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:       time_since_restore 2.83332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:         time_this_iter_s 2.83332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:             time_total_s 2.83332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:                timestamp 1689729311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb:  View run FSR_Trainable_95e32c40 at: https://wandb.ai/seokjin/FSR-prediction/runs/95e32c40\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543160)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101515-95e32c40/logs\n",
      "2023-07-19 10:15:30,058\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.864 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:15:30,064\tWARNING util.py:315 -- The `process_trial_result` operation took 1.870 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:15:30,065\tWARNING util.py:315 -- Processing trial results took 1.872 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:15:30,068\tWARNING util.py:315 -- The `process_trial_result` operation took 1.875 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_1eeb4d5f_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-15-09/wandb/run-20230719_101531-1eeb4d5f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: Syncing run FSR_Trainable_1eeb4d5f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/1eeb4d5f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:                      mae 474.3499\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:                     mape 1.140369999208248e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:                     rmse 619.73147\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:       time_since_restore 2.87408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:         time_this_iter_s 2.87408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:             time_total_s 2.87408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:                timestamp 1689729328\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb:  View run FSR_Trainable_1eeb4d5f at: https://wandb.ai/seokjin/FSR-prediction/runs/1eeb4d5f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543387)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101531-1eeb4d5f/logs\n",
      "2023-07-19 10:15:46,791\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.212 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:15:46,798\tWARNING util.py:315 -- The `process_trial_result` operation took 2.220 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:15:46,800\tWARNING util.py:315 -- Processing trial results took 2.222 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:15:46,801\tWARNING util.py:315 -- The `process_trial_result` operation took 2.224 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_bc84585a_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-15-25/wandb/run-20230719_101547-bc84585a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb: Syncing run FSR_Trainable_bc84585a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/bc84585a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:                      mae 247.4417\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:                     mape 167760340.72797\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:                     rmse 389.46639\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:       time_since_restore 4.12492\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:         time_this_iter_s 4.12492\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:             time_total_s 4.12492\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:                timestamp 1689729344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb:  View run FSR_Trainable_bc84585a at: https://wandb.ai/seokjin/FSR-prediction/runs/bc84585a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543611)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101547-bc84585a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101121-dc44db54/logs\n",
      "2023-07-19 10:16:01,355\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.326 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:16:01,361\tWARNING util.py:315 -- The `process_trial_result` operation took 2.333 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:16:01,363\tWARNING util.py:315 -- Processing trial results took 2.335 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:16:01,366\tWARNING util.py:315 -- The `process_trial_result` operation took 2.338 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=541063)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_5db9a04e_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-15-40/wandb/run-20230719_101602-5db9a04e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb: Syncing run FSR_Trainable_5db9a04e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5db9a04e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:                      mae 250.24793\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:                     mape 171001364.06423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:                     rmse 396.15507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:       time_since_restore 3.38838\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:         time_this_iter_s 3.38838\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:             time_total_s 3.38838\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:                timestamp 1689729359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb:  View run FSR_Trainable_5db9a04e at: https://wandb.ai/seokjin/FSR-prediction/runs/5db9a04e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101602-5db9a04e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=543848)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 10:16:08,961\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.969 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:16:08,963\tWARNING util.py:315 -- The `process_trial_result` operation took 1.972 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:16:08,968\tWARNING util.py:315 -- Processing trial results took 1.977 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:16:08,969\tWARNING util.py:315 -- The `process_trial_result` operation took 1.979 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_2284a968_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-15-55/wandb/run-20230719_101612-2284a968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: Syncing run FSR_Trainable_2284a968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2284a968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:                      mae 173.06158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:                     mape 42516613.19406\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:                     rmse 323.04603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:       time_since_restore 68.33799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:         time_this_iter_s 4.47624\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:             time_total_s 68.33799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:                timestamp 1689729370\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb:  View run FSR_Trainable_ab9d6e57 at: https://wandb.ai/seokjin/FSR-prediction/runs/ab9d6e57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=542944)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101505-ab9d6e57/logs\n",
      "2023-07-19 10:16:20,659\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.895 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:16:20,663\tWARNING util.py:315 -- The `process_trial_result` operation took 1.900 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:16:20,667\tWARNING util.py:315 -- Processing trial results took 1.904 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:16:20,672\tWARNING util.py:315 -- The `process_trial_result` operation took 1.908 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_7cafa4c8_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-16-05/wandb/run-20230719_101623-7cafa4c8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Syncing run FSR_Trainable_7cafa4c8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7cafa4c8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:                      mae 111.20679\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:                     mape 9.81428976663767e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:                     rmse 212.21752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:       time_since_restore 650.2225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:         time_this_iter_s 6.05599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:             time_total_s 650.2225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:                timestamp 1689729381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb:  View run FSR_Trainable_4056ce2c at: https://wandb.ai/seokjin/FSR-prediction/runs/4056ce2c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=538044)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_100527-4056ce2c/logs\n",
      "2023-07-19 10:16:31,765\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.594 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:16:31,768\tWARNING util.py:315 -- The `process_trial_result` operation took 1.597 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:16:31,770\tWARNING util.py:315 -- Processing trial results took 1.600 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:16:31,772\tWARNING util.py:315 -- The `process_trial_result` operation took 1.601 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_28b2c667_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-16-17/wandb/run-20230719_101635-28b2c667\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Syncing run FSR_Trainable_28b2c667\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/28b2c667\n",
      "2023-07-19 10:16:44,594\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.625 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:16:44,597\tWARNING util.py:315 -- The `process_trial_result` operation took 1.628 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:16:44,600\tWARNING util.py:315 -- Processing trial results took 1.632 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:16:44,604\tWARNING util.py:315 -- The `process_trial_result` operation took 1.636 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_ebea30a5_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-16-28/wandb/run-20230719_101647-ebea30a5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: Syncing run FSR_Trainable_ebea30a5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ebea30a5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:                      mae 186.48046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:                     mape 3.8357331567430584e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:                     rmse 354.34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:       time_since_restore 4.48729\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:         time_this_iter_s 2.1845\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:             time_total_s 4.48729\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:                timestamp 1689729406\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb:  View run FSR_Trainable_ebea30a5 at: https://wandb.ai/seokjin/FSR-prediction/runs/ebea30a5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544738)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101647-ebea30a5/logs\n",
      "2023-07-19 10:17:01,397\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.879 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:17:01,400\tWARNING util.py:315 -- The `process_trial_result` operation took 1.883 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:17:01,402\tWARNING util.py:315 -- Processing trial results took 1.885 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:17:01,403\tWARNING util.py:315 -- The `process_trial_result` operation took 1.886 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_57d3b397_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-16-40/wandb/run-20230719_101704-57d3b397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: Syncing run FSR_Trainable_57d3b397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/57d3b397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:                      mae 179.87809\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:                     mape 1.7824747884608254e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:                     rmse 359.10742\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:       time_since_restore 4.07643\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:         time_this_iter_s 1.95767\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:             time_total_s 4.07643\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:                timestamp 1689729423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb:  View run FSR_Trainable_57d3b397 at: https://wandb.ai/seokjin/FSR-prediction/runs/57d3b397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544973)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101704-57d3b397/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_c5a39582_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-16-57/wandb/run-20230719_101721-c5a39582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb: Syncing run FSR_Trainable_c5a39582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c5a39582\n",
      "2023-07-19 10:17:23,332\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.469 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:17:23,345\tWARNING util.py:315 -- The `process_trial_result` operation took 1.483 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:17:23,347\tWARNING util.py:315 -- Processing trial results took 1.484 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:17:23,348\tWARNING util.py:315 -- The `process_trial_result` operation took 1.486 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:                      mae 109.82652\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:                     mape 36273341.20517\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:                     rmse 212.43288\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:       time_since_restore 100.87518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:         time_this_iter_s 0.95184\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:             time_total_s 100.87518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:                timestamp 1689729477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb:  View run FSR_Trainable_2284a968 at: https://wandb.ai/seokjin/FSR-prediction/runs/2284a968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544062)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101612-2284a968/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:                      mae 187.46293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:                     mape 1.998279141509369e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:                     rmse 344.57222\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:       time_since_restore 53.55302\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:         time_this_iter_s 6.66918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:             time_total_s 53.55302\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:                timestamp 1689729489\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb:  View run FSR_Trainable_c5a39582 at: https://wandb.ai/seokjin/FSR-prediction/runs/c5a39582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101721-c5a39582/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_ad0eb2b4_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-17-14/wandb/run-20230719_101815-ad0eb2b4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb: Syncing run FSR_Trainable_ad0eb2b4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ad0eb2b4\n",
      "2023-07-19 10:18:17,984\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.609 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:18:17,989\tWARNING util.py:315 -- The `process_trial_result` operation took 1.615 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:18:17,991\tWARNING util.py:315 -- Processing trial results took 1.618 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:18:17,993\tWARNING util.py:315 -- The `process_trial_result` operation took 1.619 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544298)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:                      mae 276.94296\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:                     mape 2.494997584556088e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:                     rmse 547.25642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:       time_since_restore 8.08494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:         time_this_iter_s 8.08494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:             time_total_s 8.08494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:                timestamp 1689729496\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb:  View run FSR_Trainable_ad0eb2b4 at: https://wandb.ai/seokjin/FSR-prediction/runs/ad0eb2b4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545471)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101815-ad0eb2b4/logs\n",
      "2023-07-19 10:18:22,388\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.647 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:18:22,391\tWARNING util.py:315 -- The `process_trial_result` operation took 1.651 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:18:22,393\tWARNING util.py:315 -- Processing trial results took 1.652 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:18:22,395\tWARNING util.py:315 -- The `process_trial_result` operation took 1.654 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_d4185b3f_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-18-08/wandb/run-20230719_101824-d4185b3f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Syncing run FSR_Trainable_d4185b3f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d4185b3f\n",
      "2023-07-19 10:18:31,449\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.415 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:18:31,455\tWARNING util.py:315 -- The `process_trial_result` operation took 1.421 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:18:31,456\tWARNING util.py:315 -- Processing trial results took 1.422 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:18:31,457\tWARNING util.py:315 -- The `process_trial_result` operation took 1.424 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=544524)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_4e830b65_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-18-19/wandb/run-20230719_101834-4e830b65\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb: Syncing run FSR_Trainable_4e830b65\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4e830b65\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:                      mae 241.08085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:                     mape 138276987.78184\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:                     rmse 376.93694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:       time_since_restore 2.44092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:         time_this_iter_s 1.04323\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:             time_total_s 2.44092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:                timestamp 1689729512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb:  View run FSR_Trainable_4e830b65 at: https://wandb.ai/seokjin/FSR-prediction/runs/4e830b65\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545945)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101834-4e830b65/logs\n",
      "2023-07-19 10:18:40,850\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.690 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:18:40,854\tWARNING util.py:315 -- The `process_trial_result` operation took 1.694 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:18:40,856\tWARNING util.py:315 -- Processing trial results took 1.696 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:18:40,857\tWARNING util.py:315 -- The `process_trial_result` operation took 1.698 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_9ac72315_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-18-28/wandb/run-20230719_101843-9ac72315\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb: Syncing run FSR_Trainable_9ac72315\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9ac72315\n",
      "2023-07-19 10:18:51,607\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.882 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:18:51,610\tWARNING util.py:315 -- The `process_trial_result` operation took 1.886 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:18:51,611\tWARNING util.py:315 -- Processing trial results took 1.887 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:18:51,612\tWARNING util.py:315 -- The `process_trial_result` operation took 1.888 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_d2a94033_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-18-38/wandb/run-20230719_101854-d2a94033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: Syncing run FSR_Trainable_d2a94033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d2a94033\n",
      "2023-07-19 10:19:03,432\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.946 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:19:03,435\tWARNING util.py:315 -- The `process_trial_result` operation took 1.950 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:19:03,436\tWARNING util.py:315 -- Processing trial results took 1.951 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:19:03,437\tWARNING util.py:315 -- The `process_trial_result` operation took 1.952 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_0b303d44_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-18-48/wandb/run-20230719_101906-0b303d44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb: Syncing run FSR_Trainable_0b303d44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0b303d44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:                      mae 99.38561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:                     mape 31540959.77812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:                     rmse 190.90844\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:       time_since_restore 60.377\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:         time_this_iter_s 0.62811\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:             time_total_s 60.377\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:                timestamp 1689729595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb:  View run FSR_Trainable_d2a94033 at: https://wandb.ai/seokjin/FSR-prediction/runs/d2a94033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546383)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101854-d2a94033/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:                      mae 103.08707\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:                     mape 29101165.04616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:                     rmse 196.39673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:       time_since_restore 58.91115\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:         time_this_iter_s 0.57633\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:             time_total_s 58.91115\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:                timestamp 1689729603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb:  View run FSR_Trainable_0b303d44 at: https://wandb.ai/seokjin/FSR-prediction/runs/0b303d44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546598)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101906-0b303d44/logs\n",
      "2023-07-19 10:20:09,192\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.208 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:20:09,195\tWARNING util.py:315 -- The `process_trial_result` operation took 2.213 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:20:09,197\tWARNING util.py:315 -- Processing trial results took 2.215 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:20:09,198\tWARNING util.py:315 -- The `process_trial_result` operation took 2.216 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_4209070a_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-19-00/wandb/run-20230719_102012-4209070a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb: Syncing run FSR_Trainable_4209070a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4209070a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:                      mae 104.69612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:                     mape 34191346.2421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:                     rmse 199.64125\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:       time_since_restore 82.81151\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:         time_this_iter_s 1.26722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:             time_total_s 82.81151\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:                timestamp 1689729607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb:  View run FSR_Trainable_9ac72315 at: https://wandb.ai/seokjin/FSR-prediction/runs/9ac72315\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546165)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101843-9ac72315/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_101824-d4185b3f/logs\n",
      "2023-07-19 10:20:19,359\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.943 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:20:19,362\tWARNING util.py:315 -- The `process_trial_result` operation took 1.947 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:20:19,363\tWARNING util.py:315 -- Processing trial results took 1.948 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:20:19,365\tWARNING util.py:315 -- The `process_trial_result` operation took 1.950 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=545713)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_24560de9_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-20-06/wandb/run-20230719_102022-24560de9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Syncing run FSR_Trainable_24560de9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/24560de9\n",
      "2023-07-19 10:20:30,123\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.869 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:20:30,125\tWARNING util.py:315 -- The `process_trial_result` operation took 1.872 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:20:30,126\tWARNING util.py:315 -- Processing trial results took 1.873 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:20:30,127\tWARNING util.py:315 -- The `process_trial_result` operation took 1.874 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_8312c849_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-20-16/wandb/run-20230719_102033-8312c849\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb: Syncing run FSR_Trainable_8312c849\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8312c849\n",
      "2023-07-19 10:20:43,454\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.957 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:20:43,455\tWARNING util.py:315 -- The `process_trial_result` operation took 1.959 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:20:43,458\tWARNING util.py:315 -- Processing trial results took 1.962 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:20:43,461\tWARNING util.py:315 -- The `process_trial_result` operation took 1.965 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_2813b9b2_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-20-27/wandb/run-20230719_102046-2813b9b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: Syncing run FSR_Trainable_2813b9b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2813b9b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:                      mae 98.00297\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:                     mape 26921818.27948\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:                     rmse 186.05713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:       time_since_restore 59.51017\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:         time_this_iter_s 0.62883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:             time_total_s 59.51017\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:                timestamp 1689729675\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb:  View run FSR_Trainable_4209070a at: https://wandb.ai/seokjin/FSR-prediction/runs/4209070a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=546901)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102012-4209070a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-07-19 10:21:28,796\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.055 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:21:28,800\tWARNING util.py:315 -- The `process_trial_result` operation took 2.060 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:21:28,801\tWARNING util.py:315 -- Processing trial results took 2.061 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:21:28,803\tWARNING util.py:315 -- The `process_trial_result` operation took 2.062 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102022-24560de9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102022-24560de9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102022-24560de9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102022-24560de9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102022-24560de9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102022-24560de9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102022-24560de9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102022-24560de9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102022-24560de9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102022-24560de9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102022-24560de9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102022-24560de9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102022-24560de9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102022-24560de9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102022-24560de9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547141)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102022-24560de9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_4f59511a_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-20-40/wandb/run-20230719_102131-4f59511a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb: Syncing run FSR_Trainable_4f59511a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4f59511a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 10:21:40,830\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.033 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:21:40,832\tWARNING util.py:315 -- The `process_trial_result` operation took 2.036 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:21:40,834\tWARNING util.py:315 -- Processing trial results took 2.038 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:21:40,838\tWARNING util.py:315 -- The `process_trial_result` operation took 2.042 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:                      mae 100.66645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:                     mape 24953935.03722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:                     rmse 194.05232\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:       time_since_restore 62.41405\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:         time_this_iter_s 0.6011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:             time_total_s 62.41405\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:                timestamp 1689729697\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb:  View run FSR_Trainable_8312c849 at: https://wandb.ai/seokjin/FSR-prediction/runs/8312c849\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547361)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102033-8312c849/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_d06a69bc_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-21-26/wandb/run-20230719_102143-d06a69bc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb: Syncing run FSR_Trainable_d06a69bc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d06a69bc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:                      mae 97.93281\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:                     mape 23660579.59976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:                     rmse 190.5173\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:       time_since_restore 56.48344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:         time_this_iter_s 0.62873\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:             time_total_s 56.48344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:                timestamp 1689729705\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb:  View run FSR_Trainable_2813b9b2 at: https://wandb.ai/seokjin/FSR-prediction/runs/2813b9b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547581)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102046-2813b9b2/logs\n",
      "2023-07-19 10:21:52,507\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.217 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:21:52,510\tWARNING util.py:315 -- The `process_trial_result` operation took 2.221 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:21:52,512\tWARNING util.py:315 -- Processing trial results took 2.223 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:21:52,516\tWARNING util.py:315 -- The `process_trial_result` operation took 2.226 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_f97d6a66_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-21-37/wandb/run-20230719_102155-f97d6a66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: Syncing run FSR_Trainable_f97d6a66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f97d6a66\n",
      "2023-07-19 10:22:04,220\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.829 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:22:04,222\tWARNING util.py:315 -- The `process_trial_result` operation took 1.833 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:22:04,223\tWARNING util.py:315 -- Processing trial results took 1.834 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:22:04,225\tWARNING util.py:315 -- The `process_trial_result` operation took 1.836 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_d645a338_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-21-49/wandb/run-20230719_102207-d645a338\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Syncing run FSR_Trainable_d645a338\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d645a338\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:                      mae 106.17672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:                     mape 28098376.69715\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:                     rmse 200.87597\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:       time_since_restore 37.24571\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:         time_this_iter_s 0.50188\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:             time_total_s 37.24571\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:                timestamp 1689729732\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb:  View run FSR_Trainable_4f59511a at: https://wandb.ai/seokjin/FSR-prediction/runs/4f59511a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=547866)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102131-4f59511a/logs\n",
      "2023-07-19 10:22:25,927\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.935 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:22:25,929\tWARNING util.py:315 -- The `process_trial_result` operation took 1.938 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:22:25,931\tWARNING util.py:315 -- Processing trial results took 1.940 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:22:25,933\tWARNING util.py:315 -- The `process_trial_result` operation took 1.942 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_c94a92ec_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-22-01/wandb/run-20230719_102229-c94a92ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: Syncing run FSR_Trainable_c94a92ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c94a92ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:                      mae 99.07016\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:                     mape 26944770.73266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:                     rmse 192.67812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:       time_since_restore 56.83711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:         time_this_iter_s 0.48557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:             time_total_s 56.83711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:                timestamp 1689729764\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb:  View run FSR_Trainable_d06a69bc at: https://wandb.ai/seokjin/FSR-prediction/runs/d06a69bc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548097)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102143-d06a69bc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:                      mae 100.46338\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:                     mape 27400571.51347\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:                     rmse 193.5092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:       time_since_restore 55.1545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:         time_this_iter_s 0.66488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:             time_total_s 55.1545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:                timestamp 1689729772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb:  View run FSR_Trainable_f97d6a66 at: https://wandb.ai/seokjin/FSR-prediction/runs/f97d6a66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548331)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102155-f97d6a66/logs\n",
      "2023-07-19 10:22:58,182\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.083 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:22:58,184\tWARNING util.py:315 -- The `process_trial_result` operation took 2.086 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:22:58,198\tWARNING util.py:315 -- Processing trial results took 2.100 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:22:58,207\tWARNING util.py:315 -- The `process_trial_result` operation took 2.109 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_65daf275_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-22-23/wandb/run-20230719_102301-65daf275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb: Syncing run FSR_Trainable_65daf275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/65daf275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:                      mae 121.66207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:                     mape 38902766.88478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:                     rmse 231.13661\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:       time_since_restore 2.87505\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:         time_this_iter_s 0.59785\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:             time_total_s 2.87505\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:                timestamp 1689729780\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb:  View run FSR_Trainable_65daf275 at: https://wandb.ai/seokjin/FSR-prediction/runs/65daf275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549072)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102301-65daf275/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: \n",
      "2023-07-19 10:23:09,268\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.835 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:23:09,269\tWARNING util.py:315 -- The `process_trial_result` operation took 1.838 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:23:09,272\tWARNING util.py:315 -- Processing trial results took 1.840 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:23:09,273\tWARNING util.py:315 -- The `process_trial_result` operation took 1.842 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_73ceb852_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-22-55/wandb/run-20230719_102312-73ceb852\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Syncing run FSR_Trainable_73ceb852\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/73ceb852\n",
      "2023-07-19 10:23:20,016\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.799 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:23:20,019\tWARNING util.py:315 -- The `process_trial_result` operation took 1.803 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:23:20,023\tWARNING util.py:315 -- Processing trial results took 1.807 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:23:20,024\tWARNING util.py:315 -- The `process_trial_result` operation took 1.808 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_72d6fcf5_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-23-06/wandb/run-20230719_102323-72d6fcf5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: Syncing run FSR_Trainable_72d6fcf5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/72d6fcf5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 10:23:32,249\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.773 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:23:32,252\tWARNING util.py:315 -- The `process_trial_result` operation took 1.778 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:23:32,257\tWARNING util.py:315 -- Processing trial results took 1.782 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:23:32,258\tWARNING util.py:315 -- The `process_trial_result` operation took 1.784 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:                      mae 100.38529\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:                     mape 36530270.01673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:                     rmse 189.85472\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:       time_since_restore 55.20944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:         time_this_iter_s 0.53581\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:             time_total_s 55.20944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:                timestamp 1689729807\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb:  View run FSR_Trainable_c94a92ec at: https://wandb.ai/seokjin/FSR-prediction/runs/c94a92ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=548802)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102229-c94a92ec/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_1042a77b_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-23-17/wandb/run-20230719_102335-1042a77b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb: Syncing run FSR_Trainable_1042a77b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/1042a77b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:                      mae 109.17992\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:                     mape 34084808.14799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:                     rmse 211.09391\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:       time_since_restore 8.19376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:         time_this_iter_s 0.90569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:             time_total_s 8.19376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:                timestamp 1689729819\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb:  View run FSR_Trainable_1042a77b at: https://wandb.ai/seokjin/FSR-prediction/runs/1042a77b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549758)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102335-1042a77b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 10:23:44,465\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.861 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:23:44,467\tWARNING util.py:315 -- The `process_trial_result` operation took 1.864 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:23:44,471\tWARNING util.py:315 -- Processing trial results took 1.868 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:23:44,473\tWARNING util.py:315 -- The `process_trial_result` operation took 1.870 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102312-73ceb852/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_cfb64bff_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-23-29/wandb/run-20230719_102347-cfb64bff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb: Syncing run FSR_Trainable_cfb64bff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/cfb64bff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549308)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:                      mae 173.77702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:                     mape 77452831.11715\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:                     rmse 315.96439\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:       time_since_restore 2.03192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:         time_this_iter_s 0.93082\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:             time_total_s 2.03192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:                timestamp 1689729825\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb:  View run FSR_Trainable_cfb64bff at: https://wandb.ai/seokjin/FSR-prediction/runs/cfb64bff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102347-cfb64bff/logs\n",
      "2023-07-19 10:23:54,163\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.083 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:23:54,166\tWARNING util.py:315 -- The `process_trial_result` operation took 2.087 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:23:54,173\tWARNING util.py:315 -- Processing trial results took 2.095 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:23:54,175\tWARNING util.py:315 -- The `process_trial_result` operation took 2.096 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549991)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_ec866daa_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-23-41/wandb/run-20230719_102357-ec866daa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb: Syncing run FSR_Trainable_ec866daa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ec866daa\n",
      "2023-07-19 10:24:04,869\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.893 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:24:04,872\tWARNING util.py:315 -- The `process_trial_result` operation took 1.897 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:24:04,875\tWARNING util.py:315 -- Processing trial results took 1.899 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:24:04,882\tWARNING util.py:315 -- The `process_trial_result` operation took 1.907 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_4d2b310e_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-23-51/wandb/run-20230719_102408-4d2b310e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb: Syncing run FSR_Trainable_4d2b310e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4d2b310e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 10:24:17,731\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.458 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:24:17,735\tWARNING util.py:315 -- The `process_trial_result` operation took 2.464 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:24:17,737\tWARNING util.py:315 -- Processing trial results took 2.466 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:24:17,738\tWARNING util.py:315 -- The `process_trial_result` operation took 2.467 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:                      mae 105.54245\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:                     mape 34321161.76462\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:                     rmse 202.67221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:       time_since_restore 9.72917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:         time_this_iter_s 0.55519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:             time_total_s 9.72917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:                timestamp 1689729854\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb:  View run FSR_Trainable_4d2b310e at: https://wandb.ai/seokjin/FSR-prediction/runs/4d2b310e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550445)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102408-4d2b310e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_fb905107_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-24-02/wandb/run-20230719_102421-fb905107\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb: Syncing run FSR_Trainable_fb905107\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fb905107\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 10:24:30,710\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.926 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:24:30,713\tWARNING util.py:315 -- The `process_trial_result` operation took 1.930 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:24:30,716\tWARNING util.py:315 -- Processing trial results took 1.933 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:24:30,717\tWARNING util.py:315 -- The `process_trial_result` operation took 1.935 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:                      mae 98.1076\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:                     mape 26478590.69707\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:                     rmse 199.70762\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:       time_since_restore 57.32476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:         time_this_iter_s 0.90263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:             time_total_s 57.32476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:                timestamp 1689729866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb:  View run FSR_Trainable_72d6fcf5 at: https://wandb.ai/seokjin/FSR-prediction/runs/72d6fcf5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=549537)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102323-72d6fcf5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_3e4279dd_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-24-14/wandb/run-20230719_102433-3e4279dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: Syncing run FSR_Trainable_3e4279dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/3e4279dd\n",
      "2023-07-19 10:24:43,245\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.612 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:24:43,247\tWARNING util.py:315 -- The `process_trial_result` operation took 1.615 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:24:43,251\tWARNING util.py:315 -- Processing trial results took 1.619 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:24:43,252\tWARNING util.py:315 -- The `process_trial_result` operation took 1.620 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_b6bd78e9_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-24-27/wandb/run-20230719_102446-b6bd78e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: Syncing run FSR_Trainable_b6bd78e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b6bd78e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: \\ 0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: | 0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:                      mae 111.76307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:                     mape 42258788.67063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:                     rmse 214.82504\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:       time_since_restore 4.19851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:         time_this_iter_s 0.79531\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:             time_total_s 4.19851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:                timestamp 1689729885\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb:  View run FSR_Trainable_b6bd78e9 at: https://wandb.ai/seokjin/FSR-prediction/runs/b6bd78e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551126)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102446-b6bd78e9/logs\n",
      "2023-07-19 10:24:59,805\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.842 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:24:59,807\tWARNING util.py:315 -- The `process_trial_result` operation took 1.845 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:24:59,823\tWARNING util.py:315 -- Processing trial results took 1.861 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:24:59,826\tWARNING util.py:315 -- The `process_trial_result` operation took 1.864 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_168fed47_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-24-40/wandb/run-20230719_102503-168fed47\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb: Syncing run FSR_Trainable_168fed47\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/168fed47\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:                      mae 97.53769\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:                     mape 36656787.06499\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:                     rmse 190.3998\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:       time_since_restore 58.96219\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:         time_this_iter_s 0.57451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:             time_total_s 58.96219\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:                timestamp 1689729904\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb:  View run FSR_Trainable_ec866daa at: https://wandb.ai/seokjin/FSR-prediction/runs/ec866daa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550223)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102357-ec866daa/logs\n",
      "2023-07-19 10:25:18,190\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.018 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:25:18,192\tWARNING util.py:315 -- The `process_trial_result` operation took 2.021 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:25:18,195\tWARNING util.py:315 -- Processing trial results took 2.023 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:25:18,197\tWARNING util.py:315 -- The `process_trial_result` operation took 2.025 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_734d5dd2_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-24-56/wandb/run-20230719_102521-734d5dd2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb: Syncing run FSR_Trainable_734d5dd2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/734d5dd2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:                      mae 101.30758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:                     mape 25887474.68245\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:                     rmse 190.19069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:       time_since_restore 59.5719\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:         time_this_iter_s 0.50304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:             time_total_s 59.5719\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:                timestamp 1689729926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb:  View run FSR_Trainable_fb905107 at: https://wandb.ai/seokjin/FSR-prediction/runs/fb905107\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550667)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102421-fb905107/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551604)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb: Waiting for W&B process to finish... (success).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:       training_iteration \n",
      "2023-07-19 10:25:37,597\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.825 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:25:37,601\tWARNING util.py:315 -- The `process_trial_result` operation took 1.830 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:25:37,604\tWARNING util.py:315 -- Processing trial results took 1.832 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:25:37,606\tWARNING util.py:315 -- The `process_trial_result` operation took 1.835 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb: iterations_since_restore 32\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:                      mae 101.7736\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:                     mape 32724386.17696\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:                     rmse 201.00438\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:       time_since_restore 26.80556\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:         time_this_iter_s 0.80396\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:             time_total_s 26.80556\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:                timestamp 1689729927\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:       training_iteration 32\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb:  View run FSR_Trainable_168fed47 at: https://wandb.ai/seokjin/FSR-prediction/runs/168fed47\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551366)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102503-168fed47/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "wandb: \\ Waiting for wandb.init()...866)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_ece22011_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-25-15/wandb/run-20230719_102540-ece22011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: Syncing run FSR_Trainable_ece22011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ece22011\n",
      "2023-07-19 10:25:49,780\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.977 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:25:49,785\tWARNING util.py:315 -- The `process_trial_result` operation took 1.983 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:25:49,787\tWARNING util.py:315 -- Processing trial results took 1.985 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:25:49,789\tWARNING util.py:315 -- The `process_trial_result` operation took 1.986 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_44149a06_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-25-34/wandb/run-20230719_102551-44149a06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: Syncing run FSR_Trainable_44149a06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/44149a06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 10:25:59,470\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.041 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:25:59,474\tWARNING util.py:315 -- The `process_trial_result` operation took 2.045 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:25:59,476\tWARNING util.py:315 -- Processing trial results took 2.047 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:25:59,477\tWARNING util.py:315 -- The `process_trial_result` operation took 2.049 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:                      mae 216.35629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:                     mape 1.011957113646146e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:                     rmse 379.68046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:       time_since_restore 2.37917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:         time_this_iter_s 2.37917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:             time_total_s 2.37917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:                timestamp 1689729947\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb:  View run FSR_Trainable_44149a06 at: https://wandb.ai/seokjin/FSR-prediction/runs/44149a06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552081)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102551-44149a06/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_d676d4d4_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-25-45/wandb/run-20230719_102602-d676d4d4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: Syncing run FSR_Trainable_d676d4d4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d676d4d4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=550899)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 10:26:07,826\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.114 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:26:07,828\tWARNING util.py:315 -- The `process_trial_result` operation took 2.116 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:26:07,829\tWARNING util.py:315 -- Processing trial results took 2.117 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:26:07,830\tWARNING util.py:315 -- The `process_trial_result` operation took 2.119 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:                      mae 236.0279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:                     mape 1.1521440199819182e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:                     rmse 388.65617\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:       time_since_restore 2.32756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:         time_this_iter_s 2.32756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:             time_total_s 2.32756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:                timestamp 1689729957\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb:  View run FSR_Trainable_d676d4d4 at: https://wandb.ai/seokjin/FSR-prediction/runs/d676d4d4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102602-d676d4d4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102602-d676d4d4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102602-d676d4d4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_7cf331a6_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-25-55/wandb/run-20230719_102610-7cf331a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb: Syncing run FSR_Trainable_7cf331a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7cf331a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552299)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:                      mae 119.08148\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:                     mape 41607935.10341\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:                     rmse 215.87524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:       time_since_restore 2.93141\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:         time_this_iter_s 0.64727\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:             time_total_s 2.93141\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:                timestamp 1689729969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb:  View run FSR_Trainable_7cf331a6 at: https://wandb.ai/seokjin/FSR-prediction/runs/7cf331a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102610-7cf331a6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552532)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 10:26:17,865\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.077 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:26:17,867\tWARNING util.py:315 -- The `process_trial_result` operation took 2.080 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:26:17,869\tWARNING util.py:315 -- Processing trial results took 2.082 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:26:17,871\tWARNING util.py:315 -- The `process_trial_result` operation took 2.084 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_ffed4354_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-26-04/wandb/run-20230719_102620-ffed4354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb: Syncing run FSR_Trainable_ffed4354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ffed4354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:                      mae 112.10623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:                     mape 45332726.33567\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:                     rmse 207.8524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:       time_since_restore 5.02855\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:         time_this_iter_s 0.55757\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:             time_total_s 5.02855\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:                timestamp 1689729982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb:  View run FSR_Trainable_ffed4354 at: https://wandb.ai/seokjin/FSR-prediction/runs/ffed4354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552766)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102620-ffed4354/logs\n",
      "2023-07-19 10:26:28,052\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.088 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:26:28,055\tWARNING util.py:315 -- The `process_trial_result` operation took 2.091 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:26:28,058\tWARNING util.py:315 -- Processing trial results took 2.095 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:26:28,060\tWARNING util.py:315 -- The `process_trial_result` operation took 2.096 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_f04af01c_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-26-14/wandb/run-20230719_102631-f04af01c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: Syncing run FSR_Trainable_f04af01c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f04af01c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:                      mae 114.63824\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:                     mape 35795689.37103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:                     rmse 214.13437\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:       time_since_restore 2.62756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:         time_this_iter_s 0.57256\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:             time_total_s 2.62756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:                timestamp 1689729989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb:  View run FSR_Trainable_f04af01c at: https://wandb.ai/seokjin/FSR-prediction/runs/f04af01c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=552995)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102631-f04af01c/logs\n",
      "2023-07-19 10:26:37,851\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.737 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:26:37,855\tWARNING util.py:315 -- The `process_trial_result` operation took 1.741 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:26:37,856\tWARNING util.py:315 -- Processing trial results took 1.742 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:26:37,857\tWARNING util.py:315 -- The `process_trial_result` operation took 1.743 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_a64dc20d_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-26-25/wandb/run-20230719_102640-a64dc20d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Syncing run FSR_Trainable_a64dc20d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a64dc20d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102640-a64dc20d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102640-a64dc20d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102640-a64dc20d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102640-a64dc20d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102640-a64dc20d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102640-a64dc20d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102640-a64dc20d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102640-a64dc20d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102640-a64dc20d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102640-a64dc20d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102640-a64dc20d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102640-a64dc20d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102640-a64dc20d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553218)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102640-a64dc20d/logs\n",
      "2023-07-19 10:26:50,510\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.792 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:26:50,512\tWARNING util.py:315 -- The `process_trial_result` operation took 1.795 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:26:50,515\tWARNING util.py:315 -- Processing trial results took 1.798 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:26:50,517\tWARNING util.py:315 -- The `process_trial_result` operation took 1.800 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_8e36da61_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-26-35/wandb/run-20230719_102650-8e36da61\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: Syncing run FSR_Trainable_8e36da61\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8e36da61\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 10:26:56,342\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.994 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:26:56,344\tWARNING util.py:315 -- The `process_trial_result` operation took 1.997 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:26:56,345\tWARNING util.py:315 -- Processing trial results took 1.999 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:26:56,347\tWARNING util.py:315 -- The `process_trial_result` operation took 2.000 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:                      mae 169.6346\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:                     mape 46859399.75212\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:                     rmse 330.17625\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:       time_since_restore 4.32085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:         time_this_iter_s 4.32085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:             time_total_s 4.32085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:                timestamp 1689730008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb:  View run FSR_Trainable_8e36da61 at: https://wandb.ai/seokjin/FSR-prediction/runs/8e36da61\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553438)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102650-8e36da61/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...656)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_276c1961_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-26-44/wandb/run-20230719_102659-276c1961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: Syncing run FSR_Trainable_276c1961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/276c1961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:                      mae 119.32834\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:                     mape 42507392.61571\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:                     rmse 215.27777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:       time_since_restore 3.34939\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:         time_this_iter_s 0.74791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:             time_total_s 3.34939\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:                timestamp 1689730018\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb:  View run FSR_Trainable_276c1961 at: https://wandb.ai/seokjin/FSR-prediction/runs/276c1961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553656)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102659-276c1961/logs\n",
      "2023-07-19 10:27:06,033\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.781 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:27:06,035\tWARNING util.py:315 -- The `process_trial_result` operation took 1.785 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:27:06,038\tWARNING util.py:315 -- Processing trial results took 1.787 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:27:06,039\tWARNING util.py:315 -- The `process_trial_result` operation took 1.789 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102540-ece22011/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_52f6ac72_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-26-53/wandb/run-20230719_102708-52f6ac72\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb: Syncing run FSR_Trainable_52f6ac72\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/52f6ac72\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=551866)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:                      mae 184.62032\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:                     mape 49204586.04399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:                     rmse 332.99256\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:       time_since_restore 1.02115\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:         time_this_iter_s 1.02115\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:             time_total_s 1.02115\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:                timestamp 1689730024\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb:  View run FSR_Trainable_52f6ac72 at: https://wandb.ai/seokjin/FSR-prediction/runs/52f6ac72\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102708-52f6ac72/logs\n",
      "2023-07-19 10:27:15,215\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.842 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:27:15,221\tWARNING util.py:315 -- The `process_trial_result` operation took 1.848 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:27:15,224\tWARNING util.py:315 -- Processing trial results took 1.851 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:27:15,225\tWARNING util.py:315 -- The `process_trial_result` operation took 1.852 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=553891)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_90e6ac13_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-27-03/wandb/run-20230719_102716-90e6ac13\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb: Syncing run FSR_Trainable_90e6ac13\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/90e6ac13\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 10:27:20,793\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.745 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:27:20,794\tWARNING util.py:315 -- The `process_trial_result` operation took 1.747 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:27:20,797\tWARNING util.py:315 -- Processing trial results took 1.750 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:27:20,798\tWARNING util.py:315 -- The `process_trial_result` operation took 1.750 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:                      mae 181.50451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:                     mape 2.429115119599393e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:                     rmse 340.92452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:       time_since_restore 1.91954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:         time_this_iter_s 1.91954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:             time_total_s 1.91954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:                timestamp 1689730033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb:  View run FSR_Trainable_90e6ac13 at: https://wandb.ai/seokjin/FSR-prediction/runs/90e6ac13\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102716-90e6ac13/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554120)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_e1fcce58_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-27-11/wandb/run-20230719_102723-e1fcce58\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb: Syncing run FSR_Trainable_e1fcce58\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e1fcce58\n",
      "2023-07-19 10:27:30,182\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.173 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:27:30,186\tWARNING util.py:315 -- The `process_trial_result` operation took 2.178 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:27:30,191\tWARNING util.py:315 -- Processing trial results took 2.184 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:27:30,192\tWARNING util.py:315 -- The `process_trial_result` operation took 2.185 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_1870f879_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-27-18/wandb/run-20230719_102733-1870f879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: Syncing run FSR_Trainable_1870f879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/1870f879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 10:27:37,382\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.051 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:27:37,386\tWARNING util.py:315 -- The `process_trial_result` operation took 2.055 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:27:37,388\tWARNING util.py:315 -- Processing trial results took 2.057 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:27:37,388\tWARNING util.py:315 -- The `process_trial_result` operation took 2.058 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:                      mae 151.76969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:                     mape 40779955.73692\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:                     rmse 304.91193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:       time_since_restore 1.48483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:         time_this_iter_s 0.73233\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:             time_total_s 1.48483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:                timestamp 1689730050\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb:  View run FSR_Trainable_1870f879 at: https://wandb.ai/seokjin/FSR-prediction/runs/1870f879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554523)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102733-1870f879/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_999096be_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-27-27/wandb/run-20230719_102740-999096be\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: Syncing run FSR_Trainable_999096be\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/999096be\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554302)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102740-999096be/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102740-999096be/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102740-999096be/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102740-999096be/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102740-999096be/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102740-999096be/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: iterations_since_restore 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb:                      mae 125.87871\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb:                     mape 49795790.98976\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb:                     rmse 231.83773\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb:       time_since_restore 1.56969\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb:         time_this_iter_s 0.75374\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb:             time_total_s 1.56969\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb:                timestamp 1689730058\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb:       training_iteration 2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb:  View run FSR_Trainable_999096be at: https://wandb.ai/seokjin/FSR-prediction/runs/999096be\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102740-999096be/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2023-07-19 10:27:46,126\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.813 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:27:46,128\tWARNING util.py:315 -- The `process_trial_result` operation took 1.815 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:27:46,130\tWARNING util.py:315 -- Processing trial results took 1.818 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:27:46,133\tWARNING util.py:315 -- The `process_trial_result` operation took 1.821 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554701)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_b9aeaa9e_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-27-34/wandb/run-20230719_102748-b9aeaa9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Syncing run FSR_Trainable_b9aeaa9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b9aeaa9e\n",
      "2023-07-19 10:27:55,516\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.818 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:27:55,520\tWARNING util.py:315 -- The `process_trial_result` operation took 1.823 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:27:55,521\tWARNING util.py:315 -- Processing trial results took 1.824 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:27:55,522\tWARNING util.py:315 -- The `process_trial_result` operation took 1.825 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_d41a892c_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-27-43/wandb/run-20230719_102758-d41a892c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb: Syncing run FSR_Trainable_d41a892c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d41a892c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:                      mae 240.53901\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:                     mape 173325347.2508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:                     rmse 395.02731\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:       time_since_restore 0.9261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:         time_this_iter_s 0.9261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:             time_total_s 0.9261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:                timestamp 1689730073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb:  View run FSR_Trainable_d41a892c at: https://wandb.ai/seokjin/FSR-prediction/runs/d41a892c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555159)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102758-d41a892c/logs\n",
      "2023-07-19 10:28:05,307\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.934 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:28:05,309\tWARNING util.py:315 -- The `process_trial_result` operation took 1.936 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:28:05,312\tWARNING util.py:315 -- Processing trial results took 1.939 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:28:05,313\tWARNING util.py:315 -- The `process_trial_result` operation took 1.941 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_bdc4ad54_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-27-52/wandb/run-20230719_102807-bdc4ad54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: Syncing run FSR_Trainable_bdc4ad54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/bdc4ad54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:                      mae 228.63437\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:                     mape 150996004.43554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:                     rmse 377.94164\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:       time_since_restore 1.0249\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:         time_this_iter_s 1.0249\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:             time_total_s 1.0249\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:                timestamp 1689730083\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb:  View run FSR_Trainable_bdc4ad54 at: https://wandb.ai/seokjin/FSR-prediction/runs/bdc4ad54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555379)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102807-bdc4ad54/logs\n",
      "2023-07-19 10:28:16,325\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.906 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:28:16,330\tWARNING util.py:315 -- The `process_trial_result` operation took 1.912 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:28:16,332\tWARNING util.py:315 -- Processing trial results took 1.914 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:28:16,334\tWARNING util.py:315 -- The `process_trial_result` operation took 1.916 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_9e9a8c9b_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-28-02/wandb/run-20230719_102818-9e9a8c9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb: Syncing run FSR_Trainable_9e9a8c9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9e9a8c9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_e9a1363c_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-28-11/wandb/run-20230719_102828-e9a1363c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb: Syncing run FSR_Trainable_e9a1363c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e9a1363c\n",
      "2023-07-19 10:28:34,852\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.940 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:28:34,856\tWARNING util.py:315 -- The `process_trial_result` operation took 1.945 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:28:34,859\tWARNING util.py:315 -- Processing trial results took 1.948 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:28:34,861\tWARNING util.py:315 -- The `process_trial_result` operation took 1.950 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb: - 0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb: \\ 0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:                      mae 209.0423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:                     mape 96428940.36688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:                     rmse 380.69292\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:       time_since_restore 11.36953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:         time_this_iter_s 11.36953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:             time_total_s 11.36953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:                timestamp 1689730112\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb:  View run FSR_Trainable_e9a1363c at: https://wandb.ai/seokjin/FSR-prediction/runs/e9a1363c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555823)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102828-e9a1363c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_4f7583e2_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-28-21/wandb/run-20230719_102839-4f7583e2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb: Syncing run FSR_Trainable_4f7583e2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4f7583e2\n",
      "2023-07-19 10:28:42,861\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.642 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:28:42,866\tWARNING util.py:315 -- The `process_trial_result` operation took 1.647 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:28:42,868\tWARNING util.py:315 -- Processing trial results took 1.649 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:28:42,869\tWARNING util.py:315 -- The `process_trial_result` operation took 1.650 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:28:50,503\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.939 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:28:50,507\tWARNING util.py:315 -- The `process_trial_result` operation took 1.945 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:28:50,508\tWARNING util.py:315 -- Processing trial results took 1.946 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:28:50,509\tWARNING util.py:315 -- The `process_trial_result` operation took 1.947 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_d6458403_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-28-32/wandb/run-20230719_102852-d6458403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb: Syncing run FSR_Trainable_d6458403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d6458403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:                      mae 145.38192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:                     mape 70872539.3632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:                     rmse 265.23733\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:       time_since_restore 16.54789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:         time_this_iter_s 7.82714\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:             time_total_s 16.54789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:                timestamp 1689730130\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb:  View run FSR_Trainable_4f7583e2 at: https://wandb.ai/seokjin/FSR-prediction/runs/4f7583e2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556027)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102839-4f7583e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb:       training_iteration \n",
      "2023-07-19 10:29:04,527\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.341 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:29:04,532\tWARNING util.py:315 -- The `process_trial_result` operation took 2.347 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:29:04,532\tWARNING util.py:315 -- Processing trial results took 2.348 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:29:04,533\tWARNING util.py:315 -- The `process_trial_result` operation took 2.349 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=554935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_8b356e81_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-28-45/wandb/run-20230719_102908-8b356e81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb: Syncing run FSR_Trainable_8b356e81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8b356e81\n",
      "2023-07-19 10:29:17,796\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.808 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:29:17,799\tWARNING util.py:315 -- The `process_trial_result` operation took 1.812 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:29:17,800\tWARNING util.py:315 -- Processing trial results took 1.813 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:29:17,801\tWARNING util.py:315 -- The `process_trial_result` operation took 1.814 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_ef63f3e9_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-29-01/wandb/run-20230719_102921-ef63f3e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: Syncing run FSR_Trainable_ef63f3e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ef63f3e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:                      mae 123.25377\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:                     mape 1.8262465120120326e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:                     rmse 221.07351\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:       time_since_restore 2.49535\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:         time_this_iter_s 1.17801\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:             time_total_s 2.49535\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:                timestamp 1689730158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb:  View run FSR_Trainable_ef63f3e9 at: https://wandb.ai/seokjin/FSR-prediction/runs/ef63f3e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556720)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102921-ef63f3e9/logs\n",
      "2023-07-19 10:29:34,037\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.794 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:29:34,040\tWARNING util.py:315 -- The `process_trial_result` operation took 1.798 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:29:34,041\tWARNING util.py:315 -- Processing trial results took 1.800 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:29:34,043\tWARNING util.py:315 -- The `process_trial_result` operation took 1.801 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_87828ea3_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-29-14/wandb/run-20230719_102937-87828ea3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb: Syncing run FSR_Trainable_87828ea3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/87828ea3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:                      mae 112.81533\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:                     mape 1.1682250327067294e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:                     rmse 204.51664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:       time_since_restore 16.1139\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:         time_this_iter_s 0.96073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:             time_total_s 16.1139\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:                timestamp 1689730189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb:  View run FSR_Trainable_87828ea3 at: https://wandb.ai/seokjin/FSR-prediction/runs/87828ea3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556950)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102937-87828ea3/logs\n",
      "2023-07-19 10:30:04,731\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.560 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:30:04,734\tWARNING util.py:315 -- The `process_trial_result` operation took 2.565 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:30:04,735\tWARNING util.py:315 -- Processing trial results took 2.565 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:30:04,736\tWARNING util.py:315 -- The `process_trial_result` operation took 2.566 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_04550823_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-29-30/wandb/run-20230719_103008-04550823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb: Syncing run FSR_Trainable_04550823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/04550823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:                      mae 112.42097\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:                     mape 57222060.29655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:                     rmse 205.80363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:       time_since_restore 5.73553\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:         time_this_iter_s 0.7181\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:             time_total_s 5.73553\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:                timestamp 1689730209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb:  View run FSR_Trainable_04550823 at: https://wandb.ai/seokjin/FSR-prediction/runs/04550823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557193)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_103008-04550823/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 10:30:24,802\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.472 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:30:24,805\tWARNING util.py:315 -- The `process_trial_result` operation took 2.476 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:30:24,806\tWARNING util.py:315 -- Processing trial results took 2.477 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:30:24,807\tWARNING util.py:315 -- The `process_trial_result` operation took 2.478 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:                      mae 96.43397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:                     mape 8.11254599995656e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:                     rmse 184.29147\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:       time_since_restore 68.65158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:         time_this_iter_s 0.635\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:             time_total_s 68.65158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:                timestamp 1689730221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb:  View run FSR_Trainable_8b356e81 at: https://wandb.ai/seokjin/FSR-prediction/runs/8b356e81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102908-8b356e81/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_966fc53a_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-30-01/wandb/run-20230719_103028-966fc53a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: Syncing run FSR_Trainable_966fc53a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/966fc53a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:                      mae 119.90531\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:                     mape 49706188.43063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:                     rmse 220.48523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:       time_since_restore 1.78873\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:         time_this_iter_s 0.87149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:             time_total_s 1.78873\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:                timestamp 1689730225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb:  View run FSR_Trainable_966fc53a at: https://wandb.ai/seokjin/FSR-prediction/runs/966fc53a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557428)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_103028-966fc53a/logs\n",
      "2023-07-19 10:30:37,251\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.094 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:30:37,253\tWARNING util.py:315 -- The `process_trial_result` operation took 2.096 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:30:37,256\tWARNING util.py:315 -- Processing trial results took 2.100 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:30:37,258\tWARNING util.py:315 -- The `process_trial_result` operation took 2.101 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_cd929861_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-30-21/wandb/run-20230719_103040-cd929861\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: Syncing run FSR_Trainable_cd929861\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/cd929861\n",
      "2023-07-19 10:30:50,323\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.102 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:30:50,325\tWARNING util.py:315 -- The `process_trial_result` operation took 2.104 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:30:50,326\tWARNING util.py:315 -- Processing trial results took 2.105 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:30:50,326\tWARNING util.py:315 -- The `process_trial_result` operation took 2.106 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_7b92b6a6_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-30-34/wandb/run-20230719_103053-7b92b6a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: Syncing run FSR_Trainable_7b92b6a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7b92b6a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:                      mae 99.71736\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:                     mape 27805847.27737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:                     rmse 202.34909\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:       time_since_restore 143.26793\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:         time_this_iter_s 2.66432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:             time_total_s 143.26793\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:                timestamp 1689730253\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb:  View run FSR_Trainable_9e9a8c9b at: https://wandb.ai/seokjin/FSR-prediction/runs/9e9a8c9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=555603)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102818-9e9a8c9b/logs\n",
      "2023-07-19 10:31:08,399\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.941 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:31:08,402\tWARNING util.py:315 -- The `process_trial_result` operation took 1.945 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:31:08,404\tWARNING util.py:315 -- Processing trial results took 1.946 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:31:08,405\tWARNING util.py:315 -- The `process_trial_result` operation took 1.948 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_133b3320_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-30-47/wandb/run-20230719_103112-133b3320\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb: Syncing run FSR_Trainable_133b3320\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/133b3320\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:                      mae 100.44395\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:                     mape 27659420.17741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:                     rmse 201.88034\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:       time_since_restore 144.43858\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:         time_this_iter_s 2.18106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:             time_total_s 144.43858\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:                timestamp 1689730286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb:  View run FSR_Trainable_d6458403 at: https://wandb.ai/seokjin/FSR-prediction/runs/d6458403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=556262)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_102852-d6458403/logs\n",
      "2023-07-19 10:31:40,886\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.995 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:31:40,891\tWARNING util.py:315 -- The `process_trial_result` operation took 2.001 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:31:40,892\tWARNING util.py:315 -- Processing trial results took 2.002 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:31:40,893\tWARNING util.py:315 -- The `process_trial_result` operation took 2.003 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_cb86380a_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-31-05/wandb/run-20230719_103144-cb86380a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb: Syncing run FSR_Trainable_cb86380a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/cb86380a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:                      mae 101.75859\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:                     mape 1.0518768999351819e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:                     rmse 181.07293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:       time_since_restore 67.78956\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:         time_this_iter_s 0.65979\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:             time_total_s 67.78956\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:                timestamp 1689730312\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb:  View run FSR_Trainable_cd929861 at: https://wandb.ai/seokjin/FSR-prediction/runs/cd929861\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557658)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_103040-cd929861/logs\n",
      "2023-07-19 10:32:06,997\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.873 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:32:06,999\tWARNING util.py:315 -- The `process_trial_result` operation took 1.877 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:32:07,000\tWARNING util.py:315 -- Processing trial results took 1.878 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:32:07,002\tWARNING util.py:315 -- The `process_trial_result` operation took 1.880 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_87af99ee_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-31-38/wandb/run-20230719_103210-87af99ee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: Syncing run FSR_Trainable_87af99ee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/87af99ee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:                      mae 254.58987\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:                     mape 3.8919061031778963e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:                     rmse 393.63523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:       time_since_restore 1.13525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:         time_this_iter_s 1.13525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:             time_total_s 1.13525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:                timestamp 1689730325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb:  View run FSR_Trainable_87af99ee at: https://wandb.ai/seokjin/FSR-prediction/runs/87af99ee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558609)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_103210-87af99ee/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 10:32:22,737\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.223 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:32:22,743\tWARNING util.py:315 -- The `process_trial_result` operation took 2.229 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:32:22,745\tWARNING util.py:315 -- Processing trial results took 2.232 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:32:22,747\tWARNING util.py:315 -- The `process_trial_result` operation took 2.234 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:                      mae 93.88375\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:                     mape 7.54612112647505e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:                     rmse 180.62565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:       time_since_restore 82.13784\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:         time_this_iter_s 1.05363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:             time_total_s 82.13784\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:                timestamp 1689730340\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb:  View run FSR_Trainable_7b92b6a6 at: https://wandb.ai/seokjin/FSR-prediction/runs/7b92b6a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=557879)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_103053-7b92b6a6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_c3f9a74e_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-32-04/wandb/run-20230719_103225-c3f9a74e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: Syncing run FSR_Trainable_c3f9a74e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c3f9a74e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:                      mae 333.0888\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:                     mape 4.8513081576527974e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:                     rmse 561.13626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:       time_since_restore 1.17752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:         time_this_iter_s 1.17752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:             time_total_s 1.17752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:                timestamp 1689730340\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb:  View run FSR_Trainable_c3f9a74e at: https://wandb.ai/seokjin/FSR-prediction/runs/c3f9a74e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558842)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_103225-c3f9a74e/logs\n",
      "2023-07-19 10:32:34,407\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.118 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:32:34,410\tWARNING util.py:315 -- The `process_trial_result` operation took 2.122 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:32:34,412\tWARNING util.py:315 -- Processing trial results took 2.123 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:32:34,413\tWARNING util.py:315 -- The `process_trial_result` operation took 2.125 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_77c94b7f_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-32-19/wandb/run-20230719_103237-77c94b7f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb: Syncing run FSR_Trainable_77c94b7f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/77c94b7f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-07-19 10:32:47,007\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.934 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:32:47,010\tWARNING util.py:315 -- The `process_trial_result` operation took 1.937 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:32:47,011\tWARNING util.py:315 -- Processing trial results took 1.939 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:32:47,012\tWARNING util.py:315 -- The `process_trial_result` operation took 1.940 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:                      mae 95.34809\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:                     mape 8.831093019106966e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:                     rmse 180.33789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:       time_since_restore 82.41499\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:         time_this_iter_s 0.75321\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:             time_total_s 82.41499\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:                timestamp 1689730361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb:  View run FSR_Trainable_133b3320 at: https://wandb.ai/seokjin/FSR-prediction/runs/133b3320\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558113)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_103112-133b3320/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_4ea2105a_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-32-31/wandb/run-20230719_103250-4ea2105a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb: Syncing run FSR_Trainable_4ea2105a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4ea2105a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-07-19 10:33:00,135\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.832 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:33:00,137\tWARNING util.py:315 -- The `process_trial_result` operation took 1.835 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:33:00,138\tWARNING util.py:315 -- Processing trial results took 1.837 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:33:00,140\tWARNING util.py:315 -- The `process_trial_result` operation took 1.838 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:                      mae 96.79202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:                     mape 9.527100925184651e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:                     rmse 175.38349\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:       time_since_restore 66.09122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:         time_this_iter_s 0.86042\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:             time_total_s 66.09122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:                timestamp 1689730378\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb:  View run FSR_Trainable_cb86380a at: https://wandb.ai/seokjin/FSR-prediction/runs/cb86380a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=558363)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_103144-cb86380a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_96c49e29_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-32-44/wandb/run-20230719_103303-96c49e29\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: Syncing run FSR_Trainable_96c49e29\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/96c49e29\n",
      "2023-07-19 10:33:14,573\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.807 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:33:14,575\tWARNING util.py:315 -- The `process_trial_result` operation took 1.810 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:33:14,578\tWARNING util.py:315 -- Processing trial results took 1.813 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:33:14,580\tWARNING util.py:315 -- The `process_trial_result` operation took 1.815 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_93b20787_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-32-57/wandb/run-20230719_103318-93b20787\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb: Syncing run FSR_Trainable_93b20787\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/93b20787\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:                      mae 94.23823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:                     mape 8.807867112255685e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:                     rmse 178.30428\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:       time_since_restore 66.54046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:         time_this_iter_s 0.67621\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:             time_total_s 66.54046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:                timestamp 1689730429\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb:  View run FSR_Trainable_77c94b7f at: https://wandb.ai/seokjin/FSR-prediction/runs/77c94b7f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559080)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_103237-77c94b7f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:                      mae 98.16975\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:                     mape 9.248900701903326e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:                     rmse 181.61614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:       time_since_restore 65.23679\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:         time_this_iter_s 0.62144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:             time_total_s 65.23679\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:                timestamp 1689730438\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb:  View run FSR_Trainable_4ea2105a at: https://wandb.ai/seokjin/FSR-prediction/runs/4ea2105a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559304)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_103250-4ea2105a/logs\n",
      "2023-07-19 10:34:03,355\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.219 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:34:03,356\tWARNING util.py:315 -- The `process_trial_result` operation took 2.222 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:34:03,359\tWARNING util.py:315 -- Processing trial results took 2.224 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:34:03,360\tWARNING util.py:315 -- The `process_trial_result` operation took 2.225 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_861548a0_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index_2023-07-19_10-33-11/wandb/run-20230719_103407-861548a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: Syncing run FSR_Trainable_861548a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/861548a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 10:34:17,497\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.996 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:34:17,499\tWARNING util.py:315 -- The `process_trial_result` operation took 1.999 s, which may be a performance bottleneck.\n",
      "2023-07-19 10:34:17,500\tWARNING util.py:315 -- Processing trial results took 1.999 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 10:34:17,501\tWARNING util.py:315 -- The `process_trial_result` operation took 2.000 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:                      mae 97.818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:                     mape 9.273316517614691e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:                     rmse 181.83237\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:       time_since_restore 67.05392\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:         time_this_iter_s 0.59238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:             time_total_s 67.05392\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:                timestamp 1689730453\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb:  View run FSR_Trainable_96c49e29 at: https://wandb.ai/seokjin/FSR-prediction/runs/96c49e29\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559532)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_103303-96c49e29/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_bca622bf_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,inde_2023-07-19_10-34-00/wandb/run-20230719_103420-bca622bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb: Syncing run FSR_Trainable_bca622bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/bca622bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:                      mae 95.83398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:                     mape 8.530514445850264e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:                     rmse 181.76119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:       time_since_restore 65.11577\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:         time_this_iter_s 0.5755\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:             time_total_s 65.11577\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:                timestamp 1689730466\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb:  View run FSR_Trainable_93b20787 at: https://wandb.ai/seokjin/FSR-prediction/runs/93b20787\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=559764)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_103318-93b20787/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:                      mae 102.70651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:                     mape 1.0935293803517069e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:                     rmse 186.33856\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:       time_since_restore 50.48275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:         time_this_iter_s 0.52822\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:             time_total_s 50.48275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:                timestamp 1689730497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb:  View run FSR_Trainable_861548a0 at: https://wandb.ai/seokjin/FSR-prediction/runs/861548a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560051)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_103407-861548a0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:                      mae 100.85106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:                     mape 1.0319394583994859e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:                     rmse 183.53259\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:       time_since_restore 50.09868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:         time_this_iter_s 0.38837\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:             time_total_s 50.09868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:                timestamp 1689730508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb:  View run FSR_Trainable_bca622bf at: https://wandb.ai/seokjin/FSR-prediction/runs/bca622bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=560279)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_103420-bca622bf/logs\n",
      "2023-07-19 10:35:12,507\tERROR tune.py:1107 -- Trials did not complete: [FSR_Trainable_2a147f32]\n",
      "2023-07-19 10:35:12,508\tINFO tune.py:1111 -- Total run time: 1819.78 seconds (1815.83 seconds for the tuning loop).\n",
      "2023-07-19 10:35:12,740\tWARNING experiment_analysis.py:910 -- Failed to read the results for 1 trials:\n",
      "- /home/seokj/ray_results/FSR_Trainable_2023-07-19_10-04-48/FSR_Trainable_2a147f32_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,index_X=FSR_for_force,index__2023-07-19_10-08-30\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
