{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1_CNN_LSTM\n",
    "\n",
    "Index_X = FSR_for_force, FSR_for_coord\n",
    "\n",
    "Index_y = force, x_coord, y_coord\n",
    "\n",
    "Data = Splited by Time\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-07-18_22-57-54/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-07-18_22-57-54\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "97.911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.CNN_LSTM'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    imputer = trial.suggest_categorical('imputer', ['sklearn.impute.SimpleImputer'])\n",
    "    if imputer == 'sklearn.impute.SimpleImputer':\n",
    "        trial.suggest_categorical('imputer_args/strategy', [\n",
    "            'mean',\n",
    "            'median',\n",
    "        ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': ['FSR_for_force', 'FSR_for_coord'],\n",
    "        'index_y': ['force', 'x_coord', 'y_coord'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_time'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-18 22:57:54,612] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:57:56,745\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "2023-07-18 22:57:57,831\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-18 23:39:47</td></tr>\n",
       "<tr><td>Running for: </td><td>00:41:49.17        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.6/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=100<br>Bracket: Iter 64.000: -113.75521335908682 | Iter 32.000: -117.63608687415093 | Iter 16.000: -127.08140949368126 | Iter 8.000: -135.6114098224928 | Iter 4.000: -157.50163058138094 | Iter 2.000: -200.54621780367265 | Iter 1.000: -251.98073532124522<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>imputer             </th><th>imputer_args/strateg\n",
       "y       </th><th>index_X             </th><th>index_y             </th><th>model             </th><th style=\"text-align: right;\">    model_args/cnn_hidde\n",
       "n_size</th><th style=\"text-align: right;\">  model_args/cnn_num_l\n",
       "ayer</th><th style=\"text-align: right;\">    model_args/lstm_hidd\n",
       "en_size</th><th style=\"text-align: right;\">  model_args/lstm_num_\n",
       "layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">       mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_9fe93f5e</td><td>TERMINATED</td><td>172.26.215.93:252679</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_2840</td><td>[&#x27;force&#x27;, &#x27;x_co_28c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000145264</td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        37.7601 </td><td style=\"text-align: right;\">285.706 </td><td style=\"text-align: right;\"> 79.9957</td><td style=\"text-align: right;\">2.05553e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_a7398582</td><td>TERMINATED</td><td>172.26.215.93:252743</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_3140</td><td>[&#x27;force&#x27;, &#x27;x_co_4c40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0888229  </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       173.559  </td><td style=\"text-align: right;\">125.387 </td><td style=\"text-align: right;\"> 38.1562</td><td style=\"text-align: right;\">2.62672e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_4b419ffa</td><td>TERMINATED</td><td>172.26.215.93:252921</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_dec0</td><td>[&#x27;force&#x27;, &#x27;x_co_5b00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">8</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.17481e-05</td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        35.7283 </td><td style=\"text-align: right;\">272.943 </td><td style=\"text-align: right;\"> 78.9946</td><td style=\"text-align: right;\">7.38729e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_90b14738</td><td>TERMINATED</td><td>172.26.215.93:253095</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_1a00</td><td>[&#x27;force&#x27;, &#x27;x_co_0fc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000131945</td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.54146</td><td style=\"text-align: right;\">338.683 </td><td style=\"text-align: right;\">126.824 </td><td style=\"text-align: right;\">1.7319e+17 </td></tr>\n",
       "<tr><td>FSR_Trainable_a24f6e3c</td><td>TERMINATED</td><td>172.26.215.93:253431</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_e040</td><td>[&#x27;force&#x27;, &#x27;x_co_efc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.86356e-05</td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.53862</td><td style=\"text-align: right;\">326.228 </td><td style=\"text-align: right;\">125.068 </td><td style=\"text-align: right;\">1.72319e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_8d3faa09</td><td>TERMINATED</td><td>172.26.215.93:253625</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_0940</td><td>[&#x27;force&#x27;, &#x27;x_co_c900</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        8.81541e-05</td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        59.6949 </td><td style=\"text-align: right;\">262.553 </td><td style=\"text-align: right;\"> 88.3501</td><td style=\"text-align: right;\">1.40399e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_d51dd6ac</td><td>TERMINATED</td><td>172.26.215.93:253860</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_6f40</td><td>[&#x27;force&#x27;, &#x27;x_co_15c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00706728 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.6251 </td><td style=\"text-align: right;\">287.891 </td><td style=\"text-align: right;\"> 81.3362</td><td style=\"text-align: right;\">6.13812e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_f74e668e</td><td>TERMINATED</td><td>172.26.215.93:254067</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_0700</td><td>[&#x27;force&#x27;, &#x27;x_co_0fc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0650942  </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        23.9141 </td><td style=\"text-align: right;\">260.941 </td><td style=\"text-align: right;\"> 88.3416</td><td style=\"text-align: right;\">2.82474e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_d8007d6c</td><td>TERMINATED</td><td>172.26.215.93:254332</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_7980</td><td>[&#x27;force&#x27;, &#x27;x_co_5e00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0307242  </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        13.1614 </td><td style=\"text-align: right;\">266.449 </td><td style=\"text-align: right;\"> 81.1787</td><td style=\"text-align: right;\">2.2596e+08 </td></tr>\n",
       "<tr><td>FSR_Trainable_2e4dba12</td><td>TERMINATED</td><td>172.26.215.93:254581</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_5800</td><td>[&#x27;force&#x27;, &#x27;x_co_4280</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00314685 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">        94.6973 </td><td style=\"text-align: right;\">142.482 </td><td style=\"text-align: right;\"> 42.309 </td><td style=\"text-align: right;\">2.62402e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_064adcc1</td><td>TERMINATED</td><td>172.26.215.93:254768</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_f600</td><td>[&#x27;force&#x27;, &#x27;x_co_6f00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.025011   </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        16.3863 </td><td style=\"text-align: right;\">259.905 </td><td style=\"text-align: right;\"> 89.673 </td><td style=\"text-align: right;\">1.47741e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_cdff7ec2</td><td>TERMINATED</td><td>172.26.215.93:254995</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_2c00</td><td>[&#x27;force&#x27;, &#x27;x_co_1dc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00208441 </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         9.14809</td><td style=\"text-align: right;\">261.833 </td><td style=\"text-align: right;\"> 87.3424</td><td style=\"text-align: right;\">2.95309e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_fc918be6</td><td>TERMINATED</td><td>172.26.215.93:255274</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_7400</td><td>[&#x27;force&#x27;, &#x27;x_co_6c40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00373204 </td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        26.4492 </td><td style=\"text-align: right;\">266.981 </td><td style=\"text-align: right;\"> 80.8627</td><td style=\"text-align: right;\">1.80737e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_25519ac6</td><td>TERMINATED</td><td>172.26.215.93:255460</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_2200</td><td>[&#x27;force&#x27;, &#x27;x_co_93c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0142595  </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.52168</td><td style=\"text-align: right;\">438.717 </td><td style=\"text-align: right;\">156.032 </td><td style=\"text-align: right;\">1.6992e+17 </td></tr>\n",
       "<tr><td>FSR_Trainable_b900f871</td><td>TERMINATED</td><td>172.26.215.93:255709</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_f080</td><td>[&#x27;force&#x27;, &#x27;x_co_2380</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0107266  </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.54926</td><td style=\"text-align: right;\">342.017 </td><td style=\"text-align: right;\">112.374 </td><td style=\"text-align: right;\">9.87011e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_c0961f31</td><td>TERMINATED</td><td>172.26.215.93:255932</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_8f80</td><td>[&#x27;force&#x27;, &#x27;x_co_c500</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0595635  </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        30.837  </td><td style=\"text-align: right;\">225.774 </td><td style=\"text-align: right;\"> 63.5424</td><td style=\"text-align: right;\">1.52743e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_7e1f0626</td><td>TERMINATED</td><td>172.26.215.93:256149</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_6a00</td><td>[&#x27;force&#x27;, &#x27;x_co_1800</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0793467  </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        27.5943 </td><td style=\"text-align: right;\">259.131 </td><td style=\"text-align: right;\"> 82.1034</td><td style=\"text-align: right;\">2.77263e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_64598154</td><td>TERMINATED</td><td>172.26.215.93:256370</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_cd00</td><td>[&#x27;force&#x27;, &#x27;x_co_3f00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0801582  </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.84695</td><td style=\"text-align: right;\">270.958 </td><td style=\"text-align: right;\"> 85.515 </td><td style=\"text-align: right;\">3.22279e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_1a311da9</td><td>TERMINATED</td><td>172.26.215.93:256634</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_0380</td><td>[&#x27;force&#x27;, &#x27;x_co_7340</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0831031  </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.99643</td><td style=\"text-align: right;\">269.383 </td><td style=\"text-align: right;\"> 81.1456</td><td style=\"text-align: right;\">8.4721e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_da8def6e</td><td>TERMINATED</td><td>172.26.215.93:256846</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_4dc0</td><td>[&#x27;force&#x27;, &#x27;x_co_c840</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00130828 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.38564</td><td style=\"text-align: right;\">287.848 </td><td style=\"text-align: right;\">113.735 </td><td style=\"text-align: right;\">1.95658e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_4fe847ca</td><td>TERMINATED</td><td>172.26.215.93:256958</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_2f80</td><td>[&#x27;force&#x27;, &#x27;x_co_4e40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0009298  </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.70054</td><td style=\"text-align: right;\">414.821 </td><td style=\"text-align: right;\">130.384 </td><td style=\"text-align: right;\">1.32515e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_f7b1917c</td><td>TERMINATED</td><td>172.26.215.93:257259</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_db80</td><td>[&#x27;force&#x27;, &#x27;x_co_da80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000664701</td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.62439</td><td style=\"text-align: right;\">316.137 </td><td style=\"text-align: right;\"> 99.3735</td><td style=\"text-align: right;\">1.09957e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_5cfdb725</td><td>TERMINATED</td><td>172.26.215.93:257465</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_fdc0</td><td>[&#x27;force&#x27;, &#x27;x_co_4300</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0276241  </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        13.6277 </td><td style=\"text-align: right;\">253.949 </td><td style=\"text-align: right;\"> 75.8615</td><td style=\"text-align: right;\">2.76513e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_5efe99c1</td><td>TERMINATED</td><td>172.26.215.93:257711</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_1ac0</td><td>[&#x27;force&#x27;, &#x27;x_co_1740</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.027832   </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         7.20134</td><td style=\"text-align: right;\">264.668 </td><td style=\"text-align: right;\"> 82.5371</td><td style=\"text-align: right;\">2.71345e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_6d014408</td><td>TERMINATED</td><td>172.26.215.93:257933</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_18c0</td><td>[&#x27;force&#x27;, &#x27;x_co_1580</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0269743  </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        29.2717 </td><td style=\"text-align: right;\">221.877 </td><td style=\"text-align: right;\"> 63.5823</td><td style=\"text-align: right;\">1.61383e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_a3285c46</td><td>TERMINATED</td><td>172.26.215.93:258157</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_1f00</td><td>[&#x27;force&#x27;, &#x27;x_co_10c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00556062 </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">        80.6754 </td><td style=\"text-align: right;\">141.869 </td><td style=\"text-align: right;\"> 42.0936</td><td style=\"text-align: right;\">6.15576e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_4b114ad5</td><td>TERMINATED</td><td>172.26.215.93:258383</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_ec40</td><td>[&#x27;force&#x27;, &#x27;x_co_dd40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00523559 </td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.83469</td><td style=\"text-align: right;\">290.617 </td><td style=\"text-align: right;\"> 83.8139</td><td style=\"text-align: right;\">5.91126e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_367ceef8</td><td>TERMINATED</td><td>172.26.215.93:258596</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_3c00</td><td>[&#x27;force&#x27;, &#x27;x_co_e880</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0048315  </td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.59644</td><td style=\"text-align: right;\">255.824 </td><td style=\"text-align: right;\"> 74.9434</td><td style=\"text-align: right;\">4.58481e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_45a55fd7</td><td>TERMINATED</td><td>172.26.215.93:258815</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_c840</td><td>[&#x27;force&#x27;, &#x27;x_co_1540</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0117185  </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        10.3772 </td><td style=\"text-align: right;\">259.115 </td><td style=\"text-align: right;\"> 90.4196</td><td style=\"text-align: right;\">2.91515e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_534128b7</td><td>TERMINATED</td><td>172.26.215.93:259057</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_bc00</td><td>[&#x27;force&#x27;, &#x27;x_co_a0c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0122203  </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.71084</td><td style=\"text-align: right;\">269.968 </td><td style=\"text-align: right;\"> 81.8702</td><td style=\"text-align: right;\">8.67404e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_ec17fec0</td><td>TERMINATED</td><td>172.26.215.93:259273</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_c500</td><td>[&#x27;force&#x27;, &#x27;x_co_f840</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0118805  </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.49293</td><td style=\"text-align: right;\">270.133 </td><td style=\"text-align: right;\"> 81.1435</td><td style=\"text-align: right;\">8.455e+16  </td></tr>\n",
       "<tr><td>FSR_Trainable_1d77cd69</td><td>TERMINATED</td><td>172.26.215.93:259506</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_fe00</td><td>[&#x27;force&#x27;, &#x27;x_co_c540</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0416373  </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        15.6323 </td><td style=\"text-align: right;\">263.36  </td><td style=\"text-align: right;\"> 78.7336</td><td style=\"text-align: right;\">8.013e+16  </td></tr>\n",
       "<tr><td>FSR_Trainable_b80f3bce</td><td>TERMINATED</td><td>172.26.215.93:259732</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_f180</td><td>[&#x27;force&#x27;, &#x27;x_co_5940</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0404901  </td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.63662</td><td style=\"text-align: right;\">268.781 </td><td style=\"text-align: right;\"> 81.8961</td><td style=\"text-align: right;\">2.52218e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_9bc1d71b</td><td>TERMINATED</td><td>172.26.215.93:259951</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_dc00</td><td>[&#x27;force&#x27;, &#x27;x_co_1ec0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0466551  </td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.33664</td><td style=\"text-align: right;\">277.87  </td><td style=\"text-align: right;\"> 80.5152</td><td style=\"text-align: right;\">2.09955e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_55cc134f</td><td>TERMINATED</td><td>172.26.215.93:260180</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_c480</td><td>[&#x27;force&#x27;, &#x27;x_co_2040</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0971196  </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        16.3668 </td><td style=\"text-align: right;\">257.965 </td><td style=\"text-align: right;\"> 82.3883</td><td style=\"text-align: right;\">2.17281e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_c21fded1</td><td>TERMINATED</td><td>172.26.215.93:260408</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_ee80</td><td>[&#x27;force&#x27;, &#x27;x_co_fb80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0179374  </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        17.3665 </td><td style=\"text-align: right;\">258.153 </td><td style=\"text-align: right;\"> 84.5181</td><td style=\"text-align: right;\">2.61893e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_a485388c</td><td>TERMINATED</td><td>172.26.215.93:260634</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_14c0</td><td>[&#x27;force&#x27;, &#x27;x_co_8200</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0175201  </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        27.7263 </td><td style=\"text-align: right;\">240.02  </td><td style=\"text-align: right;\"> 72.1016</td><td style=\"text-align: right;\">2.05102e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_6a09b823</td><td>TERMINATED</td><td>172.26.215.93:260854</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_db40</td><td>[&#x27;force&#x27;, &#x27;x_co_8d00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0206974  </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.28672</td><td style=\"text-align: right;\">308.159 </td><td style=\"text-align: right;\"> 99.6   </td><td style=\"text-align: right;\">3.70758e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_5249ea04</td><td>TERMINATED</td><td>172.26.215.93:261086</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_0580</td><td>[&#x27;force&#x27;, &#x27;x_co_0d00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00306093 </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      1064.19   </td><td style=\"text-align: right;\">118.71  </td><td style=\"text-align: right;\"> 38.0661</td><td style=\"text-align: right;\">7.1347e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_4df8f562</td><td>TERMINATED</td><td>172.26.215.93:261310</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_92c0</td><td>[&#x27;force&#x27;, &#x27;x_co_9280</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00827524 </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       157.163  </td><td style=\"text-align: right;\">162.42  </td><td style=\"text-align: right;\"> 46.5734</td><td style=\"text-align: right;\">5.8809e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_60677baa</td><td>TERMINATED</td><td>172.26.215.93:261528</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_8bc0</td><td>[&#x27;force&#x27;, &#x27;x_co_80c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00778455 </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        20.7539 </td><td style=\"text-align: right;\">265.494 </td><td style=\"text-align: right;\"> 81.4512</td><td style=\"text-align: right;\">2.32775e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_adab90cb</td><td>TERMINATED</td><td>172.26.215.93:261757</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_0a80</td><td>[&#x27;force&#x27;, &#x27;x_co_8380</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0028918  </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       524.214  </td><td style=\"text-align: right;\">124.766 </td><td style=\"text-align: right;\"> 36.9461</td><td style=\"text-align: right;\">2.66068e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_745564df</td><td>TERMINATED</td><td>172.26.215.93:262033</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_ca40</td><td>[&#x27;force&#x27;, &#x27;x_co_c180</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00888705 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        18.3304 </td><td style=\"text-align: right;\">278.84  </td><td style=\"text-align: right;\"> 80.505 </td><td style=\"text-align: right;\">6.71699e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_3e453153</td><td>TERMINATED</td><td>172.26.215.93:262268</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_9080</td><td>[&#x27;force&#x27;, &#x27;x_co_a080</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00323923 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.28399</td><td style=\"text-align: right;\">266.438 </td><td style=\"text-align: right;\"> 81.6839</td><td style=\"text-align: right;\">9.36822e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_bdbce26d</td><td>TERMINATED</td><td>172.26.215.93:262494</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_8700</td><td>[&#x27;force&#x27;, &#x27;x_co_b080</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00365341 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.96408</td><td style=\"text-align: right;\">265.554 </td><td style=\"text-align: right;\"> 83.5152</td><td style=\"text-align: right;\">1.06796e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_7a74365c</td><td>TERMINATED</td><td>172.26.215.93:262691</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_edc0</td><td>[&#x27;force&#x27;, &#x27;x_co_c340</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.040493   </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         7.5442 </td><td style=\"text-align: right;\">252.513 </td><td style=\"text-align: right;\"> 78.1556</td><td style=\"text-align: right;\">2.89537e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_934971da</td><td>TERMINATED</td><td>172.26.215.93:262958</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_a600</td><td>[&#x27;force&#x27;, &#x27;x_co_f600</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0527251  </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        15.1349 </td><td style=\"text-align: right;\">230.09  </td><td style=\"text-align: right;\"> 65.998 </td><td style=\"text-align: right;\">1.94836e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_c1fb91e4</td><td>TERMINATED</td><td>172.26.215.93:263195</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_cc00</td><td>[&#x27;force&#x27;, &#x27;x_co_5800</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0482398  </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        15.6148 </td><td style=\"text-align: right;\">239.525 </td><td style=\"text-align: right;\"> 73.1601</td><td style=\"text-align: right;\">2.13187e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_fa31eec0</td><td>TERMINATED</td><td>172.26.215.93:263376</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_0400</td><td>[&#x27;force&#x27;, &#x27;x_co_3b80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0066609  </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       126.315  </td><td style=\"text-align: right;\">162.968 </td><td style=\"text-align: right;\"> 48.3548</td><td style=\"text-align: right;\">6.02114e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_a6d1cf80</td><td>TERMINATED</td><td>172.26.215.93:263652</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_a3c0</td><td>[&#x27;force&#x27;, &#x27;x_co_a940</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00664678 </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       138.973  </td><td style=\"text-align: right;\">163     </td><td style=\"text-align: right;\"> 51.8296</td><td style=\"text-align: right;\">8.57149e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_18af9e99</td><td>TERMINATED</td><td>172.26.215.93:263934</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_1a40</td><td>[&#x27;force&#x27;, &#x27;x_co_c440</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00740279 </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       191.601  </td><td style=\"text-align: right;\">159.196 </td><td style=\"text-align: right;\"> 47.4791</td><td style=\"text-align: right;\">6.02684e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_7541177b</td><td>TERMINATED</td><td>172.26.215.93:264174</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_2240</td><td>[&#x27;force&#x27;, &#x27;x_co_1f00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00232983 </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       302.521  </td><td style=\"text-align: right;\"> 97.9106</td><td style=\"text-align: right;\"> 30.1337</td><td style=\"text-align: right;\">5.03843e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_d263706c</td><td>TERMINATED</td><td>172.26.215.93:264500</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_9dc0</td><td>[&#x27;force&#x27;, &#x27;x_co_9c40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00193845 </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         5.98309</td><td style=\"text-align: right;\">255.256 </td><td style=\"text-align: right;\"> 85.652 </td><td style=\"text-align: right;\">3.00292e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_9d20d9d5</td><td>TERMINATED</td><td>172.26.215.93:264729</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_6740</td><td>[&#x27;force&#x27;, &#x27;x_co_adc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00283359 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.65253</td><td style=\"text-align: right;\">312.428 </td><td style=\"text-align: right;\">105.087 </td><td style=\"text-align: right;\">9.22481e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_7e985cc1</td><td>TERMINATED</td><td>172.26.215.93:264959</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_1b00</td><td>[&#x27;force&#x27;, &#x27;x_co_3ac0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00228077 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       308.473  </td><td style=\"text-align: right;\">132.772 </td><td style=\"text-align: right;\"> 40.6256</td><td style=\"text-align: right;\">3.95129e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_6f761e95</td><td>TERMINATED</td><td>172.26.215.93:265139</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_4600</td><td>[&#x27;force&#x27;, &#x27;x_co_5f40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00823101 </td><td>sklearn.preproc_8510</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       106.626  </td><td style=\"text-align: right;\">167.043 </td><td style=\"text-align: right;\"> 50.0297</td><td style=\"text-align: right;\">7.18768e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_44d49a12</td><td>TERMINATED</td><td>172.26.215.93:265462</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_c840</td><td>[&#x27;force&#x27;, &#x27;x_co_c700</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00476247 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       167.752  </td><td style=\"text-align: right;\">170.785 </td><td style=\"text-align: right;\"> 50.7015</td><td style=\"text-align: right;\">4.28152e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_38305194</td><td>TERMINATED</td><td>172.26.215.93:265707</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_6740</td><td>[&#x27;force&#x27;, &#x27;x_co_e1c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00214788 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       290.811  </td><td style=\"text-align: right;\">118.222 </td><td style=\"text-align: right;\"> 34.8271</td><td style=\"text-align: right;\">2.62644e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_865addfd</td><td>TERMINATED</td><td>172.26.215.93:266021</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_e6c0</td><td>[&#x27;force&#x27;, &#x27;x_co_f680</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00216112 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       166.174  </td><td style=\"text-align: right;\">114.506 </td><td style=\"text-align: right;\"> 35.7914</td><td style=\"text-align: right;\">3.33115e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_0c2584bd</td><td>TERMINATED</td><td>172.26.215.93:266268</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_8b40</td><td>[&#x27;force&#x27;, &#x27;x_co_b800</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00167083 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       122.522  </td><td style=\"text-align: right;\">111.085 </td><td style=\"text-align: right;\"> 36.0062</td><td style=\"text-align: right;\">3.62777e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_2d68f3af</td><td>TERMINATED</td><td>172.26.215.93:266579</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_cc80</td><td>[&#x27;force&#x27;, &#x27;x_co_c240</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00201311 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       312.12   </td><td style=\"text-align: right;\">124.872 </td><td style=\"text-align: right;\"> 36.8227</td><td style=\"text-align: right;\">3.29994e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_f8ccb7f8</td><td>TERMINATED</td><td>172.26.215.93:266815</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>median</td><td>[&#x27;FSR_for_force_dac0</td><td>[&#x27;force&#x27;, &#x27;x_co_fd40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0018403  </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       129.943  </td><td style=\"text-align: right;\">121.622 </td><td style=\"text-align: right;\"> 37.8393</td><td style=\"text-align: right;\">3.52706e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_47cee5a0</td><td>TERMINATED</td><td>172.26.215.93:267016</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_4ac0</td><td>[&#x27;force&#x27;, &#x27;x_co_5c40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000547751</td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.71803</td><td style=\"text-align: right;\">314.955 </td><td style=\"text-align: right;\">115.575 </td><td style=\"text-align: right;\">1.57188e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_8a604e62</td><td>TERMINATED</td><td>172.26.215.93:267245</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_f540</td><td>[&#x27;force&#x27;, &#x27;x_co_c1c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000633762</td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.75073</td><td style=\"text-align: right;\">308.893 </td><td style=\"text-align: right;\"> 98.0849</td><td style=\"text-align: right;\">1.07143e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_aad3656a</td><td>TERMINATED</td><td>172.26.215.93:267477</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_ebc0</td><td>[&#x27;force&#x27;, &#x27;x_co_f2c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00143472 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       132.96   </td><td style=\"text-align: right;\">112.832 </td><td style=\"text-align: right;\"> 34.2805</td><td style=\"text-align: right;\">2.67181e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_9dd92bce</td><td>TERMINATED</td><td>172.26.215.93:267707</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_58c0</td><td>[&#x27;force&#x27;, &#x27;x_co_4480</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00125576 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       129.782  </td><td style=\"text-align: right;\">106.472 </td><td style=\"text-align: right;\"> 33.0225</td><td style=\"text-align: right;\">2.81191e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_6845741b</td><td>TERMINATED</td><td>172.26.215.93:268042</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_7500</td><td>[&#x27;force&#x27;, &#x27;x_co_5040</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00151671 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       129.027  </td><td style=\"text-align: right;\">108.381 </td><td style=\"text-align: right;\"> 34.0735</td><td style=\"text-align: right;\">3.15061e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_1f0f220a</td><td>TERMINATED</td><td>172.26.215.93:268299</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_53c0</td><td>[&#x27;force&#x27;, &#x27;x_co_6580</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00152111 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       126.204  </td><td style=\"text-align: right;\">107.191 </td><td style=\"text-align: right;\"> 32.9294</td><td style=\"text-align: right;\">2.57721e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_d0ec6762</td><td>TERMINATED</td><td>172.26.215.93:268505</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_5400</td><td>[&#x27;force&#x27;, &#x27;x_co_2b80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00136903 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       129.751  </td><td style=\"text-align: right;\">103.191 </td><td style=\"text-align: right;\"> 31.7957</td><td style=\"text-align: right;\">2.55833e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_2789cfee</td><td>TERMINATED</td><td>172.26.215.93:268832</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_5d40</td><td>[&#x27;force&#x27;, &#x27;x_co_6580</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0013226  </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       126.506  </td><td style=\"text-align: right;\">111.736 </td><td style=\"text-align: right;\"> 34.8392</td><td style=\"text-align: right;\">2.88833e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_22b34823</td><td>TERMINATED</td><td>172.26.215.93:269034</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_0b00</td><td>[&#x27;force&#x27;, &#x27;x_co_f980</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00128497 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       121.203  </td><td style=\"text-align: right;\">105.063 </td><td style=\"text-align: right;\"> 33.8352</td><td style=\"text-align: right;\">2.97435e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_cb33ebc6</td><td>TERMINATED</td><td>172.26.215.93:269307</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_7600</td><td>[&#x27;force&#x27;, &#x27;x_co_7c00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00125137 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       123.722  </td><td style=\"text-align: right;\">106.306 </td><td style=\"text-align: right;\"> 32.9884</td><td style=\"text-align: right;\">2.81811e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_93c2f3ed</td><td>TERMINATED</td><td>172.26.215.93:269519</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_3380</td><td>[&#x27;force&#x27;, &#x27;x_co_1d00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00129669 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       122.785  </td><td style=\"text-align: right;\">106.499 </td><td style=\"text-align: right;\"> 33.3694</td><td style=\"text-align: right;\">2.78309e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_67d1a213</td><td>TERMINATED</td><td>172.26.215.93:269853</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_d140</td><td>[&#x27;force&#x27;, &#x27;x_co_2a40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000984466</td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.16652</td><td style=\"text-align: right;\">233.592 </td><td style=\"text-align: right;\"> 74.1289</td><td style=\"text-align: right;\">9.54956e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_5cb3b86e</td><td>TERMINATED</td><td>172.26.215.93:270054</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_9dc0</td><td>[&#x27;force&#x27;, &#x27;x_co_0d40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00102881 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.09949</td><td style=\"text-align: right;\">220.366 </td><td style=\"text-align: right;\"> 68.8727</td><td style=\"text-align: right;\">8.60629e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_4e2d7f45</td><td>TERMINATED</td><td>172.26.215.93:270283</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_9e00</td><td>[&#x27;force&#x27;, &#x27;x_co_9940</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00128529 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.42421</td><td style=\"text-align: right;\">221.92  </td><td style=\"text-align: right;\"> 67.5334</td><td style=\"text-align: right;\">7.43766e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_f3b05426</td><td>TERMINATED</td><td>172.26.215.93:270522</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_b940</td><td>[&#x27;force&#x27;, &#x27;x_co_b1c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00140695 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       104.765  </td><td style=\"text-align: right;\">103.51  </td><td style=\"text-align: right;\"> 31.8616</td><td style=\"text-align: right;\">2.55116e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_c13d6c02</td><td>TERMINATED</td><td>172.26.215.93:270752</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_ae80</td><td>[&#x27;force&#x27;, &#x27;x_co_8ac0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000815252</td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.44895</td><td style=\"text-align: right;\">259.883 </td><td style=\"text-align: right;\"> 82.8446</td><td style=\"text-align: right;\">1.0721e+17 </td></tr>\n",
       "<tr><td>FSR_Trainable_eba671e0</td><td>TERMINATED</td><td>172.26.215.93:270988</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_4d00</td><td>[&#x27;force&#x27;, &#x27;x_co_6780</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000798061</td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.80325</td><td style=\"text-align: right;\">227.422 </td><td style=\"text-align: right;\"> 70.2536</td><td style=\"text-align: right;\">8.6968e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_6e12bef7</td><td>TERMINATED</td><td>172.26.215.93:271213</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_2080</td><td>[&#x27;force&#x27;, &#x27;x_co_3e80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000541066</td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.74243</td><td style=\"text-align: right;\">264.358 </td><td style=\"text-align: right;\"> 77.1556</td><td style=\"text-align: right;\">5.74314e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_b4d9d36a</td><td>TERMINATED</td><td>172.26.215.93:271416</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_5ec0</td><td>[&#x27;force&#x27;, &#x27;x_co_a200</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000388363</td><td>sklearn.preproc_87b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.07568</td><td style=\"text-align: right;\">266.45  </td><td style=\"text-align: right;\"> 78.4856</td><td style=\"text-align: right;\">6.14366e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_f0ab078d</td><td>TERMINATED</td><td>172.26.215.93:271613</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_c640</td><td>[&#x27;force&#x27;, &#x27;x_co_40c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00110675 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       122.403  </td><td style=\"text-align: right;\">105.425 </td><td style=\"text-align: right;\"> 33.3085</td><td style=\"text-align: right;\">2.90881e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_b6491f21</td><td>TERMINATED</td><td>172.26.215.93:271889</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_a3c0</td><td>[&#x27;force&#x27;, &#x27;x_co_be00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00151269 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       145.606  </td><td style=\"text-align: right;\">112.863 </td><td style=\"text-align: right;\"> 34.9905</td><td style=\"text-align: right;\">2.99021e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_98078ce8</td><td>TERMINATED</td><td>172.26.215.93:272096</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_e940</td><td>[&#x27;force&#x27;, &#x27;x_co_fb40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00110691 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       143.005  </td><td style=\"text-align: right;\">110.701 </td><td style=\"text-align: right;\"> 35.3823</td><td style=\"text-align: right;\">3.71133e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_abebba07</td><td>TERMINATED</td><td>172.26.215.93:272412</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_56c0</td><td>[&#x27;force&#x27;, &#x27;x_co_6e80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00115133 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       136.889  </td><td style=\"text-align: right;\">104.516 </td><td style=\"text-align: right;\"> 32.1622</td><td style=\"text-align: right;\">2.66666e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_60d63a1b</td><td>TERMINATED</td><td>172.26.215.93:272694</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_aec0</td><td>[&#x27;force&#x27;, &#x27;x_co_3780</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.001146   </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.01219</td><td style=\"text-align: right;\">242.732 </td><td style=\"text-align: right;\"> 76.255 </td><td style=\"text-align: right;\">1.02588e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_e22fdcc1</td><td>TERMINATED</td><td>172.26.215.93:272900</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_e340</td><td>[&#x27;force&#x27;, &#x27;x_co_f940</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000948183</td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         6.11805</td><td style=\"text-align: right;\">170.602 </td><td style=\"text-align: right;\"> 54.0515</td><td style=\"text-align: right;\">6.7497e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_40c34eba</td><td>TERMINATED</td><td>172.26.215.93:273130</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_f380</td><td>[&#x27;force&#x27;, &#x27;x_co_c200</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00108255 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.03851</td><td style=\"text-align: right;\">260.965 </td><td style=\"text-align: right;\"> 83.6496</td><td style=\"text-align: right;\">1.0994e+17 </td></tr>\n",
       "<tr><td>FSR_Trainable_f3b152dd</td><td>TERMINATED</td><td>172.26.215.93:273360</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_9080</td><td>[&#x27;force&#x27;, &#x27;x_co_a7c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00164055 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.96236</td><td style=\"text-align: right;\">264.137 </td><td style=\"text-align: right;\"> 82.2224</td><td style=\"text-align: right;\">8.81166e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_88ceeae9</td><td>TERMINATED</td><td>172.26.215.93:273580</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_5400</td><td>[&#x27;force&#x27;, &#x27;x_co_ca80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00250631 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.94671</td><td style=\"text-align: right;\">263.58  </td><td style=\"text-align: right;\"> 78.6611</td><td style=\"text-align: right;\">7.63778e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_3c726ff3</td><td>TERMINATED</td><td>172.26.215.93:273803</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_cf40</td><td>[&#x27;force&#x27;, &#x27;x_co_e1c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00250525 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">        80.946  </td><td style=\"text-align: right;\">118.87  </td><td style=\"text-align: right;\"> 38.4485</td><td style=\"text-align: right;\">3.70862e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_9e13e128</td><td>TERMINATED</td><td>172.26.215.93:274030</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_4b80</td><td>[&#x27;force&#x27;, &#x27;x_co_6e80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00413017 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.34967</td><td style=\"text-align: right;\">234.93  </td><td style=\"text-align: right;\"> 72.2649</td><td style=\"text-align: right;\">7.30652e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_3cdef93d</td><td>TERMINATED</td><td>172.26.215.93:274266</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_1480</td><td>[&#x27;force&#x27;, &#x27;x_co_3680</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00256229 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.73307</td><td style=\"text-align: right;\">269.274 </td><td style=\"text-align: right;\"> 79.8065</td><td style=\"text-align: right;\">7.82879e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_7b63b965</td><td>TERMINATED</td><td>172.26.215.93:274486</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_f200</td><td>[&#x27;force&#x27;, &#x27;x_co_1480</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00246211 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.24606</td><td style=\"text-align: right;\">264.199 </td><td style=\"text-align: right;\"> 84.8948</td><td style=\"text-align: right;\">1.0963e+17 </td></tr>\n",
       "<tr><td>FSR_Trainable_40f07262</td><td>TERMINATED</td><td>172.26.215.93:274712</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_ff40</td><td>[&#x27;force&#x27;, &#x27;x_co_6480</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00128461 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       143.629  </td><td style=\"text-align: right;\">111.049 </td><td style=\"text-align: right;\"> 34.5236</td><td style=\"text-align: right;\">3.03829e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_36f1eec2</td><td>TERMINATED</td><td>172.26.215.93:274945</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_f980</td><td>[&#x27;force&#x27;, &#x27;x_co_cdc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00125012 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       145.71   </td><td style=\"text-align: right;\">109.95  </td><td style=\"text-align: right;\"> 34.0817</td><td style=\"text-align: right;\">2.98493e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_cc3c4bbf</td><td>TERMINATED</td><td>172.26.215.93:275161</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_55c0</td><td>[&#x27;force&#x27;, &#x27;x_co_7c80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00179399 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        46.7343 </td><td style=\"text-align: right;\">118.461 </td><td style=\"text-align: right;\"> 37.7465</td><td style=\"text-align: right;\">3.61412e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_3292865f</td><td>TERMINATED</td><td>172.26.215.93:275462</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_1480</td><td>[&#x27;force&#x27;, &#x27;x_co_3940</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00133054 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       129.862  </td><td style=\"text-align: right;\">110.372 </td><td style=\"text-align: right;\"> 33.7659</td><td style=\"text-align: right;\">2.76842e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_3cb749d0</td><td>TERMINATED</td><td>172.26.215.93:275700</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_4180</td><td>[&#x27;force&#x27;, &#x27;x_co_6c40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00173056 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">        88.4422 </td><td style=\"text-align: right;\">113.941 </td><td style=\"text-align: right;\"> 35.7589</td><td style=\"text-align: right;\">3.21579e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_499989a2</td><td>TERMINATED</td><td>172.26.215.93:275996</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8810</td><td>sklearn.impute._36e0</td><td>mean  </td><td>[&#x27;FSR_for_force_b7c0</td><td>[&#x27;force&#x27;, &#x27;x_co_ee80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00134281 </td><td>sklearn.preproc_8750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        79.9734 </td><td style=\"text-align: right;\">114.194 </td><td style=\"text-align: right;\"> 35.4965</td><td style=\"text-align: right;\">3.3334e+16 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 22:57:57,865\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">       mape</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_064adcc1</td><td>2023-07-18_23-00-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 89.673 </td><td style=\"text-align: right;\">1.47741e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">254768</td><td style=\"text-align: right;\">259.905 </td><td style=\"text-align: right;\">            16.3863 </td><td style=\"text-align: right;\">          1.75382 </td><td style=\"text-align: right;\">      16.3863 </td><td style=\"text-align: right;\"> 1689688820</td><td style=\"text-align: right;\">                   8</td><td>064adcc1  </td></tr>\n",
       "<tr><td>FSR_Trainable_0c2584bd</td><td>2023-07-18_23-22-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 36.0062</td><td style=\"text-align: right;\">3.62777e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">266268</td><td style=\"text-align: right;\">111.085 </td><td style=\"text-align: right;\">           122.522  </td><td style=\"text-align: right;\">          1.25495 </td><td style=\"text-align: right;\">     122.522  </td><td style=\"text-align: right;\"> 1689690125</td><td style=\"text-align: right;\">                 100</td><td>0c2584bd  </td></tr>\n",
       "<tr><td>FSR_Trainable_18af9e99</td><td>2023-07-18_23-13-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 47.4791</td><td style=\"text-align: right;\">6.02684e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">263934</td><td style=\"text-align: right;\">159.196 </td><td style=\"text-align: right;\">           191.601  </td><td style=\"text-align: right;\">          3.49781 </td><td style=\"text-align: right;\">     191.601  </td><td style=\"text-align: right;\"> 1689689615</td><td style=\"text-align: right;\">                  32</td><td>18af9e99  </td></tr>\n",
       "<tr><td>FSR_Trainable_1a311da9</td><td>2023-07-18_23-01-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 81.1456</td><td style=\"text-align: right;\">8.4721e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">256634</td><td style=\"text-align: right;\">269.383 </td><td style=\"text-align: right;\">             2.99643</td><td style=\"text-align: right;\">          2.99643 </td><td style=\"text-align: right;\">       2.99643</td><td style=\"text-align: right;\"> 1689688912</td><td style=\"text-align: right;\">                   1</td><td>1a311da9  </td></tr>\n",
       "<tr><td>FSR_Trainable_1d77cd69</td><td>2023-07-18_23-04-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 78.7336</td><td style=\"text-align: right;\">8.013e+16  </td><td>172.26.215.93</td><td style=\"text-align: right;\">259506</td><td style=\"text-align: right;\">263.36  </td><td style=\"text-align: right;\">            15.6323 </td><td style=\"text-align: right;\">          1.61063 </td><td style=\"text-align: right;\">      15.6323 </td><td style=\"text-align: right;\"> 1689689044</td><td style=\"text-align: right;\">                   8</td><td>1d77cd69  </td></tr>\n",
       "<tr><td>FSR_Trainable_1f0f220a</td><td>2023-07-18_23-27-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 32.9294</td><td style=\"text-align: right;\">2.57721e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">268299</td><td style=\"text-align: right;\">107.191 </td><td style=\"text-align: right;\">           126.204  </td><td style=\"text-align: right;\">          1.32933 </td><td style=\"text-align: right;\">     126.204  </td><td style=\"text-align: right;\"> 1689690473</td><td style=\"text-align: right;\">                 100</td><td>1f0f220a  </td></tr>\n",
       "<tr><td>FSR_Trainable_22b34823</td><td>2023-07-18_23-29-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 33.8352</td><td style=\"text-align: right;\">2.97435e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">269034</td><td style=\"text-align: right;\">105.063 </td><td style=\"text-align: right;\">           121.203  </td><td style=\"text-align: right;\">          1.19845 </td><td style=\"text-align: right;\">     121.203  </td><td style=\"text-align: right;\"> 1689690587</td><td style=\"text-align: right;\">                 100</td><td>22b34823  </td></tr>\n",
       "<tr><td>FSR_Trainable_25519ac6</td><td>2023-07-18_23-00-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">156.032 </td><td style=\"text-align: right;\">1.6992e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">255460</td><td style=\"text-align: right;\">438.717 </td><td style=\"text-align: right;\">             2.52168</td><td style=\"text-align: right;\">          2.52168 </td><td style=\"text-align: right;\">       2.52168</td><td style=\"text-align: right;\"> 1689688844</td><td style=\"text-align: right;\">                   1</td><td>25519ac6  </td></tr>\n",
       "<tr><td>FSR_Trainable_2789cfee</td><td>2023-07-18_23-29-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 34.8392</td><td style=\"text-align: right;\">2.88833e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">268832</td><td style=\"text-align: right;\">111.736 </td><td style=\"text-align: right;\">           126.506  </td><td style=\"text-align: right;\">          1.32005 </td><td style=\"text-align: right;\">     126.506  </td><td style=\"text-align: right;\"> 1689690580</td><td style=\"text-align: right;\">                 100</td><td>2789cfee  </td></tr>\n",
       "<tr><td>FSR_Trainable_2d68f3af</td><td>2023-07-18_23-27-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 36.8227</td><td style=\"text-align: right;\">3.29994e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">266579</td><td style=\"text-align: right;\">124.872 </td><td style=\"text-align: right;\">           312.12   </td><td style=\"text-align: right;\">          4.72271 </td><td style=\"text-align: right;\">     312.12   </td><td style=\"text-align: right;\"> 1689690434</td><td style=\"text-align: right;\">                  64</td><td>2d68f3af  </td></tr>\n",
       "<tr><td>FSR_Trainable_2e4dba12</td><td>2023-07-18_23-01-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 42.309 </td><td style=\"text-align: right;\">2.62402e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">254581</td><td style=\"text-align: right;\">142.482 </td><td style=\"text-align: right;\">            94.6973 </td><td style=\"text-align: right;\">          1.50084 </td><td style=\"text-align: right;\">      94.6973 </td><td style=\"text-align: right;\"> 1689688898</td><td style=\"text-align: right;\">                  64</td><td>2e4dba12  </td></tr>\n",
       "<tr><td>FSR_Trainable_3292865f</td><td>2023-07-18_23-38-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 33.7659</td><td style=\"text-align: right;\">2.76842e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">275462</td><td style=\"text-align: right;\">110.372 </td><td style=\"text-align: right;\">           129.862  </td><td style=\"text-align: right;\">          1.00484 </td><td style=\"text-align: right;\">     129.862  </td><td style=\"text-align: right;\"> 1689691135</td><td style=\"text-align: right;\">                 100</td><td>3292865f  </td></tr>\n",
       "<tr><td>FSR_Trainable_367ceef8</td><td>2023-07-18_23-03-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 74.9434</td><td style=\"text-align: right;\">4.58481e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">258596</td><td style=\"text-align: right;\">255.824 </td><td style=\"text-align: right;\">             3.59644</td><td style=\"text-align: right;\">          1.67141 </td><td style=\"text-align: right;\">       3.59644</td><td style=\"text-align: right;\"> 1689688993</td><td style=\"text-align: right;\">                   2</td><td>367ceef8  </td></tr>\n",
       "<tr><td>FSR_Trainable_36f1eec2</td><td>2023-07-18_23-38-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 34.0817</td><td style=\"text-align: right;\">2.98493e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">274945</td><td style=\"text-align: right;\">109.95  </td><td style=\"text-align: right;\">           145.71   </td><td style=\"text-align: right;\">          1.34258 </td><td style=\"text-align: right;\">     145.71   </td><td style=\"text-align: right;\"> 1689691101</td><td style=\"text-align: right;\">                 100</td><td>36f1eec2  </td></tr>\n",
       "<tr><td>FSR_Trainable_38305194</td><td>2023-07-18_23-21-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 34.8271</td><td style=\"text-align: right;\">2.62644e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">265707</td><td style=\"text-align: right;\">118.222 </td><td style=\"text-align: right;\">           290.811  </td><td style=\"text-align: right;\">          2.89802 </td><td style=\"text-align: right;\">     290.811  </td><td style=\"text-align: right;\"> 1689690102</td><td style=\"text-align: right;\">                 100</td><td>38305194  </td></tr>\n",
       "<tr><td>FSR_Trainable_3c726ff3</td><td>2023-07-18_23-36-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 38.4485</td><td style=\"text-align: right;\">3.70862e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">273803</td><td style=\"text-align: right;\">118.87  </td><td style=\"text-align: right;\">            80.946  </td><td style=\"text-align: right;\">          1.31598 </td><td style=\"text-align: right;\">      80.946  </td><td style=\"text-align: right;\"> 1689690984</td><td style=\"text-align: right;\">                  64</td><td>3c726ff3  </td></tr>\n",
       "<tr><td>FSR_Trainable_3cb749d0</td><td>2023-07-18_23-38-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 35.7589</td><td style=\"text-align: right;\">3.21579e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">275700</td><td style=\"text-align: right;\">113.941 </td><td style=\"text-align: right;\">            88.4422 </td><td style=\"text-align: right;\">          1.07578 </td><td style=\"text-align: right;\">      88.4422 </td><td style=\"text-align: right;\"> 1689691117</td><td style=\"text-align: right;\">                  64</td><td>3cb749d0  </td></tr>\n",
       "<tr><td>FSR_Trainable_3cdef93d</td><td>2023-07-18_23-35-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 79.8065</td><td style=\"text-align: right;\">7.82879e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">274266</td><td style=\"text-align: right;\">269.274 </td><td style=\"text-align: right;\">             4.73307</td><td style=\"text-align: right;\">          4.73307 </td><td style=\"text-align: right;\">       4.73307</td><td style=\"text-align: right;\"> 1689690918</td><td style=\"text-align: right;\">                   1</td><td>3cdef93d  </td></tr>\n",
       "<tr><td>FSR_Trainable_3e453153</td><td>2023-07-18_23-06-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 81.6839</td><td style=\"text-align: right;\">9.36822e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">262268</td><td style=\"text-align: right;\">266.438 </td><td style=\"text-align: right;\">             5.28399</td><td style=\"text-align: right;\">          5.28399 </td><td style=\"text-align: right;\">       5.28399</td><td style=\"text-align: right;\"> 1689689188</td><td style=\"text-align: right;\">                   1</td><td>3e453153  </td></tr>\n",
       "<tr><td>FSR_Trainable_40c34eba</td><td>2023-07-18_23-34-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 83.6496</td><td style=\"text-align: right;\">1.0994e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">273130</td><td style=\"text-align: right;\">260.965 </td><td style=\"text-align: right;\">             2.03851</td><td style=\"text-align: right;\">          2.03851 </td><td style=\"text-align: right;\">       2.03851</td><td style=\"text-align: right;\"> 1689690862</td><td style=\"text-align: right;\">                   1</td><td>40c34eba  </td></tr>\n",
       "<tr><td>FSR_Trainable_40f07262</td><td>2023-07-18_23-38-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 34.5236</td><td style=\"text-align: right;\">3.03829e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">274712</td><td style=\"text-align: right;\">111.049 </td><td style=\"text-align: right;\">           143.629  </td><td style=\"text-align: right;\">          1.37934 </td><td style=\"text-align: right;\">     143.629  </td><td style=\"text-align: right;\"> 1689691090</td><td style=\"text-align: right;\">                 100</td><td>40f07262  </td></tr>\n",
       "<tr><td>FSR_Trainable_44d49a12</td><td>2023-07-18_23-19-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 50.7015</td><td style=\"text-align: right;\">4.28152e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">265462</td><td style=\"text-align: right;\">170.785 </td><td style=\"text-align: right;\">           167.752  </td><td style=\"text-align: right;\">          5.57545 </td><td style=\"text-align: right;\">     167.752  </td><td style=\"text-align: right;\"> 1689689947</td><td style=\"text-align: right;\">                  32</td><td>44d49a12  </td></tr>\n",
       "<tr><td>FSR_Trainable_45a55fd7</td><td>2023-07-18_23-03-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 90.4196</td><td style=\"text-align: right;\">2.91515e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">258815</td><td style=\"text-align: right;\">259.115 </td><td style=\"text-align: right;\">            10.3772 </td><td style=\"text-align: right;\">          5.19282 </td><td style=\"text-align: right;\">      10.3772 </td><td style=\"text-align: right;\"> 1689689009</td><td style=\"text-align: right;\">                   2</td><td>45a55fd7  </td></tr>\n",
       "<tr><td>FSR_Trainable_47cee5a0</td><td>2023-07-18_23-22-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">115.575 </td><td style=\"text-align: right;\">1.57188e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">267016</td><td style=\"text-align: right;\">314.955 </td><td style=\"text-align: right;\">             1.71803</td><td style=\"text-align: right;\">          1.71803 </td><td style=\"text-align: right;\">       1.71803</td><td style=\"text-align: right;\"> 1689690152</td><td style=\"text-align: right;\">                   1</td><td>47cee5a0  </td></tr>\n",
       "<tr><td>FSR_Trainable_499989a2</td><td>2023-07-18_23-39-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 35.4965</td><td style=\"text-align: right;\">3.3334e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">275996</td><td style=\"text-align: right;\">114.194 </td><td style=\"text-align: right;\">            79.9734 </td><td style=\"text-align: right;\">          0.641827</td><td style=\"text-align: right;\">      79.9734 </td><td style=\"text-align: right;\"> 1689691186</td><td style=\"text-align: right;\">                 100</td><td>499989a2  </td></tr>\n",
       "<tr><td>FSR_Trainable_4b114ad5</td><td>2023-07-18_23-02-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 83.8139</td><td style=\"text-align: right;\">5.91126e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">258383</td><td style=\"text-align: right;\">290.617 </td><td style=\"text-align: right;\">             1.83469</td><td style=\"text-align: right;\">          1.83469 </td><td style=\"text-align: right;\">       1.83469</td><td style=\"text-align: right;\"> 1689688979</td><td style=\"text-align: right;\">                   1</td><td>4b114ad5  </td></tr>\n",
       "<tr><td>FSR_Trainable_4b419ffa</td><td>2023-07-18_22-58-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 78.9946</td><td style=\"text-align: right;\">7.38729e+14</td><td>172.26.215.93</td><td style=\"text-align: right;\">252921</td><td style=\"text-align: right;\">272.943 </td><td style=\"text-align: right;\">            35.7283 </td><td style=\"text-align: right;\">         16.8505  </td><td style=\"text-align: right;\">      35.7283 </td><td style=\"text-align: right;\"> 1689688732</td><td style=\"text-align: right;\">                   2</td><td>4b419ffa  </td></tr>\n",
       "<tr><td>FSR_Trainable_4df8f562</td><td>2023-07-18_23-07-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 46.5734</td><td style=\"text-align: right;\">5.8809e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">261310</td><td style=\"text-align: right;\">162.42  </td><td style=\"text-align: right;\">           157.163  </td><td style=\"text-align: right;\">          6.05557 </td><td style=\"text-align: right;\">     157.163  </td><td style=\"text-align: right;\"> 1689689264</td><td style=\"text-align: right;\">                  16</td><td>4df8f562  </td></tr>\n",
       "<tr><td>FSR_Trainable_4e2d7f45</td><td>2023-07-18_23-30-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 67.5334</td><td style=\"text-align: right;\">7.43766e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">270283</td><td style=\"text-align: right;\">221.92  </td><td style=\"text-align: right;\">             3.42421</td><td style=\"text-align: right;\">          1.5458  </td><td style=\"text-align: right;\">       3.42421</td><td style=\"text-align: right;\"> 1689690623</td><td style=\"text-align: right;\">                   2</td><td>4e2d7f45  </td></tr>\n",
       "<tr><td>FSR_Trainable_4fe847ca</td><td>2023-07-18_23-02-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">130.384 </td><td style=\"text-align: right;\">1.32515e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">256958</td><td style=\"text-align: right;\">414.821 </td><td style=\"text-align: right;\">             1.70054</td><td style=\"text-align: right;\">          1.70054 </td><td style=\"text-align: right;\">       1.70054</td><td style=\"text-align: right;\"> 1689688926</td><td style=\"text-align: right;\">                   1</td><td>4fe847ca  </td></tr>\n",
       "<tr><td>FSR_Trainable_5249ea04</td><td>2023-07-18_23-22-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 38.0661</td><td style=\"text-align: right;\">7.1347e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">261086</td><td style=\"text-align: right;\">118.71  </td><td style=\"text-align: right;\">          1064.19   </td><td style=\"text-align: right;\">          6.77001 </td><td style=\"text-align: right;\">    1064.19   </td><td style=\"text-align: right;\"> 1689690172</td><td style=\"text-align: right;\">                 100</td><td>5249ea04  </td></tr>\n",
       "<tr><td>FSR_Trainable_534128b7</td><td>2023-07-18_23-03-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 81.8702</td><td style=\"text-align: right;\">8.67404e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">259057</td><td style=\"text-align: right;\">269.968 </td><td style=\"text-align: right;\">             2.71084</td><td style=\"text-align: right;\">          2.71084 </td><td style=\"text-align: right;\">       2.71084</td><td style=\"text-align: right;\"> 1689689010</td><td style=\"text-align: right;\">                   1</td><td>534128b7  </td></tr>\n",
       "<tr><td>FSR_Trainable_55cc134f</td><td>2023-07-18_23-04-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 82.3883</td><td style=\"text-align: right;\">2.17281e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">260180</td><td style=\"text-align: right;\">257.965 </td><td style=\"text-align: right;\">            16.3668 </td><td style=\"text-align: right;\">          2.84891 </td><td style=\"text-align: right;\">      16.3668 </td><td style=\"text-align: right;\"> 1689689073</td><td style=\"text-align: right;\">                   4</td><td>55cc134f  </td></tr>\n",
       "<tr><td>FSR_Trainable_5cb3b86e</td><td>2023-07-18_23-30-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 68.8727</td><td style=\"text-align: right;\">8.60629e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">270054</td><td style=\"text-align: right;\">220.366 </td><td style=\"text-align: right;\">             3.09949</td><td style=\"text-align: right;\">          1.35762 </td><td style=\"text-align: right;\">       3.09949</td><td style=\"text-align: right;\"> 1689690610</td><td style=\"text-align: right;\">                   2</td><td>5cb3b86e  </td></tr>\n",
       "<tr><td>FSR_Trainable_5cfdb725</td><td>2023-07-18_23-02-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 75.8615</td><td style=\"text-align: right;\">2.76513e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">257465</td><td style=\"text-align: right;\">253.949 </td><td style=\"text-align: right;\">            13.6277 </td><td style=\"text-align: right;\">          1.54629 </td><td style=\"text-align: right;\">      13.6277 </td><td style=\"text-align: right;\"> 1689688957</td><td style=\"text-align: right;\">                   8</td><td>5cfdb725  </td></tr>\n",
       "<tr><td>FSR_Trainable_5efe99c1</td><td>2023-07-18_23-02-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 82.5371</td><td style=\"text-align: right;\">2.71345e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">257711</td><td style=\"text-align: right;\">264.668 </td><td style=\"text-align: right;\">             7.20134</td><td style=\"text-align: right;\">          1.40656 </td><td style=\"text-align: right;\">       7.20134</td><td style=\"text-align: right;\"> 1689688958</td><td style=\"text-align: right;\">                   4</td><td>5efe99c1  </td></tr>\n",
       "<tr><td>FSR_Trainable_60677baa</td><td>2023-07-18_23-05-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 81.4512</td><td style=\"text-align: right;\">2.32775e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">261528</td><td style=\"text-align: right;\">265.494 </td><td style=\"text-align: right;\">            20.7539 </td><td style=\"text-align: right;\">         20.7539  </td><td style=\"text-align: right;\">      20.7539 </td><td style=\"text-align: right;\"> 1689689136</td><td style=\"text-align: right;\">                   1</td><td>60677baa  </td></tr>\n",
       "<tr><td>FSR_Trainable_60d63a1b</td><td>2023-07-18_23-33-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 76.255 </td><td style=\"text-align: right;\">1.02588e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">272694</td><td style=\"text-align: right;\">242.732 </td><td style=\"text-align: right;\">             3.01219</td><td style=\"text-align: right;\">          1.36508 </td><td style=\"text-align: right;\">       3.01219</td><td style=\"text-align: right;\"> 1689690834</td><td style=\"text-align: right;\">                   2</td><td>60d63a1b  </td></tr>\n",
       "<tr><td>FSR_Trainable_64598154</td><td>2023-07-18_23-01-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 85.515 </td><td style=\"text-align: right;\">3.22279e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">256370</td><td style=\"text-align: right;\">270.958 </td><td style=\"text-align: right;\">             3.84695</td><td style=\"text-align: right;\">          3.84695 </td><td style=\"text-align: right;\">       3.84695</td><td style=\"text-align: right;\"> 1689688898</td><td style=\"text-align: right;\">                   1</td><td>64598154  </td></tr>\n",
       "<tr><td>FSR_Trainable_67d1a213</td><td>2023-07-18_23-29-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 74.1289</td><td style=\"text-align: right;\">9.54956e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">269853</td><td style=\"text-align: right;\">233.592 </td><td style=\"text-align: right;\">             3.16652</td><td style=\"text-align: right;\">          1.47188 </td><td style=\"text-align: right;\">       3.16652</td><td style=\"text-align: right;\"> 1689690597</td><td style=\"text-align: right;\">                   2</td><td>67d1a213  </td></tr>\n",
       "<tr><td>FSR_Trainable_6845741b</td><td>2023-07-18_23-27-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 34.0735</td><td style=\"text-align: right;\">3.15061e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">268042</td><td style=\"text-align: right;\">108.381 </td><td style=\"text-align: right;\">           129.027  </td><td style=\"text-align: right;\">          1.30142 </td><td style=\"text-align: right;\">     129.027  </td><td style=\"text-align: right;\"> 1689690432</td><td style=\"text-align: right;\">                 100</td><td>6845741b  </td></tr>\n",
       "<tr><td>FSR_Trainable_6a09b823</td><td>2023-07-18_23-04-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 99.6   </td><td style=\"text-align: right;\">3.70758e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">260854</td><td style=\"text-align: right;\">308.159 </td><td style=\"text-align: right;\">             4.28672</td><td style=\"text-align: right;\">          4.28672 </td><td style=\"text-align: right;\">       4.28672</td><td style=\"text-align: right;\"> 1689689089</td><td style=\"text-align: right;\">                   1</td><td>6a09b823  </td></tr>\n",
       "<tr><td>FSR_Trainable_6d014408</td><td>2023-07-18_23-03-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 63.5823</td><td style=\"text-align: right;\">1.61383e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">257933</td><td style=\"text-align: right;\">221.877 </td><td style=\"text-align: right;\">            29.2717 </td><td style=\"text-align: right;\">          2.32306 </td><td style=\"text-align: right;\">      29.2717 </td><td style=\"text-align: right;\"> 1689688993</td><td style=\"text-align: right;\">                  16</td><td>6d014408  </td></tr>\n",
       "<tr><td>FSR_Trainable_6e12bef7</td><td>2023-07-18_23-31-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 77.1556</td><td style=\"text-align: right;\">5.74314e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">271213</td><td style=\"text-align: right;\">264.358 </td><td style=\"text-align: right;\">             1.74243</td><td style=\"text-align: right;\">          1.74243 </td><td style=\"text-align: right;\">       1.74243</td><td style=\"text-align: right;\"> 1689690663</td><td style=\"text-align: right;\">                   1</td><td>6e12bef7  </td></tr>\n",
       "<tr><td>FSR_Trainable_6f761e95</td><td>2023-07-18_23-16-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 50.0297</td><td style=\"text-align: right;\">7.18768e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">265139</td><td style=\"text-align: right;\">167.043 </td><td style=\"text-align: right;\">           106.626  </td><td style=\"text-align: right;\">          3.88713 </td><td style=\"text-align: right;\">     106.626  </td><td style=\"text-align: right;\"> 1689689791</td><td style=\"text-align: right;\">                  16</td><td>6f761e95  </td></tr>\n",
       "<tr><td>FSR_Trainable_745564df</td><td>2023-07-18_23-06-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 80.505 </td><td style=\"text-align: right;\">6.71699e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">262033</td><td style=\"text-align: right;\">278.84  </td><td style=\"text-align: right;\">            18.3304 </td><td style=\"text-align: right;\">         18.3304  </td><td style=\"text-align: right;\">      18.3304 </td><td style=\"text-align: right;\"> 1689689169</td><td style=\"text-align: right;\">                   1</td><td>745564df  </td></tr>\n",
       "<tr><td>FSR_Trainable_7541177b</td><td>2023-07-18_23-16-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 30.1337</td><td style=\"text-align: right;\">5.03843e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">264174</td><td style=\"text-align: right;\"> 97.9106</td><td style=\"text-align: right;\">           302.521  </td><td style=\"text-align: right;\">          2.87633 </td><td style=\"text-align: right;\">     302.521  </td><td style=\"text-align: right;\"> 1689689764</td><td style=\"text-align: right;\">                 100</td><td>7541177b  </td></tr>\n",
       "<tr><td>FSR_Trainable_7a74365c</td><td>2023-07-18_23-07-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 78.1556</td><td style=\"text-align: right;\">2.89537e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">262691</td><td style=\"text-align: right;\">252.513 </td><td style=\"text-align: right;\">             7.5442 </td><td style=\"text-align: right;\">          1.61221 </td><td style=\"text-align: right;\">       7.5442 </td><td style=\"text-align: right;\"> 1689689227</td><td style=\"text-align: right;\">                   4</td><td>7a74365c  </td></tr>\n",
       "<tr><td>FSR_Trainable_7b63b965</td><td>2023-07-18_23-35-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 84.8948</td><td style=\"text-align: right;\">1.0963e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">274486</td><td style=\"text-align: right;\">264.199 </td><td style=\"text-align: right;\">             4.24606</td><td style=\"text-align: right;\">          4.24606 </td><td style=\"text-align: right;\">       4.24606</td><td style=\"text-align: right;\"> 1689690928</td><td style=\"text-align: right;\">                   1</td><td>7b63b965  </td></tr>\n",
       "<tr><td>FSR_Trainable_7e1f0626</td><td>2023-07-18_23-01-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 82.1034</td><td style=\"text-align: right;\">2.77263e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">256149</td><td style=\"text-align: right;\">259.131 </td><td style=\"text-align: right;\">            27.5943 </td><td style=\"text-align: right;\">          3.13135 </td><td style=\"text-align: right;\">      27.5943 </td><td style=\"text-align: right;\"> 1689688912</td><td style=\"text-align: right;\">                   8</td><td>7e1f0626  </td></tr>\n",
       "<tr><td>FSR_Trainable_7e985cc1</td><td>2023-07-18_23-19-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 40.6256</td><td style=\"text-align: right;\">3.95129e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">264959</td><td style=\"text-align: right;\">132.772 </td><td style=\"text-align: right;\">           308.473  </td><td style=\"text-align: right;\">          5.40453 </td><td style=\"text-align: right;\">     308.473  </td><td style=\"text-align: right;\"> 1689689984</td><td style=\"text-align: right;\">                  64</td><td>7e985cc1  </td></tr>\n",
       "<tr><td>FSR_Trainable_865addfd</td><td>2023-07-18_23-22-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 35.7914</td><td style=\"text-align: right;\">3.33115e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">266021</td><td style=\"text-align: right;\">114.506 </td><td style=\"text-align: right;\">           166.174  </td><td style=\"text-align: right;\">          1.72706 </td><td style=\"text-align: right;\">     166.174  </td><td style=\"text-align: right;\"> 1689690133</td><td style=\"text-align: right;\">                 100</td><td>865addfd  </td></tr>\n",
       "<tr><td>FSR_Trainable_88ceeae9</td><td>2023-07-18_23-34-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 78.6611</td><td style=\"text-align: right;\">7.63778e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">273580</td><td style=\"text-align: right;\">263.58  </td><td style=\"text-align: right;\">             1.94671</td><td style=\"text-align: right;\">          1.94671 </td><td style=\"text-align: right;\">       1.94671</td><td style=\"text-align: right;\"> 1689690881</td><td style=\"text-align: right;\">                   1</td><td>88ceeae9  </td></tr>\n",
       "<tr><td>FSR_Trainable_8a604e62</td><td>2023-07-18_23-22-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 98.0849</td><td style=\"text-align: right;\">1.07143e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">267245</td><td style=\"text-align: right;\">308.893 </td><td style=\"text-align: right;\">             1.75073</td><td style=\"text-align: right;\">          1.75073 </td><td style=\"text-align: right;\">       1.75073</td><td style=\"text-align: right;\"> 1689690169</td><td style=\"text-align: right;\">                   1</td><td>8a604e62  </td></tr>\n",
       "<tr><td>FSR_Trainable_8d3faa09</td><td>2023-07-18_22-59-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 88.3501</td><td style=\"text-align: right;\">1.40399e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">253625</td><td style=\"text-align: right;\">262.553 </td><td style=\"text-align: right;\">            59.6949 </td><td style=\"text-align: right;\">         14.6625  </td><td style=\"text-align: right;\">      59.6949 </td><td style=\"text-align: right;\"> 1689688795</td><td style=\"text-align: right;\">                   4</td><td>8d3faa09  </td></tr>\n",
       "<tr><td>FSR_Trainable_90b14738</td><td>2023-07-18_22-58-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">126.824 </td><td style=\"text-align: right;\">1.7319e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">253095</td><td style=\"text-align: right;\">338.683 </td><td style=\"text-align: right;\">             5.54146</td><td style=\"text-align: right;\">          5.54146 </td><td style=\"text-align: right;\">       5.54146</td><td style=\"text-align: right;\"> 1689688709</td><td style=\"text-align: right;\">                   1</td><td>90b14738  </td></tr>\n",
       "<tr><td>FSR_Trainable_934971da</td><td>2023-07-18_23-07-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 65.998 </td><td style=\"text-align: right;\">1.94836e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">262958</td><td style=\"text-align: right;\">230.09  </td><td style=\"text-align: right;\">            15.1349 </td><td style=\"text-align: right;\">          1.75629 </td><td style=\"text-align: right;\">      15.1349 </td><td style=\"text-align: right;\"> 1689689256</td><td style=\"text-align: right;\">                   8</td><td>934971da  </td></tr>\n",
       "<tr><td>FSR_Trainable_93c2f3ed</td><td>2023-07-18_23-30-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 33.3694</td><td style=\"text-align: right;\">2.78309e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">269519</td><td style=\"text-align: right;\">106.499 </td><td style=\"text-align: right;\">           122.785  </td><td style=\"text-align: right;\">          1.15708 </td><td style=\"text-align: right;\">     122.785  </td><td style=\"text-align: right;\"> 1689690636</td><td style=\"text-align: right;\">                 100</td><td>93c2f3ed  </td></tr>\n",
       "<tr><td>FSR_Trainable_98078ce8</td><td>2023-07-18_23-34-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 35.3823</td><td style=\"text-align: right;\">3.71133e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">272096</td><td style=\"text-align: right;\">110.701 </td><td style=\"text-align: right;\">           143.005  </td><td style=\"text-align: right;\">          1.20029 </td><td style=\"text-align: right;\">     143.005  </td><td style=\"text-align: right;\"> 1689690857</td><td style=\"text-align: right;\">                 100</td><td>98078ce8  </td></tr>\n",
       "<tr><td>FSR_Trainable_9bc1d71b</td><td>2023-07-18_23-04-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 80.5152</td><td style=\"text-align: right;\">2.09955e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">259951</td><td style=\"text-align: right;\">277.87  </td><td style=\"text-align: right;\">             2.33664</td><td style=\"text-align: right;\">          2.33664 </td><td style=\"text-align: right;\">       2.33664</td><td style=\"text-align: right;\"> 1689689048</td><td style=\"text-align: right;\">                   1</td><td>9bc1d71b  </td></tr>\n",
       "<tr><td>FSR_Trainable_9d20d9d5</td><td>2023-07-18_23-14-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">105.087 </td><td style=\"text-align: right;\">9.22481e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">264729</td><td style=\"text-align: right;\">312.428 </td><td style=\"text-align: right;\">             5.65253</td><td style=\"text-align: right;\">          5.65253 </td><td style=\"text-align: right;\">       5.65253</td><td style=\"text-align: right;\"> 1689689652</td><td style=\"text-align: right;\">                   1</td><td>9d20d9d5  </td></tr>\n",
       "<tr><td>FSR_Trainable_9dd92bce</td><td>2023-07-18_23-25-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 33.0225</td><td style=\"text-align: right;\">2.81191e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">267707</td><td style=\"text-align: right;\">106.472 </td><td style=\"text-align: right;\">           129.782  </td><td style=\"text-align: right;\">          1.16164 </td><td style=\"text-align: right;\">     129.782  </td><td style=\"text-align: right;\"> 1689690332</td><td style=\"text-align: right;\">                 100</td><td>9dd92bce  </td></tr>\n",
       "<tr><td>FSR_Trainable_9e13e128</td><td>2023-07-18_23-35-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 72.2649</td><td style=\"text-align: right;\">7.30652e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">274030</td><td style=\"text-align: right;\">234.93  </td><td style=\"text-align: right;\">             3.34967</td><td style=\"text-align: right;\">          1.66665 </td><td style=\"text-align: right;\">       3.34967</td><td style=\"text-align: right;\"> 1689690906</td><td style=\"text-align: right;\">                   2</td><td>9e13e128  </td></tr>\n",
       "<tr><td>FSR_Trainable_9fe93f5e</td><td>2023-07-18_22-58-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 79.9957</td><td style=\"text-align: right;\">2.05553e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">252679</td><td style=\"text-align: right;\">285.706 </td><td style=\"text-align: right;\">            37.7601 </td><td style=\"text-align: right;\">          2.56723 </td><td style=\"text-align: right;\">      37.7601 </td><td style=\"text-align: right;\"> 1689688723</td><td style=\"text-align: right;\">                  16</td><td>9fe93f5e  </td></tr>\n",
       "<tr><td>FSR_Trainable_a24f6e3c</td><td>2023-07-18_22-58-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">125.068 </td><td style=\"text-align: right;\">1.72319e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">253431</td><td style=\"text-align: right;\">326.228 </td><td style=\"text-align: right;\">             4.53862</td><td style=\"text-align: right;\">          4.53862 </td><td style=\"text-align: right;\">       4.53862</td><td style=\"text-align: right;\"> 1689688725</td><td style=\"text-align: right;\">                   1</td><td>a24f6e3c  </td></tr>\n",
       "<tr><td>FSR_Trainable_a3285c46</td><td>2023-07-18_23-04-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 42.0936</td><td style=\"text-align: right;\">6.15576e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">258157</td><td style=\"text-align: right;\">141.869 </td><td style=\"text-align: right;\">            80.6754 </td><td style=\"text-align: right;\">          1.16695 </td><td style=\"text-align: right;\">      80.6754 </td><td style=\"text-align: right;\"> 1689689062</td><td style=\"text-align: right;\">                  64</td><td>a3285c46  </td></tr>\n",
       "<tr><td>FSR_Trainable_a485388c</td><td>2023-07-18_23-05-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 72.1016</td><td style=\"text-align: right;\">2.05102e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">260634</td><td style=\"text-align: right;\">240.02  </td><td style=\"text-align: right;\">            27.7263 </td><td style=\"text-align: right;\">          3.78683 </td><td style=\"text-align: right;\">      27.7263 </td><td style=\"text-align: right;\"> 1689689107</td><td style=\"text-align: right;\">                   8</td><td>a485388c  </td></tr>\n",
       "<tr><td>FSR_Trainable_a6d1cf80</td><td>2023-07-18_23-10-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 51.8296</td><td style=\"text-align: right;\">8.57149e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">263652</td><td style=\"text-align: right;\">163     </td><td style=\"text-align: right;\">           138.973  </td><td style=\"text-align: right;\">          6.66599 </td><td style=\"text-align: right;\">     138.973  </td><td style=\"text-align: right;\"> 1689689442</td><td style=\"text-align: right;\">                  16</td><td>a6d1cf80  </td></tr>\n",
       "<tr><td>FSR_Trainable_a7398582</td><td>2023-07-18_23-01-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 38.1562</td><td style=\"text-align: right;\">2.62672e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">252743</td><td style=\"text-align: right;\">125.387 </td><td style=\"text-align: right;\">           173.559  </td><td style=\"text-align: right;\">          1.75623 </td><td style=\"text-align: right;\">     173.559  </td><td style=\"text-align: right;\"> 1689688882</td><td style=\"text-align: right;\">                 100</td><td>a7398582  </td></tr>\n",
       "<tr><td>FSR_Trainable_aad3656a</td><td>2023-07-18_23-25-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 34.2805</td><td style=\"text-align: right;\">2.67181e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">267477</td><td style=\"text-align: right;\">112.832 </td><td style=\"text-align: right;\">           132.96   </td><td style=\"text-align: right;\">          1.25979 </td><td style=\"text-align: right;\">     132.96   </td><td style=\"text-align: right;\"> 1689690324</td><td style=\"text-align: right;\">                 100</td><td>aad3656a  </td></tr>\n",
       "<tr><td>FSR_Trainable_abebba07</td><td>2023-07-18_23-35-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 32.1622</td><td style=\"text-align: right;\">2.66666e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">272412</td><td style=\"text-align: right;\">104.516 </td><td style=\"text-align: right;\">           136.889  </td><td style=\"text-align: right;\">          1.07583 </td><td style=\"text-align: right;\">     136.889  </td><td style=\"text-align: right;\"> 1689690913</td><td style=\"text-align: right;\">                 100</td><td>abebba07  </td></tr>\n",
       "<tr><td>FSR_Trainable_adab90cb</td><td>2023-07-18_23-14-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 36.9461</td><td style=\"text-align: right;\">2.66068e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">261757</td><td style=\"text-align: right;\">124.766 </td><td style=\"text-align: right;\">           524.214  </td><td style=\"text-align: right;\">          5.92642 </td><td style=\"text-align: right;\">     524.214  </td><td style=\"text-align: right;\"> 1689689667</td><td style=\"text-align: right;\">                 100</td><td>adab90cb  </td></tr>\n",
       "<tr><td>FSR_Trainable_b4d9d36a</td><td>2023-07-18_23-31-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 78.4856</td><td style=\"text-align: right;\">6.14366e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">271416</td><td style=\"text-align: right;\">266.45  </td><td style=\"text-align: right;\">             2.07568</td><td style=\"text-align: right;\">          2.07568 </td><td style=\"text-align: right;\">       2.07568</td><td style=\"text-align: right;\"> 1689690672</td><td style=\"text-align: right;\">                   1</td><td>b4d9d36a  </td></tr>\n",
       "<tr><td>FSR_Trainable_b6491f21</td><td>2023-07-18_23-34-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 34.9905</td><td style=\"text-align: right;\">2.99021e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">271889</td><td style=\"text-align: right;\">112.863 </td><td style=\"text-align: right;\">           145.606  </td><td style=\"text-align: right;\">          1.40008 </td><td style=\"text-align: right;\">     145.606  </td><td style=\"text-align: right;\"> 1689690847</td><td style=\"text-align: right;\">                 100</td><td>b6491f21  </td></tr>\n",
       "<tr><td>FSR_Trainable_b80f3bce</td><td>2023-07-18_23-03-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 81.8961</td><td style=\"text-align: right;\">2.52218e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">259732</td><td style=\"text-align: right;\">268.781 </td><td style=\"text-align: right;\">             2.63662</td><td style=\"text-align: right;\">          2.63662 </td><td style=\"text-align: right;\">       2.63662</td><td style=\"text-align: right;\"> 1689689038</td><td style=\"text-align: right;\">                   1</td><td>b80f3bce  </td></tr>\n",
       "<tr><td>FSR_Trainable_b900f871</td><td>2023-07-18_23-01-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">112.374 </td><td style=\"text-align: right;\">9.87011e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">255709</td><td style=\"text-align: right;\">342.017 </td><td style=\"text-align: right;\">             2.54926</td><td style=\"text-align: right;\">          2.54926 </td><td style=\"text-align: right;\">       2.54926</td><td style=\"text-align: right;\"> 1689688860</td><td style=\"text-align: right;\">                   1</td><td>b900f871  </td></tr>\n",
       "<tr><td>FSR_Trainable_bdbce26d</td><td>2023-07-18_23-06-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 83.5152</td><td style=\"text-align: right;\">1.06796e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">262494</td><td style=\"text-align: right;\">265.554 </td><td style=\"text-align: right;\">             1.96408</td><td style=\"text-align: right;\">          1.96408 </td><td style=\"text-align: right;\">       1.96408</td><td style=\"text-align: right;\"> 1689689204</td><td style=\"text-align: right;\">                   1</td><td>bdbce26d  </td></tr>\n",
       "<tr><td>FSR_Trainable_c0961f31</td><td>2023-07-18_23-01-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 63.5424</td><td style=\"text-align: right;\">1.52743e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">255932</td><td style=\"text-align: right;\">225.774 </td><td style=\"text-align: right;\">            30.837  </td><td style=\"text-align: right;\">          3.14238 </td><td style=\"text-align: right;\">      30.837  </td><td style=\"text-align: right;\"> 1689688906</td><td style=\"text-align: right;\">                   8</td><td>c0961f31  </td></tr>\n",
       "<tr><td>FSR_Trainable_c13d6c02</td><td>2023-07-18_23-30-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 82.8446</td><td style=\"text-align: right;\">1.0721e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">270752</td><td style=\"text-align: right;\">259.883 </td><td style=\"text-align: right;\">             1.44895</td><td style=\"text-align: right;\">          1.44895 </td><td style=\"text-align: right;\">       1.44895</td><td style=\"text-align: right;\"> 1689690641</td><td style=\"text-align: right;\">                   1</td><td>c13d6c02  </td></tr>\n",
       "<tr><td>FSR_Trainable_c1fb91e4</td><td>2023-07-18_23-08-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 73.1601</td><td style=\"text-align: right;\">2.13187e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">263195</td><td style=\"text-align: right;\">239.525 </td><td style=\"text-align: right;\">            15.6148 </td><td style=\"text-align: right;\">          1.94978 </td><td style=\"text-align: right;\">      15.6148 </td><td style=\"text-align: right;\"> 1689689287</td><td style=\"text-align: right;\">                   8</td><td>c1fb91e4  </td></tr>\n",
       "<tr><td>FSR_Trainable_c21fded1</td><td>2023-07-18_23-04-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 84.5181</td><td style=\"text-align: right;\">2.61893e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">260408</td><td style=\"text-align: right;\">258.153 </td><td style=\"text-align: right;\">            17.3665 </td><td style=\"text-align: right;\">          8.32899 </td><td style=\"text-align: right;\">      17.3665 </td><td style=\"text-align: right;\"> 1689689083</td><td style=\"text-align: right;\">                   2</td><td>c21fded1  </td></tr>\n",
       "<tr><td>FSR_Trainable_cb33ebc6</td><td>2023-07-18_23-30-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 32.9884</td><td style=\"text-align: right;\">2.81811e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">269307</td><td style=\"text-align: right;\">106.306 </td><td style=\"text-align: right;\">           123.722  </td><td style=\"text-align: right;\">          1.15082 </td><td style=\"text-align: right;\">     123.722  </td><td style=\"text-align: right;\"> 1689690618</td><td style=\"text-align: right;\">                 100</td><td>cb33ebc6  </td></tr>\n",
       "<tr><td>FSR_Trainable_cc3c4bbf</td><td>2023-07-18_23-36-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 37.7465</td><td style=\"text-align: right;\">3.61412e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">275161</td><td style=\"text-align: right;\">118.461 </td><td style=\"text-align: right;\">            46.7343 </td><td style=\"text-align: right;\">          1.39895 </td><td style=\"text-align: right;\">      46.7343 </td><td style=\"text-align: right;\"> 1689691009</td><td style=\"text-align: right;\">                  32</td><td>cc3c4bbf  </td></tr>\n",
       "<tr><td>FSR_Trainable_cdff7ec2</td><td>2023-07-18_23-00-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 87.3424</td><td style=\"text-align: right;\">2.95309e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">254995</td><td style=\"text-align: right;\">261.833 </td><td style=\"text-align: right;\">             9.14809</td><td style=\"text-align: right;\">          2.00405 </td><td style=\"text-align: right;\">       9.14809</td><td style=\"text-align: right;\"> 1689688823</td><td style=\"text-align: right;\">                   4</td><td>cdff7ec2  </td></tr>\n",
       "<tr><td>FSR_Trainable_d0ec6762</td><td>2023-07-18_23-28-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 31.7957</td><td style=\"text-align: right;\">2.55833e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">268505</td><td style=\"text-align: right;\">103.191 </td><td style=\"text-align: right;\">           129.751  </td><td style=\"text-align: right;\">          1.60622 </td><td style=\"text-align: right;\">     129.751  </td><td style=\"text-align: right;\"> 1689690490</td><td style=\"text-align: right;\">                 100</td><td>d0ec6762  </td></tr>\n",
       "<tr><td>FSR_Trainable_d263706c</td><td>2023-07-18_23-13-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 85.652 </td><td style=\"text-align: right;\">3.00292e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">264500</td><td style=\"text-align: right;\">255.256 </td><td style=\"text-align: right;\">             5.98309</td><td style=\"text-align: right;\">          2.90253 </td><td style=\"text-align: right;\">       5.98309</td><td style=\"text-align: right;\"> 1689689634</td><td style=\"text-align: right;\">                   2</td><td>d263706c  </td></tr>\n",
       "<tr><td>FSR_Trainable_d51dd6ac</td><td>2023-07-18_22-59-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 81.3362</td><td style=\"text-align: right;\">6.13812e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">253860</td><td style=\"text-align: right;\">287.891 </td><td style=\"text-align: right;\">            13.6251 </td><td style=\"text-align: right;\">         13.6251  </td><td style=\"text-align: right;\">      13.6251 </td><td style=\"text-align: right;\"> 1689688756</td><td style=\"text-align: right;\">                   1</td><td>d51dd6ac  </td></tr>\n",
       "<tr><td>FSR_Trainable_d8007d6c</td><td>2023-07-18_22-59-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 81.1787</td><td style=\"text-align: right;\">2.2596e+08 </td><td>172.26.215.93</td><td style=\"text-align: right;\">254332</td><td style=\"text-align: right;\">266.449 </td><td style=\"text-align: right;\">            13.1614 </td><td style=\"text-align: right;\">          5.67965 </td><td style=\"text-align: right;\">      13.1614 </td><td style=\"text-align: right;\"> 1689688786</td><td style=\"text-align: right;\">                   2</td><td>d8007d6c  </td></tr>\n",
       "<tr><td>FSR_Trainable_da8def6e</td><td>2023-07-18_23-01-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">113.735 </td><td style=\"text-align: right;\">1.95658e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">256846</td><td style=\"text-align: right;\">287.848 </td><td style=\"text-align: right;\">             1.38564</td><td style=\"text-align: right;\">          1.38564 </td><td style=\"text-align: right;\">       1.38564</td><td style=\"text-align: right;\"> 1689688919</td><td style=\"text-align: right;\">                   1</td><td>da8def6e  </td></tr>\n",
       "<tr><td>FSR_Trainable_e22fdcc1</td><td>2023-07-18_23-34-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 54.0515</td><td style=\"text-align: right;\">6.7497e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">272900</td><td style=\"text-align: right;\">170.602 </td><td style=\"text-align: right;\">             6.11805</td><td style=\"text-align: right;\">          1.26712 </td><td style=\"text-align: right;\">       6.11805</td><td style=\"text-align: right;\"> 1689690855</td><td style=\"text-align: right;\">                   4</td><td>e22fdcc1  </td></tr>\n",
       "<tr><td>FSR_Trainable_eba671e0</td><td>2023-07-18_23-30-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 70.2536</td><td style=\"text-align: right;\">8.6968e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">270988</td><td style=\"text-align: right;\">227.422 </td><td style=\"text-align: right;\">             2.80325</td><td style=\"text-align: right;\">          1.29798 </td><td style=\"text-align: right;\">       2.80325</td><td style=\"text-align: right;\"> 1689690655</td><td style=\"text-align: right;\">                   2</td><td>eba671e0  </td></tr>\n",
       "<tr><td>FSR_Trainable_ec17fec0</td><td>2023-07-18_23-03-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 81.1435</td><td style=\"text-align: right;\">8.455e+16  </td><td>172.26.215.93</td><td style=\"text-align: right;\">259273</td><td style=\"text-align: right;\">270.133 </td><td style=\"text-align: right;\">             2.49293</td><td style=\"text-align: right;\">          2.49293 </td><td style=\"text-align: right;\">       2.49293</td><td style=\"text-align: right;\"> 1689689019</td><td style=\"text-align: right;\">                   1</td><td>ec17fec0  </td></tr>\n",
       "<tr><td>FSR_Trainable_f0ab078d</td><td>2023-07-18_23-33-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 33.3085</td><td style=\"text-align: right;\">2.90881e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">271613</td><td style=\"text-align: right;\">105.425 </td><td style=\"text-align: right;\">           122.403  </td><td style=\"text-align: right;\">          1.22209 </td><td style=\"text-align: right;\">     122.403  </td><td style=\"text-align: right;\"> 1689690813</td><td style=\"text-align: right;\">                 100</td><td>f0ab078d  </td></tr>\n",
       "<tr><td>FSR_Trainable_f3b05426</td><td>2023-07-18_23-32-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 31.8616</td><td style=\"text-align: right;\">2.55116e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">270522</td><td style=\"text-align: right;\">103.51  </td><td style=\"text-align: right;\">           104.765  </td><td style=\"text-align: right;\">          1.14862 </td><td style=\"text-align: right;\">     104.765  </td><td style=\"text-align: right;\"> 1689690749</td><td style=\"text-align: right;\">                 100</td><td>f3b05426  </td></tr>\n",
       "<tr><td>FSR_Trainable_f3b152dd</td><td>2023-07-18_23-34-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 82.2224</td><td style=\"text-align: right;\">8.81166e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">273360</td><td style=\"text-align: right;\">264.137 </td><td style=\"text-align: right;\">             1.96236</td><td style=\"text-align: right;\">          1.96236 </td><td style=\"text-align: right;\">       1.96236</td><td style=\"text-align: right;\"> 1689690871</td><td style=\"text-align: right;\">                   1</td><td>f3b152dd  </td></tr>\n",
       "<tr><td>FSR_Trainable_f74e668e</td><td>2023-07-18_22-59-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 88.3416</td><td style=\"text-align: right;\">2.82474e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">254067</td><td style=\"text-align: right;\">260.941 </td><td style=\"text-align: right;\">            23.9141 </td><td style=\"text-align: right;\">          6.80847 </td><td style=\"text-align: right;\">      23.9141 </td><td style=\"text-align: right;\"> 1689688779</td><td style=\"text-align: right;\">                   4</td><td>f74e668e  </td></tr>\n",
       "<tr><td>FSR_Trainable_f7b1917c</td><td>2023-07-18_23-02-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 99.3735</td><td style=\"text-align: right;\">1.09957e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">257259</td><td style=\"text-align: right;\">316.137 </td><td style=\"text-align: right;\">             2.62439</td><td style=\"text-align: right;\">          2.62439 </td><td style=\"text-align: right;\">       2.62439</td><td style=\"text-align: right;\"> 1689688935</td><td style=\"text-align: right;\">                   1</td><td>f7b1917c  </td></tr>\n",
       "<tr><td>FSR_Trainable_f8ccb7f8</td><td>2023-07-18_23-24-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 37.8393</td><td style=\"text-align: right;\">3.52706e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">266815</td><td style=\"text-align: right;\">121.622 </td><td style=\"text-align: right;\">           129.943  </td><td style=\"text-align: right;\">          1.3016  </td><td style=\"text-align: right;\">     129.943  </td><td style=\"text-align: right;\"> 1689690280</td><td style=\"text-align: right;\">                 100</td><td>f8ccb7f8  </td></tr>\n",
       "<tr><td>FSR_Trainable_fa31eec0</td><td>2023-07-18_23-10-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 48.3548</td><td style=\"text-align: right;\">6.02114e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">263376</td><td style=\"text-align: right;\">162.968 </td><td style=\"text-align: right;\">           126.315  </td><td style=\"text-align: right;\">          4.66526 </td><td style=\"text-align: right;\">     126.315  </td><td style=\"text-align: right;\"> 1689689409</td><td style=\"text-align: right;\">                  16</td><td>fa31eec0  </td></tr>\n",
       "<tr><td>FSR_Trainable_fc918be6</td><td>2023-07-18_23-00-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 80.8627</td><td style=\"text-align: right;\">1.80737e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">255274</td><td style=\"text-align: right;\">266.981 </td><td style=\"text-align: right;\">            26.4492 </td><td style=\"text-align: right;\">          6.59144 </td><td style=\"text-align: right;\">      26.4492 </td><td style=\"text-align: right;\"> 1689688859</td><td style=\"text-align: right;\">                   4</td><td>fc918be6  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_9fe93f5e_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-18_22-57-57/wandb/run-20230718_225806-9fe93f5e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: Syncing run FSR_Trainable_9fe93f5e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9fe93f5e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_a7398582_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-18_22-58-01/wandb/run-20230718_225813-a7398582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: Syncing run FSR_Trainable_a7398582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a7398582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_4b419ffa_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-18_22-58-07/wandb/run-20230718_225821-4b419ffa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Syncing run FSR_Trainable_4b419ffa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4b419ffa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_90b14738_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-18_22-58-15/wandb/run-20230718_225831-90b14738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: Syncing run FSR_Trainable_90b14738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/90b14738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb:                      mae 126.82376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb:                     mape 1.731899847609691e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb:                     rmse 338.6834\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb:       time_since_restore 5.54146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb:         time_this_iter_s 5.54146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb:             time_total_s 5.54146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb:                timestamp 1689688709\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: 🚀 View run FSR_Trainable_90b14738 at: https://wandb.ai/seokjin/FSR-prediction/runs/90b14738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253259)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225831-90b14738/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb:                      mae ██▇▇▆▆▅▅▄▄▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb:                     mape ██▇▇▆▆▅▅▄▄▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb:                     rmse ██▇▇▆▆▅▅▄▄▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb:         time_this_iter_s ▂▁▄▄▃▄▇▆▆█▆▄▄▄▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb:                timestamp ▁▂▂▂▃▃▄▄▅▅▆▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb:                      mae 79.99575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb:                     mape 2055526592141987.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb:                     rmse 285.70585\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb:       time_since_restore 37.76011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb:         time_this_iter_s 2.56723\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb:             time_total_s 37.76011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb:                timestamp 1689688723\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: 🚀 View run FSR_Trainable_9fe93f5e at: https://wandb.ai/seokjin/FSR-prediction/runs/9fe93f5e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252742)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225806-9fe93f5e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_a24f6e3c_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-18_22-58-24/wandb/run-20230718_225848-a24f6e3c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: Syncing run FSR_Trainable_a24f6e3c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a24f6e3c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb:                      mae 125.06779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb:                     mape 1.7231900334317114e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb:                     rmse 326.22825\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb:       time_since_restore 4.53862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb:         time_this_iter_s 4.53862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb:             time_total_s 4.53862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb:                timestamp 1689688725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: 🚀 View run FSR_Trainable_a24f6e3c at: https://wandb.ai/seokjin/FSR-prediction/runs/a24f6e3c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253501)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225848-a24f6e3c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_8d3faa09_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-18_22-58-41/wandb/run-20230718_225859-8d3faa09\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: Syncing run FSR_Trainable_8d3faa09\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8d3faa09\n",
      "2023-07-18 22:59:09,529\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.186 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:59:09,530\tWARNING util.py:315 -- The `process_trial_result` operation took 2.188 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:59:09,532\tWARNING util.py:315 -- Processing trial results took 2.190 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:59:09,535\tWARNING util.py:315 -- The `process_trial_result` operation took 2.193 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_d51dd6ac_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-18_22-58-53/wandb/run-20230718_225909-d51dd6ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: Syncing run FSR_Trainable_d51dd6ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d51dd6ac\n",
      "2023-07-18 22:59:18,423\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.068 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:59:18,428\tWARNING util.py:315 -- The `process_trial_result` operation took 2.073 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:59:18,429\tWARNING util.py:315 -- Processing trial results took 2.075 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:59:18,432\tWARNING util.py:315 -- The `process_trial_result` operation took 2.077 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-18 22:59:21,402\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.260 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:59:21,410\tWARNING util.py:315 -- The `process_trial_result` operation took 2.269 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:59:21,412\tWARNING util.py:315 -- Processing trial results took 2.272 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:59:21,417\tWARNING util.py:315 -- The `process_trial_result` operation took 2.276 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_f74e668e_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-18_22-59-02/wandb/run-20230718_225920-f74e668e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: Syncing run FSR_Trainable_f74e668e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f74e668e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb:                      mae 81.33621\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb:                     mape 6.138116999664591e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb:                     rmse 287.89082\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb:       time_since_restore 13.62514\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb:         time_this_iter_s 13.62514\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb:             time_total_s 13.62514\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb:                timestamp 1689688756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: 🚀 View run FSR_Trainable_d51dd6ac at: https://wandb.ai/seokjin/FSR-prediction/runs/d51dd6ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253956)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225909-d51dd6ac/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-18 22:59:40,443\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.229 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:59:40,446\tWARNING util.py:315 -- The `process_trial_result` operation took 2.232 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:59:40,449\tWARNING util.py:315 -- Processing trial results took 2.235 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:59:40,451\tWARNING util.py:315 -- The `process_trial_result` operation took 2.238 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_d8007d6c_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-18_22-59-13/wandb/run-20230718_225939-d8007d6c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Syncing run FSR_Trainable_d8007d6c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d8007d6c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb:                      mae ▄▁▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb:                     mape ▂▁▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb:                     rmse ▁█▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb:       time_since_restore ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb:         time_this_iter_s ▄▁▄█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb:             time_total_s ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb:                timestamp ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb:                      mae 88.34156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb:                     mape 282474434.21375\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb:                     rmse 260.94135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb:       time_since_restore 23.91409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb:         time_this_iter_s 6.80847\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb:             time_total_s 23.91409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb:                timestamp 1689688779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: 🚀 View run FSR_Trainable_f74e668e at: https://wandb.ai/seokjin/FSR-prediction/runs/f74e668e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254168)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225920-f74e668e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225939-d8007d6c/logs\n",
      "2023-07-18 22:59:55,331\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.059 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:59:55,337\tWARNING util.py:315 -- The `process_trial_result` operation took 2.065 s, which may be a performance bottleneck.\n",
      "2023-07-18 22:59:55,338\tWARNING util.py:315 -- Processing trial results took 2.066 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 22:59:55,340\tWARNING util.py:315 -- The `process_trial_result` operation took 2.068 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254405)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_2e4dba12_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-59-30/wandb/run-20230718_225958-2e4dba12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: Syncing run FSR_Trainable_2e4dba12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2e4dba12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb:                      mae ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb:                     mape ▁▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb:                     rmse █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb:         time_this_iter_s ▁▄█▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb:                timestamp ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb:                      mae 88.3501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb:                     mape 1.403985161042392e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb:                     rmse 262.55311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb:       time_since_restore 59.6949\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb:         time_this_iter_s 14.66255\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb:             time_total_s 59.6949\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb:                timestamp 1689688795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: 🚀 View run FSR_Trainable_8d3faa09 at: https://wandb.ai/seokjin/FSR-prediction/runs/8d3faa09\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225859-8d3faa09/logs\n",
      "2023-07-18 23:00:05,989\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.972 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:00:05,991\tWARNING util.py:315 -- The `process_trial_result` operation took 1.975 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:00:05,996\tWARNING util.py:315 -- Processing trial results took 1.980 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:00:05,998\tWARNING util.py:315 -- The `process_trial_result` operation took 1.982 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253737)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_064adcc1_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_22-59-51/wandb/run-20230718_230008-064adcc1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: Syncing run FSR_Trainable_064adcc1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/064adcc1\n",
      "2023-07-18 23:00:16,884\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.739 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:00:16,886\tWARNING util.py:315 -- The `process_trial_result` operation took 1.742 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:00:16,887\tWARNING util.py:315 -- Processing trial results took 1.743 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:00:16,888\tWARNING util.py:315 -- The `process_trial_result` operation took 1.744 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_cdff7ec2_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-00-01/wandb/run-20230718_230019-cdff7ec2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Syncing run FSR_Trainable_cdff7ec2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cdff7ec2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb:                      mae █▇▅▄▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb:                     mape ▇█▇▅▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb:                     rmse █▄▃▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb:         time_this_iter_s █▄▁▂▅▂▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb:                timestamp ▁▃▃▄▅▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb:                      mae 89.67303\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb:                     mape 1.477410429454563e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb:                     rmse 259.90495\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb:       time_since_restore 16.38627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb:         time_this_iter_s 1.75382\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb:             time_total_s 16.38627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb:                timestamp 1689688820\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: 🚀 View run FSR_Trainable_064adcc1 at: https://wandb.ai/seokjin/FSR-prediction/runs/064adcc1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254881)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230008-064adcc1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb:                      mae █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb:                     mape █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb:                     rmse █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb:         time_this_iter_s █▅▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230019-cdff7ec2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-18 23:00:39,778\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.061 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:00:39,780\tWARNING util.py:315 -- The `process_trial_result` operation took 2.064 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:00:39,783\tWARNING util.py:315 -- Processing trial results took 2.067 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:00:39,785\tWARNING util.py:315 -- The `process_trial_result` operation took 2.068 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_fc918be6_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-00-12/wandb/run-20230718_230038-fc918be6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: Syncing run FSR_Trainable_fc918be6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fc918be6\n",
      "2023-07-18 23:00:46,593\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.906 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:00:46,597\tWARNING util.py:315 -- The `process_trial_result` operation took 1.912 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:00:46,599\tWARNING util.py:315 -- Processing trial results took 1.913 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:00:46,600\tWARNING util.py:315 -- The `process_trial_result` operation took 1.914 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_25519ac6_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-00-31/wandb/run-20230718_230048-25519ac6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: Syncing run FSR_Trainable_25519ac6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/25519ac6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb:                      mae 156.0324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb:                     mape 1.6991984820836822e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb:                     rmse 438.71741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb:       time_since_restore 2.52168\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb:         time_this_iter_s 2.52168\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb:             time_total_s 2.52168\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb:                timestamp 1689688844\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: 🚀 View run FSR_Trainable_25519ac6 at: https://wandb.ai/seokjin/FSR-prediction/runs/25519ac6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255559)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230048-25519ac6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:01:02,937\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.055 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:01:02,942\tWARNING util.py:315 -- The `process_trial_result` operation took 2.061 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:01:02,944\tWARNING util.py:315 -- Processing trial results took 2.063 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:01:02,945\tWARNING util.py:315 -- The `process_trial_result` operation took 2.064 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb:                      mae ▁▁█▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb:                     mape ▁▂██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb:                     rmse ▄▁▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb:         time_this_iter_s █▂▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb:                timestamp ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb:                      mae 80.86265\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb:                     mape 1.807370399512458e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb:                     rmse 266.98078\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb:       time_since_restore 26.4492\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb:         time_this_iter_s 6.59144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb:             time_total_s 26.4492\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb:                timestamp 1689688859\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: 🚀 View run FSR_Trainable_fc918be6 at: https://wandb.ai/seokjin/FSR-prediction/runs/fc918be6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255345)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230038-fc918be6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_b900f871_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-00-42/wandb/run-20230718_230105-b900f871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: Syncing run FSR_Trainable_b900f871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b900f871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb:                      mae 112.37373\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb:                     mape 9.870107922883478e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb:                     rmse 342.01731\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb:       time_since_restore 2.54926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb:         time_this_iter_s 2.54926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb:             time_total_s 2.54926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb:                timestamp 1689688860\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: 🚀 View run FSR_Trainable_b900f871 at: https://wandb.ai/seokjin/FSR-prediction/runs/b900f871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255792)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230105-b900f871/logs\n",
      "2023-07-18 23:01:17,261\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.181 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:01:17,266\tWARNING util.py:315 -- The `process_trial_result` operation took 2.187 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:01:17,268\tWARNING util.py:315 -- Processing trial results took 2.189 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:01:17,269\tWARNING util.py:315 -- The `process_trial_result` operation took 2.190 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_c0961f31_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-00-58/wandb/run-20230718_230118-c0961f31\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: Syncing run FSR_Trainable_c0961f31\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c0961f31\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb:                      mae █▅▄▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb:                     mape █▇▃▂▁▁▂▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb:                     rmse █▅▄▄▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb:         time_this_iter_s ▄▁▃▁▄▁▂▂▃▁▅▂▁▃▂▅▃▂█▇▃▁▄▂▅▅▃▃▂▂▅▃▄▄▃▅▁▂▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb:                      mae 38.15618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb:                     mape 2.626724587192279e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb:                     rmse 125.38659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb:       time_since_restore 173.55882\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb:         time_this_iter_s 1.75623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb:             time_total_s 173.55882\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb:                timestamp 1689688882\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: 🚀 View run FSR_Trainable_a7398582 at: https://wandb.ai/seokjin/FSR-prediction/runs/a7398582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252920)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225813-a7398582/logs\n",
      "2023-07-18 23:01:29,392\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.156 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:01:29,409\tWARNING util.py:315 -- The `process_trial_result` operation took 2.173 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:01:29,416\tWARNING util.py:315 -- Processing trial results took 2.181 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:01:29,420\tWARNING util.py:315 -- The `process_trial_result` operation took 2.184 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_7e1f0626_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-01-11/wandb/run-20230718_230129-7e1f0626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: Syncing run FSR_Trainable_7e1f0626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7e1f0626\n",
      "2023-07-18 23:01:40,302\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.035 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:01:40,307\tWARNING util.py:315 -- The `process_trial_result` operation took 2.041 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:01:40,309\tWARNING util.py:315 -- Processing trial results took 2.043 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:01:40,321\tWARNING util.py:315 -- The `process_trial_result` operation took 2.055 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_64598154_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-01-22/wandb/run-20230718_230141-64598154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb: Syncing run FSR_Trainable_64598154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/64598154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb:                      mae █▆▅▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb:                     mape █▆▅▅▃▂▁▁▁▂▃▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb:                     rmse █▇▆▅▃▃▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb:         time_this_iter_s █▄▁▂▅▅▄▃▆▅▃▃▂▂▂▄▆▄▃▅▄▃▄▃▃▆▄▂▁▂▄▄▄▂▅▄▅▃▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb:                      mae 42.30902\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb:                     mape 2.6240223937575016e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb:                     rmse 142.48208\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb:       time_since_restore 94.69728\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb:         time_this_iter_s 1.50084\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb:             time_total_s 94.69728\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb:                timestamp 1689688898\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: 🚀 View run FSR_Trainable_2e4dba12 at: https://wandb.ai/seokjin/FSR-prediction/runs/2e4dba12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254653)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_225958-2e4dba12/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256477)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb:                      mae ▅▄▆█▂▄▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb:                     mape █▅▇▇▁▄▅▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb:                     rmse █▃█▇▆▅▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb:         time_this_iter_s ▄▁▄▃▆█▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: iterations_since_restore 8\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb:                      mae 63.54237\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb:                     mape 152743272.24823\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb:                     rmse 225.77361\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb:       time_since_restore 30.837\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb:         time_this_iter_s 3.14238\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb:             time_total_s 30.837\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb:                timestamp 1689688906\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb:       training_iteration 8\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: 🚀 View run FSR_Trainable_c0961f31 at: https://wandb.ai/seokjin/FSR-prediction/runs/c0961f31\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256029)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230118-c0961f31/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:01:54,735\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.902 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:01:54,740\tWARNING util.py:315 -- The `process_trial_result` operation took 1.908 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:01:54,742\tWARNING util.py:315 -- Processing trial results took 1.910 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:01:54,744\tWARNING util.py:315 -- The `process_trial_result` operation took 1.912 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb:                      mae ▆▅█▂▆▅▁▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb:                     mape ▅▃█▃▇▄▁▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb:                     rmse ▆▆█▃▆▆▁▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb:       time_since_restore ▁▂▄▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb:         time_this_iter_s █▇▆▃▂▂▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb:             time_total_s ▁▂▄▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb:                timestamp ▁▃▄▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb:                      mae 82.10341\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb:                     mape 277262806.7792\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb:                     rmse 259.13065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb:       time_since_restore 27.59432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb:         time_this_iter_s 3.13135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb:             time_total_s 27.59432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb:                timestamp 1689688912\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: 🚀 View run FSR_Trainable_7e1f0626 at: https://wandb.ai/seokjin/FSR-prediction/runs/7e1f0626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256248)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230129-7e1f0626/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_1a311da9_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-01-34/wandb/run-20230718_230155-1a311da9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Syncing run FSR_Trainable_1a311da9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1a311da9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:02:01,536\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.080 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:02:01,541\tWARNING util.py:315 -- The `process_trial_result` operation took 2.086 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:02:01,544\tWARNING util.py:315 -- Processing trial results took 2.089 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:02:01,546\tWARNING util.py:315 -- The `process_trial_result` operation took 2.091 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230155-1a311da9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230155-1a311da9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230155-1a311da9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230155-1a311da9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230155-1a311da9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230155-1a311da9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230155-1a311da9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230155-1a311da9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230155-1a311da9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230155-1a311da9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230155-1a311da9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230155-1a311da9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230155-1a311da9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256726)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230155-1a311da9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_da8def6e_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-01-49/wandb/run-20230718_230203-da8def6e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: Syncing run FSR_Trainable_da8def6e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/da8def6e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:02:08,970\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.989 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:02:08,974\tWARNING util.py:315 -- The `process_trial_result` operation took 1.994 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:02:08,976\tWARNING util.py:315 -- Processing trial results took 1.995 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:02:08,977\tWARNING util.py:315 -- The `process_trial_result` operation took 1.997 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb:                      mae 113.73514\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb:                     mape 1.9565802403613677e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb:                     rmse 287.84809\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb:       time_since_restore 1.38564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb:         time_this_iter_s 1.38564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb:             time_total_s 1.38564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb:                timestamp 1689688919\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: 🚀 View run FSR_Trainable_da8def6e at: https://wandb.ai/seokjin/FSR-prediction/runs/da8def6e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256957)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230203-da8def6e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_4fe847ca_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-01-58/wandb/run-20230718_230211-4fe847ca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: Syncing run FSR_Trainable_4fe847ca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4fe847ca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-07-18 23:02:17,839\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.033 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:02:17,881\tWARNING util.py:315 -- The `process_trial_result` operation took 2.076 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:02:17,890\tWARNING util.py:315 -- Processing trial results took 2.085 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:02:17,892\tWARNING util.py:315 -- The `process_trial_result` operation took 2.087 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb:                      mae 130.38353\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb:                     mape 1.3251468919813523e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb:                     rmse 414.82066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb:       time_since_restore 1.70054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb:         time_this_iter_s 1.70054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb:             time_total_s 1.70054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb:                timestamp 1689688926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: 🚀 View run FSR_Trainable_4fe847ca at: https://wandb.ai/seokjin/FSR-prediction/runs/4fe847ca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257143)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230211-4fe847ca/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:02:25,236\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.853 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:02:25,255\tWARNING util.py:315 -- The `process_trial_result` operation took 1.873 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:02:25,258\tWARNING util.py:315 -- Processing trial results took 1.876 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:02:25,259\tWARNING util.py:315 -- The `process_trial_result` operation took 1.878 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230219-f7b1917c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_5cfdb725_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-02-13/wandb/run-20230718_230227-5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: Syncing run FSR_Trainable_5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257350)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "2023-07-18 23:02:33,769\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.164 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:02:33,772\tWARNING util.py:315 -- The `process_trial_result` operation took 2.168 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:02:33,774\tWARNING util.py:315 -- Processing trial results took 2.171 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:02:33,776\tWARNING util.py:315 -- The `process_trial_result` operation took 2.173 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_5efe99c1_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-02-21/wandb/run-20230718_230235-5efe99c1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Syncing run FSR_Trainable_5efe99c1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5efe99c1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb:                      mae ▄▇▂▁▃█▂█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb:                     mape ▄▅▂▁▄█▃█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb:                     rmse ▂▆▁▁▁█▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb:         time_this_iter_s █▃▂▁▆▅▅▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb:                timestamp ▁▃▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb:                      mae 75.86148\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb:                     mape 276513109.58408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb:                     rmse 253.94877\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb:       time_since_restore 13.62774\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb:         time_this_iter_s 1.54629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb:             time_total_s 13.62774\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb:                timestamp 1689688957\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: 🚀 View run FSR_Trainable_5cfdb725 at: https://wandb.ai/seokjin/FSR-prediction/runs/5cfdb725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257591)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230227-5cfdb725/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:02:42,900\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.991 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:02:42,906\tWARNING util.py:315 -- The `process_trial_result` operation took 1.997 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:02:42,908\tWARNING util.py:315 -- Processing trial results took 1.999 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:02:42,911\tWARNING util.py:315 -- The `process_trial_result` operation took 2.002 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb:                      mae ▁▁▂█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb:                     mape ▃▁▂█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb:                     rmse ▁▁▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb:         time_this_iter_s █▇▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230235-5efe99c1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_6d014408_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-02-29/wandb/run-20230718_230244-6d014408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: Syncing run FSR_Trainable_6d014408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6d014408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6d014408\n",
      "2023-07-18 23:02:50,836\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.926 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:02:50,840\tWARNING util.py:315 -- The `process_trial_result` operation took 1.930 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:02:50,841\tWARNING util.py:315 -- Processing trial results took 1.932 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:02:50,843\tWARNING util.py:315 -- The `process_trial_result` operation took 1.933 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257818)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_a3285c46_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-02-38/wandb/run-20230718_230253-a3285c46\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: Syncing run FSR_Trainable_a3285c46\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a3285c46\n",
      "2023-07-18 23:03:01,071\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.029 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:03:01,076\tWARNING util.py:315 -- The `process_trial_result` operation took 2.035 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:03:01,078\tWARNING util.py:315 -- Processing trial results took 2.037 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:03:01,080\tWARNING util.py:315 -- The `process_trial_result` operation took 2.038 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_4b114ad5_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-02-47/wandb/run-20230718_230304-4b114ad5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: Syncing run FSR_Trainable_4b114ad5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4b114ad5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb:                      mae 83.81393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb:                     mape 5911264843057541.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb:                     rmse 290.6167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb:       time_since_restore 1.83469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb:         time_this_iter_s 1.83469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb:             time_total_s 1.83469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb:                timestamp 1689688979\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: 🚀 View run FSR_Trainable_4b114ad5 at: https://wandb.ai/seokjin/FSR-prediction/runs/4b114ad5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258483)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230304-4b114ad5/logs\n",
      "2023-07-18 23:03:11,489\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.381 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:03:11,494\tWARNING util.py:315 -- The `process_trial_result` operation took 2.386 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:03:11,496\tWARNING util.py:315 -- Processing trial results took 2.389 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:03:11,499\tWARNING util.py:315 -- The `process_trial_result` operation took 2.392 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_367ceef8_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-02-57/wandb/run-20230718_230314-367ceef8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Syncing run FSR_Trainable_367ceef8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/367ceef8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb:                      mae ▂▁▂▂▁▂▄▁▂▃▅▃▂█▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb:                     mape ▃▁▄▄▃▄▆▃▃▅▆▄▄█▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb:                     rmse ▂▂▂▃▂▃▄▁▂▄▅▃▂█▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▄▅▆▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb:         time_this_iter_s ▆▄▂▁▅▆▂▄▅▅▆▂▂▇▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▄▅▆▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb:                timestamp ▁▂▂▂▃▄▄▄▅▅▆▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb:                      mae 63.58235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb:                     mape 161383242.39055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb:                     rmse 221.8772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb:       time_since_restore 29.27168\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb:         time_this_iter_s 2.32306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb:             time_total_s 29.27168\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb:                timestamp 1689688993\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: 🚀 View run FSR_Trainable_6d014408 at: https://wandb.ai/seokjin/FSR-prediction/runs/6d014408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258038)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230244-6d014408/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb:       training_iteration ▁█\n",
      "2023-07-18 23:03:24,211\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.784 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:03:24,213\tWARNING util.py:315 -- The `process_trial_result` operation took 1.787 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:03:24,219\tWARNING util.py:315 -- Processing trial results took 1.793 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:03:24,220\tWARNING util.py:315 -- The `process_trial_result` operation took 1.794 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258702)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_45a55fd7_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-03-07/wandb/run-20230718_230324-45a55fd7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: Syncing run FSR_Trainable_45a55fd7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/45a55fd7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:03:32,251\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.237 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:03:32,257\tWARNING util.py:315 -- The `process_trial_result` operation took 2.244 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:03:32,277\tWARNING util.py:315 -- Processing trial results took 2.264 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:03:32,279\tWARNING util.py:315 -- The `process_trial_result` operation took 2.266 s, which may be a performance bottleneck.\n",
      "wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)04 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb:                      mae 90.41956\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb:                     mape 291515188.64206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb:                     rmse 259.11489\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb:       time_since_restore 10.3772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb:         time_this_iter_s 5.19282\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb:             time_total_s 10.3772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb:                timestamp 1689689009\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: 🚀 View run FSR_Trainable_45a55fd7 at: https://wandb.ai/seokjin/FSR-prediction/runs/45a55fd7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258936)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230324-45a55fd7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_534128b7_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-03-17/wandb/run-20230718_230334-534128b7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Syncing run FSR_Trainable_534128b7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/534128b7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230334-534128b7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230334-534128b7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230334-534128b7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230334-534128b7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230334-534128b7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230334-534128b7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230334-534128b7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230334-534128b7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230334-534128b7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230334-534128b7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230334-534128b7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230334-534128b7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230334-534128b7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259160)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230334-534128b7/logs\n",
      "2023-07-18 23:03:41,230\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.885 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:03:41,236\tWARNING util.py:315 -- The `process_trial_result` operation took 1.892 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:03:41,238\tWARNING util.py:315 -- Processing trial results took 1.894 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:03:41,239\tWARNING util.py:315 -- The `process_trial_result` operation took 1.895 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_ec17fec0_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-03-27/wandb/run-20230718_230343-ec17fec0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: Syncing run FSR_Trainable_ec17fec0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ec17fec0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb:                      mae 81.14349\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb:                     mape 8.455004331350403e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb:                     rmse 270.13335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb:       time_since_restore 2.49293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb:         time_this_iter_s 2.49293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb:             time_total_s 2.49293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb:                timestamp 1689689019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: 🚀 View run FSR_Trainable_ec17fec0 at: https://wandb.ai/seokjin/FSR-prediction/runs/ec17fec0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259388)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230343-ec17fec0/logs\n",
      "2023-07-18 23:03:50,531\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.174 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:03:50,534\tWARNING util.py:315 -- The `process_trial_result` operation took 2.177 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:03:50,536\tWARNING util.py:315 -- Processing trial results took 2.179 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:03:50,538\tWARNING util.py:315 -- The `process_trial_result` operation took 2.181 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_1d77cd69_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-03-36/wandb/run-20230718_230352-1d77cd69\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: Syncing run FSR_Trainable_1d77cd69\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1d77cd69\n",
      "2023-07-18 23:04:00,979\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.311 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:04:00,982\tWARNING util.py:315 -- The `process_trial_result` operation took 2.316 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:04:00,987\tWARNING util.py:315 -- Processing trial results took 2.321 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:04:00,988\tWARNING util.py:315 -- The `process_trial_result` operation took 2.322 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_b80f3bce_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-03-46/wandb/run-20230718_230403-b80f3bce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: Syncing run FSR_Trainable_b80f3bce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b80f3bce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb:                      mae ▄▁▃▁▁▃█▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb:                     mape ▂▁▃▂▂▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb:                     rmse ▆▂▃▂▁▃█▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb:         time_this_iter_s ▇▄▁▁█▇▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb:                timestamp ▁▃▄▄▅▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb:                      mae 78.73355\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb:                     mape 8.012998760994178e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb:                     rmse 263.35954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb:       time_since_restore 15.63226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb:         time_this_iter_s 1.61063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb:             time_total_s 15.63226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb:                timestamp 1689689044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: 🚀 View run FSR_Trainable_1d77cd69 at: https://wandb.ai/seokjin/FSR-prediction/runs/1d77cd69\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230352-1d77cd69/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230403-b80f3bce/logs\n",
      "2023-07-18 23:04:10,508\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.856 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:04:10,513\tWARNING util.py:315 -- The `process_trial_result` operation took 1.861 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:04:10,515\tWARNING util.py:315 -- Processing trial results took 1.863 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:04:10,517\tWARNING util.py:315 -- The `process_trial_result` operation took 1.865 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259615)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_9bc1d71b_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-03-56/wandb/run-20230718_230412-9bc1d71b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: Syncing run FSR_Trainable_9bc1d71b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9bc1d71b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9bc1d71b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259837)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb:                      mae 80.51522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb:                     mape 2.0995470514447264e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb:                     rmse 277.8703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb:       time_since_restore 2.33664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb:         time_this_iter_s 2.33664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb:             time_total_s 2.33664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb:                timestamp 1689689048\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: 🚀 View run FSR_Trainable_9bc1d71b at: https://wandb.ai/seokjin/FSR-prediction/runs/9bc1d71b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230412-9bc1d71b/logs\n",
      "2023-07-18 23:04:21,249\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.074 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:04:21,254\tWARNING util.py:315 -- The `process_trial_result` operation took 2.080 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:04:21,276\tWARNING util.py:315 -- Processing trial results took 2.102 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:04:21,278\tWARNING util.py:315 -- The `process_trial_result` operation took 2.103 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260060)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_55cc134f_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-04-06/wandb/run-20230718_230421-55cc134f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: Syncing run FSR_Trainable_55cc134f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/55cc134f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb:                      mae █▆▅▅▄▄▇▆▄▃▃▄▂▂▂▂▂▂▁▁▂▁▂▂▁▂▁▂▂▂▂▂▂▂▂▂▂▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb:                     mape █▅▅▃▃▂▆▃▂▁▂▃▂▂▂▁▁▁▁▁▂▁▁▂▂▂▃▃▃▂▃▂▃▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb:                     rmse █▆▅▆▄▄▇▇▅▄▄▅▃▃▃▂▂▂▂▂▂▁▂▃▂▂▂▂▂▂▂▂▂▂▃▂▂▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb:         time_this_iter_s ▆▇▃▅█▆▄▃▄▆▃▂▃▅▅▄▃▆▄▂▂▆▄▂▁▇▄▄▃▆▆▃▄▆▅▂▁▆▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb:                      mae 42.09365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb:                     mape 61557575.968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb:                     rmse 141.86868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb:       time_since_restore 80.67536\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb:         time_this_iter_s 1.16695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb:             time_total_s 80.67536\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb:                timestamp 1689689062\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: 🚀 View run FSR_Trainable_a3285c46 at: https://wandb.ai/seokjin/FSR-prediction/runs/a3285c46\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230253-a3285c46/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258270)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_c21fded1_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-04-15/wandb/run-20230718_230431-c21fded1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: Syncing run FSR_Trainable_c21fded1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c21fded1\n",
      "2023-07-18 23:04:34,962\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.326 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:04:34,964\tWARNING util.py:315 -- The `process_trial_result` operation took 1.329 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:04:34,967\tWARNING util.py:315 -- Processing trial results took 1.332 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:04:34,969\tWARNING util.py:315 -- The `process_trial_result` operation took 1.334 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb:                      mae █▁▂▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb:                     mape ▇▆█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb:                     rmse █▃▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb:         time_this_iter_s ▅█▇▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb:                      mae 82.38828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb:                     mape 217280816.75986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb:                     rmse 257.96476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb:       time_since_restore 16.36677\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb:         time_this_iter_s 2.84891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb:             time_total_s 16.36677\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb:                timestamp 1689689073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: 🚀 View run FSR_Trainable_55cc134f at: https://wandb.ai/seokjin/FSR-prediction/runs/55cc134f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260289)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230421-55cc134f/logs\n",
      "2023-07-18 23:04:41,174\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.156 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:04:41,214\tWARNING util.py:315 -- The `process_trial_result` operation took 2.196 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:04:41,216\tWARNING util.py:315 -- Processing trial results took 2.198 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:04:41,218\tWARNING util.py:315 -- The `process_trial_result` operation took 2.200 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_a485388c_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-04-24/wandb/run-20230718_230441-a485388c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: Syncing run FSR_Trainable_a485388c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a485388c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: 🚀 View run FSR_Trainable_c21fded1 at: https://wandb.ai/seokjin/FSR-prediction/runs/c21fded1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260518)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230431-c21fded1/logs\n",
      "2023-07-18 23:04:51,374\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.051 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:04:51,378\tWARNING util.py:315 -- The `process_trial_result` operation took 2.056 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:04:51,382\tWARNING util.py:315 -- Processing trial results took 2.060 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:04:51,383\tWARNING util.py:315 -- The `process_trial_result` operation took 2.061 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_6a09b823_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-04-35/wandb/run-20230718_230451-6a09b823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: Syncing run FSR_Trainable_6a09b823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6a09b823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb:                      mae 99.60005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb:                     mape 370758127.33481\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb:                     rmse 308.15924\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb:       time_since_restore 4.28672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb:         time_this_iter_s 4.28672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb:             time_total_s 4.28672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb:                timestamp 1689689089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: 🚀 View run FSR_Trainable_6a09b823 at: https://wandb.ai/seokjin/FSR-prediction/runs/6a09b823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260969)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230451-6a09b823/logs\n",
      "2023-07-18 23:05:00,044\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.068 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:05:00,047\tWARNING util.py:315 -- The `process_trial_result` operation took 2.072 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:05:00,051\tWARNING util.py:315 -- Processing trial results took 2.076 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:05:00,057\tWARNING util.py:315 -- The `process_trial_result` operation took 2.082 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_5249ea04_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-04-45/wandb/run-20230718_230501-5249ea04\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: Syncing run FSR_Trainable_5249ea04\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5249ea04\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb:                      mae █▄▂▁▁▅▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb:                     mape █▄▂▁▁▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb:                     rmse █▁▁▁▁▅▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb:         time_this_iter_s █▄▆▅▁█▅▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb:                      mae 72.10159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb:                     mape 205102481.2447\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb:                     rmse 240.02049\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb:       time_since_restore 27.72626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb:         time_this_iter_s 3.78683\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb:             time_total_s 27.72626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb:                timestamp 1689689107\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: 🚀 View run FSR_Trainable_a485388c at: https://wandb.ai/seokjin/FSR-prediction/runs/a485388c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260740)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230441-a485388c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-18 23:05:13,221\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.732 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:05:13,223\tWARNING util.py:315 -- The `process_trial_result` operation took 1.735 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:05:13,226\tWARNING util.py:315 -- Processing trial results took 1.738 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:05:13,227\tWARNING util.py:315 -- The `process_trial_result` operation took 1.740 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_4df8f562_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-04-54/wandb/run-20230718_230512-4df8f562\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: Syncing run FSR_Trainable_4df8f562\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4df8f562\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_60677baa_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-05-05/wandb/run-20230718_230525-60677baa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: Syncing run FSR_Trainable_60677baa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/60677baa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-18 23:05:37,105\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.295 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:05:37,108\tWARNING util.py:315 -- The `process_trial_result` operation took 2.299 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:05:37,110\tWARNING util.py:315 -- Processing trial results took 2.301 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:05:37,111\tWARNING util.py:315 -- The `process_trial_result` operation took 2.302 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_adab90cb_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-05-15/wandb/run-20230718_230536-adab90cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: Syncing run FSR_Trainable_adab90cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/adab90cb\n",
      "2023-07-18 23:05:39,894\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.723 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:05:39,896\tWARNING util.py:315 -- The `process_trial_result` operation took 2.726 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:05:39,899\tWARNING util.py:315 -- Processing trial results took 2.729 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:05:39,901\tWARNING util.py:315 -- The `process_trial_result` operation took 2.731 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb:                      mae 81.45119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb:                     mape 232775104.99251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb:                     rmse 265.4941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb:       time_since_restore 20.75389\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb:         time_this_iter_s 20.75389\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb:             time_total_s 20.75389\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb:                timestamp 1689689136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: 🚀 View run FSR_Trainable_60677baa at: https://wandb.ai/seokjin/FSR-prediction/runs/60677baa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261641)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230525-60677baa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_745564df_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-05-29/wandb/run-20230718_230559-745564df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: Syncing run FSR_Trainable_745564df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/745564df\n",
      "2023-07-18 23:06:11,826\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.007 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:06:11,830\tWARNING util.py:315 -- The `process_trial_result` operation took 2.011 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:06:11,843\tWARNING util.py:315 -- Processing trial results took 2.025 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:06:11,851\tWARNING util.py:315 -- The `process_trial_result` operation took 2.032 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb:                      mae 80.50504\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb:                     mape 6.716991326515752e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb:                     rmse 278.8401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb:       time_since_restore 18.33039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb:         time_this_iter_s 18.33039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb:             time_total_s 18.33039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb:                timestamp 1689689169\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: 🚀 View run FSR_Trainable_745564df at: https://wandb.ai/seokjin/FSR-prediction/runs/745564df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262087)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230559-745564df/logs\n",
      "2023-07-18 23:06:31,072\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.283 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:06:31,077\tWARNING util.py:315 -- The `process_trial_result` operation took 2.289 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:06:31,079\tWARNING util.py:315 -- Processing trial results took 2.290 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:06:31,080\tWARNING util.py:315 -- The `process_trial_result` operation took 2.291 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_3e453153_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-05-51/wandb/run-20230718_230631-3e453153\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: Syncing run FSR_Trainable_3e453153\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3e453153\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb:                      mae 81.6839\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb:                     mape 9.368216328165426e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb:                     rmse 266.43762\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb:       time_since_restore 5.28399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb:         time_this_iter_s 5.28399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb:             time_total_s 5.28399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb:                timestamp 1689689188\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: 🚀 View run FSR_Trainable_3e453153 at: https://wandb.ai/seokjin/FSR-prediction/runs/3e453153\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262324)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230631-3e453153/logs\n",
      "2023-07-18 23:06:46,472\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.045 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:06:46,475\tWARNING util.py:315 -- The `process_trial_result` operation took 2.049 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:06:46,480\tWARNING util.py:315 -- Processing trial results took 2.054 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:06:46,482\tWARNING util.py:315 -- The `process_trial_result` operation took 2.056 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_bdbce26d_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-06-23/wandb/run-20230718_230650-bdbce26d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: Syncing run FSR_Trainable_bdbce26d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/bdbce26d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb:                      mae 83.51525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb:                     mape 1.0679597174768949e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb:                     rmse 265.5544\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb:       time_since_restore 1.96408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb:         time_this_iter_s 1.96408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb:             time_total_s 1.96408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb:                timestamp 1689689204\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: 🚀 View run FSR_Trainable_bdbce26d at: https://wandb.ai/seokjin/FSR-prediction/runs/bdbce26d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262551)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230650-bdbce26d/logs\n",
      "2023-07-18 23:07:02,262\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.398 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:07:02,267\tWARNING util.py:315 -- The `process_trial_result` operation took 2.403 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:07:02,268\tWARNING util.py:315 -- Processing trial results took 2.405 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:07:02,271\tWARNING util.py:315 -- The `process_trial_result` operation took 2.407 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_7a74365c_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-06-42/wandb/run-20230718_230705-7a74365c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: Syncing run FSR_Trainable_7a74365c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7a74365c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb:                      mae ▃▁▃█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb:                     mape ▅▁▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb:                     rmse ▁▁▃█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb:         time_this_iter_s █▃▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb:                      mae 78.15564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb:                     mape 289537104.87476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb:                     rmse 252.51296\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb:       time_since_restore 7.5442\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb:         time_this_iter_s 1.61221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb:             time_total_s 7.5442\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb:                timestamp 1689689227\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: 🚀 View run FSR_Trainable_7a74365c at: https://wandb.ai/seokjin/FSR-prediction/runs/7a74365c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262775)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230705-7a74365c/logs\n",
      "2023-07-18 23:07:23,827\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.272 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:07:23,832\tWARNING util.py:315 -- The `process_trial_result` operation took 2.277 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:07:23,833\tWARNING util.py:315 -- Processing trial results took 2.278 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:07:23,835\tWARNING util.py:315 -- The `process_trial_result` operation took 2.280 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_934971da_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-06-57/wandb/run-20230718_230726-934971da\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: Syncing run FSR_Trainable_934971da\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/934971da\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb:                      mae █▄▅▄▂▁▇▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb:                     mape ▇▄██▂▁█▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb:                     rmse ▆▄▅▄▃▁█▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb:         time_this_iter_s █▆▃▄▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb:                timestamp ▁▃▄▅▆▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb:                      mae 65.99805\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb:                     mape 194835719.41788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb:                     rmse 230.09026\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb:       time_since_restore 15.13492\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb:         time_this_iter_s 1.75629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb:             time_total_s 15.13492\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb:                timestamp 1689689256\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: 🚀 View run FSR_Trainable_934971da at: https://wandb.ai/seokjin/FSR-prediction/runs/934971da\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263014)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230726-934971da/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb:                      mae ▇▇▅▂▃▃▂▃█▄▄▅▇▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb:                     mape ▅▄▁▃▃▃▃▆▆▄▃▅▇█▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb:                     rmse ▇▇▆▃▄▄▃▃█▄▅▄▆▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb:       time_since_restore ▁▁▂▃▄▄▅▅▆▆▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb:         time_this_iter_s ▁▄██▆▅▃▃▃▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb:             time_total_s ▁▁▂▃▄▄▅▅▆▆▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb:                timestamp ▁▂▂▃▄▅▅▅▆▆▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb:                      mae 46.57342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb:                     mape 58809038.58917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb:                     rmse 162.41969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb:       time_since_restore 157.1631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb:         time_this_iter_s 6.05557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb:             time_total_s 157.1631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb:                timestamp 1689689264\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: 🚀 View run FSR_Trainable_4df8f562 at: https://wandb.ai/seokjin/FSR-prediction/runs/4df8f562\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261413)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230512-4df8f562/logs\n",
      "2023-07-18 23:07:53,132\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.487 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:07:53,135\tWARNING util.py:315 -- The `process_trial_result` operation took 2.491 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:07:53,138\tWARNING util.py:315 -- Processing trial results took 2.494 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:07:53,142\tWARNING util.py:315 -- The `process_trial_result` operation took 2.498 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_c1fb91e4_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-07-19/wandb/run-20230718_230756-c1fb91e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: Syncing run FSR_Trainable_c1fb91e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c1fb91e4\n",
      "2023-07-18 23:08:05,532\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.944 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:08:05,534\tWARNING util.py:315 -- The `process_trial_result` operation took 1.947 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:08:05,536\tWARNING util.py:315 -- Processing trial results took 1.949 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:08:05,538\tWARNING util.py:315 -- The `process_trial_result` operation took 1.951 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_fa31eec0_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-07-48/wandb/run-20230718_230807-fa31eec0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: Syncing run FSR_Trainable_fa31eec0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fa31eec0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb:                      mae █▇▅▂▁▂▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb:                     mape █▃▄▂▃▄▁▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb:                     rmse ▇▆▆▄▃▅▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb:         time_this_iter_s █▄▄▁▄▂▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb:                timestamp ▁▃▄▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb:                      mae 73.1601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb:                     mape 213187355.44737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb:                     rmse 239.52491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb:       time_since_restore 15.61484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb:         time_this_iter_s 1.94978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb:             time_total_s 15.61484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb:                timestamp 1689689287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: 🚀 View run FSR_Trainable_c1fb91e4 at: https://wandb.ai/seokjin/FSR-prediction/runs/c1fb91e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263257)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230756-c1fb91e4/logs\n",
      "2023-07-18 23:08:26,372\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.906 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:08:26,377\tWARNING util.py:315 -- The `process_trial_result` operation took 2.912 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:08:26,379\tWARNING util.py:315 -- Processing trial results took 2.913 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:08:26,381\tWARNING util.py:315 -- The `process_trial_result` operation took 2.915 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_a6d1cf80_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-08-00/wandb/run-20230718_230828-a6d1cf80\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: Syncing run FSR_Trainable_a6d1cf80\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a6d1cf80\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb:                      mae ▃▃█▅▄▂▃▅▄▆▂▆▃▅▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb:                     mape ▂▆▃▃▄▃▆█▁▇▄▅▅▅▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb:                     rmse ▅▂█▅▄▃▄▄▄▅▄▅▃▅▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▃▄▅▆▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb:         time_this_iter_s ▁▁▁▂▅███▅▄▄▃▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▃▄▅▆▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb:                timestamp ▁▁▁▂▂▃▄▅▆▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb:                      mae 48.35483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb:                     mape 60211368.26125\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb:                     rmse 162.9676\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb:       time_since_restore 126.31493\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb:         time_this_iter_s 4.66526\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb:             time_total_s 126.31493\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb:                timestamp 1689689409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: 🚀 View run FSR_Trainable_fa31eec0 at: https://wandb.ai/seokjin/FSR-prediction/runs/fa31eec0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263476)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230807-fa31eec0/logs\n",
      "2023-07-18 23:10:25,603\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.137 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:10:25,607\tWARNING util.py:315 -- The `process_trial_result` operation took 2.142 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:10:25,609\tWARNING util.py:315 -- Processing trial results took 2.143 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:10:25,611\tWARNING util.py:315 -- The `process_trial_result` operation took 2.146 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_18af9e99_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-08-19/wandb/run-20230718_231027-18af9e99\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: Syncing run FSR_Trainable_18af9e99\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/18af9e99\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb:                      mae ▆▆▇▅█▄▄▄▇▄▃▃▁▁▂▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb:                     mape ▁▂█▆▆▄▅▄█▄▃▆▂▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb:                     rmse ▇▇█▄▇▃▂▃▅▃▃▂▁▁▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▅▅▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb:         time_this_iter_s ▁▂▂▄▆███▅▄▄▄▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▅▅▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb:                timestamp ▁▁▂▂▃▃▄▅▅▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb:                      mae 51.82956\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb:                     mape 85714922.29039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb:                     rmse 163.00022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb:       time_since_restore 138.97307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb:         time_this_iter_s 6.66599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb:             time_total_s 138.97307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb:                timestamp 1689689442\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: 🚀 View run FSR_Trainable_a6d1cf80 at: https://wandb.ai/seokjin/FSR-prediction/runs/a6d1cf80\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263707)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230828-a6d1cf80/logs\n",
      "2023-07-18 23:10:59,041\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.395 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:10:59,045\tWARNING util.py:315 -- The `process_trial_result` operation took 2.400 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:10:59,078\tWARNING util.py:315 -- Processing trial results took 2.433 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:10:59,079\tWARNING util.py:315 -- The `process_trial_result` operation took 2.434 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_7541177b_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-10-20/wandb/run-20230718_231100-7541177b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: Syncing run FSR_Trainable_7541177b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7541177b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb:                      mae ▇▆█▇█▆▆▃▄▃▄▅▂▁▃▄▂▂▂▃▁▂▅▄▄▄▂▃▃▃▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb:                     mape ▅▂▇▅▃▄▇▄▇▄▂█▃▃▃▇▂▃▂▁▃▄▃▆▄▄▃▅▄▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb:                     rmse ███▆█▅▅▃▃▄▃▄▂▂▂▃▂▂▂▃▁▂▅▄▅▄▂▃▃▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▃▃▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb:         time_this_iter_s ▁▁▂▃████▆▆▄▄▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▃▃▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb:                timestamp ▁▁▁▂▂▃▃▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb:                      mae 47.47906\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb:                     mape 60268383.39179\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb:                     rmse 159.19582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb:       time_since_restore 191.60069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb:         time_this_iter_s 3.49781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb:             time_total_s 191.60069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb:                timestamp 1689689615\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: 🚀 View run FSR_Trainable_18af9e99 at: https://wandb.ai/seokjin/FSR-prediction/runs/18af9e99\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263991)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_231027-18af9e99/logs\n",
      "2023-07-18 23:13:51,950\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.304 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:13:51,954\tWARNING util.py:315 -- The `process_trial_result` operation took 2.308 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:13:51,957\tWARNING util.py:315 -- Processing trial results took 2.312 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:13:51,959\tWARNING util.py:315 -- The `process_trial_result` operation took 2.314 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...556)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_d263706c_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-10-53/wandb/run-20230718_231354-d263706c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: Syncing run FSR_Trainable_d263706c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d263706c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb:                      mae 85.65201\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb:                     mape 300291681.32553\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb:                     rmse 255.25582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb:       time_since_restore 5.98309\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb:         time_this_iter_s 2.90253\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb:             time_total_s 5.98309\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb:                timestamp 1689689634\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: 🚀 View run FSR_Trainable_d263706c at: https://wandb.ai/seokjin/FSR-prediction/runs/d263706c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264556)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_231354-d263706c/logs\n",
      "2023-07-18 23:14:15,064\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.474 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:14:15,070\tWARNING util.py:315 -- The `process_trial_result` operation took 2.481 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:14:15,071\tWARNING util.py:315 -- Processing trial results took 2.482 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:14:15,074\tWARNING util.py:315 -- The `process_trial_result` operation took 2.484 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_9d20d9d5_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-13-46/wandb/run-20230718_231415-9d20d9d5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: Syncing run FSR_Trainable_9d20d9d5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9d20d9d5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb:                      mae 105.08697\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb:                     mape 9.224813824345666e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb:                     rmse 312.42847\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb:       time_since_restore 5.65253\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb:         time_this_iter_s 5.65253\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb:             time_total_s 5.65253\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb:                timestamp 1689689652\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: 🚀 View run FSR_Trainable_9d20d9d5 at: https://wandb.ai/seokjin/FSR-prediction/runs/9d20d9d5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264789)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_231415-9d20d9d5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb:                      mae █▅▄▄▄▄▄▄▃▄▄▄▄▄▄▃▃▃▃▃▄▃▃▃▃▂▂▂▂▂▂▁▂▂▁▁▁▁▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb:                     mape █▂▂▂▃▂▂▂▂▂▃▂▃▁▂▂▃▃▃▂▅▂▃▃▃▂▂▂▂▂▂▁▂▃▂▂▁▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb:                     rmse █▅▄▄▃▄▄▄▃▃▄▄▄▄▄▃▃▃▃▂▃▃▃▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb:         time_this_iter_s ▆▂▃▁▃▃▃▅▃▄▂▄▃▆▄▄▄▂▂▂▁▃▃▄▆▆▅▃▃▄▅▄▄▆█▆▆▇█▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb:                      mae 36.94614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb:                     mape 2.660684528263719e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb:                     rmse 124.76601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb:       time_since_restore 524.21405\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb:         time_this_iter_s 5.92642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb:             time_total_s 524.21405\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb:                timestamp 1689689667\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: 🚀 View run FSR_Trainable_adab90cb at: https://wandb.ai/seokjin/FSR-prediction/runs/adab90cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261844)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230536-adab90cb/logs\n",
      "2023-07-18 23:14:35,877\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.511 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:14:35,883\tWARNING util.py:315 -- The `process_trial_result` operation took 2.518 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-18 23:14:35,895\tWARNING util.py:315 -- Processing trial results took 2.530 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:14:35,905\tWARNING util.py:315 -- The `process_trial_result` operation took 2.540 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_7e985cc1_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-14-07/wandb/run-20230718_231435-7e985cc1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: Syncing run FSR_Trainable_7e985cc1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7e985cc1\n",
      "2023-07-18 23:14:47,535\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.979 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:14:47,538\tWARNING util.py:315 -- The `process_trial_result` operation took 1.982 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:14:47,539\tWARNING util.py:315 -- Processing trial results took 1.983 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:14:47,541\tWARNING util.py:315 -- The `process_trial_result` operation took 1.985 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_6f761e95_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-14-26/wandb/run-20230718_231448-6f761e95\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: Syncing run FSR_Trainable_6f761e95\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6f761e95\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb:                      mae ▆▅▄▄▄▄▄▄▄▃▄▃▃▂▂▂▃▂▂█▄▃▃▂▂▂▂▂▂▂▁▁▃▃▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb:                     mape ▁▁▂▁▃▃▂▃▃▂▂▂▃▃▂▄▄▂▃█▃▂▂▂▁▂▂▂▂▂▂▂▃▄▁▁▂▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb:                     rmse ▇▇▅▅▄▄▃▃▄▃▄▃▃▂▂▂▄▂▂█▄▃▃▂▁▁▂▁▂▁▁▁▃▃▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb:         time_this_iter_s ▆▃▁▂▁▂▅▃▄▄▅▅▄▄▃▄▃█▃▃▇▇▄▆▂▇▂▂▇▅▁▁▃▃▃▄▄▃▆▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb:                      mae 30.13365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb:                     mape 50384312.25617\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb:                     rmse 97.91058\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb:       time_since_restore 302.52097\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb:         time_this_iter_s 2.87633\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb:             time_total_s 302.52097\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb:                timestamp 1689689764\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: 🚀 View run FSR_Trainable_7541177b at: https://wandb.ai/seokjin/FSR-prediction/runs/7541177b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264231)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_231100-7541177b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-18 23:16:23,771\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.297 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:16:23,776\tWARNING util.py:315 -- The `process_trial_result` operation took 2.303 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:16:23,778\tWARNING util.py:315 -- Processing trial results took 2.305 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:16:23,779\tWARNING util.py:315 -- The `process_trial_result` operation took 2.306 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_44d49a12_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-14-41/wandb/run-20230718_231623-44d49a12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: Syncing run FSR_Trainable_44d49a12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/44d49a12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb:                      mae ▆▅▆▅▂▂▁▃▂▂█▅▄▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb:                     mape ▃▃▃▂▃▁▁▂▂▂▇▃█▅▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb:                     rmse ▆▆█▅▂▃▁▃▁▂█▆▃▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb:       time_since_restore ▁▁▂▃▄▄▅▆▆▆▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb:         time_this_iter_s ▁▁▄██▆▅▅▃▂▁▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb:             time_total_s ▁▁▂▃▄▄▅▆▆▆▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb:                timestamp ▁▁▂▃▄▄▅▆▆▆▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb:                      mae 50.02971\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb:                     mape 71876754.83764\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb:                     rmse 167.04289\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb:       time_since_restore 106.62605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb:         time_this_iter_s 3.88713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb:             time_total_s 106.62605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb:                timestamp 1689689791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: 🚀 View run FSR_Trainable_6f761e95 at: https://wandb.ai/seokjin/FSR-prediction/runs/6f761e95\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265244)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_231448-6f761e95/logs\n",
      "2023-07-18 23:16:50,135\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.673 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:16:50,141\tWARNING util.py:315 -- The `process_trial_result` operation took 2.679 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:16:50,143\tWARNING util.py:315 -- Processing trial results took 2.681 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:16:50,144\tWARNING util.py:315 -- The `process_trial_result` operation took 2.683 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_38305194_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-16-15/wandb/run-20230718_231652-38305194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: Syncing run FSR_Trainable_38305194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/38305194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb:                      mae █▄▄▃▂▂▂▂▂▂▂▁▂▂▃▂▅▄▃▃▁▁▁▂▂▁▄▃▂▃▃▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb:                     mape █▄▂▂▁▁▂▃▂▂▂▂▂▂▂▁▅▃▂▄▂▂▂▂▁▁▅▄▃▄▂▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb:                     rmse █▄▄▄▂▂▂▂▂▂▁▁▁▂▃▂▅▄▃▂▁▁▁▂▂▂▃▂▂▃▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb:         time_this_iter_s █▁▂▄▂▆▅▁▂▂▁▂▁▂▁▂▂▃█▅▂▂▁▁▂▂▂▄▄▄▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb:                      mae 50.70151\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb:                     mape 4.281522954030336e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb:                     rmse 170.78477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb:       time_since_restore 167.75213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb:         time_this_iter_s 5.57545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb:             time_total_s 167.75213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb:                timestamp 1689689947\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: 🚀 View run FSR_Trainable_44d49a12 at: https://wandb.ai/seokjin/FSR-prediction/runs/44d49a12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265522)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_231623-44d49a12/logs\n",
      "2023-07-18 23:19:23,557\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.029 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:19:23,559\tWARNING util.py:315 -- The `process_trial_result` operation took 2.033 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:19:23,562\tWARNING util.py:315 -- Processing trial results took 2.036 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:19:23,563\tWARNING util.py:315 -- The `process_trial_result` operation took 2.037 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_865addfd_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-16-43/wandb/run-20230718_231926-865addfd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: Syncing run FSR_Trainable_865addfd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/865addfd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb:                      mae █▅▃▃▃▃▃▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▃▂▂▁▁▂▁▁▂▂▁▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb:                     mape █▄▃▃▃▄▄▃▃▃▂▂▂▂▂▂▁▁▁▂▁▃▁▄▂▂▁▂▂▁▂▂▂▂▂▂▂▁▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb:                     rmse █▄▃▂▂▂▂▂▂▂▂▂▂▃▃▂▃▂▂▂▃▂▂▃▃▂▁▁▂▁▁▂▂▁▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb:         time_this_iter_s █▄▃▂▁▁▁▂▃▃▂▄▂▂▄▂▃▂▅▂▃▂▃▃▃▃▇▄▂▂▃▂▂▃▃▃▂▃▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb:                      mae 40.62557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb:                     mape 3.951290856271003e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb:                     rmse 132.77249\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb:       time_since_restore 308.47281\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb:         time_this_iter_s 5.40453\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb:             time_total_s 308.47281\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb:                timestamp 1689689984\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: 🚀 View run FSR_Trainable_7e985cc1 at: https://wandb.ai/seokjin/FSR-prediction/runs/7e985cc1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265016)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_231435-7e985cc1/logs\n",
      "2023-07-18 23:20:00,063\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.370 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:20:00,066\tWARNING util.py:315 -- The `process_trial_result` operation took 2.375 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:20:00,067\tWARNING util.py:315 -- Processing trial results took 2.376 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:20:00,069\tWARNING util.py:315 -- The `process_trial_result` operation took 2.378 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_0c2584bd_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-19-19/wandb/run-20230718_232003-0c2584bd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: Syncing run FSR_Trainable_0c2584bd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0c2584bd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb:                      mae █▄▄▄▄▄▄▄▄▄▄▄▃▃▃▂▂▂▂▄▂▂▂▂▂▁▂▁▂▂▃▃▂▁▂▄▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb:                     mape █▄▅▅▄▄▃▃▄▃▃▃▂▁▁▁▁▁▂▃▂▁▁▂▂▁▁▂▂▂▂▃▂▂▂▅▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb:                     rmse █▄▃▃▃▃▃▃▃▄▄▄▃▃▃▃▂▂▂▄▂▂▂▂▂▂▂▁▂▂▃▃▂▁▁▃▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb:         time_this_iter_s █▅▃▃▃▃▂▃▄▅▄▃▃▃▂▂▂▄▃▃▃▃▂▅▁▂▁▂▃▁▂▂▂▃▁▁▂▁▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb:                      mae 34.82708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb:                     mape 2.6264395474059772e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb:                     rmse 118.22234\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb:       time_since_restore 290.81118\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb:         time_this_iter_s 2.89802\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb:             time_total_s 290.81118\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb:                timestamp 1689690102\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: 🚀 View run FSR_Trainable_38305194 at: https://wandb.ai/seokjin/FSR-prediction/runs/38305194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265763)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_231652-38305194/logs\n",
      "2023-07-18 23:22:01,441\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.747 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:22:01,443\tWARNING util.py:315 -- The `process_trial_result` operation took 1.750 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:22:01,447\tWARNING util.py:315 -- Processing trial results took 1.754 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:22:01,449\tWARNING util.py:315 -- The `process_trial_result` operation took 1.756 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...637)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_2d68f3af_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-19-56/wandb/run-20230718_232201-2d68f3af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Syncing run FSR_Trainable_2d68f3af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2d68f3af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb:                      mae █▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb:                     mape █▂▄▄▄▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▃▂▂▂▃▁▁▂▂▂▂▁▂▁▁▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb:                     rmse █▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb:         time_this_iter_s █▄▄▃▄▄▄▅▄▃▄▅▄▅▃▄▄▃▅▃▃▅▄▄▄▃▅▃▄▃▃▄▃▁▄▂▄█▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb:                      mae 36.00618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb:                     mape 3.627771071276566e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb:                     rmse 111.08517\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb:       time_since_restore 122.5218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb:         time_this_iter_s 1.25495\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb:             time_total_s 122.5218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb:                timestamp 1689690125\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: 🚀 View run FSR_Trainable_0c2584bd at: https://wandb.ai/seokjin/FSR-prediction/runs/0c2584bd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266325)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232003-0c2584bd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb:                      mae █▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb:                     mape █▂▂▃▃▂▂▂▁▁▂▁▁▂▂▃▃▃▂▂▂▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb:                     rmse █▄▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb:         time_this_iter_s █▃▁▃▄▄▂▂▄▃▂▃▂▃▃▂▂▃▃▂▂▂▂▁▄▂▃▂▂▄▃▂▃▂▂▁▅▅▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb:                      mae 35.79135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb:                     mape 3.3311531993075816e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb:                     rmse 114.50586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb:       time_since_restore 166.17421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb:         time_this_iter_s 1.72706\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb:             time_total_s 166.17421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb:                timestamp 1689690133\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: 🚀 View run FSR_Trainable_865addfd at: https://wandb.ai/seokjin/FSR-prediction/runs/865addfd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266079)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_231926-865addfd/logs\n",
      "2023-07-18 23:22:20,673\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.299 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:22:20,676\tWARNING util.py:315 -- The `process_trial_result` operation took 2.303 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:22:20,678\tWARNING util.py:315 -- Processing trial results took 2.305 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:22:20,686\tWARNING util.py:315 -- The `process_trial_result` operation took 2.313 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_f8ccb7f8_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-21-53/wandb/run-20230718_232224-f8ccb7f8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: Syncing run FSR_Trainable_f8ccb7f8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f8ccb7f8\n",
      "2023-07-18 23:22:34,851\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.362 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:22:34,855\tWARNING util.py:315 -- The `process_trial_result` operation took 2.367 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:22:34,857\tWARNING util.py:315 -- Processing trial results took 2.369 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:22:34,859\tWARNING util.py:315 -- The `process_trial_result` operation took 2.371 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...103)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_47cee5a0_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-22-16/wandb/run-20230718_232238-47cee5a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: Syncing run FSR_Trainable_47cee5a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/47cee5a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb:                      mae 115.57523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb:                     mape 1.571883616271652e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb:                     rmse 314.95517\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb:       time_since_restore 1.71803\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb:         time_this_iter_s 1.71803\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb:             time_total_s 1.71803\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb:                timestamp 1689690152\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: 🚀 View run FSR_Trainable_47cee5a0 at: https://wandb.ai/seokjin/FSR-prediction/runs/47cee5a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267103)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232238-47cee5a0/logs\n",
      "2023-07-18 23:22:51,578\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.181 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:22:51,584\tWARNING util.py:315 -- The `process_trial_result` operation took 2.188 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:22:51,586\tWARNING util.py:315 -- Processing trial results took 2.190 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:22:51,588\tWARNING util.py:315 -- The `process_trial_result` operation took 2.193 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_8a604e62_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-22-30/wandb/run-20230718_232255-8a604e62\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Syncing run FSR_Trainable_8a604e62\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8a604e62\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb:                      mae █▆▅▄▄▂▄▃▃▃▃▂▃▂▂▂▂▂▃▂▂▂▁▃▃▂▂▁▁▁▁▁▁▂▂▂▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb:                     mape █▃▂▄▃▂▄▂▂▂▂▃▃▃▃▂▂▂▄▂▂▃▂▃▂▁▃▁▂▁▁▂▂▃▃▂▂▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb:                     rmse █▇▄▄▅▂▄▄▃▄▃▂▂▂▂▂▂▂▃▂▂▂▁▃▂▂▂▁▁▁▁▁▁▂▂▂▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb:       time_since_restore ▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb:         time_this_iter_s ▁▁▁▂▂▂▃▅▆▆██▇▇▇▆▇▇▇▇▆▆▆▅▅▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb:             time_total_s ▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb:                timestamp ▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb:                      mae 38.06607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb:                     mape 71346983.91232\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb:                     rmse 118.70983\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb:       time_since_restore 1064.19085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb:         time_this_iter_s 6.77001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb:             time_total_s 1064.19085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb:                timestamp 1689690172\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: 🚀 View run FSR_Trainable_5249ea04 at: https://wandb.ai/seokjin/FSR-prediction/runs/5249ea04\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261194)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230501-5249ea04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_230501-5249ea04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232255-8a604e62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232255-8a604e62/logs\n",
      "2023-07-18 23:23:06,243\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.778 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:23:06,245\tWARNING util.py:315 -- The `process_trial_result` operation took 1.781 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:23:06,246\tWARNING util.py:315 -- Processing trial results took 1.782 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:23:06,247\tWARNING util.py:315 -- The `process_trial_result` operation took 1.783 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_aad3656a_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-22-47/wandb/run-20230718_232310-aad3656a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: Syncing run FSR_Trainable_aad3656a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/aad3656a\n",
      "2023-07-18 23:23:20,393\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.213 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:23:20,398\tWARNING util.py:315 -- The `process_trial_result` operation took 2.219 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:23:20,400\tWARNING util.py:315 -- Processing trial results took 2.221 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:23:20,402\tWARNING util.py:315 -- The `process_trial_result` operation took 2.223 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_9dd92bce_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-23-02/wandb/run-20230718_232324-9dd92bce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: Syncing run FSR_Trainable_9dd92bce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9dd92bce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)03 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb:                      mae █▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb:                     mape █▆███▇▇▆▆▅▅▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▂▂▂▂▁▁▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb:                     rmse █▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb:         time_this_iter_s ▇▆▃▂▃▅▄▃█▆▃▁█▃▄▄▅▄▄▅▆▄▇▃▃▄▄▄▄▄▄▄▃▅▄▃▄▃▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb:                      mae 37.83934\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb:                     mape 3.5270607280814668e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb:                     rmse 121.62205\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb:       time_since_restore 129.94275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb:         time_this_iter_s 1.3016\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb:             time_total_s 129.94275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb:                timestamp 1689690280\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: 🚀 View run FSR_Trainable_f8ccb7f8 at: https://wandb.ai/seokjin/FSR-prediction/runs/f8ccb7f8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266879)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232224-f8ccb7f8/logs\n",
      "2023-07-18 23:24:57,257\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.842 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:24:57,261\tWARNING util.py:315 -- The `process_trial_result` operation took 1.847 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:24:57,262\tWARNING util.py:315 -- Processing trial results took 1.848 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:24:57,263\tWARNING util.py:315 -- The `process_trial_result` operation took 1.849 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_6845741b_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-23-16/wandb/run-20230718_232501-6845741b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: Syncing run FSR_Trainable_6845741b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6845741b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb:                      mae █▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb:                     mape █▃▇▇▆▆▅▅▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▂▂▂▂▁▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb:                     rmse █▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb:         time_this_iter_s █▅▃▃▅▃▅▃▆▅▅▁▂▃▃▂▃▃▁▃▂▃▄▂▂▄▂▅▃▃▃▄▆▂▂▃▃▃▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb:                      mae 34.28055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb:                     mape 2.67180762916453e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb:                     rmse 112.83249\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb:       time_since_restore 132.96046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb:         time_this_iter_s 1.25979\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb:             time_total_s 132.96046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb:                timestamp 1689690324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: 🚀 View run FSR_Trainable_aad3656a at: https://wandb.ai/seokjin/FSR-prediction/runs/aad3656a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267576)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232310-aad3656a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb:                      mae █▄▂▂▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb:                     mape █▇▃▂▂▂▂▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb:                     rmse █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb:         time_this_iter_s █▆▄▆▄█▃▄▂▂▂▃▂▄▂▃▂▃▂▁▄▂▄▃▃▄▂▂▅▆▂▄▄▄▃▁▃▃▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb:                      mae 33.02249\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb:                     mape 2.8119054805122984e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb:                     rmse 106.47202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb:       time_since_restore 129.78237\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb:         time_this_iter_s 1.16164\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb:             time_total_s 129.78237\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb:                timestamp 1689690332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: 🚀 View run FSR_Trainable_9dd92bce at: https://wandb.ai/seokjin/FSR-prediction/runs/9dd92bce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267793)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232324-9dd92bce/logs\n",
      "2023-07-18 23:25:40,480\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.390 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:25:40,483\tWARNING util.py:315 -- The `process_trial_result` operation took 2.394 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:25:40,485\tWARNING util.py:315 -- Processing trial results took 2.396 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:25:40,489\tWARNING util.py:315 -- The `process_trial_result` operation took 2.400 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_1f0f220a_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-24-53/wandb/run-20230718_232544-1f0f220a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: Syncing run FSR_Trainable_1f0f220a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1f0f220a\n",
      "2023-07-18 23:25:54,837\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.033 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:25:54,841\tWARNING util.py:315 -- The `process_trial_result` operation took 2.038 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:25:54,844\tWARNING util.py:315 -- Processing trial results took 2.040 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:25:54,845\tWARNING util.py:315 -- The `process_trial_result` operation took 2.042 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_d0ec6762_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-25-36/wandb/run-20230718_232558-d0ec6762\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: Syncing run FSR_Trainable_d0ec6762\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d0ec6762\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb:                      mae █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb:                     mape █▄▁▁▂▂▂▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▄▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb:                     rmse █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb:         time_this_iter_s █▆▃▂▄▃▃▄▂▄▃▃▄▄▅▁▅▂▄▂▂▄▂▃▃▂▂▄▂▃▃▃▃▃▂▃▃▂▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb:                      mae 34.07352\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb:                     mape 3.150609474969413e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb:                     rmse 108.38119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb:       time_since_restore 129.02693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb:         time_this_iter_s 1.30142\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb:             time_total_s 129.02693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb:                timestamp 1689690432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: 🚀 View run FSR_Trainable_6845741b at: https://wandb.ai/seokjin/FSR-prediction/runs/6845741b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268101)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232501-6845741b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb:                      mae █▄▃▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▃▂▁▂▂▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb:                     mape █▃▂▃▃▃▃▃▃▃▃▂▂▁▁▁▁▁▁▂▂▂▂▁▂▂▂▂▃▃▂▂▂▃▂▂▂▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb:                     rmse █▄▃▂▂▃▃▂▂▂▃▃▂▂▃▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▃▂▂▂▂▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb:         time_this_iter_s █▄▃▄▃▅▄▅▁▅▄▆▆▄▄▄▅▃▄▄▄▄▅▃▄▄▄▃▅▅▅▄▄▄▄▄▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232201-2d68f3af/logs\n",
      "2023-07-18 23:27:26,817\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.762 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:27:26,820\tWARNING util.py:315 -- The `process_trial_result` operation took 1.767 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:27:26,822\tWARNING util.py:315 -- Processing trial results took 1.769 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:27:26,823\tWARNING util.py:315 -- The `process_trial_result` operation took 1.770 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_2789cfee_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-25-51/wandb/run-20230718_232730-2789cfee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: Syncing run FSR_Trainable_2789cfee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2789cfee\n",
      "2023-07-18 23:27:40,501\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.727 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:27:40,503\tWARNING util.py:315 -- The `process_trial_result` operation took 1.731 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:27:40,506\tWARNING util.py:315 -- Processing trial results took 1.734 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:27:40,508\tWARNING util.py:315 -- The `process_trial_result` operation took 1.735 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_22b34823_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-27-23/wandb/run-20230718_232744-22b34823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Syncing run FSR_Trainable_22b34823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/22b34823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb:                      mae █▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▂▂▁▁▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb:                     mape █▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb:                     rmse █▂▁▁▁▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb:         time_this_iter_s █▅▂▁▅▂▂▂▃▃▂▂▄▃▃▅▃▃▃▃▃▄▃▃▄▂▃▂▃▄▃▁▅▃▂▂▄▃▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb:                      mae 32.92936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb:                     mape 2.577214548780875e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb:                     rmse 107.19094\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb:       time_since_restore 126.20353\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb:         time_this_iter_s 1.32933\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb:             time_total_s 126.20353\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb:                timestamp 1689690473\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: 🚀 View run FSR_Trainable_1f0f220a at: https://wandb.ai/seokjin/FSR-prediction/runs/1f0f220a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268366)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232544-1f0f220a/logs\n",
      "2023-07-18 23:28:08,914\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.788 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:28:08,917\tWARNING util.py:315 -- The `process_trial_result` operation took 1.792 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:28:08,918\tWARNING util.py:315 -- Processing trial results took 1.793 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:28:08,919\tWARNING util.py:315 -- The `process_trial_result` operation took 1.794 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_cb33ebc6_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-27-37/wandb/run-20230718_232812-cb33ebc6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Syncing run FSR_Trainable_cb33ebc6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cb33ebc6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb:                      mae █▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▁▁▁▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb:                     mape █▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb:                     rmse █▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb:         time_this_iter_s █▅▅▆▃▃▄▄▆▄▄▄▄▄▄▄▃▄▃▃▄▄▄▄▄▄▁▃▄▄▃▅▃▃▃▃▅▃▃▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb:                      mae 31.79569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb:                     mape 2.5583304167144616e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb:                     rmse 103.1909\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb:       time_since_restore 129.75073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb:         time_this_iter_s 1.60622\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb:             time_total_s 129.75073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb:                timestamp 1689690490\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: 🚀 View run FSR_Trainable_d0ec6762 at: https://wandb.ai/seokjin/FSR-prediction/runs/d0ec6762\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268588)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232558-d0ec6762/logs\n",
      "2023-07-18 23:28:26,269\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.991 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:28:26,272\tWARNING util.py:315 -- The `process_trial_result` operation took 1.995 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:28:26,274\tWARNING util.py:315 -- Processing trial results took 1.996 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:28:26,276\tWARNING util.py:315 -- The `process_trial_result` operation took 1.999 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_93c2f3ed_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-28-05/wandb/run-20230718_232829-93c2f3ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: Syncing run FSR_Trainable_93c2f3ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/93c2f3ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb:                      mae █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb:                     mape █▅▂▂▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb:                     rmse █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb:         time_this_iter_s █▃▃▃▆▃▃▃▂█▃▄█▅▁▃▅▃▅▂▂▂▃▃▃▄▂▃▂▄▂▃▂▂▂▁▂▁▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb:                      mae 34.83923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb:                     mape 2.8883295191531228e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb:                     rmse 111.7364\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb:       time_since_restore 126.50572\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb:         time_this_iter_s 1.32005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb:             time_total_s 126.50572\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb:                timestamp 1689690580\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: 🚀 View run FSR_Trainable_2789cfee at: https://wandb.ai/seokjin/FSR-prediction/runs/2789cfee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268903)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232730-2789cfee/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:                      mae █▅▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:                     mape █▆▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:                     rmse █▅▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:         time_this_iter_s █▄▃▄▃▃▃▂▂▇▃▃▃▄▂▂▃▃▂▃▂▂▂▂▃▃▃▃▂▄▁▂▃▃▄▃▃▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:                      mae 33.83521\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:                     mape 2.9743530085364732e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:                     rmse 105.0628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:       time_since_restore 121.20348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:         time_this_iter_s 1.19845\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:             time_total_s 121.20348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:                timestamp 1689690587\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: 🚀 View run FSR_Trainable_22b34823 at: https://wandb.ai/seokjin/FSR-prediction/runs/22b34823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232744-22b34823/logs\n",
      "2023-07-18 23:29:55,762\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.964 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:29:55,763\tWARNING util.py:315 -- The `process_trial_result` operation took 1.967 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:29:55,765\tWARNING util.py:315 -- Processing trial results took 1.969 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:29:55,766\tWARNING util.py:315 -- The `process_trial_result` operation took 1.970 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_67d1a213_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-28-22/wandb/run-20230718_232959-67d1a213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: Syncing run FSR_Trainable_67d1a213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/67d1a213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb:                      mae 74.12887\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb:                     mape 9.549558846833338e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb:                     rmse 233.59208\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb:       time_since_restore 3.16652\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb:         time_this_iter_s 1.47188\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb:             time_total_s 3.16652\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb:                timestamp 1689690597\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: 🚀 View run FSR_Trainable_67d1a213 at: https://wandb.ai/seokjin/FSR-prediction/runs/67d1a213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269920)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232959-67d1a213/logs\n",
      "2023-07-18 23:30:08,651\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.784 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:30:08,654\tWARNING util.py:315 -- The `process_trial_result` operation took 1.787 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:30:08,655\tWARNING util.py:315 -- Processing trial results took 1.788 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:30:08,657\tWARNING util.py:315 -- The `process_trial_result` operation took 1.790 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_5cb3b86e_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-29-52/wandb/run-20230718_233012-5cb3b86e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: Syncing run FSR_Trainable_5cb3b86e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cb3b86e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb:                      mae 68.87266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb:                     mape 8.606292564495517e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb:                     rmse 220.36636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb:       time_since_restore 3.09949\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb:         time_this_iter_s 1.35762\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb:             time_total_s 3.09949\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb:                timestamp 1689690610\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: 🚀 View run FSR_Trainable_5cb3b86e at: https://wandb.ai/seokjin/FSR-prediction/runs/5cb3b86e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270149)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233012-5cb3b86e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:30:21,833\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.680 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:30:21,836\tWARNING util.py:315 -- The `process_trial_result` operation took 1.684 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:30:21,837\tWARNING util.py:315 -- Processing trial results took 1.685 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:30:21,839\tWARNING util.py:315 -- The `process_trial_result` operation took 1.687 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb:                      mae █▄▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb:                     mape █▅▃▂▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb:                     rmse █▄▂▁▁▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb:         time_this_iter_s █▅▂▂▃▄▃▃▃▃▂▄▂▂▃▃▂▆▃▃▃▂▄▂▃▃▂▆▃▃▄▁▆▇▂▁█▃▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232812-cb33ebc6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232812-cb33ebc6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232812-cb33ebc6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232812-cb33ebc6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232812-cb33ebc6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232812-cb33ebc6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232812-cb33ebc6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232812-cb33ebc6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232812-cb33ebc6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232812-cb33ebc6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232812-cb33ebc6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232812-cb33ebc6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232812-cb33ebc6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269367)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232812-cb33ebc6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_4e2d7f45_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-30-05/wandb/run-20230718_233025-4e2d7f45\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: Syncing run FSR_Trainable_4e2d7f45\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4e2d7f45\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:30:32,365\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.834 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:30:32,376\tWARNING util.py:315 -- The `process_trial_result` operation took 1.846 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:30:32,378\tWARNING util.py:315 -- Processing trial results took 1.847 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:30:32,379\tWARNING util.py:315 -- The `process_trial_result` operation took 1.848 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb:                      mae 67.53341\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb:                     mape 7.437659273014597e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb:                     rmse 221.92047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb:       time_since_restore 3.42421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb:         time_this_iter_s 1.5458\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb:             time_total_s 3.42421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb:                timestamp 1689690623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: 🚀 View run FSR_Trainable_4e2d7f45 at: https://wandb.ai/seokjin/FSR-prediction/runs/4e2d7f45\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270377)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233025-4e2d7f45/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_f3b05426_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-30-18/wandb/run-20230718_233035-f3b05426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: Syncing run FSR_Trainable_f3b05426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f3b05426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb:                      mae █▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb:                     mape █▅▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb:                     rmse █▄▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb:         time_this_iter_s █▅▅▅▅▄▅▅▄▅▅▄▅▅▄▄▄▅▄▅▄▅▆▅▆▅▄▆▅▄▂▆▃▄▃▇▁▂▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb:                      mae 33.36939\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb:                     mape 2.7830894594350692e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb:                     rmse 106.49854\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb:       time_since_restore 122.7848\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb:         time_this_iter_s 1.15708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb:             time_total_s 122.7848\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb:                timestamp 1689690636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: 🚀 View run FSR_Trainable_93c2f3ed at: https://wandb.ai/seokjin/FSR-prediction/runs/93c2f3ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269603)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_232829-93c2f3ed/logs\n",
      "2023-07-18 23:30:43,823\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.877 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:30:43,828\tWARNING util.py:315 -- The `process_trial_result` operation took 1.882 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:30:43,829\tWARNING util.py:315 -- Processing trial results took 1.884 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:30:43,831\tWARNING util.py:315 -- The `process_trial_result` operation took 1.886 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_c13d6c02_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-30-29/wandb/run-20230718_233046-c13d6c02\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: Syncing run FSR_Trainable_c13d6c02\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c13d6c02\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb:                      mae 82.84462\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb:                     mape 1.072102164240937e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb:                     rmse 259.88321\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb:       time_since_restore 1.44895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb:         time_this_iter_s 1.44895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb:             time_total_s 1.44895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb:                timestamp 1689690641\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: 🚀 View run FSR_Trainable_c13d6c02 at: https://wandb.ai/seokjin/FSR-prediction/runs/c13d6c02\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270850)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233046-c13d6c02/logs\n",
      "2023-07-18 23:30:54,307\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.722 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:30:54,310\tWARNING util.py:315 -- The `process_trial_result` operation took 1.726 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:30:54,311\tWARNING util.py:315 -- Processing trial results took 1.727 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:30:54,313\tWARNING util.py:315 -- The `process_trial_result` operation took 1.729 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_eba671e0_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-30-40/wandb/run-20230718_233058-eba671e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: Syncing run FSR_Trainable_eba671e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/eba671e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:31:05,232\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.891 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:31:05,237\tWARNING util.py:315 -- The `process_trial_result` operation took 1.896 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:31:05,239\tWARNING util.py:315 -- Processing trial results took 1.898 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:31:05,241\tWARNING util.py:315 -- The `process_trial_result` operation took 1.900 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb:                      mae 70.25358\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb:                     mape 8.696800674808248e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb:                     rmse 227.42206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb:       time_since_restore 2.80325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb:         time_this_iter_s 1.29798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb:             time_total_s 2.80325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb:                timestamp 1689690655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: 🚀 View run FSR_Trainable_eba671e0 at: https://wandb.ai/seokjin/FSR-prediction/runs/eba671e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271076)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233058-eba671e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "2023-07-18 23:31:14,396\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.992 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:31:14,400\tWARNING util.py:315 -- The `process_trial_result` operation took 1.997 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:31:14,402\tWARNING util.py:315 -- Processing trial results took 1.999 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:31:14,404\tWARNING util.py:315 -- The `process_trial_result` operation took 2.001 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_6e12bef7_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-30-51/wandb/run-20230718_233112-6e12bef7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: Syncing run FSR_Trainable_6e12bef7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6e12bef7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb:                      mae 77.15558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb:                     mape 5743139430805755.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb:                     rmse 264.35819\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb:       time_since_restore 1.74243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb:         time_this_iter_s 1.74243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb:             time_total_s 1.74243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb:                timestamp 1689690663\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: 🚀 View run FSR_Trainable_6e12bef7 at: https://wandb.ai/seokjin/FSR-prediction/runs/6e12bef7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271300)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233112-6e12bef7/logs\n",
      "2023-07-18 23:31:22,863\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.863 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:31:22,866\tWARNING util.py:315 -- The `process_trial_result` operation took 1.867 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:31:22,867\tWARNING util.py:315 -- Processing trial results took 1.869 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:31:22,868\tWARNING util.py:315 -- The `process_trial_result` operation took 1.870 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233117-b4d9d36a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_f0ab078d_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-31-10/wandb/run-20230718_233126-f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: Syncing run FSR_Trainable_f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "2023-07-18 23:31:35,588\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.954 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:31:35,592\tWARNING util.py:315 -- The `process_trial_result` operation took 1.960 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:31:35,594\tWARNING util.py:315 -- Processing trial results took 1.961 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:31:35,596\tWARNING util.py:315 -- The `process_trial_result` operation took 1.963 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_b6491f21_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-31-19/wandb/run-20230718_233138-b6491f21\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: Syncing run FSR_Trainable_b6491f21\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b6491f21\n",
      "2023-07-18 23:31:46,686\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.875 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:31:46,688\tWARNING util.py:315 -- The `process_trial_result` operation took 1.878 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:31:46,690\tWARNING util.py:315 -- Processing trial results took 1.880 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:31:46,691\tWARNING util.py:315 -- The `process_trial_result` operation took 1.881 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_98078ce8_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-31-31/wandb/run-20230718_233150-98078ce8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Syncing run FSR_Trainable_98078ce8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/98078ce8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb:                      mae █▄▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb:                     mape █▄▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb:                     rmse █▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb:         time_this_iter_s █▆▄▁▆▄▂▃▇▅▂▂▅▁▁▅▅▂█▆▅▅█▅▆▄▅█▅█▆▇▆▆▄▅▆▆▅▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb:                      mae 31.86162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb:                     mape 2.5511605899695416e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb:                     rmse 103.5096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb:       time_since_restore 104.76513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb:         time_this_iter_s 1.14862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb:             time_total_s 104.76513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb:                timestamp 1689690749\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: 🚀 View run FSR_Trainable_f3b05426 at: https://wandb.ai/seokjin/FSR-prediction/runs/f3b05426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270618)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233035-f3b05426/logs\n",
      "2023-07-18 23:32:45,197\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.798 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:32:45,199\tWARNING util.py:315 -- The `process_trial_result` operation took 1.801 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:32:45,201\tWARNING util.py:315 -- Processing trial results took 1.803 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:32:45,202\tWARNING util.py:315 -- The `process_trial_result` operation took 1.805 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_abebba07_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-31-43/wandb/run-20230718_233248-abebba07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Syncing run FSR_Trainable_abebba07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/abebba07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb:                      mae █▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb:                     mape █▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb:                     rmse █▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb:         time_this_iter_s ▇▄▂▁▄▃▄▅▃▅▃▅▄▃▃▆▄▂▆▂▄▄▄▄▃▆█▆▄▃▅▄▃▃▄▄▄▄▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb:                      mae 33.30852\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb:                     mape 2.9088061896265744e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb:                     rmse 105.42506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb:       time_since_restore 122.40292\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb:         time_this_iter_s 1.22209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb:             time_total_s 122.40292\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb:                timestamp 1689690813\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: 🚀 View run FSR_Trainable_f0ab078d at: https://wandb.ai/seokjin/FSR-prediction/runs/f0ab078d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271729)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233126-f0ab078d/logs\n",
      "2023-07-18 23:33:53,200\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.919 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:33:53,204\tWARNING util.py:315 -- The `process_trial_result` operation took 1.923 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:33:53,205\tWARNING util.py:315 -- Processing trial results took 1.924 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:33:53,206\tWARNING util.py:315 -- The `process_trial_result` operation took 1.925 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_60d63a1b_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-32-41/wandb/run-20230718_233357-60d63a1b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: Syncing run FSR_Trainable_60d63a1b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/60d63a1b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb:                      mae 76.25498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb:                     mape 1.0258755291321474e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb:                     rmse 242.73169\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb:       time_since_restore 3.01219\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb:         time_this_iter_s 1.36508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb:             time_total_s 3.01219\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb:                timestamp 1689690834\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: 🚀 View run FSR_Trainable_60d63a1b at: https://wandb.ai/seokjin/FSR-prediction/runs/60d63a1b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272754)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233357-60d63a1b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:34:11,281\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.853 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:34:11,285\tWARNING util.py:315 -- The `process_trial_result` operation took 1.858 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:34:11,286\tWARNING util.py:315 -- Processing trial results took 1.859 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:34:11,288\tWARNING util.py:315 -- The `process_trial_result` operation took 1.861 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb:                      mae █▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▂▂▁▁▂▂▁▁▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb:                     mape █▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb:                     rmse █▄▂▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb:         time_this_iter_s ▅▂▃▁▂▃▃▂▃▃▃▂▂▃▁▃▄▂▂▅▆▂▂▂▃▃▃▃▂▃▂▇█▃█▅▅▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb:                      mae 34.99051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb:                     mape 2.9902055998995764e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb:                     rmse 112.86347\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb:       time_since_restore 145.60554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb:         time_this_iter_s 1.40008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb:             time_total_s 145.60554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb:                timestamp 1689690847\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: 🚀 View run FSR_Trainable_b6491f21 at: https://wandb.ai/seokjin/FSR-prediction/runs/b6491f21\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271981)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233138-b6491f21/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_e22fdcc1_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-33-49/wandb/run-20230718_233414-e22fdcc1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: Syncing run FSR_Trainable_e22fdcc1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e22fdcc1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb:                      mae █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb:                     mape █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb:                     rmse █▅▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb:         time_this_iter_s █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb:                      mae 54.05152\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb:                     mape 6.749699379785176e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb:                     rmse 170.60248\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb:       time_since_restore 6.11805\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb:         time_this_iter_s 1.26712\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb:             time_total_s 6.11805\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb:                timestamp 1689690855\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: 🚀 View run FSR_Trainable_e22fdcc1 at: https://wandb.ai/seokjin/FSR-prediction/runs/e22fdcc1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272991)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233414-e22fdcc1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb:                      mae █▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▁▂▂▁▁▂▂▂▂▂▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb:                     mape █▄▂▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▃▃▂▃▃▂▂▃▃▃▂▃▃▁▂▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb:                     rmse █▂▁▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb:         time_this_iter_s ▄▂▂▁▂▂▂▂▁▂▂▂▂▂▂▁▂▂▃▂▂▂▁▁▂▂▂▃▂▆█▂▇▄▃▃▂▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233150-98078ce8/logs\n",
      "2023-07-18 23:34:24,225\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.023 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:34:24,230\tWARNING util.py:315 -- The `process_trial_result` operation took 2.029 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:34:24,231\tWARNING util.py:315 -- Processing trial results took 2.030 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:34:24,232\tWARNING util.py:315 -- The `process_trial_result` operation took 2.031 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_40c34eba_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-34-07/wandb/run-20230718_233426-40c34eba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: Syncing run FSR_Trainable_40c34eba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/40c34eba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb:                      mae 83.64958\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb:                     mape 1.0993950059041214e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb:                     rmse 260.96517\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb:       time_since_restore 2.03851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb:         time_this_iter_s 2.03851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb:             time_total_s 2.03851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb:                timestamp 1689690862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: 🚀 View run FSR_Trainable_40c34eba at: https://wandb.ai/seokjin/FSR-prediction/runs/40c34eba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233426-40c34eba/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273234)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-18 23:34:34,044\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.087 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:34:34,049\tWARNING util.py:315 -- The `process_trial_result` operation took 2.093 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:34:34,051\tWARNING util.py:315 -- Processing trial results took 2.095 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:34:34,054\tWARNING util.py:315 -- The `process_trial_result` operation took 2.098 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_f3b152dd_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-34-20/wandb/run-20230718_233436-f3b152dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: Syncing run FSR_Trainable_f3b152dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f3b152dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb:                      mae 82.22237\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb:                     mape 8.811656933701053e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb:                     rmse 264.13685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb:       time_since_restore 1.96236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb:         time_this_iter_s 1.96236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb:             time_total_s 1.96236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb:                timestamp 1689690871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: 🚀 View run FSR_Trainable_f3b152dd at: https://wandb.ai/seokjin/FSR-prediction/runs/f3b152dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273464)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233436-f3b152dd/logs\n",
      "2023-07-18 23:34:43,775\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.057 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:34:43,779\tWARNING util.py:315 -- The `process_trial_result` operation took 2.062 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:34:43,781\tWARNING util.py:315 -- Processing trial results took 2.064 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:34:43,783\tWARNING util.py:315 -- The `process_trial_result` operation took 2.066 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_88ceeae9_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-34-30/wandb/run-20230718_233446-88ceeae9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: Syncing run FSR_Trainable_88ceeae9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/88ceeae9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb:                      mae 78.66111\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb:                     mape 7.63778168627929e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb:                     rmse 263.5803\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb:       time_since_restore 1.94671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb:         time_this_iter_s 1.94671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb:             time_total_s 1.94671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb:                timestamp 1689690881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: 🚀 View run FSR_Trainable_88ceeae9 at: https://wandb.ai/seokjin/FSR-prediction/runs/88ceeae9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273688)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233446-88ceeae9/logs\n",
      "2023-07-18 23:34:54,205\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.322 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:34:54,219\tWARNING util.py:315 -- The `process_trial_result` operation took 2.339 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:34:54,221\tWARNING util.py:315 -- Processing trial results took 2.340 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:34:54,222\tWARNING util.py:315 -- The `process_trial_result` operation took 2.342 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_3c726ff3_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-34-39/wandb/run-20230718_233457-3c726ff3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: Syncing run FSR_Trainable_3c726ff3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3c726ff3\n",
      "2023-07-18 23:35:04,510\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.737 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:35:04,513\tWARNING util.py:315 -- The `process_trial_result` operation took 1.741 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:35:04,516\tWARNING util.py:315 -- Processing trial results took 1.743 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:35:04,517\tWARNING util.py:315 -- The `process_trial_result` operation took 1.745 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_9e13e128_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-34-49/wandb/run-20230718_233508-9e13e128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: Syncing run FSR_Trainable_9e13e128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9e13e128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb:                      mae 72.26491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb:                     mape 7.306524542126013e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb:                     rmse 234.93032\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb:       time_since_restore 3.34967\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb:         time_this_iter_s 1.66665\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb:             time_total_s 3.34967\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb:                timestamp 1689690906\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: 🚀 View run FSR_Trainable_9e13e128 at: https://wandb.ai/seokjin/FSR-prediction/runs/9e13e128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274136)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233508-9e13e128/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb:                      mae █▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb:                     mape █▅▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb:                     rmse █▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb:         time_this_iter_s █▅▃▃▄▄▃▄▄▃▄▄▂▄█▄█▆▆▅▄▄▆▄▁▃▁▁▃▄▁▃▁▂▃▂▄▆▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233248-abebba07/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233248-abebba07/logs\n",
      "2023-07-18 23:35:20,751\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.013 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:35:20,754\tWARNING util.py:315 -- The `process_trial_result` operation took 2.016 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:35:20,756\tWARNING util.py:315 -- Processing trial results took 2.018 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:35:20,760\tWARNING util.py:315 -- The `process_trial_result` operation took 2.022 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=272473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_3cdef93d_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-35-01/wandb/run-20230718_233521-3cdef93d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: Syncing run FSR_Trainable_3cdef93d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3cdef93d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb:                      mae 79.80652\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb:                     mape 7.82879146797539e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb:                     rmse 269.2739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb:       time_since_restore 4.73307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb:         time_this_iter_s 4.73307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb:             time_total_s 4.73307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb:                timestamp 1689690918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: 🚀 View run FSR_Trainable_3cdef93d at: https://wandb.ai/seokjin/FSR-prediction/runs/3cdef93d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233521-3cdef93d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274363)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-18 23:35:30,361\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.862 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:35:30,366\tWARNING util.py:315 -- The `process_trial_result` operation took 1.868 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:35:30,375\tWARNING util.py:315 -- Processing trial results took 1.877 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:35:30,377\tWARNING util.py:315 -- The `process_trial_result` operation took 1.879 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_7b63b965_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-35-14/wandb/run-20230718_233531-7b63b965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: Syncing run FSR_Trainable_7b63b965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7b63b965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb:                      mae 84.89478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb:                     mape 1.0962963475704851e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb:                     rmse 264.19925\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb:       time_since_restore 4.24606\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb:         time_this_iter_s 4.24606\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb:             time_total_s 4.24606\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb:                timestamp 1689690928\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: 🚀 View run FSR_Trainable_7b63b965 at: https://wandb.ai/seokjin/FSR-prediction/runs/7b63b965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274596)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233531-7b63b965/logs\n",
      "2023-07-18 23:35:38,308\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.285 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:35:38,311\tWARNING util.py:315 -- The `process_trial_result` operation took 2.290 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:35:38,313\tWARNING util.py:315 -- Processing trial results took 2.292 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:35:38,315\tWARNING util.py:315 -- The `process_trial_result` operation took 2.294 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_40f07262_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-35-24/wandb/run-20230718_233541-40f07262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: Syncing run FSR_Trainable_40f07262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/40f07262\n",
      "2023-07-18 23:35:49,151\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.149 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:35:49,161\tWARNING util.py:315 -- The `process_trial_result` operation took 2.159 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:35:49,162\tWARNING util.py:315 -- Processing trial results took 2.161 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:35:49,165\tWARNING util.py:315 -- The `process_trial_result` operation took 2.163 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_36f1eec2_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-35-34/wandb/run-20230718_233553-36f1eec2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: Syncing run FSR_Trainable_36f1eec2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/36f1eec2\n",
      "2023-07-18 23:36:01,733\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.022 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:36:01,737\tWARNING util.py:315 -- The `process_trial_result` operation took 2.031 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:36:01,738\tWARNING util.py:315 -- Processing trial results took 2.032 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:36:01,740\tWARNING util.py:315 -- The `process_trial_result` operation took 2.034 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_cc3c4bbf_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-35-45/wandb/run-20230718_233605-cc3c4bbf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: Syncing run FSR_Trainable_cc3c4bbf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cc3c4bbf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb:                      mae █▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb:                     mape █▇▅▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb:                     rmse █▆▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▂▁▁▁▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb:         time_this_iter_s █▃▂▂▂▂▅▂▃▂▃▅▃▂▁▃▃▂▁▂▃▃▂▂▂▃▄▃▄▄▅▃▅▄▅▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb:                      mae 38.44847\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb:                     mape 3.708624966520151e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb:                     rmse 118.86999\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb:       time_since_restore 80.94602\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb:         time_this_iter_s 1.31598\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb:             time_total_s 80.94602\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb:                timestamp 1689690984\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: 🚀 View run FSR_Trainable_3c726ff3 at: https://wandb.ai/seokjin/FSR-prediction/runs/3c726ff3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=273912)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233457-3c726ff3/logs\n",
      "2023-07-18 23:36:41,467\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.891 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:36:41,470\tWARNING util.py:315 -- The `process_trial_result` operation took 1.894 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:36:41,473\tWARNING util.py:315 -- Processing trial results took 1.898 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:36:41,475\tWARNING util.py:315 -- The `process_trial_result` operation took 1.899 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_3292865f_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-35-58/wandb/run-20230718_233645-3292865f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: Syncing run FSR_Trainable_3292865f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3292865f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb:                      mae █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb:                     mape █▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb:                     rmse █▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb:         time_this_iter_s ▇█▂▄▇▇▇▄▃▃▂▃▃▄▂▄▃▆▅▅▂▄▃▃▃▇▁▆▇▂▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb:                      mae 37.74647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb:                     mape 3.614121613275885e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb:                     rmse 118.46133\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb:       time_since_restore 46.73426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb:         time_this_iter_s 1.39895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb:             time_total_s 46.73426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb:                timestamp 1689691009\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: 🚀 View run FSR_Trainable_cc3c4bbf at: https://wandb.ai/seokjin/FSR-prediction/runs/cc3c4bbf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275265)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233605-cc3c4bbf/logs\n",
      "2023-07-18 23:37:06,125\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.984 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:37:06,127\tWARNING util.py:315 -- The `process_trial_result` operation took 1.987 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:37:06,128\tWARNING util.py:315 -- Processing trial results took 1.989 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:37:06,129\tWARNING util.py:315 -- The `process_trial_result` operation took 1.990 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_3cb749d0_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-18_23-36-37/wandb/run-20230718_233709-3cb749d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: Syncing run FSR_Trainable_3cb749d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3cb749d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb:                      mae █▃▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb:                     mape █▄▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb:                     rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▁▁▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb:         time_this_iter_s ▇▁▁▇▄▇█▇█▄▃▃▇▅▃▅▆▆▃▄▃▆█▄▂▆▄▂▄▃▂▃▄▅▃▄▄▂▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb:                      mae 34.52358\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb:                     mape 3.0382924112128896e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb:                     rmse 111.04883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb:       time_since_restore 143.62913\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb:         time_this_iter_s 1.37934\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb:             time_total_s 143.62913\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb:                timestamp 1689691090\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: 🚀 View run FSR_Trainable_40f07262 at: https://wandb.ai/seokjin/FSR-prediction/runs/40f07262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=274822)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233541-40f07262/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:38:26,499\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.790 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:38:26,502\tWARNING util.py:315 -- The `process_trial_result` operation took 1.794 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:38:26,503\tWARNING util.py:315 -- Processing trial results took 1.795 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:38:26,503\tWARNING util.py:315 -- The `process_trial_result` operation took 1.796 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb:                      mae █▃▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb:                     mape █▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▂▁▁▂▁▁▂▂▂▂▁▂▂▁▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb:                     rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb:         time_this_iter_s █▆▆▂▇▆▃▃▃▁▄▃▂▆▂▂▄▂▃█▂▁▂▂▄▃▂▃▃▃▃▁▃▂▂▄▄▄▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb:                      mae 34.08174\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb:                     mape 2.9849263457615204e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb:                     rmse 109.95015\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb:       time_since_restore 145.71029\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb:         time_this_iter_s 1.34258\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb:             time_total_s 145.71029\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb:                timestamp 1689691101\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: 🚀 View run FSR_Trainable_36f1eec2 at: https://wandb.ai/seokjin/FSR-prediction/runs/36f1eec2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275046)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233553-36f1eec2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_22-57-54/FSR_Trainable_499989a2_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_Simple_2023-07-18_23-37-02/wandb/run-20230718_233829-499989a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: Syncing run FSR_Trainable_499989a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/499989a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb:                      mae █▅▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▂▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb:                     mape █▄▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▁▁▂▂▁▁▂▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb:                     rmse █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▂▁▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb:         time_this_iter_s █▅▂▃▃▂▄▂▃▃▃▃▃▃▃▃▃▃▃▄▂▃▂▃▃▂▃▂▄▃▄▃▃▂▃▄▂▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb:                      mae 35.75892\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb:                     mape 3.215794016239928e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb:                     rmse 113.94075\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb:       time_since_restore 88.4422\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb:         time_this_iter_s 1.07578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb:             time_total_s 88.4422\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb:                timestamp 1689691117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: 🚀 View run FSR_Trainable_3cb749d0 at: https://wandb.ai/seokjin/FSR-prediction/runs/3cb749d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275759)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233709-3cb749d0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb:                      mae █▄▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▂▂▂▁▁▁▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb:                     mape █▅▃▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▂▁▁▂▂▂▁▁▁▁▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb:                     rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb:         time_this_iter_s █▆▄▄▅▄█▇▄▄▆▃▄▄▄▄▄▄▅▃▄▅▃▅▃▅▃▄▆▅▄▄▂▁▁▃▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb:                      mae 33.76585\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb:                     mape 2.768422483304513e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb:                     rmse 110.37152\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb:       time_since_restore 129.86235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb:         time_this_iter_s 1.00484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb:             time_total_s 129.86235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb:                timestamp 1689691135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: 🚀 View run FSR_Trainable_3292865f at: https://wandb.ai/seokjin/FSR-prediction/runs/3292865f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=275520)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233645-3292865f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb:                      mae █▃▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb:                     mape █▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb:                     rmse █▃▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb:         time_this_iter_s █▆▅▄▄▃▂▃▂▃▃▄▂▂▂▁▂▂▁▂▁▁▂▂▁▁▂▂▁▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb:                      mae 35.49651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb:                     mape 3.333398615160274e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb:                     rmse 114.19382\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb:       time_since_restore 79.97342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb:         time_this_iter_s 0.64183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb:             time_total_s 79.97342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb:                timestamp 1689691186\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: 🚀 View run FSR_Trainable_499989a2 at: https://wandb.ai/seokjin/FSR-prediction/runs/499989a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=276060)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_233829-499989a2/logs\n",
      "2023-07-18 23:39:53,912\tINFO tune.py:1111 -- Total run time: 2516.08 seconds (2509.14 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__randomstate_ctor() takes from 0 to 1 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39;49mrestore(\n\u001b[1;32m      2\u001b[0m     path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/home/seokj/ray_results/FSR_Trainable_2023-07-04_21-31-27\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     trainable\u001b[39m=\u001b[39;49mFSR_Trainable\n\u001b[1;32m      4\u001b[0m )\u001b[39m.\u001b[39mfit()\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/tuner.py:244\u001b[0m, in \u001b[0;36mTuner.restore\u001b[0;34m(cls, path, trainable, resume_unfinished, resume_errored, restart_errored, param_space)\u001b[0m\n\u001b[1;32m    237\u001b[0m resume_config \u001b[39m=\u001b[39m _ResumeConfig(\n\u001b[1;32m    238\u001b[0m     resume_unfinished\u001b[39m=\u001b[39mresume_unfinished,\n\u001b[1;32m    239\u001b[0m     resume_errored\u001b[39m=\u001b[39mresume_errored,\n\u001b[1;32m    240\u001b[0m     restart_errored\u001b[39m=\u001b[39mrestart_errored,\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    243\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ray\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mray\u001b[39m.\u001b[39mis_connected():\n\u001b[0;32m--> 244\u001b[0m     tuner_internal \u001b[39m=\u001b[39m TunerInternal(\n\u001b[1;32m    245\u001b[0m         restore_path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m    246\u001b[0m         resume_config\u001b[39m=\u001b[39;49mresume_config,\n\u001b[1;32m    247\u001b[0m         trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m    248\u001b[0m         param_space\u001b[39m=\u001b[39;49mparam_space,\n\u001b[1;32m    249\u001b[0m     )\n\u001b[1;32m    250\u001b[0m     \u001b[39mreturn\u001b[39;00m Tuner(_tuner_internal\u001b[39m=\u001b[39mtuner_internal)\n\u001b[1;32m    251\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py:111\u001b[0m, in \u001b[0;36mTunerInternal.__init__\u001b[0;34m(self, restore_path, resume_config, trainable, param_space, tune_config, run_config, _tuner_kwargs, _entrypoint)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39m# Restore from Tuner checkpoint.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39mif\u001b[39;00m restore_path:\n\u001b[0;32m--> 111\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_restore_from_path_or_uri(\n\u001b[1;32m    112\u001b[0m         path_or_uri\u001b[39m=\u001b[39;49mrestore_path,\n\u001b[1;32m    113\u001b[0m         trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m    114\u001b[0m         overwrite_param_space\u001b[39m=\u001b[39;49mparam_space,\n\u001b[1;32m    115\u001b[0m         resume_config\u001b[39m=\u001b[39;49mresume_config,\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39m# Start from fresh\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py:394\u001b[0m, in \u001b[0;36mTunerInternal._restore_from_path_or_uri\u001b[0;34m(self, path_or_uri, trainable, overwrite_param_space, resume_config)\u001b[0m\n\u001b[1;32m    388\u001b[0m (\n\u001b[1;32m    389\u001b[0m     restoring_from_cloud,\n\u001b[1;32m    390\u001b[0m     local_experiment_checkpoint_dir,\n\u001b[1;32m    391\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_sync_down_tuner_state(path_or_uri)\n\u001b[1;32m    392\u001b[0m experiment_checkpoint_path \u001b[39m=\u001b[39m Path(local_experiment_checkpoint_dir)\n\u001b[0;32m--> 394\u001b[0m old_trainable_name, flattened_param_space_keys \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_tuner_state(\n\u001b[1;32m    395\u001b[0m     experiment_checkpoint_path \u001b[39m/\u001b[39;49m _TUNER_PKL\n\u001b[1;32m    396\u001b[0m )\n\u001b[1;32m    398\u001b[0m \u001b[39m# Perform validation and set the re-specified `trainable` and `param_space`\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_trainable_on_restore(\n\u001b[1;32m    400\u001b[0m     trainable\u001b[39m=\u001b[39mtrainable, old_trainable_name\u001b[39m=\u001b[39mold_trainable_name\n\u001b[1;32m    401\u001b[0m )\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py:353\u001b[0m, in \u001b[0;36mTunerInternal._load_tuner_state\u001b[0;34m(self, tuner_pkl_path)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    347\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not find Tuner state in restore directory. Did you pass\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    348\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthe correct path (the top-level experiment directory?) Got: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    349\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtuner_pkl_path\u001b[39m.\u001b[39mparent\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    350\u001b[0m     )\n\u001b[1;32m    352\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(tuner_pkl_path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m fp:\n\u001b[0;32m--> 353\u001b[0m     tuner_state \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(fp)\n\u001b[1;32m    355\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tuner_state, TunerInternal):\n\u001b[1;32m    356\u001b[0m         \u001b[39m# TODO(ml-team): Remove in 2.7.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m         \u001b[39m# Backwards compatibility: ray<=2.4 pickles the full Tuner object\u001b[39;00m\n\u001b[1;32m    358\u001b[0m         \u001b[39m# within `tuner.pkl`. ray>=2.5 pickles the object state as a dict.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m         tuner: TunerInternal \u001b[39m=\u001b[39m tuner_state\n",
      "\u001b[0;31mTypeError\u001b[0m: __randomstate_ctor() takes from 0 to 1 positional arguments but 2 were given"
     ]
    }
   ],
   "source": [
    "results = tuner.restore(\n",
    "    path='/home/seokj/ray_results/FSR_Trainable_2023-07-04_21-31-27',\n",
    "    trainable=FSR_Trainable\n",
    ").fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
