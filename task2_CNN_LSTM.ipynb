{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2\n",
    "\n",
    "Index_X = FSR_for_force\n",
    "\n",
    "Index_y = force\n",
    "\n",
    "Data = Splited by Time\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-07-19_00-13-04/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-07-19_00-13-04\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "178.705"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.CNN_LSTM'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': ['FSR_for_force'],\n",
    "        'index_y': ['force'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_time'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-19 00:13:04,285] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:13:06,535\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-07-19 00:13:08,192\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-19 01:05:09</td></tr>\n",
       "<tr><td>Running for: </td><td>00:52:01.27        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.0/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=100<br>Bracket: Iter 64.000: -207.17896001891182 | Iter 32.000: -230.2894805983455 | Iter 16.000: -234.42931829760778 | Iter 8.000: -239.30190096165703 | Iter 4.000: -249.41174512157716 | Iter 2.000: -275.99723166856046 | Iter 1.000: -390.4076222791572<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>index_X          </th><th>index_y  </th><th>model             </th><th style=\"text-align: right;\">    model_args/cnn_hidde\n",
       "n_size</th><th style=\"text-align: right;\">  model_args/cnn_num_l\n",
       "ayer</th><th style=\"text-align: right;\">    model_args/lstm_hidd\n",
       "en_size</th><th style=\"text-align: right;\">  model_args/lstm_num_\n",
       "layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">       mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_76355ad3</td><td>TERMINATED</td><td>172.26.215.93:315205</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00585293 </td><td>sklearn.preproc_c750</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       151.213  </td><td style=\"text-align: right;\">417.883</td><td style=\"text-align: right;\">225.857 </td><td style=\"text-align: right;\">5.27505e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_bfef460c</td><td>TERMINATED</td><td>172.26.215.93:315277</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0326195  </td><td>sklearn.preproc_c750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.65694</td><td style=\"text-align: right;\">448.246</td><td style=\"text-align: right;\">218.658 </td><td style=\"text-align: right;\">4.55232e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_2a1b74b4</td><td>TERMINATED</td><td>172.26.215.93:315432</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        1.84398e-05</td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.83932</td><td style=\"text-align: right;\">461.782</td><td style=\"text-align: right;\">274.434 </td><td style=\"text-align: right;\">1.02037e+09</td></tr>\n",
       "<tr><td>FSR_Trainable_8ad11431</td><td>TERMINATED</td><td>172.26.215.93:315627</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0418351  </td><td>sklearn.preproc_c750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.2492 </td><td style=\"text-align: right;\">567.482</td><td style=\"text-align: right;\">291.359 </td><td style=\"text-align: right;\">4.41569e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_5c398036</td><td>TERMINATED</td><td>172.26.215.93:315927</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0026188  </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.4188 </td><td style=\"text-align: right;\">464.611</td><td style=\"text-align: right;\">246.519 </td><td style=\"text-align: right;\">2.81647e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_02909daa</td><td>TERMINATED</td><td>172.26.215.93:316144</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        4.76116e-05</td><td>sklearn.preproc_c750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.07693</td><td style=\"text-align: right;\">476.313</td><td style=\"text-align: right;\">250.714 </td><td style=\"text-align: right;\">7.36968e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_0d54ed40</td><td>TERMINATED</td><td>172.26.215.93:316372</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0259847  </td><td>sklearn.preproc_c750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.01879</td><td style=\"text-align: right;\">476.687</td><td style=\"text-align: right;\">241.022 </td><td style=\"text-align: right;\">4.9783e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_cf38a0e6</td><td>TERMINATED</td><td>172.26.215.93:316596</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0235658  </td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         9.68815</td><td style=\"text-align: right;\">454.933</td><td style=\"text-align: right;\">258.69  </td><td style=\"text-align: right;\">8.56003e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_99e2ed26</td><td>TERMINATED</td><td>172.26.215.93:316818</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">7</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000531362</td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         6.06384</td><td style=\"text-align: right;\">456.817</td><td style=\"text-align: right;\">255.734 </td><td style=\"text-align: right;\">7.20232e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_85a5743c</td><td>TERMINATED</td><td>172.26.215.93:317034</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">8</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        8.79116e-05</td><td>sklearn.preproc_c750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.11551</td><td style=\"text-align: right;\">484.883</td><td style=\"text-align: right;\">255.407 </td><td style=\"text-align: right;\">6.14994e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_88386bcc</td><td>TERMINATED</td><td>172.26.215.93:317267</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00971809 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         6.01007</td><td style=\"text-align: right;\">460.191</td><td style=\"text-align: right;\">243.936 </td><td style=\"text-align: right;\">2.84995e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_fbddafdd</td><td>TERMINATED</td><td>172.26.215.93:317484</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000427721</td><td>sklearn.preproc_c750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.61098</td><td style=\"text-align: right;\">465.527</td><td style=\"text-align: right;\">237.478 </td><td style=\"text-align: right;\">3.03145e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_8e3bd690</td><td>TERMINATED</td><td>172.26.215.93:317712</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0943639  </td><td>sklearn.preproc_c750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.89814</td><td style=\"text-align: right;\">463.973</td><td style=\"text-align: right;\">255.675 </td><td style=\"text-align: right;\">6.87926e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_fa9d0228</td><td>TERMINATED</td><td>172.26.215.93:317953</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.099748   </td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.8477 </td><td style=\"text-align: right;\">473.399</td><td style=\"text-align: right;\">257.1   </td><td style=\"text-align: right;\">9.47609e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_4ca3bae3</td><td>TERMINATED</td><td>172.26.215.93:318159</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00770674 </td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        44.9717 </td><td style=\"text-align: right;\">450.782</td><td style=\"text-align: right;\">255.19  </td><td style=\"text-align: right;\">7.31718e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_d8fbd1ca</td><td>TERMINATED</td><td>172.26.215.93:318385</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00769548 </td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       225.578  </td><td style=\"text-align: right;\">365.856</td><td style=\"text-align: right;\">177.568 </td><td style=\"text-align: right;\">3.43176e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_c767eb4c</td><td>TERMINATED</td><td>172.26.215.93:318602</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0087468  </td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        10.1017 </td><td style=\"text-align: right;\">367.292</td><td style=\"text-align: right;\">184.904 </td><td style=\"text-align: right;\">5.78297e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_e2e2c0c8</td><td>TERMINATED</td><td>172.26.215.93:318878</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00897701 </td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         8.77879</td><td style=\"text-align: right;\">386.839</td><td style=\"text-align: right;\">194.585 </td><td style=\"text-align: right;\">6.40034e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_681f7398</td><td>TERMINATED</td><td>172.26.215.93:319077</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00539277 </td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         6.95826</td><td style=\"text-align: right;\">383.33 </td><td style=\"text-align: right;\">191.16  </td><td style=\"text-align: right;\">6.23321e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_03bd954e</td><td>TERMINATED</td><td>172.26.215.93:319289</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00347247 </td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       220.056  </td><td style=\"text-align: right;\">360.282</td><td style=\"text-align: right;\">173.642 </td><td style=\"text-align: right;\">2.9408e+08 </td></tr>\n",
       "<tr><td>FSR_Trainable_9e15ec76</td><td>TERMINATED</td><td>172.26.215.93:319514</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00159279 </td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       218.292  </td><td style=\"text-align: right;\">214.087</td><td style=\"text-align: right;\">108.995 </td><td style=\"text-align: right;\">8.86654e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_cb5882fa</td><td>TERMINATED</td><td>172.26.215.93:319732</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00128075 </td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        23.4704 </td><td style=\"text-align: right;\">290.017</td><td style=\"text-align: right;\">149.844 </td><td style=\"text-align: right;\">2.47721e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_64b635d6</td><td>TERMINATED</td><td>172.26.215.93:320016</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00163024 </td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        51.2478 </td><td style=\"text-align: right;\">290.128</td><td style=\"text-align: right;\">144.866 </td><td style=\"text-align: right;\">1.7078e+08 </td></tr>\n",
       "<tr><td>FSR_Trainable_45a78ad0</td><td>TERMINATED</td><td>172.26.215.93:320271</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00155926 </td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        13.6866 </td><td style=\"text-align: right;\">366.142</td><td style=\"text-align: right;\">178.85  </td><td style=\"text-align: right;\">4.75424e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_96e0adc1</td><td>TERMINATED</td><td>172.26.215.93:320503</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00155111 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.61475</td><td style=\"text-align: right;\">578.32 </td><td style=\"text-align: right;\">381.814 </td><td style=\"text-align: right;\">4.95448e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_caeff74f</td><td>TERMINATED</td><td>172.26.215.93:320706</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00133672 </td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        12.9829 </td><td style=\"text-align: right;\">365.548</td><td style=\"text-align: right;\">180.972 </td><td style=\"text-align: right;\">4.66414e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_cce72218</td><td>TERMINATED</td><td>172.26.215.93:320958</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000742008</td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.84218</td><td style=\"text-align: right;\">454.444</td><td style=\"text-align: right;\">265.402 </td><td style=\"text-align: right;\">9.318e+08  </td></tr>\n",
       "<tr><td>FSR_Trainable_457af1e2</td><td>TERMINATED</td><td>172.26.215.93:321172</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000690108</td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         4.46166</td><td style=\"text-align: right;\">445.806</td><td style=\"text-align: right;\">260.408 </td><td style=\"text-align: right;\">8.66208e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_b0e7957c</td><td>TERMINATED</td><td>172.26.215.93:321371</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000292626</td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        10.4823 </td><td style=\"text-align: right;\">439.388</td><td style=\"text-align: right;\">254.628 </td><td style=\"text-align: right;\">8.45837e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_629e0313</td><td>TERMINATED</td><td>172.26.215.93:321591</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000282997</td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        17.9091 </td><td style=\"text-align: right;\">454.166</td><td style=\"text-align: right;\">234.661 </td><td style=\"text-align: right;\">9.89556e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_2e026fe5</td><td>TERMINATED</td><td>172.26.215.93:321823</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000254222</td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        30.2395 </td><td style=\"text-align: right;\">384.463</td><td style=\"text-align: right;\">191.387 </td><td style=\"text-align: right;\">6.36127e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_43d5dcc3</td><td>TERMINATED</td><td>172.26.215.93:322097</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00248659 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       518.356  </td><td style=\"text-align: right;\">217.499</td><td style=\"text-align: right;\">105.722 </td><td style=\"text-align: right;\">5.42532e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_dd20017e</td><td>TERMINATED</td><td>172.26.215.93:322278</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00297308 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        36.2022 </td><td style=\"text-align: right;\">294.487</td><td style=\"text-align: right;\">144.933 </td><td style=\"text-align: right;\">8.70726e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_9e451d85</td><td>TERMINATED</td><td>172.26.215.93:322492</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00262646 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        34.9123 </td><td style=\"text-align: right;\">291.095</td><td style=\"text-align: right;\">143.344 </td><td style=\"text-align: right;\">8.4727e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_aa30a5a6</td><td>TERMINATED</td><td>172.26.215.93:322709</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00314364 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.36285</td><td style=\"text-align: right;\">453.158</td><td style=\"text-align: right;\">260.515 </td><td style=\"text-align: right;\">4.00187e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_33e1f2a1</td><td>TERMINATED</td><td>172.26.215.93:322961</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00296969 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.26529</td><td style=\"text-align: right;\">455.733</td><td style=\"text-align: right;\">255.004 </td><td style=\"text-align: right;\">3.57444e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_41fe8a69</td><td>TERMINATED</td><td>172.26.215.93:323182</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0027463  </td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        36.5922 </td><td style=\"text-align: right;\">289.8  </td><td style=\"text-align: right;\">155.147 </td><td style=\"text-align: right;\">3.04647e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_e0e7b4f2</td><td>TERMINATED</td><td>172.26.215.93:323394</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00109905 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       275.934  </td><td style=\"text-align: right;\">210.39 </td><td style=\"text-align: right;\">101.95  </td><td style=\"text-align: right;\">5.56643e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_a30a0694</td><td>TERMINATED</td><td>172.26.215.93:323608</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000994471</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       269.768  </td><td style=\"text-align: right;\">214.239</td><td style=\"text-align: right;\">106.656 </td><td style=\"text-align: right;\">6.64336e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_5b1dc23d</td><td>TERMINATED</td><td>172.26.215.93:323882</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00109216 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       275.215  </td><td style=\"text-align: right;\">211.646</td><td style=\"text-align: right;\">107.337 </td><td style=\"text-align: right;\">7.5027e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_ac90a52e</td><td>TERMINATED</td><td>172.26.215.93:324220</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00100116 </td><td>sklearn.preproc_c510</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        60.0716 </td><td style=\"text-align: right;\">273.129</td><td style=\"text-align: right;\">139.8   </td><td style=\"text-align: right;\">2.74343e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_e2ad4b4f</td><td>TERMINATED</td><td>172.26.215.93:324400</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000896913</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       276.397  </td><td style=\"text-align: right;\">202.461</td><td style=\"text-align: right;\">102.348 </td><td style=\"text-align: right;\">5.83899e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_a25dc63a</td><td>TERMINATED</td><td>172.26.215.93:324673</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000952464</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       274.374  </td><td style=\"text-align: right;\">199.53 </td><td style=\"text-align: right;\"> 99.6831</td><td style=\"text-align: right;\">5.44077e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_f0123c53</td><td>TERMINATED</td><td>172.26.215.93:324908</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000865751</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       272.281  </td><td style=\"text-align: right;\">219.142</td><td style=\"text-align: right;\">112.765 </td><td style=\"text-align: right;\">8.06259e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_fb124383</td><td>TERMINATED</td><td>172.26.215.93:325176</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000964313</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       176.883  </td><td style=\"text-align: right;\">230.909</td><td style=\"text-align: right;\">118.18  </td><td style=\"text-align: right;\">1.07129e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_99f0e4a9</td><td>TERMINATED</td><td>172.26.215.93:325464</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000891687</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       276.04   </td><td style=\"text-align: right;\">196.017</td><td style=\"text-align: right;\"> 97.455 </td><td style=\"text-align: right;\">6.23441e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_65b325f2</td><td>TERMINATED</td><td>172.26.215.93:325696</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000872567</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       276.99   </td><td style=\"text-align: right;\">214.934</td><td style=\"text-align: right;\">106.764 </td><td style=\"text-align: right;\">6.76256e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_09efce4f</td><td>TERMINATED</td><td>172.26.215.93:325927</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000477399</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       178.02   </td><td style=\"text-align: right;\">223.287</td><td style=\"text-align: right;\">111.663 </td><td style=\"text-align: right;\">7.17547e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_a0320404</td><td>TERMINATED</td><td>172.26.215.93:326116</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000360511</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       260.678  </td><td style=\"text-align: right;\">195.54 </td><td style=\"text-align: right;\"> 99.3032</td><td style=\"text-align: right;\">6.70003e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_c370576b</td><td>TERMINATED</td><td>172.26.215.93:326472</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000457505</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       258.362  </td><td style=\"text-align: right;\">183.889</td><td style=\"text-align: right;\"> 94.5365</td><td style=\"text-align: right;\">7.04451e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_ee736811</td><td>TERMINATED</td><td>172.26.215.93:326712</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000534253</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       276.961  </td><td style=\"text-align: right;\">187.756</td><td style=\"text-align: right;\"> 94.3328</td><td style=\"text-align: right;\">6.42432e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_d72330cb</td><td>TERMINATED</td><td>172.26.215.93:326942</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000601691</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       279.064  </td><td style=\"text-align: right;\">181.766</td><td style=\"text-align: right;\"> 91.0368</td><td style=\"text-align: right;\">6.68699e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_ec812aff</td><td>TERMINATED</td><td>172.26.215.93:327171</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000560311</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       275.03   </td><td style=\"text-align: right;\">185.617</td><td style=\"text-align: right;\"> 94.5672</td><td style=\"text-align: right;\">6.79898e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_23f8d543</td><td>TERMINATED</td><td>172.26.215.93:327482</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000392624</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       250.239  </td><td style=\"text-align: right;\">178.892</td><td style=\"text-align: right;\"> 93.2257</td><td style=\"text-align: right;\">7.4879e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_b590b28a</td><td>TERMINATED</td><td>172.26.215.93:327737</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000177104</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.27049</td><td style=\"text-align: right;\">802.913</td><td style=\"text-align: right;\">573.236 </td><td style=\"text-align: right;\">8.4022e+17 </td></tr>\n",
       "<tr><td>FSR_Trainable_ecda1e39</td><td>TERMINATED</td><td>172.26.215.93:327930</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000185655</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       247.01   </td><td style=\"text-align: right;\">184.304</td><td style=\"text-align: right;\"> 98.2454</td><td style=\"text-align: right;\">7.66794e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_e689db0e</td><td>TERMINATED</td><td>172.26.215.93:328172</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000515064</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       270.483  </td><td style=\"text-align: right;\">186.113</td><td style=\"text-align: right;\"> 93.7732</td><td style=\"text-align: right;\">6.77101e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_739006ac</td><td>TERMINATED</td><td>172.26.215.93:328426</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000502362</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        18.3967 </td><td style=\"text-align: right;\">292.659</td><td style=\"text-align: right;\">151.989 </td><td style=\"text-align: right;\">1.25046e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_d8e458e1</td><td>TERMINATED</td><td>172.26.215.93:328662</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000518209</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.30339</td><td style=\"text-align: right;\">425.487</td><td style=\"text-align: right;\">219.132 </td><td style=\"text-align: right;\">2.29013e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_d814fa87</td><td>TERMINATED</td><td>172.26.215.93:328885</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000578196</td><td>sklearn.preproc_c750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         6.17918</td><td style=\"text-align: right;\">355.905</td><td style=\"text-align: right;\">177.011 </td><td style=\"text-align: right;\">2.45535e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_79862139</td><td>TERMINATED</td><td>172.26.215.93:329118</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000371222</td><td>sklearn.preproc_c750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        17.6513 </td><td style=\"text-align: right;\">428.468</td><td style=\"text-align: right;\">218.229 </td><td style=\"text-align: right;\">3.78364e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_7cfd7c70</td><td>TERMINATED</td><td>172.26.215.93:329354</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">8</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000382471</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.09023</td><td style=\"text-align: right;\">567.153</td><td style=\"text-align: right;\">351.008 </td><td style=\"text-align: right;\">4.62461e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_314c8e84</td><td>TERMINATED</td><td>172.26.215.93:329560</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000165546</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.83972</td><td style=\"text-align: right;\">747.021</td><td style=\"text-align: right;\">491.078 </td><td style=\"text-align: right;\">7.36451e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_af202444</td><td>TERMINATED</td><td>172.26.215.93:329788</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000176228</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.38726</td><td style=\"text-align: right;\">486.621</td><td style=\"text-align: right;\">266.319 </td><td style=\"text-align: right;\">3.01399e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_4995aa5f</td><td>TERMINATED</td><td>172.26.215.93:330020</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000652557</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       104.509  </td><td style=\"text-align: right;\">179.18 </td><td style=\"text-align: right;\"> 91.5638</td><td style=\"text-align: right;\">6.0287e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_d001dc20</td><td>TERMINATED</td><td>172.26.215.93:330232</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000723496</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       241.099  </td><td style=\"text-align: right;\">193.529</td><td style=\"text-align: right;\"> 99.7562</td><td style=\"text-align: right;\">7.57221e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_2187bcf7</td><td>TERMINATED</td><td>172.26.215.93:330526</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00035459 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       246.27   </td><td style=\"text-align: right;\">188.338</td><td style=\"text-align: right;\"> 97.4944</td><td style=\"text-align: right;\">7.36121e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_1a247592</td><td>TERMINATED</td><td>172.26.215.93:330773</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000111335</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         5.55614</td><td style=\"text-align: right;\">329.306</td><td style=\"text-align: right;\">177.644 </td><td style=\"text-align: right;\">2.10754e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_dfabbb69</td><td>TERMINATED</td><td>172.26.215.93:330947</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000664581</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         6.27456</td><td style=\"text-align: right;\">255.124</td><td style=\"text-align: right;\">135.706 </td><td style=\"text-align: right;\">1.08853e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_38981ae1</td><td>TERMINATED</td><td>172.26.215.93:331167</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00064963 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.73179</td><td style=\"text-align: right;\">254.009</td><td style=\"text-align: right;\">137.637 </td><td style=\"text-align: right;\">1.38316e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_a74bbb80</td><td>TERMINATED</td><td>172.26.215.93:331394</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00027545 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        18.8193 </td><td style=\"text-align: right;\">276.21 </td><td style=\"text-align: right;\">146.973 </td><td style=\"text-align: right;\">1.44436e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_462c1b68</td><td>TERMINATED</td><td>172.26.215.93:331621</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000232708</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.51616</td><td style=\"text-align: right;\">430.893</td><td style=\"text-align: right;\">236.916 </td><td style=\"text-align: right;\">3.04294e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_7609fbc4</td><td>TERMINATED</td><td>172.26.215.93:331859</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000299009</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.46004</td><td style=\"text-align: right;\">450.206</td><td style=\"text-align: right;\">266.989 </td><td style=\"text-align: right;\">3.90756e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_98d31824</td><td>TERMINATED</td><td>172.26.215.93:332086</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000399253</td><td>sklearn.preproc_c750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.36361</td><td style=\"text-align: right;\">439.093</td><td style=\"text-align: right;\">216.873 </td><td style=\"text-align: right;\">9.2433e+15 </td></tr>\n",
       "<tr><td>FSR_Trainable_ea8d39b7</td><td>TERMINATED</td><td>172.26.215.93:332309</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000471434</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       283.315  </td><td style=\"text-align: right;\">186.369</td><td style=\"text-align: right;\"> 93.7526</td><td style=\"text-align: right;\">6.39661e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_cbeeb68c</td><td>TERMINATED</td><td>172.26.215.93:332517</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000678029</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       171.938  </td><td style=\"text-align: right;\">212.189</td><td style=\"text-align: right;\">102.574 </td><td style=\"text-align: right;\">6.42117e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_674faab7</td><td>TERMINATED</td><td>172.26.215.93:332814</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000535205</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       296.751  </td><td style=\"text-align: right;\">182.862</td><td style=\"text-align: right;\"> 88.7602</td><td style=\"text-align: right;\">5.6434e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_c48c7ff2</td><td>TERMINATED</td><td>172.26.215.93:333075</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000478624</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       312.067  </td><td style=\"text-align: right;\">180.734</td><td style=\"text-align: right;\"> 89.624 </td><td style=\"text-align: right;\">6.12627e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_64a77c83</td><td>TERMINATED</td><td>172.26.215.93:333314</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000573459</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        23.1322 </td><td style=\"text-align: right;\">258.557</td><td style=\"text-align: right;\">136.989 </td><td style=\"text-align: right;\">1.04719e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_65d350c0</td><td>TERMINATED</td><td>172.26.215.93:333551</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000503138</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        11.2305 </td><td style=\"text-align: right;\">318.323</td><td style=\"text-align: right;\">172.773 </td><td style=\"text-align: right;\">1.93566e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_a454708c</td><td>TERMINATED</td><td>172.26.215.93:333786</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000458783</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         6.90149</td><td style=\"text-align: right;\">316.88 </td><td style=\"text-align: right;\">177.01  </td><td style=\"text-align: right;\">2.24211e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_b7b936d0</td><td>TERMINATED</td><td>172.26.215.93:334013</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000434179</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        15.0386 </td><td style=\"text-align: right;\">257.272</td><td style=\"text-align: right;\">135.746 </td><td style=\"text-align: right;\">1.0569e+17 </td></tr>\n",
       "<tr><td>FSR_Trainable_908222cf</td><td>TERMINATED</td><td>172.26.215.93:334242</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000242117</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.54459</td><td style=\"text-align: right;\">567.671</td><td style=\"text-align: right;\">272.166 </td><td style=\"text-align: right;\">1.41924e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_93ed133c</td><td>TERMINATED</td><td>172.26.215.93:334426</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0013387  </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.6586 </td><td style=\"text-align: right;\">577.442</td><td style=\"text-align: right;\">290.327 </td><td style=\"text-align: right;\">1.50935e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_65ee2aa9</td><td>TERMINATED</td><td>172.26.215.93:334648</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000325888</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       315.883  </td><td style=\"text-align: right;\">189.09 </td><td style=\"text-align: right;\"> 97.9621</td><td style=\"text-align: right;\">7.23046e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_cc8c9340</td><td>TERMINATED</td><td>172.26.215.93:334878</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000605537</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       315.484  </td><td style=\"text-align: right;\">185.95 </td><td style=\"text-align: right;\"> 93.703 </td><td style=\"text-align: right;\">6.51511e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_dea50f73</td><td>TERMINATED</td><td>172.26.215.93:335173</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000323595</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       319.16   </td><td style=\"text-align: right;\">189.958</td><td style=\"text-align: right;\"> 95.9287</td><td style=\"text-align: right;\">6.88141e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_56bd6189</td><td>TERMINATED</td><td>172.26.215.93:335438</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00031869 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       314.95   </td><td style=\"text-align: right;\">184.687</td><td style=\"text-align: right;\"> 96.0179</td><td style=\"text-align: right;\">7.20755e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_ab5b2886</td><td>TERMINATED</td><td>172.26.215.93:335739</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000348125</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       287.446  </td><td style=\"text-align: right;\">178.705</td><td style=\"text-align: right;\"> 92.472 </td><td style=\"text-align: right;\">6.73523e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_9078fd24</td><td>TERMINATED</td><td>172.26.215.93:335919</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000749637</td><td>sklearn.preproc_c750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        11.3954 </td><td style=\"text-align: right;\">338.49 </td><td style=\"text-align: right;\">168.344 </td><td style=\"text-align: right;\">2.49365e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_fe8e305d</td><td>TERMINATED</td><td>172.26.215.93:336197</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000801596</td><td>sklearn.preproc_c750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        12.7321 </td><td style=\"text-align: right;\">328.729</td><td style=\"text-align: right;\">167.752 </td><td style=\"text-align: right;\">2.77103e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_21e9f9c9</td><td>TERMINATED</td><td>172.26.215.93:336425</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000215888</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.67308</td><td style=\"text-align: right;\">395.232</td><td style=\"text-align: right;\">204.497 </td><td style=\"text-align: right;\">2.29896e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_3e144bba</td><td>TERMINATED</td><td>172.26.215.93:336607</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00200875 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       115.914  </td><td style=\"text-align: right;\">202.186</td><td style=\"text-align: right;\">107.317 </td><td style=\"text-align: right;\">8.47588e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_93a5f695</td><td>TERMINATED</td><td>172.26.215.93:336846</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00177266 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.60008</td><td style=\"text-align: right;\">516.143</td><td style=\"text-align: right;\">287.987 </td><td style=\"text-align: right;\">3.66342e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_6b790f67</td><td>TERMINATED</td><td>172.26.215.93:337070</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00198952 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       114.564  </td><td style=\"text-align: right;\">182.518</td><td style=\"text-align: right;\"> 97.6751</td><td style=\"text-align: right;\">8.67992e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_172e5cf3</td><td>TERMINATED</td><td>172.26.215.93:337357</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000583702</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       169.312  </td><td style=\"text-align: right;\">207.214</td><td style=\"text-align: right;\">104.009 </td><td style=\"text-align: right;\">6.80158e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_6eec31ef</td><td>TERMINATED</td><td>172.26.215.93:337613</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000610974</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        55.3632 </td><td style=\"text-align: right;\">234.429</td><td style=\"text-align: right;\">124.06  </td><td style=\"text-align: right;\">1.00729e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_56a35d41</td><td>TERMINATED</td><td>172.26.215.93:337850</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00123542 </td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        29.218  </td><td style=\"text-align: right;\">241.68 </td><td style=\"text-align: right;\">129.688 </td><td style=\"text-align: right;\">1.12729e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_f6c987f1</td><td>TERMINATED</td><td>172.26.215.93:338094</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000424807</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.54014</td><td style=\"text-align: right;\">563.502</td><td style=\"text-align: right;\">418.489 </td><td style=\"text-align: right;\">7.73441e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_c68eded0</td><td>TERMINATED</td><td>172.26.215.93:338286</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_c7b0</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000396203</td><td>sklearn.preproc_c6f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.67645</td><td style=\"text-align: right;\">739.795</td><td style=\"text-align: right;\">483.977 </td><td style=\"text-align: right;\">6.37418e+17</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 00:13:08,241\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">       mape</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_02909daa</td><td>2023-07-19_00-13-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">250.714 </td><td style=\"text-align: right;\">7.36968e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">316144</td><td style=\"text-align: right;\">476.313</td><td style=\"text-align: right;\">             2.07693</td><td style=\"text-align: right;\">           2.07693</td><td style=\"text-align: right;\">       2.07693</td><td style=\"text-align: right;\"> 1689693239</td><td style=\"text-align: right;\">                   1</td><td>02909daa  </td></tr>\n",
       "<tr><td>FSR_Trainable_03bd954e</td><td>2023-07-19_00-20-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">173.642 </td><td style=\"text-align: right;\">2.9408e+08 </td><td>172.26.215.93</td><td style=\"text-align: right;\">319289</td><td style=\"text-align: right;\">360.282</td><td style=\"text-align: right;\">           220.056  </td><td style=\"text-align: right;\">           2.49121</td><td style=\"text-align: right;\">     220.056  </td><td style=\"text-align: right;\"> 1689693612</td><td style=\"text-align: right;\">                 100</td><td>03bd954e  </td></tr>\n",
       "<tr><td>FSR_Trainable_09efce4f</td><td>2023-07-19_00-35-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">111.663 </td><td style=\"text-align: right;\">7.17547e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">325927</td><td style=\"text-align: right;\">223.287</td><td style=\"text-align: right;\">           178.02   </td><td style=\"text-align: right;\">           2.75903</td><td style=\"text-align: right;\">     178.02   </td><td style=\"text-align: right;\"> 1689694527</td><td style=\"text-align: right;\">                  64</td><td>09efce4f  </td></tr>\n",
       "<tr><td>FSR_Trainable_0d54ed40</td><td>2023-07-19_00-14-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">241.022 </td><td style=\"text-align: right;\">4.9783e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">316372</td><td style=\"text-align: right;\">476.687</td><td style=\"text-align: right;\">             5.01879</td><td style=\"text-align: right;\">           5.01879</td><td style=\"text-align: right;\">       5.01879</td><td style=\"text-align: right;\"> 1689693250</td><td style=\"text-align: right;\">                   1</td><td>0d54ed40  </td></tr>\n",
       "<tr><td>FSR_Trainable_172e5cf3</td><td>2023-07-19_01-05-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">104.009 </td><td style=\"text-align: right;\">6.80158e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">337357</td><td style=\"text-align: right;\">207.214</td><td style=\"text-align: right;\">           169.312  </td><td style=\"text-align: right;\">           1.6194 </td><td style=\"text-align: right;\">     169.312  </td><td style=\"text-align: right;\"> 1689696309</td><td style=\"text-align: right;\">                  64</td><td>172e5cf3  </td></tr>\n",
       "<tr><td>FSR_Trainable_1a247592</td><td>2023-07-19_00-46-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">177.644 </td><td style=\"text-align: right;\">2.10754e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">330773</td><td style=\"text-align: right;\">329.306</td><td style=\"text-align: right;\">             5.55614</td><td style=\"text-align: right;\">           2.52755</td><td style=\"text-align: right;\">       5.55614</td><td style=\"text-align: right;\"> 1689695210</td><td style=\"text-align: right;\">                   2</td><td>1a247592  </td></tr>\n",
       "<tr><td>FSR_Trainable_2187bcf7</td><td>2023-07-19_00-50-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 97.4944</td><td style=\"text-align: right;\">7.36121e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">330526</td><td style=\"text-align: right;\">188.338</td><td style=\"text-align: right;\">           246.27   </td><td style=\"text-align: right;\">           2.47829</td><td style=\"text-align: right;\">     246.27   </td><td style=\"text-align: right;\"> 1689695430</td><td style=\"text-align: right;\">                 100</td><td>2187bcf7  </td></tr>\n",
       "<tr><td>FSR_Trainable_21e9f9c9</td><td>2023-07-19_01-00-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">204.497 </td><td style=\"text-align: right;\">2.29896e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">336425</td><td style=\"text-align: right;\">395.232</td><td style=\"text-align: right;\">             3.67308</td><td style=\"text-align: right;\">           3.67308</td><td style=\"text-align: right;\">       3.67308</td><td style=\"text-align: right;\"> 1689696029</td><td style=\"text-align: right;\">                   1</td><td>21e9f9c9  </td></tr>\n",
       "<tr><td>FSR_Trainable_23f8d543</td><td>2023-07-19_00-44-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 93.2257</td><td style=\"text-align: right;\">7.4879e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">327482</td><td style=\"text-align: right;\">178.892</td><td style=\"text-align: right;\">           250.239  </td><td style=\"text-align: right;\">           2.38987</td><td style=\"text-align: right;\">     250.239  </td><td style=\"text-align: right;\"> 1689695077</td><td style=\"text-align: right;\">                 100</td><td>23f8d543  </td></tr>\n",
       "<tr><td>FSR_Trainable_2a1b74b4</td><td>2023-07-19_00-13-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">274.434 </td><td style=\"text-align: right;\">1.02037e+09</td><td>172.26.215.93</td><td style=\"text-align: right;\">315432</td><td style=\"text-align: right;\">461.782</td><td style=\"text-align: right;\">             3.83932</td><td style=\"text-align: right;\">           3.83932</td><td style=\"text-align: right;\">       3.83932</td><td style=\"text-align: right;\"> 1689693211</td><td style=\"text-align: right;\">                   1</td><td>2a1b74b4  </td></tr>\n",
       "<tr><td>FSR_Trainable_2e026fe5</td><td>2023-07-19_00-20-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">191.387 </td><td style=\"text-align: right;\">6.36127e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">321823</td><td style=\"text-align: right;\">384.463</td><td style=\"text-align: right;\">            30.2395 </td><td style=\"text-align: right;\">           8.4224 </td><td style=\"text-align: right;\">      30.2395 </td><td style=\"text-align: right;\"> 1689693636</td><td style=\"text-align: right;\">                   4</td><td>2e026fe5  </td></tr>\n",
       "<tr><td>FSR_Trainable_314c8e84</td><td>2023-07-19_00-44-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">491.078 </td><td style=\"text-align: right;\">7.36451e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">329560</td><td style=\"text-align: right;\">747.021</td><td style=\"text-align: right;\">             2.83972</td><td style=\"text-align: right;\">           2.83972</td><td style=\"text-align: right;\">       2.83972</td><td style=\"text-align: right;\"> 1689695051</td><td style=\"text-align: right;\">                   1</td><td>314c8e84  </td></tr>\n",
       "<tr><td>FSR_Trainable_33e1f2a1</td><td>2023-07-19_00-21-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">255.004 </td><td style=\"text-align: right;\">3.57444e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">322961</td><td style=\"text-align: right;\">455.733</td><td style=\"text-align: right;\">             2.26529</td><td style=\"text-align: right;\">           2.26529</td><td style=\"text-align: right;\">       2.26529</td><td style=\"text-align: right;\"> 1689693667</td><td style=\"text-align: right;\">                   1</td><td>33e1f2a1  </td></tr>\n",
       "<tr><td>FSR_Trainable_38981ae1</td><td>2023-07-19_00-47-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">137.637 </td><td style=\"text-align: right;\">1.38316e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">331167</td><td style=\"text-align: right;\">254.009</td><td style=\"text-align: right;\">             5.73179</td><td style=\"text-align: right;\">           1.27169</td><td style=\"text-align: right;\">       5.73179</td><td style=\"text-align: right;\"> 1689695231</td><td style=\"text-align: right;\">                   4</td><td>38981ae1  </td></tr>\n",
       "<tr><td>FSR_Trainable_3e144bba</td><td>2023-07-19_01-02-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">107.317 </td><td style=\"text-align: right;\">8.47588e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">336607</td><td style=\"text-align: right;\">202.186</td><td style=\"text-align: right;\">           115.914  </td><td style=\"text-align: right;\">           1.12075</td><td style=\"text-align: right;\">     115.914  </td><td style=\"text-align: right;\"> 1689696166</td><td style=\"text-align: right;\">                 100</td><td>3e144bba  </td></tr>\n",
       "<tr><td>FSR_Trainable_41fe8a69</td><td>2023-07-19_00-21-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">155.147 </td><td style=\"text-align: right;\">3.04647e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">323182</td><td style=\"text-align: right;\">289.8  </td><td style=\"text-align: right;\">            36.5922 </td><td style=\"text-align: right;\">           4.39338</td><td style=\"text-align: right;\">      36.5922 </td><td style=\"text-align: right;\"> 1689693717</td><td style=\"text-align: right;\">                   8</td><td>41fe8a69  </td></tr>\n",
       "<tr><td>FSR_Trainable_43d5dcc3</td><td>2023-07-19_00-29-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">105.722 </td><td style=\"text-align: right;\">5.42532e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">322097</td><td style=\"text-align: right;\">217.499</td><td style=\"text-align: right;\">           518.356  </td><td style=\"text-align: right;\">           5.21393</td><td style=\"text-align: right;\">     518.356  </td><td style=\"text-align: right;\"> 1689694145</td><td style=\"text-align: right;\">                 100</td><td>43d5dcc3  </td></tr>\n",
       "<tr><td>FSR_Trainable_457af1e2</td><td>2023-07-19_00-19-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">260.408 </td><td style=\"text-align: right;\">8.66208e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">321172</td><td style=\"text-align: right;\">445.806</td><td style=\"text-align: right;\">             4.46166</td><td style=\"text-align: right;\">           2.04003</td><td style=\"text-align: right;\">       4.46166</td><td style=\"text-align: right;\"> 1689693579</td><td style=\"text-align: right;\">                   2</td><td>457af1e2  </td></tr>\n",
       "<tr><td>FSR_Trainable_45a78ad0</td><td>2023-07-19_00-18-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">178.85  </td><td style=\"text-align: right;\">4.75424e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">320271</td><td style=\"text-align: right;\">366.142</td><td style=\"text-align: right;\">            13.6866 </td><td style=\"text-align: right;\">           3.1287 </td><td style=\"text-align: right;\">      13.6866 </td><td style=\"text-align: right;\"> 1689693511</td><td style=\"text-align: right;\">                   4</td><td>45a78ad0  </td></tr>\n",
       "<tr><td>FSR_Trainable_462c1b68</td><td>2023-07-19_00-47-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">236.916 </td><td style=\"text-align: right;\">3.04294e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">331621</td><td style=\"text-align: right;\">430.893</td><td style=\"text-align: right;\">             1.51616</td><td style=\"text-align: right;\">           1.51616</td><td style=\"text-align: right;\">       1.51616</td><td style=\"text-align: right;\"> 1689695246</td><td style=\"text-align: right;\">                   1</td><td>462c1b68  </td></tr>\n",
       "<tr><td>FSR_Trainable_4995aa5f</td><td>2023-07-19_00-46-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 91.5638</td><td style=\"text-align: right;\">6.0287e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">330020</td><td style=\"text-align: right;\">179.18 </td><td style=\"text-align: right;\">           104.509  </td><td style=\"text-align: right;\">           1.06806</td><td style=\"text-align: right;\">     104.509  </td><td style=\"text-align: right;\"> 1689695193</td><td style=\"text-align: right;\">                 100</td><td>4995aa5f  </td></tr>\n",
       "<tr><td>FSR_Trainable_4ca3bae3</td><td>2023-07-19_00-16-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">255.19  </td><td style=\"text-align: right;\">7.31718e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">318159</td><td style=\"text-align: right;\">450.782</td><td style=\"text-align: right;\">            44.9717 </td><td style=\"text-align: right;\">           4.85477</td><td style=\"text-align: right;\">      44.9717 </td><td style=\"text-align: right;\"> 1689693362</td><td style=\"text-align: right;\">                   8</td><td>4ca3bae3  </td></tr>\n",
       "<tr><td>FSR_Trainable_56a35d41</td><td>2023-07-19_01-03-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">129.688 </td><td style=\"text-align: right;\">1.12729e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">337850</td><td style=\"text-align: right;\">241.68 </td><td style=\"text-align: right;\">            29.218  </td><td style=\"text-align: right;\">           3.33413</td><td style=\"text-align: right;\">      29.218  </td><td style=\"text-align: right;\"> 1689696238</td><td style=\"text-align: right;\">                   8</td><td>56a35d41  </td></tr>\n",
       "<tr><td>FSR_Trainable_56bd6189</td><td>2023-07-19_01-01-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 96.0179</td><td style=\"text-align: right;\">7.20755e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">335438</td><td style=\"text-align: right;\">184.687</td><td style=\"text-align: right;\">           314.95   </td><td style=\"text-align: right;\">           3.02554</td><td style=\"text-align: right;\">     314.95   </td><td style=\"text-align: right;\"> 1689696114</td><td style=\"text-align: right;\">                 100</td><td>56bd6189  </td></tr>\n",
       "<tr><td>FSR_Trainable_5b1dc23d</td><td>2023-07-19_00-26-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">107.337 </td><td style=\"text-align: right;\">7.5027e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">323882</td><td style=\"text-align: right;\">211.646</td><td style=\"text-align: right;\">           275.215  </td><td style=\"text-align: right;\">           3.05097</td><td style=\"text-align: right;\">     275.215  </td><td style=\"text-align: right;\"> 1689694008</td><td style=\"text-align: right;\">                 100</td><td>5b1dc23d  </td></tr>\n",
       "<tr><td>FSR_Trainable_5c398036</td><td>2023-07-19_00-13-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">246.519 </td><td style=\"text-align: right;\">2.81647e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">315927</td><td style=\"text-align: right;\">464.611</td><td style=\"text-align: right;\">            10.4188 </td><td style=\"text-align: right;\">          10.4188 </td><td style=\"text-align: right;\">      10.4188 </td><td style=\"text-align: right;\"> 1689693236</td><td style=\"text-align: right;\">                   1</td><td>5c398036  </td></tr>\n",
       "<tr><td>FSR_Trainable_629e0313</td><td>2023-07-19_00-20-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">234.661 </td><td style=\"text-align: right;\">9.89556e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">321591</td><td style=\"text-align: right;\">454.166</td><td style=\"text-align: right;\">            17.9091 </td><td style=\"text-align: right;\">           9.37014</td><td style=\"text-align: right;\">      17.9091 </td><td style=\"text-align: right;\"> 1689693611</td><td style=\"text-align: right;\">                   2</td><td>629e0313  </td></tr>\n",
       "<tr><td>FSR_Trainable_64a77c83</td><td>2023-07-19_00-51-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">136.989 </td><td style=\"text-align: right;\">1.04719e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">333314</td><td style=\"text-align: right;\">258.557</td><td style=\"text-align: right;\">            23.1322 </td><td style=\"text-align: right;\">           5.62264</td><td style=\"text-align: right;\">      23.1322 </td><td style=\"text-align: right;\"> 1689695507</td><td style=\"text-align: right;\">                   4</td><td>64a77c83  </td></tr>\n",
       "<tr><td>FSR_Trainable_64b635d6</td><td>2023-07-19_00-18-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">144.866 </td><td style=\"text-align: right;\">1.7078e+08 </td><td>172.26.215.93</td><td style=\"text-align: right;\">320016</td><td style=\"text-align: right;\">290.128</td><td style=\"text-align: right;\">            51.2478 </td><td style=\"text-align: right;\">           3.97916</td><td style=\"text-align: right;\">      51.2478 </td><td style=\"text-align: right;\"> 1689693486</td><td style=\"text-align: right;\">                  16</td><td>64b635d6  </td></tr>\n",
       "<tr><td>FSR_Trainable_65b325f2</td><td>2023-07-19_00-36-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">106.764 </td><td style=\"text-align: right;\">6.76256e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">325696</td><td style=\"text-align: right;\">214.934</td><td style=\"text-align: right;\">           276.99   </td><td style=\"text-align: right;\">           2.72576</td><td style=\"text-align: right;\">     276.99   </td><td style=\"text-align: right;\"> 1689694599</td><td style=\"text-align: right;\">                 100</td><td>65b325f2  </td></tr>\n",
       "<tr><td>FSR_Trainable_65d350c0</td><td>2023-07-19_00-52-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">172.773 </td><td style=\"text-align: right;\">1.93566e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">333551</td><td style=\"text-align: right;\">318.323</td><td style=\"text-align: right;\">            11.2305 </td><td style=\"text-align: right;\">           5.33866</td><td style=\"text-align: right;\">      11.2305 </td><td style=\"text-align: right;\"> 1689695533</td><td style=\"text-align: right;\">                   2</td><td>65d350c0  </td></tr>\n",
       "<tr><td>FSR_Trainable_65ee2aa9</td><td>2023-07-19_00-59-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 97.9621</td><td style=\"text-align: right;\">7.23046e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">334648</td><td style=\"text-align: right;\">189.09 </td><td style=\"text-align: right;\">           315.883  </td><td style=\"text-align: right;\">           2.91361</td><td style=\"text-align: right;\">     315.883  </td><td style=\"text-align: right;\"> 1689695945</td><td style=\"text-align: right;\">                 100</td><td>65ee2aa9  </td></tr>\n",
       "<tr><td>FSR_Trainable_674faab7</td><td>2023-07-19_00-54-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 88.7602</td><td style=\"text-align: right;\">5.6434e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">332814</td><td style=\"text-align: right;\">182.862</td><td style=\"text-align: right;\">           296.751  </td><td style=\"text-align: right;\">           3.35645</td><td style=\"text-align: right;\">     296.751  </td><td style=\"text-align: right;\"> 1689695672</td><td style=\"text-align: right;\">                 100</td><td>674faab7  </td></tr>\n",
       "<tr><td>FSR_Trainable_681f7398</td><td>2023-07-19_00-16-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">191.16  </td><td style=\"text-align: right;\">6.23321e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">319077</td><td style=\"text-align: right;\">383.33 </td><td style=\"text-align: right;\">             6.95826</td><td style=\"text-align: right;\">           1.73947</td><td style=\"text-align: right;\">       6.95826</td><td style=\"text-align: right;\"> 1689693380</td><td style=\"text-align: right;\">                   4</td><td>681f7398  </td></tr>\n",
       "<tr><td>FSR_Trainable_6b790f67</td><td>2023-07-19_01-03-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 97.6751</td><td style=\"text-align: right;\">8.67992e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">337070</td><td style=\"text-align: right;\">182.518</td><td style=\"text-align: right;\">           114.564  </td><td style=\"text-align: right;\">           1.10819</td><td style=\"text-align: right;\">     114.564  </td><td style=\"text-align: right;\"> 1689696194</td><td style=\"text-align: right;\">                 100</td><td>6b790f67  </td></tr>\n",
       "<tr><td>FSR_Trainable_6eec31ef</td><td>2023-07-19_01-03-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">124.06  </td><td style=\"text-align: right;\">1.00729e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">337613</td><td style=\"text-align: right;\">234.429</td><td style=\"text-align: right;\">            55.3632 </td><td style=\"text-align: right;\">           3.30497</td><td style=\"text-align: right;\">      55.3632 </td><td style=\"text-align: right;\"> 1689696239</td><td style=\"text-align: right;\">                  16</td><td>6eec31ef  </td></tr>\n",
       "<tr><td>FSR_Trainable_739006ac</td><td>2023-07-19_00-42-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">151.989 </td><td style=\"text-align: right;\">1.25046e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">328426</td><td style=\"text-align: right;\">292.659</td><td style=\"text-align: right;\">            18.3967 </td><td style=\"text-align: right;\">           4.25998</td><td style=\"text-align: right;\">      18.3967 </td><td style=\"text-align: right;\"> 1689694953</td><td style=\"text-align: right;\">                   4</td><td>739006ac  </td></tr>\n",
       "<tr><td>FSR_Trainable_7609fbc4</td><td>2023-07-19_00-47-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">266.989 </td><td style=\"text-align: right;\">3.90756e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">331859</td><td style=\"text-align: right;\">450.206</td><td style=\"text-align: right;\">             1.46004</td><td style=\"text-align: right;\">           1.46004</td><td style=\"text-align: right;\">       1.46004</td><td style=\"text-align: right;\"> 1689695260</td><td style=\"text-align: right;\">                   1</td><td>7609fbc4  </td></tr>\n",
       "<tr><td>FSR_Trainable_76355ad3</td><td>2023-07-19_00-16-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">225.857 </td><td style=\"text-align: right;\">5.27505e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">315205</td><td style=\"text-align: right;\">417.883</td><td style=\"text-align: right;\">           151.213  </td><td style=\"text-align: right;\">           1.48482</td><td style=\"text-align: right;\">     151.213  </td><td style=\"text-align: right;\"> 1689693367</td><td style=\"text-align: right;\">                 100</td><td>76355ad3  </td></tr>\n",
       "<tr><td>FSR_Trainable_79862139</td><td>2023-07-19_00-43-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">218.229 </td><td style=\"text-align: right;\">3.78364e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">329118</td><td style=\"text-align: right;\">428.468</td><td style=\"text-align: right;\">            17.6513 </td><td style=\"text-align: right;\">          17.6513 </td><td style=\"text-align: right;\">      17.6513 </td><td style=\"text-align: right;\"> 1689695019</td><td style=\"text-align: right;\">                   1</td><td>79862139  </td></tr>\n",
       "<tr><td>FSR_Trainable_7cfd7c70</td><td>2023-07-19_00-43-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">351.008 </td><td style=\"text-align: right;\">4.62461e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">329354</td><td style=\"text-align: right;\">567.153</td><td style=\"text-align: right;\">             3.09023</td><td style=\"text-align: right;\">           3.09023</td><td style=\"text-align: right;\">       3.09023</td><td style=\"text-align: right;\"> 1689695035</td><td style=\"text-align: right;\">                   1</td><td>7cfd7c70  </td></tr>\n",
       "<tr><td>FSR_Trainable_85a5743c</td><td>2023-07-19_00-14-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">255.407 </td><td style=\"text-align: right;\">6.14994e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">317034</td><td style=\"text-align: right;\">484.883</td><td style=\"text-align: right;\">             3.11551</td><td style=\"text-align: right;\">           3.11551</td><td style=\"text-align: right;\">       3.11551</td><td style=\"text-align: right;\"> 1689693276</td><td style=\"text-align: right;\">                   1</td><td>85a5743c  </td></tr>\n",
       "<tr><td>FSR_Trainable_88386bcc</td><td>2023-07-19_00-14-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">243.936 </td><td style=\"text-align: right;\">2.84995e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">317267</td><td style=\"text-align: right;\">460.191</td><td style=\"text-align: right;\">             6.01007</td><td style=\"text-align: right;\">           2.69488</td><td style=\"text-align: right;\">       6.01007</td><td style=\"text-align: right;\"> 1689693289</td><td style=\"text-align: right;\">                   2</td><td>88386bcc  </td></tr>\n",
       "<tr><td>FSR_Trainable_8ad11431</td><td>2023-07-19_00-13-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">291.359 </td><td style=\"text-align: right;\">4.41569e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">315627</td><td style=\"text-align: right;\">567.482</td><td style=\"text-align: right;\">            10.2492 </td><td style=\"text-align: right;\">          10.2492 </td><td style=\"text-align: right;\">      10.2492 </td><td style=\"text-align: right;\"> 1689693225</td><td style=\"text-align: right;\">                   1</td><td>8ad11431  </td></tr>\n",
       "<tr><td>FSR_Trainable_8e3bd690</td><td>2023-07-19_00-15-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">255.675 </td><td style=\"text-align: right;\">6.87926e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">317712</td><td style=\"text-align: right;\">463.973</td><td style=\"text-align: right;\">             2.89814</td><td style=\"text-align: right;\">           1.09508</td><td style=\"text-align: right;\">       2.89814</td><td style=\"text-align: right;\"> 1689693302</td><td style=\"text-align: right;\">                   2</td><td>8e3bd690  </td></tr>\n",
       "<tr><td>FSR_Trainable_9078fd24</td><td>2023-07-19_00-59-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">168.344 </td><td style=\"text-align: right;\">2.49365e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">335919</td><td style=\"text-align: right;\">338.49 </td><td style=\"text-align: right;\">            11.3954 </td><td style=\"text-align: right;\">           5.36747</td><td style=\"text-align: right;\">      11.3954 </td><td style=\"text-align: right;\"> 1689695983</td><td style=\"text-align: right;\">                   2</td><td>9078fd24  </td></tr>\n",
       "<tr><td>FSR_Trainable_908222cf</td><td>2023-07-19_00-53-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">272.166 </td><td style=\"text-align: right;\">1.41924e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">334242</td><td style=\"text-align: right;\">567.671</td><td style=\"text-align: right;\">             3.54459</td><td style=\"text-align: right;\">           3.54459</td><td style=\"text-align: right;\">       3.54459</td><td style=\"text-align: right;\"> 1689695594</td><td style=\"text-align: right;\">                   1</td><td>908222cf  </td></tr>\n",
       "<tr><td>FSR_Trainable_93a5f695</td><td>2023-07-19_01-00-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">287.987 </td><td style=\"text-align: right;\">3.66342e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">336846</td><td style=\"text-align: right;\">516.143</td><td style=\"text-align: right;\">             1.60008</td><td style=\"text-align: right;\">           1.60008</td><td style=\"text-align: right;\">       1.60008</td><td style=\"text-align: right;\"> 1689696053</td><td style=\"text-align: right;\">                   1</td><td>93a5f695  </td></tr>\n",
       "<tr><td>FSR_Trainable_93ed133c</td><td>2023-07-19_00-53-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">290.327 </td><td style=\"text-align: right;\">1.50935e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">334426</td><td style=\"text-align: right;\">577.442</td><td style=\"text-align: right;\">             6.6586 </td><td style=\"text-align: right;\">           6.6586 </td><td style=\"text-align: right;\">       6.6586 </td><td style=\"text-align: right;\"> 1689695610</td><td style=\"text-align: right;\">                   1</td><td>93ed133c  </td></tr>\n",
       "<tr><td>FSR_Trainable_96e0adc1</td><td>2023-07-19_00-18-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">381.814 </td><td style=\"text-align: right;\">4.95448e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">320503</td><td style=\"text-align: right;\">578.32 </td><td style=\"text-align: right;\">             3.61475</td><td style=\"text-align: right;\">           3.61475</td><td style=\"text-align: right;\">       3.61475</td><td style=\"text-align: right;\"> 1689693524</td><td style=\"text-align: right;\">                   1</td><td>96e0adc1  </td></tr>\n",
       "<tr><td>FSR_Trainable_98d31824</td><td>2023-07-19_00-47-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">216.873 </td><td style=\"text-align: right;\">9.2433e+15 </td><td>172.26.215.93</td><td style=\"text-align: right;\">332086</td><td style=\"text-align: right;\">439.093</td><td style=\"text-align: right;\">             1.36361</td><td style=\"text-align: right;\">           1.36361</td><td style=\"text-align: right;\">       1.36361</td><td style=\"text-align: right;\"> 1689695271</td><td style=\"text-align: right;\">                   1</td><td>98d31824  </td></tr>\n",
       "<tr><td>FSR_Trainable_99e2ed26</td><td>2023-07-19_00-14-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">255.734 </td><td style=\"text-align: right;\">7.20232e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">316818</td><td style=\"text-align: right;\">456.817</td><td style=\"text-align: right;\">             6.06384</td><td style=\"text-align: right;\">           2.20357</td><td style=\"text-align: right;\">       6.06384</td><td style=\"text-align: right;\"> 1689693272</td><td style=\"text-align: right;\">                   2</td><td>99e2ed26  </td></tr>\n",
       "<tr><td>FSR_Trainable_99f0e4a9</td><td>2023-07-19_00-36-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 97.455 </td><td style=\"text-align: right;\">6.23441e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">325464</td><td style=\"text-align: right;\">196.017</td><td style=\"text-align: right;\">           276.04   </td><td style=\"text-align: right;\">           2.94223</td><td style=\"text-align: right;\">     276.04   </td><td style=\"text-align: right;\"> 1689694571</td><td style=\"text-align: right;\">                 100</td><td>99f0e4a9  </td></tr>\n",
       "<tr><td>FSR_Trainable_9e15ec76</td><td>2023-07-19_00-20-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">108.995 </td><td style=\"text-align: right;\">8.86654e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">319514</td><td style=\"text-align: right;\">214.087</td><td style=\"text-align: right;\">           218.292  </td><td style=\"text-align: right;\">           1.67969</td><td style=\"text-align: right;\">     218.292  </td><td style=\"text-align: right;\"> 1689693619</td><td style=\"text-align: right;\">                 100</td><td>9e15ec76  </td></tr>\n",
       "<tr><td>FSR_Trainable_9e451d85</td><td>2023-07-19_00-21-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">143.344 </td><td style=\"text-align: right;\">8.4727e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">322492</td><td style=\"text-align: right;\">291.095</td><td style=\"text-align: right;\">            34.9123 </td><td style=\"text-align: right;\">           3.5336 </td><td style=\"text-align: right;\">      34.9123 </td><td style=\"text-align: right;\"> 1689693677</td><td style=\"text-align: right;\">                   8</td><td>9e451d85  </td></tr>\n",
       "<tr><td>FSR_Trainable_a0320404</td><td>2023-07-19_00-37-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 99.3032</td><td style=\"text-align: right;\">6.70003e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">326116</td><td style=\"text-align: right;\">195.54 </td><td style=\"text-align: right;\">           260.678  </td><td style=\"text-align: right;\">           2.51923</td><td style=\"text-align: right;\">     260.678  </td><td style=\"text-align: right;\"> 1689694623</td><td style=\"text-align: right;\">                 100</td><td>a0320404  </td></tr>\n",
       "<tr><td>FSR_Trainable_a25dc63a</td><td>2023-07-19_00-31-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 99.6831</td><td style=\"text-align: right;\">5.44077e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">324673</td><td style=\"text-align: right;\">199.53 </td><td style=\"text-align: right;\">           274.374  </td><td style=\"text-align: right;\">           2.84008</td><td style=\"text-align: right;\">     274.374  </td><td style=\"text-align: right;\"> 1689694303</td><td style=\"text-align: right;\">                 100</td><td>a25dc63a  </td></tr>\n",
       "<tr><td>FSR_Trainable_a30a0694</td><td>2023-07-19_00-26-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">106.656 </td><td style=\"text-align: right;\">6.64336e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">323608</td><td style=\"text-align: right;\">214.239</td><td style=\"text-align: right;\">           269.768  </td><td style=\"text-align: right;\">           2.74543</td><td style=\"text-align: right;\">     269.768  </td><td style=\"text-align: right;\"> 1689693974</td><td style=\"text-align: right;\">                 100</td><td>a30a0694  </td></tr>\n",
       "<tr><td>FSR_Trainable_a454708c</td><td>2023-07-19_00-52-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">177.01  </td><td style=\"text-align: right;\">2.24211e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">333786</td><td style=\"text-align: right;\">316.88 </td><td style=\"text-align: right;\">             6.90149</td><td style=\"text-align: right;\">           3.15589</td><td style=\"text-align: right;\">       6.90149</td><td style=\"text-align: right;\"> 1689695555</td><td style=\"text-align: right;\">                   2</td><td>a454708c  </td></tr>\n",
       "<tr><td>FSR_Trainable_a74bbb80</td><td>2023-07-19_00-47-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">146.973 </td><td style=\"text-align: right;\">1.44436e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">331394</td><td style=\"text-align: right;\">276.21 </td><td style=\"text-align: right;\">            18.8193 </td><td style=\"text-align: right;\">           5.00404</td><td style=\"text-align: right;\">      18.8193 </td><td style=\"text-align: right;\"> 1689695255</td><td style=\"text-align: right;\">                   4</td><td>a74bbb80  </td></tr>\n",
       "<tr><td>FSR_Trainable_aa30a5a6</td><td>2023-07-19_00-20-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">260.515 </td><td style=\"text-align: right;\">4.00187e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">322709</td><td style=\"text-align: right;\">453.158</td><td style=\"text-align: right;\">             2.36285</td><td style=\"text-align: right;\">           2.36285</td><td style=\"text-align: right;\">       2.36285</td><td style=\"text-align: right;\"> 1689693652</td><td style=\"text-align: right;\">                   1</td><td>aa30a5a6  </td></tr>\n",
       "<tr><td>FSR_Trainable_ab5b2886</td><td>2023-07-19_01-04-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 92.472 </td><td style=\"text-align: right;\">6.73523e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">335739</td><td style=\"text-align: right;\">178.705</td><td style=\"text-align: right;\">           287.446  </td><td style=\"text-align: right;\">           2.4667 </td><td style=\"text-align: right;\">     287.446  </td><td style=\"text-align: right;\"> 1689696262</td><td style=\"text-align: right;\">                 100</td><td>ab5b2886  </td></tr>\n",
       "<tr><td>FSR_Trainable_ac90a52e</td><td>2023-07-19_00-27-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">139.8   </td><td style=\"text-align: right;\">2.74343e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">324220</td><td style=\"text-align: right;\">273.129</td><td style=\"text-align: right;\">            60.0716 </td><td style=\"text-align: right;\">           3.8889 </td><td style=\"text-align: right;\">      60.0716 </td><td style=\"text-align: right;\"> 1689694046</td><td style=\"text-align: right;\">                  16</td><td>ac90a52e  </td></tr>\n",
       "<tr><td>FSR_Trainable_af202444</td><td>2023-07-19_00-44-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">266.319 </td><td style=\"text-align: right;\">3.01399e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">329788</td><td style=\"text-align: right;\">486.621</td><td style=\"text-align: right;\">             3.38726</td><td style=\"text-align: right;\">           3.38726</td><td style=\"text-align: right;\">       3.38726</td><td style=\"text-align: right;\"> 1689695068</td><td style=\"text-align: right;\">                   1</td><td>af202444  </td></tr>\n",
       "<tr><td>FSR_Trainable_b0e7957c</td><td>2023-07-19_00-19-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">254.628 </td><td style=\"text-align: right;\">8.45837e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">321371</td><td style=\"text-align: right;\">439.388</td><td style=\"text-align: right;\">            10.4823 </td><td style=\"text-align: right;\">           4.90911</td><td style=\"text-align: right;\">      10.4823 </td><td style=\"text-align: right;\"> 1689693594</td><td style=\"text-align: right;\">                   2</td><td>b0e7957c  </td></tr>\n",
       "<tr><td>FSR_Trainable_b590b28a</td><td>2023-07-19_00-41-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">573.236 </td><td style=\"text-align: right;\">8.4022e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">327737</td><td style=\"text-align: right;\">802.913</td><td style=\"text-align: right;\">             1.27049</td><td style=\"text-align: right;\">           1.27049</td><td style=\"text-align: right;\">       1.27049</td><td style=\"text-align: right;\"> 1689694879</td><td style=\"text-align: right;\">                   1</td><td>b590b28a  </td></tr>\n",
       "<tr><td>FSR_Trainable_b7b936d0</td><td>2023-07-19_00-53-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">135.746 </td><td style=\"text-align: right;\">1.0569e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">334013</td><td style=\"text-align: right;\">257.272</td><td style=\"text-align: right;\">            15.0386 </td><td style=\"text-align: right;\">           3.30178</td><td style=\"text-align: right;\">      15.0386 </td><td style=\"text-align: right;\"> 1689695586</td><td style=\"text-align: right;\">                   4</td><td>b7b936d0  </td></tr>\n",
       "<tr><td>FSR_Trainable_bfef460c</td><td>2023-07-19_00-13-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">218.658 </td><td style=\"text-align: right;\">4.55232e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">315277</td><td style=\"text-align: right;\">448.246</td><td style=\"text-align: right;\">             2.65694</td><td style=\"text-align: right;\">           1.20922</td><td style=\"text-align: right;\">       2.65694</td><td style=\"text-align: right;\"> 1689693204</td><td style=\"text-align: right;\">                   2</td><td>bfef460c  </td></tr>\n",
       "<tr><td>FSR_Trainable_c370576b</td><td>2023-07-19_00-40-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 94.5365</td><td style=\"text-align: right;\">7.04451e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">326472</td><td style=\"text-align: right;\">183.889</td><td style=\"text-align: right;\">           258.362  </td><td style=\"text-align: right;\">           2.76607</td><td style=\"text-align: right;\">     258.362  </td><td style=\"text-align: right;\"> 1689694804</td><td style=\"text-align: right;\">                 100</td><td>c370576b  </td></tr>\n",
       "<tr><td>FSR_Trainable_c48c7ff2</td><td>2023-07-19_00-56-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 89.624 </td><td style=\"text-align: right;\">6.12627e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">333075</td><td style=\"text-align: right;\">180.734</td><td style=\"text-align: right;\">           312.067  </td><td style=\"text-align: right;\">           3.15327</td><td style=\"text-align: right;\">     312.067  </td><td style=\"text-align: right;\"> 1689695770</td><td style=\"text-align: right;\">                 100</td><td>c48c7ff2  </td></tr>\n",
       "<tr><td>FSR_Trainable_c68eded0</td><td>2023-07-19_01-04-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">483.977 </td><td style=\"text-align: right;\">6.37418e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">338286</td><td style=\"text-align: right;\">739.795</td><td style=\"text-align: right;\">             1.67645</td><td style=\"text-align: right;\">           1.67645</td><td style=\"text-align: right;\">       1.67645</td><td style=\"text-align: right;\"> 1689696265</td><td style=\"text-align: right;\">                   1</td><td>c68eded0  </td></tr>\n",
       "<tr><td>FSR_Trainable_c767eb4c</td><td>2023-07-19_00-15-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">184.904 </td><td style=\"text-align: right;\">5.78297e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">318602</td><td style=\"text-align: right;\">367.292</td><td style=\"text-align: right;\">            10.1017 </td><td style=\"text-align: right;\">           2.57575</td><td style=\"text-align: right;\">      10.1017 </td><td style=\"text-align: right;\"> 1689693347</td><td style=\"text-align: right;\">                   4</td><td>c767eb4c  </td></tr>\n",
       "<tr><td>FSR_Trainable_caeff74f</td><td>2023-07-19_00-19-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">180.972 </td><td style=\"text-align: right;\">4.66414e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">320706</td><td style=\"text-align: right;\">365.548</td><td style=\"text-align: right;\">            12.9829 </td><td style=\"text-align: right;\">           2.93757</td><td style=\"text-align: right;\">      12.9829 </td><td style=\"text-align: right;\"> 1689693550</td><td style=\"text-align: right;\">                   4</td><td>caeff74f  </td></tr>\n",
       "<tr><td>FSR_Trainable_cb5882fa</td><td>2023-07-19_00-17-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">149.844 </td><td style=\"text-align: right;\">2.47721e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">319732</td><td style=\"text-align: right;\">290.017</td><td style=\"text-align: right;\">            23.4704 </td><td style=\"text-align: right;\">           2.77333</td><td style=\"text-align: right;\">      23.4704 </td><td style=\"text-align: right;\"> 1689693423</td><td style=\"text-align: right;\">                   8</td><td>cb5882fa  </td></tr>\n",
       "<tr><td>FSR_Trainable_cbeeb68c</td><td>2023-07-19_00-51-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">102.574 </td><td style=\"text-align: right;\">6.42117e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">332517</td><td style=\"text-align: right;\">212.189</td><td style=\"text-align: right;\">           171.938  </td><td style=\"text-align: right;\">           2.75754</td><td style=\"text-align: right;\">     171.938  </td><td style=\"text-align: right;\"> 1689695469</td><td style=\"text-align: right;\">                  64</td><td>cbeeb68c  </td></tr>\n",
       "<tr><td>FSR_Trainable_cc8c9340</td><td>2023-07-19_00-59-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 93.703 </td><td style=\"text-align: right;\">6.51511e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">334878</td><td style=\"text-align: right;\">185.95 </td><td style=\"text-align: right;\">           315.484  </td><td style=\"text-align: right;\">           2.96222</td><td style=\"text-align: right;\">     315.484  </td><td style=\"text-align: right;\"> 1689695954</td><td style=\"text-align: right;\">                 100</td><td>cc8c9340  </td></tr>\n",
       "<tr><td>FSR_Trainable_cce72218</td><td>2023-07-19_00-19-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">265.402 </td><td style=\"text-align: right;\">9.318e+08  </td><td>172.26.215.93</td><td style=\"text-align: right;\">320958</td><td style=\"text-align: right;\">454.444</td><td style=\"text-align: right;\">             3.84218</td><td style=\"text-align: right;\">           3.84218</td><td style=\"text-align: right;\">       3.84218</td><td style=\"text-align: right;\"> 1689693563</td><td style=\"text-align: right;\">                   1</td><td>cce72218  </td></tr>\n",
       "<tr><td>FSR_Trainable_cf38a0e6</td><td>2023-07-19_00-14-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">258.69  </td><td style=\"text-align: right;\">8.56003e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">316596</td><td style=\"text-align: right;\">454.933</td><td style=\"text-align: right;\">             9.68815</td><td style=\"text-align: right;\">           4.68112</td><td style=\"text-align: right;\">       9.68815</td><td style=\"text-align: right;\"> 1689693265</td><td style=\"text-align: right;\">                   2</td><td>cf38a0e6  </td></tr>\n",
       "<tr><td>FSR_Trainable_d001dc20</td><td>2023-07-19_00-49-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 99.7562</td><td style=\"text-align: right;\">7.57221e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">330232</td><td style=\"text-align: right;\">193.529</td><td style=\"text-align: right;\">           241.099  </td><td style=\"text-align: right;\">           2.38223</td><td style=\"text-align: right;\">     241.099  </td><td style=\"text-align: right;\"> 1689695349</td><td style=\"text-align: right;\">                 100</td><td>d001dc20  </td></tr>\n",
       "<tr><td>FSR_Trainable_d72330cb</td><td>2023-07-19_00-41-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 91.0368</td><td style=\"text-align: right;\">6.68699e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">326942</td><td style=\"text-align: right;\">181.766</td><td style=\"text-align: right;\">           279.064  </td><td style=\"text-align: right;\">           3.05201</td><td style=\"text-align: right;\">     279.064  </td><td style=\"text-align: right;\"> 1689694896</td><td style=\"text-align: right;\">                 100</td><td>d72330cb  </td></tr>\n",
       "<tr><td>FSR_Trainable_d814fa87</td><td>2023-07-19_00-43-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">177.011 </td><td style=\"text-align: right;\">2.45535e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">328885</td><td style=\"text-align: right;\">355.905</td><td style=\"text-align: right;\">             6.17918</td><td style=\"text-align: right;\">           2.77159</td><td style=\"text-align: right;\">       6.17918</td><td style=\"text-align: right;\"> 1689694991</td><td style=\"text-align: right;\">                   2</td><td>d814fa87  </td></tr>\n",
       "<tr><td>FSR_Trainable_d8e458e1</td><td>2023-07-19_00-42-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">219.132 </td><td style=\"text-align: right;\">2.29013e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">328662</td><td style=\"text-align: right;\">425.487</td><td style=\"text-align: right;\">             5.30339</td><td style=\"text-align: right;\">           5.30339</td><td style=\"text-align: right;\">       5.30339</td><td style=\"text-align: right;\"> 1689694970</td><td style=\"text-align: right;\">                   1</td><td>d8e458e1  </td></tr>\n",
       "<tr><td>FSR_Trainable_d8fbd1ca</td><td>2023-07-19_00-19-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">177.568 </td><td style=\"text-align: right;\">3.43176e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">318385</td><td style=\"text-align: right;\">365.856</td><td style=\"text-align: right;\">           225.578  </td><td style=\"text-align: right;\">           2.23398</td><td style=\"text-align: right;\">     225.578  </td><td style=\"text-align: right;\"> 1689693563</td><td style=\"text-align: right;\">                 100</td><td>d8fbd1ca  </td></tr>\n",
       "<tr><td>FSR_Trainable_dd20017e</td><td>2023-07-19_00-21-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">144.933 </td><td style=\"text-align: right;\">8.70726e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">322278</td><td style=\"text-align: right;\">294.487</td><td style=\"text-align: right;\">            36.2022 </td><td style=\"text-align: right;\">           4.6497 </td><td style=\"text-align: right;\">      36.2022 </td><td style=\"text-align: right;\"> 1689693669</td><td style=\"text-align: right;\">                   8</td><td>dd20017e  </td></tr>\n",
       "<tr><td>FSR_Trainable_dea50f73</td><td>2023-07-19_01-00-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 95.9287</td><td style=\"text-align: right;\">6.88141e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">335173</td><td style=\"text-align: right;\">189.958</td><td style=\"text-align: right;\">           319.16   </td><td style=\"text-align: right;\">           3.13956</td><td style=\"text-align: right;\">     319.16   </td><td style=\"text-align: right;\"> 1689696019</td><td style=\"text-align: right;\">                 100</td><td>dea50f73  </td></tr>\n",
       "<tr><td>FSR_Trainable_dfabbb69</td><td>2023-07-19_00-47-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">135.706 </td><td style=\"text-align: right;\">1.08853e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">330947</td><td style=\"text-align: right;\">255.124</td><td style=\"text-align: right;\">             6.27456</td><td style=\"text-align: right;\">           1.39445</td><td style=\"text-align: right;\">       6.27456</td><td style=\"text-align: right;\"> 1689695221</td><td style=\"text-align: right;\">                   4</td><td>dfabbb69  </td></tr>\n",
       "<tr><td>FSR_Trainable_e0e7b4f2</td><td>2023-07-19_00-26-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">101.95  </td><td style=\"text-align: right;\">5.56643e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">323394</td><td style=\"text-align: right;\">210.39 </td><td style=\"text-align: right;\">           275.934  </td><td style=\"text-align: right;\">           2.76678</td><td style=\"text-align: right;\">     275.934  </td><td style=\"text-align: right;\"> 1689693970</td><td style=\"text-align: right;\">                 100</td><td>e0e7b4f2  </td></tr>\n",
       "<tr><td>FSR_Trainable_e2ad4b4f</td><td>2023-07-19_00-31-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">102.348 </td><td style=\"text-align: right;\">5.83899e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">324400</td><td style=\"text-align: right;\">202.461</td><td style=\"text-align: right;\">           276.397  </td><td style=\"text-align: right;\">           2.795  </td><td style=\"text-align: right;\">     276.397  </td><td style=\"text-align: right;\"> 1689694276</td><td style=\"text-align: right;\">                 100</td><td>e2ad4b4f  </td></tr>\n",
       "<tr><td>FSR_Trainable_e2e2c0c8</td><td>2023-07-19_00-16-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">194.585 </td><td style=\"text-align: right;\">6.40034e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">318878</td><td style=\"text-align: right;\">386.839</td><td style=\"text-align: right;\">             8.77879</td><td style=\"text-align: right;\">           1.74738</td><td style=\"text-align: right;\">       8.77879</td><td style=\"text-align: right;\"> 1689693369</td><td style=\"text-align: right;\">                   4</td><td>e2e2c0c8  </td></tr>\n",
       "<tr><td>FSR_Trainable_e689db0e</td><td>2023-07-19_00-46-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 93.7732</td><td style=\"text-align: right;\">6.77101e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">328172</td><td style=\"text-align: right;\">186.113</td><td style=\"text-align: right;\">           270.483  </td><td style=\"text-align: right;\">           2.63272</td><td style=\"text-align: right;\">     270.483  </td><td style=\"text-align: right;\"> 1689695194</td><td style=\"text-align: right;\">                 100</td><td>e689db0e  </td></tr>\n",
       "<tr><td>FSR_Trainable_ea8d39b7</td><td>2023-07-19_00-52-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 93.7526</td><td style=\"text-align: right;\">6.39661e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">332309</td><td style=\"text-align: right;\">186.369</td><td style=\"text-align: right;\">           283.315  </td><td style=\"text-align: right;\">           3.11851</td><td style=\"text-align: right;\">     283.315  </td><td style=\"text-align: right;\"> 1689695578</td><td style=\"text-align: right;\">                 100</td><td>ea8d39b7  </td></tr>\n",
       "<tr><td>FSR_Trainable_ec812aff</td><td>2023-07-19_00-42-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 94.5672</td><td style=\"text-align: right;\">6.79898e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">327171</td><td style=\"text-align: right;\">185.617</td><td style=\"text-align: right;\">           275.03   </td><td style=\"text-align: right;\">           2.69298</td><td style=\"text-align: right;\">     275.03   </td><td style=\"text-align: right;\"> 1689694922</td><td style=\"text-align: right;\">                 100</td><td>ec812aff  </td></tr>\n",
       "<tr><td>FSR_Trainable_ecda1e39</td><td>2023-07-19_00-45-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 98.2454</td><td style=\"text-align: right;\">7.66794e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">327930</td><td style=\"text-align: right;\">184.304</td><td style=\"text-align: right;\">           247.01   </td><td style=\"text-align: right;\">           2.38616</td><td style=\"text-align: right;\">     247.01   </td><td style=\"text-align: right;\"> 1689695156</td><td style=\"text-align: right;\">                 100</td><td>ecda1e39  </td></tr>\n",
       "<tr><td>FSR_Trainable_ee736811</td><td>2023-07-19_00-41-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 94.3328</td><td style=\"text-align: right;\">6.42432e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">326712</td><td style=\"text-align: right;\">187.756</td><td style=\"text-align: right;\">           276.961  </td><td style=\"text-align: right;\">           2.57748</td><td style=\"text-align: right;\">     276.961  </td><td style=\"text-align: right;\"> 1689694867</td><td style=\"text-align: right;\">                 100</td><td>ee736811  </td></tr>\n",
       "<tr><td>FSR_Trainable_f0123c53</td><td>2023-07-19_00-32-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">112.765 </td><td style=\"text-align: right;\">8.06259e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">324908</td><td style=\"text-align: right;\">219.142</td><td style=\"text-align: right;\">           272.281  </td><td style=\"text-align: right;\">           2.71192</td><td style=\"text-align: right;\">     272.281  </td><td style=\"text-align: right;\"> 1689694335</td><td style=\"text-align: right;\">                 100</td><td>f0123c53  </td></tr>\n",
       "<tr><td>FSR_Trainable_f6c987f1</td><td>2023-07-19_01-04-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">418.489 </td><td style=\"text-align: right;\">7.73441e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">338094</td><td style=\"text-align: right;\">563.502</td><td style=\"text-align: right;\">             1.54014</td><td style=\"text-align: right;\">           1.54014</td><td style=\"text-align: right;\">       1.54014</td><td style=\"text-align: right;\"> 1689696251</td><td style=\"text-align: right;\">                   1</td><td>f6c987f1  </td></tr>\n",
       "<tr><td>FSR_Trainable_fa9d0228</td><td>2023-07-19_00-15-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">257.1   </td><td style=\"text-align: right;\">9.47609e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">317953</td><td style=\"text-align: right;\">473.399</td><td style=\"text-align: right;\">             3.8477 </td><td style=\"text-align: right;\">           3.8477 </td><td style=\"text-align: right;\">       3.8477 </td><td style=\"text-align: right;\"> 1689693311</td><td style=\"text-align: right;\">                   1</td><td>fa9d0228  </td></tr>\n",
       "<tr><td>FSR_Trainable_fb124383</td><td>2023-07-19_00-32-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">118.18  </td><td style=\"text-align: right;\">1.07129e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">325176</td><td style=\"text-align: right;\">230.909</td><td style=\"text-align: right;\">           176.883  </td><td style=\"text-align: right;\">           2.79405</td><td style=\"text-align: right;\">     176.883  </td><td style=\"text-align: right;\"> 1689694338</td><td style=\"text-align: right;\">                  64</td><td>fb124383  </td></tr>\n",
       "<tr><td>FSR_Trainable_fbddafdd</td><td>2023-07-19_00-14-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">237.478 </td><td style=\"text-align: right;\">3.03145e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">317484</td><td style=\"text-align: right;\">465.527</td><td style=\"text-align: right;\">             1.61098</td><td style=\"text-align: right;\">           1.61098</td><td style=\"text-align: right;\">       1.61098</td><td style=\"text-align: right;\"> 1689693291</td><td style=\"text-align: right;\">                   1</td><td>fbddafdd  </td></tr>\n",
       "<tr><td>FSR_Trainable_fe8e305d</td><td>2023-07-19_01-00-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">167.752 </td><td style=\"text-align: right;\">2.77103e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">336197</td><td style=\"text-align: right;\">328.729</td><td style=\"text-align: right;\">            12.7321 </td><td style=\"text-align: right;\">           5.10239</td><td style=\"text-align: right;\">      12.7321 </td><td style=\"text-align: right;\"> 1689696014</td><td style=\"text-align: right;\">                   2</td><td>fe8e305d  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_76355ad3_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-19_00-13-08/wandb/run-20230719_001318-76355ad3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Syncing run FSR_Trainable_76355ad3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/76355ad3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb:                      mae 218.65795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb:                     mape 4.552316375776423e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb:                     rmse 448.2464\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb:       time_since_restore 2.65694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb:         time_this_iter_s 1.20922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb:             time_total_s 2.65694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb:                timestamp 1689693204\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: 🚀 View run FSR_Trainable_bfef460c at: https://wandb.ai/seokjin/FSR-prediction/runs/bfef460c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315431)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001326-bfef460c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_2a1b74b4_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-19_00-13-20/wandb/run-20230719_001334-2a1b74b4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: Syncing run FSR_Trainable_2a1b74b4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2a1b74b4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb:                      mae 274.43448\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb:                     mape 1020371134.84996\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb:                     rmse 461.78168\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb:       time_since_restore 3.83932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb:         time_this_iter_s 3.83932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb:             time_total_s 3.83932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb:                timestamp 1689693211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: 🚀 View run FSR_Trainable_2a1b74b4 at: https://wandb.ai/seokjin/FSR-prediction/runs/2a1b74b4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315626)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001334-2a1b74b4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_8ad11431_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-19_00-13-27/wandb/run-20230719_001342-8ad11431\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: Syncing run FSR_Trainable_8ad11431\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8ad11431\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb:                      mae 291.35921\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb:                     mape 4.415692965076147e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb:                     rmse 567.48184\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb:       time_since_restore 10.24918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb:         time_this_iter_s 10.24918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb:             time_total_s 10.24918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb:                timestamp 1689693225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: 🚀 View run FSR_Trainable_8ad11431 at: https://wandb.ai/seokjin/FSR-prediction/runs/8ad11431\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315810)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001342-8ad11431/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_5c398036_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-19_00-13-35/wandb/run-20230719_001353-5c398036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: Syncing run FSR_Trainable_5c398036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5c398036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:14:01,377\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.707 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:14:01,381\tWARNING util.py:315 -- The `process_trial_result` operation took 1.713 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:14:01,383\tWARNING util.py:315 -- Processing trial results took 1.715 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:14:01,385\tWARNING util.py:315 -- The `process_trial_result` operation took 1.717 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb:                      mae 246.51915\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb:                     mape 2.8164686109736048e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb:                     rmse 464.61125\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb:       time_since_restore 10.41878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb:         time_this_iter_s 10.41878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb:             time_total_s 10.41878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb:                timestamp 1689693236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: 🚀 View run FSR_Trainable_5c398036 at: https://wandb.ai/seokjin/FSR-prediction/runs/5c398036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316019)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001353-5c398036/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_02909daa_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-19_00-13-45/wandb/run-20230719_001403-02909daa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: Syncing run FSR_Trainable_02909daa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/02909daa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb:                      mae 250.71379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb:                     mape 7369679965743370.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb:                     rmse 476.31279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb:       time_since_restore 2.07693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb:         time_this_iter_s 2.07693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb:             time_total_s 2.07693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb:                timestamp 1689693239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: 🚀 View run FSR_Trainable_02909daa at: https://wandb.ai/seokjin/FSR-prediction/runs/02909daa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316258)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001403-02909daa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 00:14:11,709\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.641 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:14:11,714\tWARNING util.py:315 -- The `process_trial_result` operation took 1.647 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:14:11,717\tWARNING util.py:315 -- Processing trial results took 1.650 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:14:11,719\tWARNING util.py:315 -- The `process_trial_result` operation took 1.652 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_0d54ed40_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-19_00-13-57/wandb/run-20230719_001411-0d54ed40\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: Syncing run FSR_Trainable_0d54ed40\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0d54ed40\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb:                      mae 241.022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb:                     mape 4.978304091789342e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb:                     rmse 476.6869\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb:       time_since_restore 5.01879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb:         time_this_iter_s 5.01879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb:             time_total_s 5.01879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb:                timestamp 1689693250\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: 🚀 View run FSR_Trainable_0d54ed40 at: https://wandb.ai/seokjin/FSR-prediction/runs/0d54ed40\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316479)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001411-0d54ed40/logs\n",
      "2023-07-19 00:14:21,025\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.862 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:14:21,030\tWARNING util.py:315 -- The `process_trial_result` operation took 1.867 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:14:21,031\tWARNING util.py:315 -- Processing trial results took 1.869 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:14:21,034\tWARNING util.py:315 -- The `process_trial_result` operation took 1.871 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_cf38a0e6_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-19_00-14-05/wandb/run-20230719_001421-cf38a0e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: Syncing run FSR_Trainable_cf38a0e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cf38a0e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb:                      mae 258.69042\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb:                     mape 856002775.79139\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb:                     rmse 454.93343\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb:       time_since_restore 9.68815\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb:         time_this_iter_s 4.68112\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb:             time_total_s 9.68815\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb:                timestamp 1689693265\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: 🚀 View run FSR_Trainable_cf38a0e6 at: https://wandb.ai/seokjin/FSR-prediction/runs/cf38a0e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316702)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001421-cf38a0e6/logs\n",
      "2023-07-19 00:14:30,152\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.237 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:14:30,156\tWARNING util.py:315 -- The `process_trial_result` operation took 2.241 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:14:30,157\tWARNING util.py:315 -- Processing trial results took 2.243 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:14:30,158\tWARNING util.py:315 -- The `process_trial_result` operation took 2.244 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_99e2ed26_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-19_00-14-14/wandb/run-20230719_001431-99e2ed26\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: Syncing run FSR_Trainable_99e2ed26\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/99e2ed26\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb:                      mae 255.73384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb:                     mape 720232412.09257\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb:                     rmse 456.81678\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb:       time_since_restore 6.06384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb:         time_this_iter_s 2.20357\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb:             time_total_s 6.06384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb:                timestamp 1689693272\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: 🚀 View run FSR_Trainable_99e2ed26 at: https://wandb.ai/seokjin/FSR-prediction/runs/99e2ed26\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=316922)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001431-99e2ed26/logs\n",
      "2023-07-19 00:14:38,376\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.652 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:14:38,380\tWARNING util.py:315 -- The `process_trial_result` operation took 1.657 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:14:38,385\tWARNING util.py:315 -- Processing trial results took 1.661 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:14:38,386\tWARNING util.py:315 -- The `process_trial_result` operation took 1.663 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_85a5743c_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-14-24/wandb/run-20230719_001439-85a5743c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: Syncing run FSR_Trainable_85a5743c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/85a5743c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: - 0.000 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb:                      mae 255.40658\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb:                     mape 6149943184119527.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb:                     rmse 484.88278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb:       time_since_restore 3.11551\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb:         time_this_iter_s 3.11551\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb:             time_total_s 3.11551\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb:                timestamp 1689693276\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: 🚀 View run FSR_Trainable_85a5743c at: https://wandb.ai/seokjin/FSR-prediction/runs/85a5743c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317150)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001439-85a5743c/logs\n",
      "2023-07-19 00:14:46,686\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.949 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:14:46,690\tWARNING util.py:315 -- The `process_trial_result` operation took 1.954 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:14:46,691\tWARNING util.py:315 -- Processing trial results took 1.955 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:14:46,693\tWARNING util.py:315 -- The `process_trial_result` operation took 1.957 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_88386bcc_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-14-33/wandb/run-20230719_001447-88386bcc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: Syncing run FSR_Trainable_88386bcc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/88386bcc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:14:53,175\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.700 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:14:53,180\tWARNING util.py:315 -- The `process_trial_result` operation took 1.705 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:14:53,183\tWARNING util.py:315 -- Processing trial results took 1.708 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:14:53,186\tWARNING util.py:315 -- The `process_trial_result` operation took 1.711 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb:                      mae 243.93648\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb:                     mape 2.8499519333581968e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb:                     rmse 460.19104\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb:       time_since_restore 6.01007\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb:         time_this_iter_s 2.69488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb:             time_total_s 6.01007\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb:                timestamp 1689693289\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: 🚀 View run FSR_Trainable_88386bcc at: https://wandb.ai/seokjin/FSR-prediction/runs/88386bcc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317376)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001447-88386bcc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_fbddafdd_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-14-41/wandb/run-20230719_001455-fbddafdd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: Syncing run FSR_Trainable_fbddafdd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fbddafdd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:15:01,795\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.228 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:15:01,798\tWARNING util.py:315 -- The `process_trial_result` operation took 2.231 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:15:01,800\tWARNING util.py:315 -- Processing trial results took 2.234 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:15:01,801\tWARNING util.py:315 -- The `process_trial_result` operation took 2.235 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb:                      mae 237.47837\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb:                     mape 3031448307417983.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb:                     rmse 465.52744\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb:       time_since_restore 1.61098\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb:         time_this_iter_s 1.61098\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb:             time_total_s 1.61098\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb:                timestamp 1689693291\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: 🚀 View run FSR_Trainable_fbddafdd at: https://wandb.ai/seokjin/FSR-prediction/runs/fbddafdd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317597)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001455-fbddafdd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_8e3bd690_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-14-49/wandb/run-20230719_001504-8e3bd690\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: Syncing run FSR_Trainable_8e3bd690\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8e3bd690\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb:                      mae 255.67494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb:                     mape 6.8792576409837416e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb:                     rmse 463.97325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb:       time_since_restore 2.89814\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb:         time_this_iter_s 1.09508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb:             time_total_s 2.89814\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb:                timestamp 1689693302\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: 🚀 View run FSR_Trainable_8e3bd690 at: https://wandb.ai/seokjin/FSR-prediction/runs/8e3bd690\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=317819)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001504-8e3bd690/logs\n",
      "2023-07-19 00:15:13,414\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.641 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:15:13,420\tWARNING util.py:315 -- The `process_trial_result` operation took 1.648 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:15:13,424\tWARNING util.py:315 -- Processing trial results took 1.652 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:15:13,426\tWARNING util.py:315 -- The `process_trial_result` operation took 1.654 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_fa9d0228_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-14-57/wandb/run-20230719_001513-fa9d0228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: Syncing run FSR_Trainable_fa9d0228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fa9d0228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb:                      mae 257.09962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb:                     mape 947608952.32826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb:                     rmse 473.3988\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb:       time_since_restore 3.8477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb:         time_this_iter_s 3.8477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb:             time_total_s 3.8477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb:                timestamp 1689693311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: 🚀 View run FSR_Trainable_fa9d0228 at: https://wandb.ai/seokjin/FSR-prediction/runs/fa9d0228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318045)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001513-fa9d0228/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 00:15:23,980\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.461 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:15:23,981\tWARNING util.py:315 -- The `process_trial_result` operation took 1.464 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:15:23,983\tWARNING util.py:315 -- Processing trial results took 1.465 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:15:23,985\tWARNING util.py:315 -- The `process_trial_result` operation took 1.467 s, which may be a performance bottleneck.\n",
      "wandb: \\ Waiting for wandb.init()...268)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_4ca3bae3_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-15-07/wandb/run-20230719_001523-4ca3bae3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: Syncing run FSR_Trainable_4ca3bae3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4ca3bae3\n",
      "2023-07-19 00:15:30,025\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.860 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:15:30,029\tWARNING util.py:315 -- The `process_trial_result` operation took 1.865 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:15:30,031\tWARNING util.py:315 -- Processing trial results took 1.866 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:15:30,032\tWARNING util.py:315 -- The `process_trial_result` operation took 1.867 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_d8fbd1ca_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-15-16/wandb/run-20230719_001531-d8fbd1ca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: Syncing run FSR_Trainable_d8fbd1ca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d8fbd1ca\n",
      "2023-07-19 00:15:40,059\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.231 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:15:40,063\tWARNING util.py:315 -- The `process_trial_result` operation took 2.236 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:15:40,065\tWARNING util.py:315 -- Processing trial results took 2.238 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:15:40,066\tWARNING util.py:315 -- The `process_trial_result` operation took 2.239 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_c767eb4c_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-15-25/wandb/run-20230719_001542-c767eb4c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: Syncing run FSR_Trainable_c767eb4c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c767eb4c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb:                      mae ▄█▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb:                     mape ▂█▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb:                     rmse ▃█▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb:         time_this_iter_s █▄▁▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb:                      mae 184.90381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb:                     mape 578297023.34982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb:                     rmse 367.29231\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb:       time_since_restore 10.10169\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb:         time_this_iter_s 2.57575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb:             time_total_s 10.10169\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb:                timestamp 1689693347\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: 🚀 View run FSR_Trainable_c767eb4c at: https://wandb.ai/seokjin/FSR-prediction/runs/c767eb4c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318698)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001542-c767eb4c/logs\n",
      "2023-07-19 00:16:02,859\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.084 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:16:02,862\tWARNING util.py:315 -- The `process_trial_result` operation took 2.087 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:16:02,866\tWARNING util.py:315 -- Processing trial results took 2.091 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:16:02,867\tWARNING util.py:315 -- The `process_trial_result` operation took 2.092 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_e2e2c0c8_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-15-35/wandb/run-20230719_001604-e2e2c0c8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: Syncing run FSR_Trainable_e2e2c0c8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e2e2c0c8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb:                      mae ▂▄▂▁▁▂█▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb:                     mape ▄▄▆▁▃▃█▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb:                     rmse ▃▆▄▁▂▃█▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb:       time_since_restore ▁▂▃▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb:         time_this_iter_s ▅█▅▅▃▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb:             time_total_s ▁▂▃▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb:                timestamp ▁▃▄▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb:                      mae 255.18987\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb:                     mape 731717840.39365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb:                     rmse 450.78202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb:       time_since_restore 44.97165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb:         time_this_iter_s 4.85477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb:             time_total_s 44.97165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb:                timestamp 1689693362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: 🚀 View run FSR_Trainable_4ca3bae3 at: https://wandb.ai/seokjin/FSR-prediction/runs/4ca3bae3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318268)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001523-4ca3bae3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: Waiting for W&B process to finish... (success).\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb:                      mae ▂▂▁▃▄▃▂▁▁▂▂▁▁▂▁▃▇▄▂▂▆▅▅▇▇▂▅▇▆▇▇█▆▇▅▅▃▂▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb:                     mape ▂▁▂▃▂▄▃▃▃▃▂▂▃▁▂▄▅▃▄▃▆▆▆▇▇▂▃█▆▇▇▇▆█▅▇▃▂▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb:                     rmse ▇▅▃▄▆▂▃▂▂▂▄▃▃▄▂▄▇▅▂▁▅▄▃▆▆▃█▆▆███▅▅▅▄▃▁▆▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb:         time_this_iter_s ▇▃▄▂▅▄▇▄█▅▄▁▄▃▆▅▇▃▆▁▅▄▃▃▃▂▂▄▃▇▃▅▄▆█▄▅▄▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001318-76355ad3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001318-76355ad3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001318-76355ad3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001318-76355ad3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001318-76355ad3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001318-76355ad3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001318-76355ad3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001318-76355ad3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001318-76355ad3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001318-76355ad3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001318-76355ad3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001318-76355ad3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001318-76355ad3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=315276)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001318-76355ad3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb:                      mae ▃█▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb:                     mape ▃█▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb:                     rmse ▃█▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb:         time_this_iter_s █▄▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb:                      mae 194.58507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb:                     mape 640034471.60902\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb:                     rmse 386.83912\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb:       time_since_restore 8.77879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb:         time_this_iter_s 1.74738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb:             time_total_s 8.77879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb:                timestamp 1689693369\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: 🚀 View run FSR_Trainable_e2e2c0c8 at: https://wandb.ai/seokjin/FSR-prediction/runs/e2e2c0c8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001604-e2e2c0c8/logs\n",
      "2023-07-19 00:16:15,340\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.686 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:16:15,343\tWARNING util.py:315 -- The `process_trial_result` operation took 1.689 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:16:15,346\tWARNING util.py:315 -- Processing trial results took 1.692 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:16:15,347\tWARNING util.py:315 -- The `process_trial_result` operation took 1.693 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_681f7398_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-15-58/wandb/run-20230719_001617-681f7398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: Syncing run FSR_Trainable_681f7398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/681f7398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318932)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:16:24,003\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.791 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:16:24,006\tWARNING util.py:315 -- The `process_trial_result` operation took 1.794 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:16:24,009\tWARNING util.py:315 -- Processing trial results took 1.797 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:16:24,012\tWARNING util.py:315 -- The `process_trial_result` operation took 1.800 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb:                      mae █▃▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb:                     mape ▇█▆▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb:                     rmse █▅▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb:       time_since_restore ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb:         time_this_iter_s █▃▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb:             time_total_s ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb:                timestamp ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb:                      mae 191.1596\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb:                     mape 623321478.14197\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb:                     rmse 383.33027\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb:       time_since_restore 6.95826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb:         time_this_iter_s 1.73947\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb:             time_total_s 6.95826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb:                timestamp 1689693380\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: 🚀 View run FSR_Trainable_681f7398 at: https://wandb.ai/seokjin/FSR-prediction/runs/681f7398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319173)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001617-681f7398/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_03bd954e_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-16-11/wandb/run-20230719_001625-03bd954e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: Syncing run FSR_Trainable_03bd954e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/03bd954e\n",
      "2023-07-19 00:16:32,887\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.988 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:16:32,891\tWARNING util.py:315 -- The `process_trial_result` operation took 1.992 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:16:32,894\tWARNING util.py:315 -- Processing trial results took 1.996 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:16:32,896\tWARNING util.py:315 -- The `process_trial_result` operation took 1.998 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_9e15ec76_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-16-19/wandb/run-20230719_001634-9e15ec76\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: Syncing run FSR_Trainable_9e15ec76\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9e15ec76\n",
      "2023-07-19 00:16:43,191\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.623 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:16:43,208\tWARNING util.py:315 -- The `process_trial_result` operation took 1.640 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:16:43,209\tWARNING util.py:315 -- Processing trial results took 1.642 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:16:43,211\tWARNING util.py:315 -- The `process_trial_result` operation took 1.644 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_cb5882fa_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-16-28/wandb/run-20230719_001644-cb5882fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: Syncing run FSR_Trainable_cb5882fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cb5882fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb:                      mae █▇▃▃▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb:                     mape █▇▅▅▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb:                     rmse █▇▄▃▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb:         time_this_iter_s █▄▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb:                timestamp ▁▃▄▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb:                      mae 149.84353\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb:                     mape 247720530.4636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb:                     rmse 290.01672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb:       time_since_restore 23.47043\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb:         time_this_iter_s 2.77333\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb:             time_total_s 23.47043\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb:                timestamp 1689693423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: 🚀 View run FSR_Trainable_cb5882fa at: https://wandb.ai/seokjin/FSR-prediction/runs/cb5882fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001644-cb5882fa/logs\n",
      "2023-07-19 00:17:18,235\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.945 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:17:18,240\tWARNING util.py:315 -- The `process_trial_result` operation took 1.950 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:17:18,241\tWARNING util.py:315 -- Processing trial results took 1.952 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:17:18,243\tWARNING util.py:315 -- The `process_trial_result` operation took 1.954 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_64b635d6_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-16-37/wandb/run-20230719_001718-64b635d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: Syncing run FSR_Trainable_64b635d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/64b635d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb:                      mae █▄▄▃▂▂▂▁▁▁▂▁▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb:                     mape █▅▅▅▃▃▂▂▁▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb:                     rmse █▅▅▄▂▂▁▁▁▁▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb:         time_this_iter_s ▆▁▁▁▁▄▄▂▄▃▁▄▅▁█▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb:                timestamp ▁▂▂▂▃▃▄▄▅▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb:                      mae 144.86642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb:                     mape 170779719.5669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb:                     rmse 290.12763\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb:       time_since_restore 51.24778\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb:         time_this_iter_s 3.97916\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb:             time_total_s 51.24778\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb:                timestamp 1689693486\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: 🚀 View run FSR_Trainable_64b635d6 at: https://wandb.ai/seokjin/FSR-prediction/runs/64b635d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320072)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001718-64b635d6/logs\n",
      "2023-07-19 00:18:22,008\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.882 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:18:22,010\tWARNING util.py:315 -- The `process_trial_result` operation took 1.885 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:18:22,013\tWARNING util.py:315 -- Processing trial results took 1.888 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:18:22,015\tWARNING util.py:315 -- The `process_trial_result` operation took 1.890 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_45a78ad0_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-17-12/wandb/run-20230719_001822-45a78ad0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: Syncing run FSR_Trainable_45a78ad0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/45a78ad0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb:                      mae █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb:                     mape █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb:                     rmse █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb:         time_this_iter_s █▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb:                timestamp ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb:                      mae 178.8497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb:                     mape 475423683.25236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb:                     rmse 366.14192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb:       time_since_restore 13.68659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb:         time_this_iter_s 3.1287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb:             time_total_s 13.68659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb:                timestamp 1689693511\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: 🚀 View run FSR_Trainable_45a78ad0 at: https://wandb.ai/seokjin/FSR-prediction/runs/45a78ad0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320324)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001822-45a78ad0/logs\n",
      "2023-07-19 00:18:46,356\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.651 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:18:46,360\tWARNING util.py:315 -- The `process_trial_result` operation took 1.655 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:18:46,361\tWARNING util.py:315 -- Processing trial results took 1.657 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:18:46,363\tWARNING util.py:315 -- The `process_trial_result` operation took 1.658 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_96e0adc1_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-18-15/wandb/run-20230719_001847-96e0adc1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: Syncing run FSR_Trainable_96e0adc1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/96e0adc1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb:                      mae 381.81424\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb:                     mape 4.954484984157756e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb:                     rmse 578.32011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb:       time_since_restore 3.61475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb:         time_this_iter_s 3.61475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb:             time_total_s 3.61475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb:                timestamp 1689693524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: 🚀 View run FSR_Trainable_96e0adc1 at: https://wandb.ai/seokjin/FSR-prediction/runs/96e0adc1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320557)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001847-96e0adc1/logs\n",
      "2023-07-19 00:19:01,051\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.772 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:19:01,055\tWARNING util.py:315 -- The `process_trial_result` operation took 1.777 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:19:01,057\tWARNING util.py:315 -- Processing trial results took 1.779 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:19:01,058\tWARNING util.py:315 -- The `process_trial_result` operation took 1.780 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_caeff74f_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-18-41/wandb/run-20230719_001901-caeff74f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: Syncing run FSR_Trainable_caeff74f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/caeff74f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb:                      mae █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb:                     mape █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb:                     rmse █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb:         time_this_iter_s █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb:                timestamp ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb:                      mae 180.97179\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb:                     mape 466413521.35396\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb:                     rmse 365.54805\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb:       time_since_restore 12.98285\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb:         time_this_iter_s 2.93757\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb:             time_total_s 12.98285\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb:                timestamp 1689693550\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: 🚀 View run FSR_Trainable_caeff74f at: https://wandb.ai/seokjin/FSR-prediction/runs/caeff74f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=320783)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001901-caeff74f/logs\n",
      "2023-07-19 00:19:24,874\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.825 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:19:24,877\tWARNING util.py:315 -- The `process_trial_result` operation took 1.829 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:19:24,879\tWARNING util.py:315 -- Processing trial results took 1.830 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:19:24,880\tWARNING util.py:315 -- The `process_trial_result` operation took 1.832 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_cce72218_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-18-55/wandb/run-20230719_001925-cce72218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Syncing run FSR_Trainable_cce72218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cce72218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb:                      mae █▄▃▂▄▂▄▃▃▅▂▂▃▅▅▅▂█▄▄▅▄▃▂▂▅▂▂▃▂▃▁▇▅▅▅▅▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb:                     mape █▄▃▂▃▂▅▂▂▅▂▁▂▅▃▅▃▃▄▂▅▅▃▂▃▅▃▁▂▂▂▂▆▄▄▄▃▃▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb:                     rmse ▇▄▂▂▄▂▃▃▃▅▂▂▄▅▆▄▁█▃▄▅▅▃▂▃▄▂▂▃▂▃▁▇▆▆▆▆▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb:         time_this_iter_s ▄▃▄▄▃▂▃▁▂▂▂▃▃▄▄▄▄▄▇▅▅▅▅▅▅█▃▃▂▂▂▂▄▂▃▁▂▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb:                      mae 177.56836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb:                     mape 343176398.44688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb:                     rmse 365.85551\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb:       time_since_restore 225.57841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb:         time_this_iter_s 2.23398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb:             time_total_s 225.57841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb:                timestamp 1689693563\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: 🚀 View run FSR_Trainable_d8fbd1ca at: https://wandb.ai/seokjin/FSR-prediction/runs/d8fbd1ca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=318488)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001531-d8fbd1ca/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001925-cce72218/logs\n",
      "2023-07-19 00:19:37,474\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.881 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:19:37,476\tWARNING util.py:315 -- The `process_trial_result` operation took 1.884 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:19:37,479\tWARNING util.py:315 -- Processing trial results took 1.886 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:19:37,481\tWARNING util.py:315 -- The `process_trial_result` operation took 1.888 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321015)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_457af1e2_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-19-19/wandb/run-20230719_001939-457af1e2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: Syncing run FSR_Trainable_457af1e2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/457af1e2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb:                      mae 260.40846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb:                     mape 866207858.48337\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb:                     rmse 445.80615\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb:       time_since_restore 4.46166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb:         time_this_iter_s 2.04003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb:             time_total_s 4.46166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb:                timestamp 1689693579\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: 🚀 View run FSR_Trainable_457af1e2 at: https://wandb.ai/seokjin/FSR-prediction/runs/457af1e2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001939-457af1e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321257)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 00:19:49,655\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.784 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:19:49,656\tWARNING util.py:315 -- The `process_trial_result` operation took 1.786 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:19:49,659\tWARNING util.py:315 -- Processing trial results took 1.789 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:19:49,663\tWARNING util.py:315 -- The `process_trial_result` operation took 1.793 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_b0e7957c_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-19-33/wandb/run-20230719_001949-b0e7957c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: Syncing run FSR_Trainable_b0e7957c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b0e7957c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb:                      mae 254.62833\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb:                     mape 845837424.11962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb:                     rmse 439.38829\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb:       time_since_restore 10.48229\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb:         time_this_iter_s 4.90911\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb:             time_total_s 10.48229\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb:                timestamp 1689693594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: 🚀 View run FSR_Trainable_b0e7957c at: https://wandb.ai/seokjin/FSR-prediction/runs/b0e7957c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321475)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001949-b0e7957c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_629e0313_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-19-42/wandb/run-20230719_001958-629e0313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: Syncing run FSR_Trainable_629e0313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/629e0313\n",
      "2023-07-19 00:20:02,191\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.677 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:20:02,195\tWARNING util.py:315 -- The `process_trial_result` operation took 1.682 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:20:02,198\tWARNING util.py:315 -- Processing trial results took 1.685 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:20:02,200\tWARNING util.py:315 -- The `process_trial_result` operation took 1.687 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_2e026fe5_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-19-52/wandb/run-20230719_002011-2e026fe5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: Syncing run FSR_Trainable_2e026fe5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2e026fe5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:20:14,333\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.435 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:20:14,336\tWARNING util.py:315 -- The `process_trial_result` operation took 1.439 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:20:14,340\tWARNING util.py:315 -- Processing trial results took 1.443 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:20:14,341\tWARNING util.py:315 -- The `process_trial_result` operation took 1.444 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb:                      mae 234.66107\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb:                     mape 989556363.0188\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb:                     rmse 454.16559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb:       time_since_restore 17.9091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb:         time_this_iter_s 9.37014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb:             time_total_s 17.9091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb:                timestamp 1689693611\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: 🚀 View run FSR_Trainable_629e0313 at: https://wandb.ai/seokjin/FSR-prediction/runs/629e0313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321693)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001958-629e0313/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb:                      mae █▄▁▂▂▁▁▁▁▁▁▂▁▁▁▁▂█▂▁▂▁▂▂▂▂▂▃▁▂▂▂▂▂▄▄▄▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb:                     mape █▆▂▂▂▁▂▁▁▁▁▁▁▁▂▁▁▄▁▁▁▂▁▁▃▂▁▁▂▁▁▂▁▂▃▄▃▃▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb:                     rmse ▇▄▁▁▂▁▁▁▂▁▁▂▂▁▂▁▃█▂▁▂▂▂▂▂▃▂▄▁▂▂▂▃▂▄▄▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb:         time_this_iter_s ▅▂▃▅▃▃▃▃▃▃▃▃▄▃▅▅▆█▄▃▆▃▃▃▃▃▃▄▃▃▃▅▂▁▅▆▄▆▃▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001625-03bd954e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319394)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb:                      mae █▄▂▃▂▂▂▂▂▂▂▂▁▂▁▃▂▂▁▂▃▂▂▁▂▁▁▁▂▁▁▁▁▂▁▂▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb:                     mape █▅▂▃▂▂▂▁▂▂▂▂▁▁▁▃▂▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb:                     rmse █▅▂▃▂▃▃▂▂▂▃▂▂▂▂▄▂▂▁▂▄▃▂▂▂▁▂▂▂▁▁▂▁▂▁▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb:         time_this_iter_s ▅▃▄▃▃▃▅▃▅▃▃▇▃▃▄▄█▃▆▃▃▃▃▃▄▄▃▃▃▄▁▂▄▁▄▅▃▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb:                      mae 108.99501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb:                     mape 88665443.93872\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb:                     rmse 214.08671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb:       time_since_restore 218.29165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb:         time_this_iter_s 1.67969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb:             time_total_s 218.29165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb:                timestamp 1689693619\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: 🚀 View run FSR_Trainable_9e15ec76 at: https://wandb.ai/seokjin/FSR-prediction/runs/9e15ec76\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_001634-9e15ec76/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=319619)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 00:20:27,707\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.695 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:20:27,711\tWARNING util.py:315 -- The `process_trial_result` operation took 1.699 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:20:27,713\tWARNING util.py:315 -- Processing trial results took 1.701 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:20:27,718\tWARNING util.py:315 -- The `process_trial_result` operation took 1.706 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_43d5dcc3_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-20-04/wandb/run-20230719_002026-43d5dcc3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: Syncing run FSR_Trainable_43d5dcc3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/43d5dcc3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 00:20:36,811\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.280 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:20:36,813\tWARNING util.py:315 -- The `process_trial_result` operation took 2.284 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:20:36,817\tWARNING util.py:315 -- Processing trial results took 2.287 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:20:36,821\tWARNING util.py:315 -- The `process_trial_result` operation took 2.291 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_dd20017e_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-20-20/wandb/run-20230719_002036-dd20017e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: Syncing run FSR_Trainable_dd20017e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/dd20017e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb:                      mae ▅█▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb:                     mape ▆█▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb:                     rmse ▅█▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb:       time_since_restore ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb:         time_this_iter_s █▁▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb:             time_total_s ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb:                timestamp ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb:                      mae 191.38743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb:                     mape 636126651.31542\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb:                     rmse 384.46329\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb:       time_since_restore 30.23955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb:         time_this_iter_s 8.4224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb:             time_total_s 30.23955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb:                timestamp 1689693636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: 🚀 View run FSR_Trainable_2e026fe5 at: https://wandb.ai/seokjin/FSR-prediction/runs/2e026fe5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=321917)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_002011-2e026fe5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 00:20:46,597\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.091 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:20:46,600\tWARNING util.py:315 -- The `process_trial_result` operation took 2.094 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:20:46,601\tWARNING util.py:315 -- Processing trial results took 2.096 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:20:46,603\tWARNING util.py:315 -- The `process_trial_result` operation took 2.098 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_9e451d85_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-20-29/wandb/run-20230719_002046-9e451d85\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: Syncing run FSR_Trainable_9e451d85\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9e451d85\n",
      "2023-07-19 00:20:54,399\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.377 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:20:54,404\tWARNING util.py:315 -- The `process_trial_result` operation took 2.384 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:20:54,409\tWARNING util.py:315 -- Processing trial results took 2.388 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:20:54,418\tWARNING util.py:315 -- The `process_trial_result` operation took 2.398 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_aa30a5a6_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-20-39/wandb/run-20230719_002056-aa30a5a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: Syncing run FSR_Trainable_aa30a5a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/aa30a5a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb:                      mae 260.51526\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb:                     mape 4.00187491685067e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb:                     rmse 453.15782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb:       time_since_restore 2.36285\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb:         time_this_iter_s 2.36285\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb:             time_total_s 2.36285\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb:                timestamp 1689693652\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: 🚀 View run FSR_Trainable_aa30a5a6 at: https://wandb.ai/seokjin/FSR-prediction/runs/aa30a5a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322811)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_002056-aa30a5a6/logs\n",
      "2023-07-19 00:21:09,588\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.068 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:21:09,591\tWARNING util.py:315 -- The `process_trial_result` operation took 2.072 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:21:09,596\tWARNING util.py:315 -- Processing trial results took 2.076 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:21:09,598\tWARNING util.py:315 -- The `process_trial_result` operation took 2.078 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_33e1f2a1_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-20-49/wandb/run-20230719_002111-33e1f2a1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: Syncing run FSR_Trainable_33e1f2a1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/33e1f2a1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb:                      mae █▃▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb:                     mape █▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb:                     rmse █▃▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb:         time_this_iter_s █▁▃▄▄▂▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb:                      mae 144.93305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb:                     mape 8.707255121158195e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb:                     rmse 294.48714\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb:       time_since_restore 36.20216\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb:         time_this_iter_s 4.6497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb:             time_total_s 36.20216\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb:                timestamp 1689693669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: 🚀 View run FSR_Trainable_dd20017e at: https://wandb.ai/seokjin/FSR-prediction/runs/dd20017e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322379)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_002036-dd20017e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323036)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb:                      mae █▃▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb:                     mape █▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb:                     rmse █▄▃▂▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb:         time_this_iter_s █▅▅▅▄▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb:                      mae 143.34395\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb:                     mape 8.472698455007858e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb:                     rmse 291.09461\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb:       time_since_restore 34.9123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb:         time_this_iter_s 3.5336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb:             time_total_s 34.9123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb:                timestamp 1689693677\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: 🚀 View run FSR_Trainable_9e451d85 at: https://wandb.ai/seokjin/FSR-prediction/runs/9e451d85\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_002046-9e451d85/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 00:21:24,761\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.733 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:21:24,767\tWARNING util.py:315 -- The `process_trial_result` operation took 1.740 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:21:24,771\tWARNING util.py:315 -- Processing trial results took 1.744 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:21:24,775\tWARNING util.py:315 -- The `process_trial_result` operation took 1.748 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_41fe8a69_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-21-05/wandb/run-20230719_002124-41fe8a69\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: Syncing run FSR_Trainable_41fe8a69\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/41fe8a69\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322594)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 00:21:33,062\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.333 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:21:33,066\tWARNING util.py:315 -- The `process_trial_result` operation took 2.338 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:21:33,071\tWARNING util.py:315 -- Processing trial results took 2.343 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:21:33,073\tWARNING util.py:315 -- The `process_trial_result` operation took 2.345 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_e0e7b4f2_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-21-18/wandb/run-20230719_002134-e0e7b4f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: Syncing run FSR_Trainable_e0e7b4f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e0e7b4f2\n",
      "2023-07-19 00:21:43,370\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.043 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:21:43,373\tWARNING util.py:315 -- The `process_trial_result` operation took 2.047 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:21:43,376\tWARNING util.py:315 -- Processing trial results took 2.050 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:21:43,379\tWARNING util.py:315 -- The `process_trial_result` operation took 2.053 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_a30a0694_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-21-27/wandb/run-20230719_002144-a30a0694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Syncing run FSR_Trainable_a30a0694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a30a0694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb:                      mae █▇▃▂▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb:                     mape ▆█▃▂▃▁▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb:                     rmse █▆▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb:         time_this_iter_s ▄▁▆▂█▅▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb:                      mae 155.14671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb:                     mape 304647169.68928\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb:                     rmse 289.79955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb:       time_since_restore 36.59221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb:         time_this_iter_s 4.39338\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb:             time_total_s 36.59221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb:                timestamp 1689693717\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: 🚀 View run FSR_Trainable_41fe8a69 at: https://wandb.ai/seokjin/FSR-prediction/runs/41fe8a69\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323280)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_002124-41fe8a69/logs\n",
      "2023-07-19 00:22:12,724\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.990 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:22:12,726\tWARNING util.py:315 -- The `process_trial_result` operation took 1.993 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:22:12,729\tWARNING util.py:315 -- Processing trial results took 1.996 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:22:12,731\tWARNING util.py:315 -- The `process_trial_result` operation took 1.998 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_5b1dc23d_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-21-37/wandb/run-20230719_002214-5b1dc23d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: Syncing run FSR_Trainable_5b1dc23d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5b1dc23d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb:                      mae █▇▆▅▅▅▄▄▄▃▃▄▄▃▃▃▃▂▂▂▃▄▃▃▃▂▂▂▂▁▂▂▁▁▁▁▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb:                     mape ██▇▆▅▅▄▄▃▂▂▂▃▁▁▁▁▁▂▂▃▃▃▃▃▃▂▃▂▁▂▁▁▂▂▁▁▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb:                     rmse █▆▅▅▅▅▅▅▅▄▄▅▅▄▄▄▄▃▃▃▃▄▃▃▃▂▃▃▂▂▃▂▂▂▁▂▂▂▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb:         time_this_iter_s █▃▃▁▂▇▂▂▃▃▁▄▁▅▂▃▂▂▃▂▂▁▁▄▂▂▂▁▂▃▁▃▂▂▂▁▁▂▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb:                      mae 101.94962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb:                     mape 5.56643141083484e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb:                     rmse 210.39014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb:       time_since_restore 275.93391\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb:         time_this_iter_s 2.76678\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb:             time_total_s 275.93391\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb:                timestamp 1689693970\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: 🚀 View run FSR_Trainable_e0e7b4f2 at: https://wandb.ai/seokjin/FSR-prediction/runs/e0e7b4f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323495)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_002134-e0e7b4f2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb:                      mae █▆▅▅▄▄▄▄▄▄▃▃▃▂▃▂▂▂▃▂▂▁▂▂▂▁▃▂▂▁▁▁▁▃▂▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb:                     mape ▇█▇▆▆▅▅▄▄▃▃▃▂▁▂▂▂▁▂▂▃▂▂▂▂▃▃▃▃▂▂▂▂▄▃▂▂▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb:                     rmse █▅▄▄▄▄▄▄▄▄▃▄▃▃▃▃▃▂▄▂▂▂▂▂▂▁▃▂▂▁▁▁▁▃▂▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb:         time_this_iter_s █▂▂▂▅▂▂▃▃▂▂▂▄▂▃▁▁▂▂▂▂▂▂▂▂▃▃▂▂▂▂▃▂▂▂▂▂▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_002144-a30a0694/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_002144-a30a0694/logs\n",
      "2023-07-19 00:26:26,653\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.275 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:26:26,658\tWARNING util.py:315 -- The `process_trial_result` operation took 2.281 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:26:26,660\tWARNING util.py:315 -- Processing trial results took 2.283 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:26:26,663\tWARNING util.py:315 -- The `process_trial_result` operation took 2.287 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323705)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_ac90a52e_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-22-07/wandb/run-20230719_002627-ac90a52e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: Syncing run FSR_Trainable_ac90a52e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ac90a52e\n",
      "2023-07-19 00:26:36,011\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.827 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:26:36,013\tWARNING util.py:315 -- The `process_trial_result` operation took 1.830 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:26:36,015\tWARNING util.py:315 -- Processing trial results took 1.832 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:26:36,016\tWARNING util.py:315 -- The `process_trial_result` operation took 1.833 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_e2ad4b4f_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-26-20/wandb/run-20230719_002637-e2ad4b4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: Syncing run FSR_Trainable_e2ad4b4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e2ad4b4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb:                      mae █▇▅▅▄▄▄▄▃▃▇▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▂▂▁▁▂▂▂▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb:                     mape █▇▇▆▅▅▅▄▃▃▇▂▂▁▁▁▁▂▁▁▁▂▁▂▂▂▂▂▃▂▂▂▂▂▃▂▂▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb:                     rmse █▆▅▅▄▄▄▄▄▄▇▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▃▂▂▁▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb:         time_this_iter_s █▂▃▃▃▂▂▂▅▂▂▃▃▂▂▂▂▂▄▂▃▂▃▂▂▂▃▂▂▃▁▂▃▃▃▁▅▃▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb:                      mae 107.33691\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb:                     mape 7.502698874805485e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb:                     rmse 211.64614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb:       time_since_restore 275.21467\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb:         time_this_iter_s 3.05097\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb:             time_total_s 275.21467\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb:                timestamp 1689694008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: 🚀 View run FSR_Trainable_5b1dc23d at: https://wandb.ai/seokjin/FSR-prediction/runs/5b1dc23d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=323938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_002214-5b1dc23d/logs\n",
      "2023-07-19 00:27:03,986\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.933 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:27:03,989\tWARNING util.py:315 -- The `process_trial_result` operation took 1.937 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:27:03,992\tWARNING util.py:315 -- Processing trial results took 1.939 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:27:03,994\tWARNING util.py:315 -- The `process_trial_result` operation took 1.941 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_a25dc63a_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-26-30/wandb/run-20230719_002705-a25dc63a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: Syncing run FSR_Trainable_a25dc63a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a25dc63a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb:                      mae █▃▆▁▂▃▃▂▄▅▁▄▂▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb:                     mape █▂▅▁▂▁▂▂▂▃▁▃▂▂▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb:                     rmse █▂▅▁▂▃▂▂▄▅▁▅▂▃▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb:         time_this_iter_s █▃▆▄▆▅▄▃▁▇▄▂▄▄▃▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb:                timestamp ▁▂▂▃▃▄▄▅▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb:                      mae 139.80006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb:                     mape 274342836.88154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb:                     rmse 273.12932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb:       time_since_restore 60.07157\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb:         time_this_iter_s 3.8889\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb:             time_total_s 60.07157\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb:                timestamp 1689694046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: 🚀 View run FSR_Trainable_ac90a52e at: https://wandb.ai/seokjin/FSR-prediction/runs/ac90a52e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324287)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_002627-ac90a52e/logs\n",
      "2023-07-19 00:27:41,378\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.937 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:27:41,380\tWARNING util.py:315 -- The `process_trial_result` operation took 1.940 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:27:41,384\tWARNING util.py:315 -- Processing trial results took 1.944 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:27:41,386\tWARNING util.py:315 -- The `process_trial_result` operation took 1.946 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_f0123c53_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-26-58/wandb/run-20230719_002742-f0123c53\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: Syncing run FSR_Trainable_f0123c53\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0123c53\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: \\ 0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: | 0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb:                      mae █▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb:                     mape █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb:                     rmse █▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb:         time_this_iter_s ▅▅▇▄▁▅▆▅▇▅▄▅▄▄▄▄▅▅▅▄▄▄▄▅▄▄▄▃▅█▅▅▄▆▅▅▄▄▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb:                      mae 105.72189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb:                     mape 5.425316916795066e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb:                     rmse 217.49856\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb:       time_since_restore 518.35571\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb:         time_this_iter_s 5.21393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb:             time_total_s 518.35571\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb:                timestamp 1689694145\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: 🚀 View run FSR_Trainable_43d5dcc3 at: https://wandb.ai/seokjin/FSR-prediction/runs/43d5dcc3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=322163)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_002026-43d5dcc3/logs\n",
      "2023-07-19 00:29:20,307\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.692 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:29:20,311\tWARNING util.py:315 -- The `process_trial_result` operation took 1.696 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:29:20,312\tWARNING util.py:315 -- Processing trial results took 1.698 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:29:20,314\tWARNING util.py:315 -- The `process_trial_result` operation took 1.699 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_fb124383_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-27-36/wandb/run-20230719_002921-fb124383\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Syncing run FSR_Trainable_fb124383\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fb124383\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb:                      mae █▇▆▅▅▄▄▄▄▄▃▄▃▃▃▃▃▃▃▃▃▂▄▂▂▂▂▁▁▁▁▂▃▂▃▆▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb:                     mape ▇██▇▆▅▄▄▄▃▃▄▄▃▂▂▃▂▂▃▅▂▃▃▃▃▃▂▂▂▃▃▄▃▃▆▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb:                     rmse █▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▄▂▂▂▃▂▁▁▂▂▃▂▂▆▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb:         time_this_iter_s █▃▃▂▄▂▂▃▁▅▃▃▃▂▂▃▂▃▃▂▂▂▂▃▂▃▂▂▃▃▂▂▂▃▂▂▂▂▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb:                      mae 102.34765\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb:                     mape 5.8389883594161544e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb:                     rmse 202.46142\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb:       time_since_restore 276.39672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb:         time_this_iter_s 2.795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb:             time_total_s 276.39672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb:                timestamp 1689694276\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: 🚀 View run FSR_Trainable_e2ad4b4f at: https://wandb.ai/seokjin/FSR-prediction/runs/e2ad4b4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324497)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_002637-e2ad4b4f/logs\n",
      "2023-07-19 00:31:31,594\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.920 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:31:31,597\tWARNING util.py:315 -- The `process_trial_result` operation took 1.925 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:31:31,599\tWARNING util.py:315 -- Processing trial results took 1.927 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:31:31,604\tWARNING util.py:315 -- The `process_trial_result` operation took 1.932 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_99f0e4a9_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-29-15/wandb/run-20230719_003133-99f0e4a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: Syncing run FSR_Trainable_99f0e4a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/99f0e4a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb:                      mae █▇▆▅▅▄▄▄▄▄▃▃▄▃▃▃▃▃▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb:                     mape ▇█▆▅▄▄▃▃▃▂▂▂▃▂▂▂▁▁▅▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb:                     rmse █▆▅▅▅▅▄▄▄▄▄▄▅▄▄▃▄▃▅▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb:         time_this_iter_s █▂▂▂▂▄▂▂▃▁▂▂▁▁▁▂▃▂▂▇▂▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▃▁▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb:                      mae 99.68314\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb:                     mape 5.440769368785551e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb:                     rmse 199.53048\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb:       time_since_restore 274.37379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb:         time_this_iter_s 2.84008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb:             time_total_s 274.37379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb:                timestamp 1689694303\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: 🚀 View run FSR_Trainable_a25dc63a at: https://wandb.ai/seokjin/FSR-prediction/runs/a25dc63a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324729)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_002705-a25dc63a/logs\n",
      "2023-07-19 00:31:58,351\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.628 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:31:58,353\tWARNING util.py:315 -- The `process_trial_result` operation took 1.631 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:31:58,354\tWARNING util.py:315 -- Processing trial results took 1.633 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:31:58,358\tWARNING util.py:315 -- The `process_trial_result` operation took 1.637 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_65b325f2_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-31-26/wandb/run-20230719_003159-65b325f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: Syncing run FSR_Trainable_65b325f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/65b325f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb:                      mae █▇▅▅▄▄▄▄▄▃▃▃▃▃▄▃▂▂▂▃▃▂▃▂▂▂▂▂▃▂▃▃▂▂▁▂▁▁▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb:                     mape ▆█▇▆▅▄▃▃▃▃▂▂▁▁▄▁▁▁▁▂▃▂▃▂▃▂▃▂▄▃▅▃▂▂▂▂▂▁▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb:                     rmse █▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▄▂▂▂▂▂▂▂▃▃▂▁▁▂▁▁▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb:         time_this_iter_s █▂▃▂▁▃▂▂▂▂▃▃▂▂▆▃▃▂▂▂▃▁▂▃▁▁▂▂▂▃▃▂▂▇▃▄▃▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb:                      mae 112.76498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb:                     mape 8.062590802045958e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb:                     rmse 219.14208\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb:       time_since_restore 272.28053\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb:         time_this_iter_s 2.71192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb:             time_total_s 272.28053\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb:                timestamp 1689694335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: 🚀 View run FSR_Trainable_f0123c53 at: https://wandb.ai/seokjin/FSR-prediction/runs/f0123c53\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_002742-f0123c53/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=324967)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb:                      mae █▆▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▃▂▂▂▂▃▂▂▃▄▁▁▂▁▁▁▂▁▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb:                     mape █▆▆▆▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▂▁▁▁▁▂▁▁▃▃▁▁▂▁▁▁▃▂▂▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb:                     rmse █▅▄▄▄▃▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▂▃▄▅▂▂▂▁▁▁▂▁▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb:         time_this_iter_s █▃▂▂▂▂▂▁▁▂▂▂▂▂▂▁▁▂▃▁▃▁▂▂▃▃▂▂▁▃▄▁▃▂▁▅▂▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "2023-07-19 00:32:29,619\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.995 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:32:29,622\tWARNING util.py:315 -- The `process_trial_result` operation took 1.998 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:32:29,626\tWARNING util.py:315 -- Processing trial results took 2.001 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:32:29,628\tWARNING util.py:315 -- The `process_trial_result` operation took 2.003 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325236)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_09efce4f_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-31-53/wandb/run-20230719_003230-09efce4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: Syncing run FSR_Trainable_09efce4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/09efce4f\n",
      "2023-07-19 00:32:39,441\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.764 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:32:39,444\tWARNING util.py:315 -- The `process_trial_result` operation took 1.767 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:32:39,445\tWARNING util.py:315 -- Processing trial results took 1.768 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:32:39,446\tWARNING util.py:315 -- The `process_trial_result` operation took 1.769 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_a0320404_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-32-24/wandb/run-20230719_003240-a0320404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: Syncing run FSR_Trainable_a0320404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a0320404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb:                      mae █▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb:                     mape █▄▅▆▆▅▅▄▄▄▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▂▂▂▂▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb:                     rmse █▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb:         time_this_iter_s █▃▄▄▂▁▂▂▂▂▃▂▃▂▂▂▂▃▂▂▂▂▂▂▂▃▁▂▃▃▂▂▂▂▂▂▃▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb:                      mae 111.66341\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb:                     mape 7.175468916096804e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb:                     rmse 223.28654\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb:       time_since_restore 178.02044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb:         time_this_iter_s 2.75903\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb:             time_total_s 178.02044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb:                timestamp 1689694527\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: 🚀 View run FSR_Trainable_09efce4f at: https://wandb.ai/seokjin/FSR-prediction/runs/09efce4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326002)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_003230-09efce4f/logs\n",
      "2023-07-19 00:35:43,584\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.108 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:35:43,587\tWARNING util.py:315 -- The `process_trial_result` operation took 2.112 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:35:43,589\tWARNING util.py:315 -- Processing trial results took 2.114 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:35:43,591\tWARNING util.py:315 -- The `process_trial_result` operation took 2.116 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_c370576b_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-32-34/wandb/run-20230719_003545-c370576b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: Syncing run FSR_Trainable_c370576b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c370576b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb:                      mae █▇▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▄▂▅▃▂▂▂▂▂▂▂▃▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb:                     mape ▇█▇▆▅▅▄▄▃▃▂▃▂▁▁▁▂▁▁▁▁▃▂▄▂▂▃▃▃▃▃▃▄▄▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb:                     rmse █▆▅▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▃▄▃▅▃▃▃▃▂▂▃▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb:         time_this_iter_s █▃▁▂▅▂▂▁▄▄▂▄▂▅▂▃▂▂▂▃▂▂▂▃▂▃▂▂▃▂▂▂▃▂▂▄▃▃▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb:                      mae 97.455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb:                     mape 6.234413392315375e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb:                     rmse 196.01666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb:       time_since_restore 276.03991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb:         time_this_iter_s 2.94223\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb:             time_total_s 276.03991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb:                timestamp 1689694571\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: 🚀 View run FSR_Trainable_99f0e4a9 at: https://wandb.ai/seokjin/FSR-prediction/runs/99f0e4a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325520)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_003133-99f0e4a9/logs\n",
      "2023-07-19 00:36:27,588\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.812 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:36:27,593\tWARNING util.py:315 -- The `process_trial_result` operation took 1.818 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:36:27,594\tWARNING util.py:315 -- Processing trial results took 1.819 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:36:27,596\tWARNING util.py:315 -- The `process_trial_result` operation took 1.821 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_ee736811_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-35-38/wandb/run-20230719_003629-ee736811\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: Syncing run FSR_Trainable_ee736811\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ee736811\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb:                      mae █▆▅▄▄▄▃▃▃▃▃▃▃▃▄▃▂▃▃▂▂▂▁▁▁▁▂▁▂▄▂▁▁▁▁▂▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb:                     mape █▆▅▄▄▃▃▂▂▂▂▂▁▁▂▁▁▂▂▂▁▂▁▂▁▂▂▁▂▄▂▁▁▁▁▂▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb:                     rmse █▅▄▄▄▄▄▄▄▄▄▄▃▄▄▃▃▄▃▃▃▂▂▁▂▂▂▂▂▃▂▁▁▁▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb:         time_this_iter_s █▃▂▂▁▃▄▂▂▃▂▃▂▂▂▂▂▂▂▃▃▁▃▂▂▃▃▃▂▂▂▁▄▂▂▃▃▂▅▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb:                      mae 106.76426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb:                     mape 6.7625581578518856e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb:                     rmse 214.93402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb:       time_since_restore 276.99049\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb:         time_this_iter_s 2.72576\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb:             time_total_s 276.99049\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb:                timestamp 1689694599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: 🚀 View run FSR_Trainable_65b325f2 at: https://wandb.ai/seokjin/FSR-prediction/runs/65b325f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=325750)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_003159-65b325f2/logs\n",
      "2023-07-19 00:36:54,958\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.272 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:36:54,962\tWARNING util.py:315 -- The `process_trial_result` operation took 2.278 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:36:54,965\tWARNING util.py:315 -- Processing trial results took 2.280 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:36:54,966\tWARNING util.py:315 -- The `process_trial_result` operation took 2.282 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_d72330cb_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-36-22/wandb/run-20230719_003656-d72330cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: Syncing run FSR_Trainable_d72330cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d72330cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb:                      mae █▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb:                     mape █▃▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb:                     rmse █▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb:         time_this_iter_s █▂▂▁▂▂▂▂▂▂▁▂▂▂▂▂▁▃▂▃▁▂▁▂▃▂▃▁▄▂▃▂▃▂▅▂▃▁▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb:                      mae 99.30319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb:                     mape 6.7000290275820104e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb:                     rmse 195.54042\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb:       time_since_restore 260.6782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb:         time_this_iter_s 2.51923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb:             time_total_s 260.6782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb:                timestamp 1689694623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: 🚀 View run FSR_Trainable_a0320404 at: https://wandb.ai/seokjin/FSR-prediction/runs/a0320404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326212)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_003240-a0320404/logs\n",
      "2023-07-19 00:37:24,117\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.366 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:37:24,122\tWARNING util.py:315 -- The `process_trial_result` operation took 2.371 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:37:24,123\tWARNING util.py:315 -- Processing trial results took 2.372 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:37:24,124\tWARNING util.py:315 -- The `process_trial_result` operation took 2.374 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_ec812aff_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-36-49/wandb/run-20230719_003726-ec812aff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: Syncing run FSR_Trainable_ec812aff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ec812aff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb:                      mae █▅▅▅▅▅▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb:                     mape █▄▅▅▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb:                     rmse █▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb:         time_this_iter_s ▅▂▂▂▂▁▁▅▂▂▂▂▂▄▅▅▃▂▃▄█▄▅▂▂▂▂▂▂▂▂▂▂▃▂▃▂▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb:                      mae 94.53645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb:                     mape 7.044508326154725e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb:                     rmse 183.8894\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb:       time_since_restore 258.36166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb:         time_this_iter_s 2.76607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb:             time_total_s 258.36166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb:                timestamp 1689694804\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: 🚀 View run FSR_Trainable_c370576b at: https://wandb.ai/seokjin/FSR-prediction/runs/c370576b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326524)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_003545-c370576b/logs\n",
      "2023-07-19 00:40:19,721\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.786 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:40:19,723\tWARNING util.py:315 -- The `process_trial_result` operation took 1.788 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:40:19,725\tWARNING util.py:315 -- Processing trial results took 1.791 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:40:19,727\tWARNING util.py:315 -- The `process_trial_result` operation took 1.792 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_23f8d543_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-37-17/wandb/run-20230719_004021-23f8d543\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Syncing run FSR_Trainable_23f8d543\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/23f8d543\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb:                      mae █▆▅▅▅▄▄▄▄▄▄▄▄▄▄▄▃▄▃▃▃▃▂▂▂▂▂▂▂▃▂▁▁▁▁▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb:                     mape █▄▅▅▄▄▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▁▁▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb:                     rmse █▆▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb:         time_this_iter_s ▅▁▂▂▂▂▁█▂▂▂▂█▃▃▁▁▂▂▁▁▂▁▂▁▂▁▂▁▁▃▁▄▂▁▂▁▁▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb:                      mae 94.3328\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb:                     mape 6.424322175680578e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb:                     rmse 187.75638\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb:       time_since_restore 276.96106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb:         time_this_iter_s 2.57748\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb:             time_total_s 276.96106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb:                timestamp 1689694867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: 🚀 View run FSR_Trainable_ee736811 at: https://wandb.ai/seokjin/FSR-prediction/runs/ee736811\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326765)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_003629-ee736811/logs\n",
      "2023-07-19 00:41:22,125\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.244 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:41:22,130\tWARNING util.py:315 -- The `process_trial_result` operation took 2.249 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:41:22,131\tWARNING util.py:315 -- Processing trial results took 2.250 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:41:22,137\tWARNING util.py:315 -- The `process_trial_result` operation took 2.257 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...790)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_b590b28a_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-40-14/wandb/run-20230719_004125-b590b28a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: Syncing run FSR_Trainable_b590b28a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b590b28a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb:                      mae 573.23619\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb:                     mape 8.402197967933343e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb:                     rmse 802.91261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb:       time_since_restore 1.27049\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb:         time_this_iter_s 1.27049\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb:             time_total_s 1.27049\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb:                timestamp 1689694879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: 🚀 View run FSR_Trainable_b590b28a at: https://wandb.ai/seokjin/FSR-prediction/runs/b590b28a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327790)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004125-b590b28a/logs\n",
      "2023-07-19 00:41:38,171\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.073 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:41:38,175\tWARNING util.py:315 -- The `process_trial_result` operation took 2.078 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:41:38,179\tWARNING util.py:315 -- Processing trial results took 2.082 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:41:38,181\tWARNING util.py:315 -- The `process_trial_result` operation took 2.084 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_ecda1e39_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-41-18/wandb/run-20230719_004140-ecda1e39\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: Syncing run FSR_Trainable_ecda1e39\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ecda1e39\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb:                      mae █▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▃▄▃▃▃▃▃▃▃▃▃▃▃▂▂▃▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb:                     mape ▄▇██▇▇▆▅▄▃▃▃▃▂▂▁▁▁▁▁▁▁▁▁▂▂▂▂▂▁▁▁▂▂▃▂▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb:                     rmse █▆▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb:         time_this_iter_s ▅▁▁█▃▂▂▃█▂▄▂▁▂▂▁▁▂▁▂▂▂▂▂▁▂▂▁▁▂▁▁▂▁▂▂▁▁▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb:                      mae 91.03682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb:                     mape 6.686993363041777e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb:                     rmse 181.76558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb:       time_since_restore 279.06362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb:         time_this_iter_s 3.05201\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb:             time_total_s 279.06362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb:                timestamp 1689694896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: 🚀 View run FSR_Trainable_d72330cb at: https://wandb.ai/seokjin/FSR-prediction/runs/d72330cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=326997)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_003656-d72330cb/logs\n",
      "2023-07-19 00:41:54,534\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.776 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:41:54,537\tWARNING util.py:315 -- The `process_trial_result` operation took 1.780 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:41:54,538\tWARNING util.py:315 -- Processing trial results took 1.781 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:41:54,539\tWARNING util.py:315 -- The `process_trial_result` operation took 1.783 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_e689db0e_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-41-33/wandb/run-20230719_004156-e689db0e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Syncing run FSR_Trainable_e689db0e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e689db0e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb:                      mae ██▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▆▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▁▃▂▂▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb:                     mape ▃██▇▇▆▆▅▅▅▄▄▄▃▃▄▃▇▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▅▁▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb:                     rmse █▆▅▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▃▂▂▂▁▂▂▁▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb:         time_this_iter_s █▂▂▄▃▄▅▂▂▂▂▂▁▁▁▂▂▂▂▂▂▂▄▂▁▂▁▂▂▁▂▁▂▁▂▂▁▂▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb:                      mae 94.56722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb:                     mape 6.798975316163965e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb:                     rmse 185.61747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb:       time_since_restore 275.03023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb:         time_this_iter_s 2.69298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb:             time_total_s 275.03023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb:                timestamp 1689694922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: 🚀 View run FSR_Trainable_ec812aff at: https://wandb.ai/seokjin/FSR-prediction/runs/ec812aff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327229)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_003726-ec812aff/logs\n",
      "2023-07-19 00:42:20,756\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.639 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:42:20,758\tWARNING util.py:315 -- The `process_trial_result` operation took 1.642 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:42:20,760\tWARNING util.py:315 -- Processing trial results took 1.643 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:42:20,761\tWARNING util.py:315 -- The `process_trial_result` operation took 1.644 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_739006ac_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-41-49/wandb/run-20230719_004220-739006ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: Syncing run FSR_Trainable_739006ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/739006ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb:                      mae █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb:                     mape ██▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb:                     rmse █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb:         time_this_iter_s █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb:                timestamp ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb:                      mae 151.98925\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb:                     mape 1.2504635175887293e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb:                     rmse 292.65925\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb:       time_since_restore 18.39669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb:         time_this_iter_s 4.25998\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb:             time_total_s 18.39669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb:                timestamp 1689694953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: 🚀 View run FSR_Trainable_739006ac at: https://wandb.ai/seokjin/FSR-prediction/runs/739006ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328480)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004220-739006ac/logs\n",
      "2023-07-19 00:42:51,892\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.862 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:42:51,895\tWARNING util.py:315 -- The `process_trial_result` operation took 1.866 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:42:51,897\tWARNING util.py:315 -- Processing trial results took 1.867 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:42:51,897\tWARNING util.py:315 -- The `process_trial_result` operation took 1.868 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_d8e458e1_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-42-13/wandb/run-20230719_004252-d8e458e1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: Syncing run FSR_Trainable_d8e458e1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d8e458e1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb:                      mae 219.13235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb:                     mape 2.2901342119467795e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb:                     rmse 425.48747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb:       time_since_restore 5.30339\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb:         time_this_iter_s 5.30339\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb:             time_total_s 5.30339\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb:                timestamp 1689694970\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: 🚀 View run FSR_Trainable_d8e458e1 at: https://wandb.ai/seokjin/FSR-prediction/runs/d8e458e1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328716)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004252-d8e458e1/logs\n",
      "2023-07-19 00:43:08,575\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.259 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:43:08,577\tWARNING util.py:315 -- The `process_trial_result` operation took 2.263 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:43:08,578\tWARNING util.py:315 -- Processing trial results took 2.264 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:43:08,581\tWARNING util.py:315 -- The `process_trial_result` operation took 2.267 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_d814fa87_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-42-44/wandb/run-20230719_004310-d814fa87\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: Syncing run FSR_Trainable_d814fa87\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d814fa87\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb:                      mae 177.01073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb:                     mape 2.4553507505769212e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb:                     rmse 355.90549\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb:       time_since_restore 6.17918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb:         time_this_iter_s 2.77159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb:             time_total_s 6.17918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb:                timestamp 1689694991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: 🚀 View run FSR_Trainable_d814fa87 at: https://wandb.ai/seokjin/FSR-prediction/runs/d814fa87\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328944)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004310-d814fa87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_79862139_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-43-03/wandb/run-20230719_004329-79862139\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: Syncing run FSR_Trainable_79862139\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/79862139\n",
      "2023-07-19 00:43:42,015\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.067 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:43:42,017\tWARNING util.py:315 -- The `process_trial_result` operation took 2.069 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:43:42,019\tWARNING util.py:315 -- Processing trial results took 2.071 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:43:42,032\tWARNING util.py:315 -- The `process_trial_result` operation took 2.084 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb:                      mae 218.22922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb:                     mape 3.783643903256976e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb:                     rmse 428.46817\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb:       time_since_restore 17.65127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb:         time_this_iter_s 17.65127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb:             time_total_s 17.65127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb:                timestamp 1689695019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: 🚀 View run FSR_Trainable_79862139 at: https://wandb.ai/seokjin/FSR-prediction/runs/79862139\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329178)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004329-79862139/logs\n",
      "2023-07-19 00:43:57,729\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.845 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:43:57,731\tWARNING util.py:315 -- The `process_trial_result` operation took 1.848 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:43:57,733\tWARNING util.py:315 -- Processing trial results took 1.849 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:43:57,734\tWARNING util.py:315 -- The `process_trial_result` operation took 1.850 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_7cfd7c70_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-43-22/wandb/run-20230719_004359-7cfd7c70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: Syncing run FSR_Trainable_7cfd7c70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7cfd7c70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb:                      mae 351.00765\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb:                     mape 4.62461270598346e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb:                     rmse 567.15266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb:       time_since_restore 3.09023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb:         time_this_iter_s 3.09023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb:             time_total_s 3.09023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb:                timestamp 1689695035\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: 🚀 View run FSR_Trainable_7cfd7c70 at: https://wandb.ai/seokjin/FSR-prediction/runs/7cfd7c70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329411)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004359-7cfd7c70/logs\n",
      "2023-07-19 00:44:13,901\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.236 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:44:13,907\tWARNING util.py:315 -- The `process_trial_result` operation took 2.242 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:44:13,908\tWARNING util.py:315 -- Processing trial results took 2.243 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:44:13,909\tWARNING util.py:315 -- The `process_trial_result` operation took 2.244 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_314c8e84_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-43-52/wandb/run-20230719_004415-314c8e84\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: Syncing run FSR_Trainable_314c8e84\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/314c8e84\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb:                      mae 491.07756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb:                     mape 7.364505522196389e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb:                     rmse 747.02123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb:       time_since_restore 2.83972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb:         time_this_iter_s 2.83972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb:             time_total_s 2.83972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb:                timestamp 1689695051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: 🚀 View run FSR_Trainable_314c8e84 at: https://wandb.ai/seokjin/FSR-prediction/runs/314c8e84\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329636)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004415-314c8e84/logs\n",
      "2023-07-19 00:44:29,974\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.684 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:44:29,978\tWARNING util.py:315 -- The `process_trial_result` operation took 1.689 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:44:29,980\tWARNING util.py:315 -- Processing trial results took 1.690 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:44:29,981\tWARNING util.py:315 -- The `process_trial_result` operation took 1.691 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_af202444_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-44-08/wandb/run-20230719_004431-af202444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: Syncing run FSR_Trainable_af202444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/af202444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb:                      mae 266.31876\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb:                     mape 3.013985086537411e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb:                     rmse 486.62058\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb:       time_since_restore 3.38726\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb:         time_this_iter_s 3.38726\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb:             time_total_s 3.38726\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb:                timestamp 1689695068\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: 🚀 View run FSR_Trainable_af202444 at: https://wandb.ai/seokjin/FSR-prediction/runs/af202444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=329863)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004431-af202444/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb:                      mae █▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb:                     mape █▃▃▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb:                     rmse █▅▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb:         time_this_iter_s █▄▂▂▃▃▄▃▂▂▃▃▅▅▁▄▃▃▄▃▂▃▂▆▃▁▃▃▄▅▃▁▂▅▃▁▂▁▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004021-23f8d543/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004021-23f8d543/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004021-23f8d543/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004021-23f8d543/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004021-23f8d543/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004021-23f8d543/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004021-23f8d543/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004021-23f8d543/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004021-23f8d543/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004021-23f8d543/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004021-23f8d543/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004021-23f8d543/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004021-23f8d543/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=327540)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004021-23f8d543/logs\n",
      "2023-07-19 00:44:44,297\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.955 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:44:44,299\tWARNING util.py:315 -- The `process_trial_result` operation took 1.958 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:44:44,301\tWARNING util.py:315 -- Processing trial results took 1.960 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:44:44,303\tWARNING util.py:315 -- The `process_trial_result` operation took 1.961 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_4995aa5f_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-44-25/wandb/run-20230719_004447-4995aa5f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: Syncing run FSR_Trainable_4995aa5f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4995aa5f\n",
      "2023-07-19 00:44:58,631\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.929 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:44:58,634\tWARNING util.py:315 -- The `process_trial_result` operation took 1.932 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:44:58,636\tWARNING util.py:315 -- Processing trial results took 1.934 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:44:58,640\tWARNING util.py:315 -- The `process_trial_result` operation took 1.938 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_d001dc20_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-44-40/wandb/run-20230719_004500-d001dc20\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: Syncing run FSR_Trainable_d001dc20\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d001dc20\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb:                      mae █▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb:                     mape █▃▂▂▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb:                     rmse █▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb:         time_this_iter_s █▇▆▄▁▂▇▄▂▁▂▇▃▅▄▂▄▅▃▂▁▃▂▄▂▂▆▂▄▂▅▂▂▂▂▂▂▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb:                      mae 98.24538\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb:                     mape 7.667935307818296e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb:                     rmse 184.30399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb:       time_since_restore 247.01011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb:         time_this_iter_s 2.38616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb:             time_total_s 247.01011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb:                timestamp 1689695156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: 🚀 View run FSR_Trainable_ecda1e39 at: https://wandb.ai/seokjin/FSR-prediction/runs/ecda1e39\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328019)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004140-ecda1e39/logs\n",
      "2023-07-19 00:46:12,341\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.666 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:46:12,345\tWARNING util.py:315 -- The `process_trial_result` operation took 1.671 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:46:12,345\tWARNING util.py:315 -- Processing trial results took 1.672 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:46:12,346\tWARNING util.py:315 -- The `process_trial_result` operation took 1.673 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_2187bcf7_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-44-53/wandb/run-20230719_004614-2187bcf7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: Syncing run FSR_Trainable_2187bcf7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2187bcf7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb:                      mae █▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb:                     mape █▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb:                     rmse █▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb:         time_this_iter_s █▄▂▂▇▃▂▂▂▂▂▃▃▂▃▂▃▃▃▂▂▂▃▃▃▂▂▃▂▁▂▅▃▃▄▂▂▂▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb:                      mae 91.56382\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb:                     mape 6.0287043933142344e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb:                     rmse 179.17951\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb:       time_since_restore 104.50942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb:         time_this_iter_s 1.06806\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb:             time_total_s 104.50942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb:                timestamp 1689695193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: 🚀 View run FSR_Trainable_4995aa5f at: https://wandb.ai/seokjin/FSR-prediction/runs/4995aa5f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330095)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004447-4995aa5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb:                      mae █▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▃▃▃▃▂▃▂▂▂▂▁▂▂▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb:                     mape █▆▇▇▇▆▆▅▅▅▄▄▄▃▃▃▂▃▃▂▂▂▂▁▂▂▂▂▁▁▂▂▂▂▂▃▂▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb:                     rmse █▆▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▃▂▂▂▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb:         time_this_iter_s █▃▄▂▄▃▅▃▅▄▇▅▂▆▃▂▄▇▄▇▄▄▇▁▅█▅▂▃▃▃▃▃▃▃▂▅▃▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004156-e689db0e/logs\n",
      "2023-07-19 00:46:48,450\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.057 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:46:48,453\tWARNING util.py:315 -- The `process_trial_result` operation took 2.061 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:46:48,454\tWARNING util.py:315 -- Processing trial results took 2.063 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:46:48,455\tWARNING util.py:315 -- The `process_trial_result` operation took 2.064 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=328247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_1a247592_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-46-07/wandb/run-20230719_004650-1a247592\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: Syncing run FSR_Trainable_1a247592\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1a247592\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:46:57,306\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.902 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:46:57,311\tWARNING util.py:315 -- The `process_trial_result` operation took 1.907 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:46:57,312\tWARNING util.py:315 -- Processing trial results took 1.908 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:46:57,313\tWARNING util.py:315 -- The `process_trial_result` operation took 1.909 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb:                      mae 177.64355\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb:                     mape 2.1075434413184656e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb:                     rmse 329.30595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb:       time_since_restore 5.55614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb:         time_this_iter_s 2.52755\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb:             time_total_s 5.55614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb:                timestamp 1689695210\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: 🚀 View run FSR_Trainable_1a247592 at: https://wandb.ai/seokjin/FSR-prediction/runs/1a247592\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330833)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004650-1a247592/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_dfabbb69_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-46-43/wandb/run-20230719_004700-dfabbb69\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: Syncing run FSR_Trainable_dfabbb69\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/dfabbb69\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb:                      mae █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb:                     mape █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb:                     rmse █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb:         time_this_iter_s █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb:                      mae 135.70614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb:                     mape 1.0885287321801384e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb:                     rmse 255.12359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb:       time_since_restore 6.27456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb:         time_this_iter_s 1.39445\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb:             time_total_s 6.27456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb:                timestamp 1689695221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: 🚀 View run FSR_Trainable_dfabbb69 at: https://wandb.ai/seokjin/FSR-prediction/runs/dfabbb69\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331050)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004700-dfabbb69/logs\n",
      "2023-07-19 00:47:07,912\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.445 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:47:07,916\tWARNING util.py:315 -- The `process_trial_result` operation took 2.451 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:47:07,918\tWARNING util.py:315 -- Processing trial results took 2.452 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:47:07,919\tWARNING util.py:315 -- The `process_trial_result` operation took 2.454 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_38981ae1_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-46-53/wandb/run-20230719_004710-38981ae1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: Syncing run FSR_Trainable_38981ae1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/38981ae1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb:                      mae █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb:                     mape █▃▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb:                     rmse █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb:         time_this_iter_s █▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb:                timestamp ▁▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb:                      mae 137.63663\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb:                     mape 1.3831592189842378e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb:                     rmse 254.00881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb:       time_since_restore 5.73179\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb:         time_this_iter_s 1.27169\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb:             time_total_s 5.73179\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb:                timestamp 1689695231\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: 🚀 View run FSR_Trainable_38981ae1 at: https://wandb.ai/seokjin/FSR-prediction/runs/38981ae1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331278)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004710-38981ae1/logs\n",
      "2023-07-19 00:47:21,474\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.100 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:47:21,477\tWARNING util.py:315 -- The `process_trial_result` operation took 2.104 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:47:21,479\tWARNING util.py:315 -- Processing trial results took 2.105 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:47:21,481\tWARNING util.py:315 -- The `process_trial_result` operation took 2.107 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...503)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_a74bbb80_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-47-03/wandb/run-20230719_004721-a74bbb80\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Syncing run FSR_Trainable_a74bbb80\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a74bbb80\n",
      "2023-07-19 00:47:28,458\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.932 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:47:28,462\tWARNING util.py:315 -- The `process_trial_result` operation took 1.936 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:47:28,463\tWARNING util.py:315 -- Processing trial results took 1.938 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:47:28,465\tWARNING util.py:315 -- The `process_trial_result` operation took 1.939 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_462c1b68_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-47-14/wandb/run-20230719_004731-462c1b68\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: Syncing run FSR_Trainable_462c1b68\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/462c1b68\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb:                      mae 236.91595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb:                     mape 3.042943284443938e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb:                     rmse 430.89325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb:       time_since_restore 1.51616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb:         time_this_iter_s 1.51616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb:             time_total_s 1.51616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb:                timestamp 1689695246\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: 🚀 View run FSR_Trainable_462c1b68 at: https://wandb.ai/seokjin/FSR-prediction/runs/462c1b68\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331719)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004731-462c1b68/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb:                      mae █▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb:                     mape █▄▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb:                     rmse █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb:       time_since_restore ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb:         time_this_iter_s █▅▁▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb:             time_total_s ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb:                timestamp ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004721-a74bbb80/logs\n",
      "2023-07-19 00:47:42,642\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.709 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:47:42,645\tWARNING util.py:315 -- The `process_trial_result` operation took 1.713 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:47:42,647\tWARNING util.py:315 -- Processing trial results took 1.715 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:47:42,650\tWARNING util.py:315 -- The `process_trial_result` operation took 1.718 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331503)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_7609fbc4_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-47-25/wandb/run-20230719_004745-7609fbc4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: Syncing run FSR_Trainable_7609fbc4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7609fbc4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb:                      mae 266.98856\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb:                     mape 3.907559293324104e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb:                     rmse 450.20601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb:       time_since_restore 1.46004\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb:         time_this_iter_s 1.46004\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb:             time_total_s 1.46004\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb:                timestamp 1689695260\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: 🚀 View run FSR_Trainable_7609fbc4 at: https://wandb.ai/seokjin/FSR-prediction/runs/7609fbc4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004745-7609fbc4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=331956)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 00:47:53,800\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.844 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:47:53,803\tWARNING util.py:315 -- The `process_trial_result` operation took 1.849 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:47:53,805\tWARNING util.py:315 -- Processing trial results took 1.850 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:47:53,806\tWARNING util.py:315 -- The `process_trial_result` operation took 1.852 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_98d31824_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-47-39/wandb/run-20230719_004756-98d31824\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: Syncing run FSR_Trainable_98d31824\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/98d31824\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb:                      mae 216.87317\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb:                     mape 9243299495089038.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb:                     rmse 439.09307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb:       time_since_restore 1.36361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb:         time_this_iter_s 1.36361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb:             time_total_s 1.36361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb:                timestamp 1689695271\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: 🚀 View run FSR_Trainable_98d31824 at: https://wandb.ai/seokjin/FSR-prediction/runs/98d31824\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332175)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004756-98d31824/logs\n",
      "2023-07-19 00:48:06,767\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.815 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:48:06,769\tWARNING util.py:315 -- The `process_trial_result` operation took 1.818 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:48:06,772\tWARNING util.py:315 -- Processing trial results took 1.821 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:48:06,773\tWARNING util.py:315 -- The `process_trial_result` operation took 1.822 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_ea8d39b7_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-47-50/wandb/run-20230719_004808-ea8d39b7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: Syncing run FSR_Trainable_ea8d39b7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ea8d39b7\n",
      "2023-07-19 00:48:17,481\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.052 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:48:17,483\tWARNING util.py:315 -- The `process_trial_result` operation took 2.055 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:48:17,488\tWARNING util.py:315 -- Processing trial results took 2.060 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:48:17,490\tWARNING util.py:315 -- The `process_trial_result` operation took 2.062 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...615)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_cbeeb68c_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-48-01/wandb/run-20230719_004818-cbeeb68c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: Syncing run FSR_Trainable_cbeeb68c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cbeeb68c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb:                      mae █▇▇▇▇▆▆▆▅▅▅▄▄▄▄▅▄▃▃▃▃▃▃▂▂▂▃▂▂▂▂▂▁▁▂▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb:                     mape ▃▇███▇▇▆▆▅▅▄▄▄▄▅▃▃▃▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb:                     rmse █▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb:         time_this_iter_s █▅▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▄▅▄▄▃▄▅▂▂▄▂▂▃▃▃▄▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb:                      mae 99.75623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb:                     mape 7.57221123238279e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb:                     rmse 193.52883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb:       time_since_restore 241.09896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb:         time_this_iter_s 2.38223\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb:             time_total_s 241.09896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb:                timestamp 1689695349\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: 🚀 View run FSR_Trainable_d001dc20 at: https://wandb.ai/seokjin/FSR-prediction/runs/d001dc20\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330314)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004500-d001dc20/logs\n",
      "2023-07-19 00:49:25,747\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.229 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:49:25,750\tWARNING util.py:315 -- The `process_trial_result` operation took 2.233 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:49:25,754\tWARNING util.py:315 -- Processing trial results took 2.237 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:49:25,756\tWARNING util.py:315 -- The `process_trial_result` operation took 2.239 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_674faab7_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-48-12/wandb/run-20230719_004927-674faab7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: Syncing run FSR_Trainable_674faab7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/674faab7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb:                      mae █▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb:                     mape █▃▄▅▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb:                     rmse █▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb:         time_this_iter_s █▅▄▄▂▁▅▂▅▇▄▆▇▃▄▅▂▆▄▆▅▄▅▅▅▅▄▄▄▇▅▅▅▅▄▄▅▄▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb:                      mae 97.49445\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb:                     mape 7.361213139680278e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb:                     rmse 188.33753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb:       time_since_restore 246.26977\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb:         time_this_iter_s 2.47829\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb:             time_total_s 246.26977\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb:                timestamp 1689695430\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: 🚀 View run FSR_Trainable_2187bcf7 at: https://wandb.ai/seokjin/FSR-prediction/runs/2187bcf7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=330581)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004614-2187bcf7/logs\n",
      "2023-07-19 00:50:47,624\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.398 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:50:47,627\tWARNING util.py:315 -- The `process_trial_result` operation took 2.402 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:50:47,628\tWARNING util.py:315 -- Processing trial results took 2.403 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:50:47,630\tWARNING util.py:315 -- The `process_trial_result` operation took 2.404 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_c48c7ff2_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-49-20/wandb/run-20230719_005049-c48c7ff2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: Syncing run FSR_Trainable_c48c7ff2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c48c7ff2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb:                      mae █▇▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▃▃▄▃▃▃▃▃▃▂▃▂▃▃▂▃▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb:                     mape ██████▇▇▇▆▆▅▅▄▄▃▃▂▂▂▂▁▁▁▁▃▂▁▁▁▁▁▁▂▂▂▁▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb:                     rmse █▆▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▃▃▃▃▃▃▂▃▂▂▃▂▂▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb:         time_this_iter_s █▃▂▃▂▂▂▂▂▂▂▂▂▃▁▆▁▂▂▁▂▂▂▂▂▁▂▂▂▂▂▁▂▁▅▄▃▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb:                      mae 102.57407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb:                     mape 6.421166053698753e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb:                     rmse 212.18863\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb:       time_since_restore 171.93814\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb:         time_this_iter_s 2.75754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb:             time_total_s 171.93814\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb:                timestamp 1689695469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: 🚀 View run FSR_Trainable_cbeeb68c at: https://wandb.ai/seokjin/FSR-prediction/runs/cbeeb68c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332615)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004818-cbeeb68c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 00:51:30,532\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.305 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:51:30,538\tWARNING util.py:315 -- The `process_trial_result` operation took 3.311 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:51:30,539\tWARNING util.py:315 -- Processing trial results took 3.313 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:51:30,540\tWARNING util.py:315 -- The `process_trial_result` operation took 3.314 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_64a77c83_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-50-41/wandb/run-20230719_005129-64a77c83\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: Syncing run FSR_Trainable_64a77c83\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/64a77c83\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb:                      mae █▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb:                     mape █▁▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb:                     rmse █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb:         time_this_iter_s █▇▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb:                timestamp ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb:                      mae 136.98851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb:                     mape 1.0471939386501218e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb:                     rmse 258.55673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb:       time_since_restore 23.13221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb:         time_this_iter_s 5.62264\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb:             time_total_s 23.13221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb:                timestamp 1689695507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: 🚀 View run FSR_Trainable_64a77c83 at: https://wandb.ai/seokjin/FSR-prediction/runs/64a77c83\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333368)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005129-64a77c83/logs\n",
      "2023-07-19 00:52:08,286\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.030 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:52:08,289\tWARNING util.py:315 -- The `process_trial_result` operation took 2.034 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:52:08,290\tWARNING util.py:315 -- Processing trial results took 2.035 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:52:08,291\tWARNING util.py:315 -- The `process_trial_result` operation took 2.036 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_65d350c0_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-51-20/wandb/run-20230719_005208-65d350c0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: Syncing run FSR_Trainable_65d350c0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/65d350c0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb:                      mae 172.77327\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb:                     mape 1.9356579033446467e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb:                     rmse 318.32333\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb:       time_since_restore 11.23051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb:         time_this_iter_s 5.33866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb:             time_total_s 11.23051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb:                timestamp 1689695533\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: 🚀 View run FSR_Trainable_65d350c0 at: https://wandb.ai/seokjin/FSR-prediction/runs/65d350c0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333605)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005208-65d350c0/logs\n",
      "2023-07-19 00:52:32,722\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.892 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:52:32,725\tWARNING util.py:315 -- The `process_trial_result` operation took 1.895 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:52:32,726\tWARNING util.py:315 -- Processing trial results took 1.897 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:52:32,731\tWARNING util.py:315 -- The `process_trial_result` operation took 1.902 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_a454708c_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-52-00/wandb/run-20230719_005234-a454708c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: Syncing run FSR_Trainable_a454708c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a454708c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb:                      mae 177.00975\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb:                     mape 2.242111442119729e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb:                     rmse 316.8802\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb:       time_since_restore 6.90149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb:         time_this_iter_s 3.15589\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb:             time_total_s 6.90149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb:                timestamp 1689695555\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: 🚀 View run FSR_Trainable_a454708c at: https://wandb.ai/seokjin/FSR-prediction/runs/a454708c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333841)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005234-a454708c/logs\n",
      "2023-07-19 00:52:55,776\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.523 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:52:55,779\tWARNING util.py:315 -- The `process_trial_result` operation took 2.526 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:52:55,780\tWARNING util.py:315 -- Processing trial results took 2.527 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:52:55,781\tWARNING util.py:315 -- The `process_trial_result` operation took 2.528 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_b7b936d0_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-52-27/wandb/run-20230719_005257-b7b936d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Syncing run FSR_Trainable_b7b936d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b7b936d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb:                      mae █▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb:                     mape █▆██▇▇▆▆▅▅▅▄▄▄▅▄▃▃▃▃▃▃▂▃▃▂▂▂▂▂▁▂▂▁▁▁▁▁▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb:                     rmse █▆▅▅▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb:         time_this_iter_s ▅▂▂▂▂▂▂▂▂▂▁▅▃▂▂▂▂▁▁▂▁▁▂▃▁▂▂▂▄█▄█▃▆▄▄▆▄▃▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb:                      mae 93.75264\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb:                     mape 6.396610838921141e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb:                     rmse 186.36877\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb:       time_since_restore 283.31478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb:         time_this_iter_s 3.11851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb:             time_total_s 283.31478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb:                timestamp 1689695578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: 🚀 View run FSR_Trainable_ea8d39b7 at: https://wandb.ai/seokjin/FSR-prediction/runs/ea8d39b7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332398)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004808-ea8d39b7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb:                      mae █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb:                     mape █▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb:                     rmse █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb:         time_this_iter_s █▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb:                timestamp ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005257-b7b936d0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005257-b7b936d0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005257-b7b936d0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005257-b7b936d0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005257-b7b936d0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005257-b7b936d0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005257-b7b936d0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005257-b7b936d0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005257-b7b936d0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005257-b7b936d0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005257-b7b936d0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005257-b7b936d0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005257-b7b936d0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334067)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005257-b7b936d0/logs\n",
      "2023-07-19 00:53:17,755\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.799 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:53:17,764\tWARNING util.py:315 -- The `process_trial_result` operation took 2.810 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:53:17,766\tWARNING util.py:315 -- Processing trial results took 2.812 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:53:17,768\tWARNING util.py:315 -- The `process_trial_result` operation took 2.814 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_908222cf_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-52-48/wandb/run-20230719_005319-908222cf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: Syncing run FSR_Trainable_908222cf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/908222cf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb:                      mae 272.16561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb:                     mape 1.419243624093937e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb:                     rmse 567.6711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb:       time_since_restore 3.54459\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb:         time_this_iter_s 3.54459\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb:             time_total_s 3.54459\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb:                timestamp 1689695594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: 🚀 View run FSR_Trainable_908222cf at: https://wandb.ai/seokjin/FSR-prediction/runs/908222cf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334312)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005319-908222cf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 00:53:31,902\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.871 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:53:31,906\tWARNING util.py:315 -- The `process_trial_result` operation took 1.876 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:53:31,908\tWARNING util.py:315 -- Processing trial results took 1.878 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:53:31,909\tWARNING util.py:315 -- The `process_trial_result` operation took 1.879 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_93ed133c_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-53-11/wandb/run-20230719_005331-93ed133c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: Syncing run FSR_Trainable_93ed133c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/93ed133c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:53:41,693\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.875 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:53:41,698\tWARNING util.py:315 -- The `process_trial_result` operation took 2.881 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:53:41,701\tWARNING util.py:315 -- Processing trial results took 2.884 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:53:41,705\tWARNING util.py:315 -- The `process_trial_result` operation took 2.887 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb:                      mae 290.3267\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb:                     mape 1.5093548367163747e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb:                     rmse 577.44229\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb:       time_since_restore 6.6586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb:         time_this_iter_s 6.6586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb:             time_total_s 6.6586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb:                timestamp 1689695610\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: 🚀 View run FSR_Trainable_93ed133c at: https://wandb.ai/seokjin/FSR-prediction/runs/93ed133c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334533)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005331-93ed133c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_65ee2aa9_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-53-23/wandb/run-20230719_005343-65ee2aa9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: Syncing run FSR_Trainable_65ee2aa9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/65ee2aa9\n",
      "2023-07-19 00:53:54,752\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.529 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:53:54,755\tWARNING util.py:315 -- The `process_trial_result` operation took 2.533 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:53:54,756\tWARNING util.py:315 -- Processing trial results took 2.535 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:53:54,757\tWARNING util.py:315 -- The `process_trial_result` operation took 2.536 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_cc8c9340_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-53-35/wandb/run-20230719_005356-cc8c9340\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: Syncing run FSR_Trainable_cc8c9340\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cc8c9340\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb:                      mae █▆▆▆▆▅▅▅▅▅▅▅▅▅▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▃▂▂▂▂▁▁▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb:                     mape █▅▇▇▇▆▆▅▅▄▄▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▁▁▁▁▁▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb:                     rmse █▆▅▅▅▅▅▅▅▄▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb:         time_this_iter_s ▅▁▁▁▂▂▁▁▁▁▁▁▂▁▁▁▁▃▅▂▅▂▅▃▂▅▃▂▃▂▅▁▄▅▄▂▂▂█▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb:                      mae 88.76016\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb:                     mape 5.643402664975108e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb:                     rmse 182.8623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb:       time_since_restore 296.75085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb:         time_this_iter_s 3.35645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb:             time_total_s 296.75085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb:                timestamp 1689695672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: 🚀 View run FSR_Trainable_674faab7 at: https://wandb.ai/seokjin/FSR-prediction/runs/674faab7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=332870)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_004927-674faab7/logs\n",
      "2023-07-19 00:54:51,897\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.879 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:54:51,900\tWARNING util.py:315 -- The `process_trial_result` operation took 2.883 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:54:51,903\tWARNING util.py:315 -- Processing trial results took 2.886 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:54:51,905\tWARNING util.py:315 -- The `process_trial_result` operation took 2.889 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_dea50f73_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-53-48/wandb/run-20230719_005453-dea50f73\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Syncing run FSR_Trainable_dea50f73\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/dea50f73\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb:                      mae █▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb:                     mape █▄▅▅▅▅▅▄▄▄▃▃▄▃▃▂▂▃▄▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb:                     rmse █▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb:         time_this_iter_s ▅▂▂▂▁▁▆▃▆▄▆▆▃▅▄▂▃▃▁▁▅▆▃▅▃▄█▅▃▆▃▃▄▄▅▃▄▃▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb:                      mae 89.62403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb:                     mape 6.126267910258165e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb:                     rmse 180.73357\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb:       time_since_restore 312.06656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb:         time_this_iter_s 3.15327\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb:             time_total_s 312.06656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb:                timestamp 1689695770\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: 🚀 View run FSR_Trainable_c48c7ff2 at: https://wandb.ai/seokjin/FSR-prediction/runs/c48c7ff2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=333129)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005049-c48c7ff2/logs\n",
      "2023-07-19 00:56:30,463\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.677 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:56:30,469\tWARNING util.py:315 -- The `process_trial_result` operation took 2.683 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:56:30,471\tWARNING util.py:315 -- Processing trial results took 2.685 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:56:30,472\tWARNING util.py:315 -- The `process_trial_result` operation took 2.687 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_56bd6189_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-54-45/wandb/run-20230719_005632-56bd6189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: Syncing run FSR_Trainable_56bd6189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/56bd6189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb:                      mae █▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▃▃▂▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb:                     mape █▃▃▄▄▄▄▄▄▃▃▃▃▃▃▃▂▃▂▃▃▃▂▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb:                     rmse █▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb:         time_this_iter_s ▇▄▄▁▃█▁▁▃▃▂▃▂▅▂▁▁▂▂▂▅▄▄▃▃▂▂▁▂▂▆▃▂▂▂▂▂▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb:                      mae 97.9621\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb:                     mape 7.230456102552712e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb:                     rmse 189.09033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb:       time_since_restore 315.88317\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb:         time_this_iter_s 2.91361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb:             time_total_s 315.88317\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb:                timestamp 1689695945\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: 🚀 View run FSR_Trainable_65ee2aa9 at: https://wandb.ai/seokjin/FSR-prediction/runs/65ee2aa9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334760)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005343-65ee2aa9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb:                      mae ██▇▇▇▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▅▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb:                     mape ▄▇██▇▆▆▅▄▄▄▄▄▃▃▃▃▂▂▃▃▅▄▃▂▂▂▂▂▂▂▃▂▁▁▁▁▁▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb:                     rmse █▆▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb:         time_this_iter_s █▄▁▄▃▂▂▄▃▂▂▃▅▃▃▄▂▃▂▄▅▃▄▃▃▃▆▂▇▆▄▃▃▄▃▃▃▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb:                      mae 93.70299\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb:                     mape 6.515112678574333e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb:                     rmse 185.94989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb:       time_since_restore 315.48394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb:         time_this_iter_s 2.96222\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb:             time_total_s 315.48394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb:                timestamp 1689695954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: 🚀 View run FSR_Trainable_cc8c9340 at: https://wandb.ai/seokjin/FSR-prediction/runs/cc8c9340\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=334986)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005356-cc8c9340/logs\n",
      "2023-07-19 00:59:23,380\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.440 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:59:23,385\tWARNING util.py:315 -- The `process_trial_result` operation took 2.444 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:59:23,386\tWARNING util.py:315 -- Processing trial results took 2.446 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:59:23,391\tWARNING util.py:315 -- The `process_trial_result` operation took 2.451 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...799)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_ab5b2886_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-56-23/wandb/run-20230719_005925-ab5b2886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Syncing run FSR_Trainable_ab5b2886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ab5b2886\n",
      "2023-07-19 00:59:38,429\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.132 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:59:38,432\tWARNING util.py:315 -- The `process_trial_result` operation took 2.136 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:59:38,434\tWARNING util.py:315 -- Processing trial results took 2.137 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:59:38,435\tWARNING util.py:315 -- The `process_trial_result` operation took 2.138 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...019)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_9078fd24_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-59-17/wandb/run-20230719_005938-9078fd24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: Syncing run FSR_Trainable_9078fd24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9078fd24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb:                      mae 168.34426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb:                     mape 2.4936472311935144e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb:                     rmse 338.49011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb:       time_since_restore 11.39537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb:         time_this_iter_s 5.36747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb:             time_total_s 11.39537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb:                timestamp 1689695983\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: 🚀 View run FSR_Trainable_9078fd24 at: https://wandb.ai/seokjin/FSR-prediction/runs/9078fd24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336019)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005938-9078fd24/logs\n",
      "2023-07-19 01:00:09,104\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.169 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:00:09,106\tWARNING util.py:315 -- The `process_trial_result` operation took 2.172 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:00:09,108\tWARNING util.py:315 -- Processing trial results took 2.174 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:00:09,109\tWARNING util.py:315 -- The `process_trial_result` operation took 2.175 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_fe8e305d_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-59-30/wandb/run-20230719_010009-fe8e305d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: Syncing run FSR_Trainable_fe8e305d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fe8e305d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb:                      mae 167.75227\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb:                     mape 2.7710347206352844e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb:                     rmse 328.72902\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb:       time_since_restore 12.73209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb:         time_this_iter_s 5.10239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb:             time_total_s 12.73209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb:                timestamp 1689696014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: 🚀 View run FSR_Trainable_fe8e305d at: https://wandb.ai/seokjin/FSR-prediction/runs/fe8e305d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336250)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_010009-fe8e305d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb:                      mae █▅▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb:                     mape █▃▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▃▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb:                     rmse █▅▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb:         time_this_iter_s █▃▂▄▂▅▂▃▂▂▃▃▅▄▅▃▃▂▄▂▂▂▄▂▂▂▃▃▃▃▂▃▃▁▃▄▅▆▆▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005453-dea50f73/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005453-dea50f73/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005453-dea50f73/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005453-dea50f73/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005453-dea50f73/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005453-dea50f73/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005453-dea50f73/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005453-dea50f73/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005453-dea50f73/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005453-dea50f73/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005453-dea50f73/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005453-dea50f73/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005453-dea50f73/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005453-dea50f73/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335229)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005453-dea50f73/logs\n",
      "2023-07-19 01:00:31,949\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.557 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:00:31,953\tWARNING util.py:315 -- The `process_trial_result` operation took 2.562 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:00:31,954\tWARNING util.py:315 -- Processing trial results took 2.564 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:00:31,960\tWARNING util.py:315 -- The `process_trial_result` operation took 2.570 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_21e9f9c9_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-59-59/wandb/run-20230719_010033-21e9f9c9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: Syncing run FSR_Trainable_21e9f9c9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/21e9f9c9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb:                      mae 204.4973\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb:                     mape 2.298959470109511e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb:                     rmse 395.23151\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb:       time_since_restore 3.67308\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb:         time_this_iter_s 3.67308\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb:             time_total_s 3.67308\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb:                timestamp 1689696029\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: 🚀 View run FSR_Trainable_21e9f9c9 at: https://wandb.ai/seokjin/FSR-prediction/runs/21e9f9c9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336486)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_010033-21e9f9c9/logs\n",
      "2023-07-19 01:00:41,531\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.302 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:00:41,535\tWARNING util.py:315 -- The `process_trial_result` operation took 2.306 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:00:41,537\tWARNING util.py:315 -- Processing trial results took 2.308 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:00:41,541\tWARNING util.py:315 -- The `process_trial_result` operation took 2.313 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_3e144bba_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-00-25/wandb/run-20230719_010045-3e144bba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: Syncing run FSR_Trainable_3e144bba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3e144bba\n",
      "2023-07-19 01:00:55,840\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.141 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:00:55,844\tWARNING util.py:315 -- The `process_trial_result` operation took 2.146 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:00:55,846\tWARNING util.py:315 -- Processing trial results took 2.147 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:00:55,847\tWARNING util.py:315 -- The `process_trial_result` operation took 2.149 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_93a5f695_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-00-37/wandb/run-20230719_010059-93a5f695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: Syncing run FSR_Trainable_93a5f695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/93a5f695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb:                      mae 287.98731\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb:                     mape 3.6634205344304275e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb:                     rmse 516.14329\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb:       time_since_restore 1.60008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb:         time_this_iter_s 1.60008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb:             time_total_s 1.60008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb:                timestamp 1689696053\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: 🚀 View run FSR_Trainable_93a5f695 at: https://wandb.ai/seokjin/FSR-prediction/runs/93a5f695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336926)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_010059-93a5f695/logs\n",
      "2023-07-19 01:01:12,991\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.258 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:01:12,993\tWARNING util.py:315 -- The `process_trial_result` operation took 2.262 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:01:12,995\tWARNING util.py:315 -- Processing trial results took 2.264 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:01:12,997\tWARNING util.py:315 -- The `process_trial_result` operation took 2.266 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_6b790f67_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-00-52/wandb/run-20230719_010116-6b790f67\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: Syncing run FSR_Trainable_6b790f67\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6b790f67\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb:                      mae █▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb:                     mape █▂▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb:                     rmse █▅▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb:         time_this_iter_s █▅▄▄▄▃▄▃▃▄▇▄▄▄▄▄▄▄▃▃▄▇▄▅▃▇▇▄▁▄▅▃▄▃▅▂▃▃▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb:                      mae 96.01792\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb:                     mape 7.207552290094003e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb:                     rmse 184.68711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb:       time_since_restore 314.94995\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb:         time_this_iter_s 3.02554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb:             time_total_s 314.94995\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb:                timestamp 1689696114\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: 🚀 View run FSR_Trainable_56bd6189 at: https://wandb.ai/seokjin/FSR-prediction/runs/56bd6189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335494)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005632-56bd6189/logs\n",
      "2023-07-19 01:02:15,823\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.237 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:02:15,827\tWARNING util.py:315 -- The `process_trial_result` operation took 2.242 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:02:15,830\tWARNING util.py:315 -- Processing trial results took 2.244 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:02:15,834\tWARNING util.py:315 -- The `process_trial_result` operation took 2.249 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_172e5cf3_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-01-09/wandb/run-20230719_010217-172e5cf3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: Syncing run FSR_Trainable_172e5cf3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/172e5cf3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb:                      mae █▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb:                     mape █▆▆▆▅▅▅▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb:                     rmse █▅▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb:         time_this_iter_s █▄▃▂▅▄▃▂▂▄▃▃▁▂▃▁▂▂▃▂▃▂▂▂▁▅▄▁▆▆▄▃▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb:                      mae 107.31702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb:                     mape 8.475882369587242e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb:                     rmse 202.18592\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb:       time_since_restore 115.91387\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb:         time_this_iter_s 1.12075\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb:             time_total_s 115.91387\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb:                timestamp 1689696166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: 🚀 View run FSR_Trainable_3e144bba at: https://wandb.ai/seokjin/FSR-prediction/runs/3e144bba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=336707)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_010045-3e144bba/logs\n",
      "2023-07-19 01:03:05,235\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.979 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:03:05,237\tWARNING util.py:315 -- The `process_trial_result` operation took 1.981 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:03:05,238\tWARNING util.py:315 -- Processing trial results took 1.982 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:03:05,239\tWARNING util.py:315 -- The `process_trial_result` operation took 1.983 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_6eec31ef_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-02-09/wandb/run-20230719_010307-6eec31ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Syncing run FSR_Trainable_6eec31ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6eec31ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb:                      mae █▅▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb:                     mape █▅▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb:                     rmse █▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb:         time_this_iter_s ▇▃▂▂▂▂▃▂▁▂▂▃▂▂▃▁▆▄▄▃▃▃▂▃▂▁▂▂▁▂▂▂▃▂▃▂▅█▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb:                      mae 97.67506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb:                     mape 8.679919178356155e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb:                     rmse 182.5182\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb:       time_since_restore 114.56424\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb:         time_this_iter_s 1.10819\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb:             time_total_s 114.56424\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb:                timestamp 1689696194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: 🚀 View run FSR_Trainable_6b790f67 at: https://wandb.ai/seokjin/FSR-prediction/runs/6b790f67\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337156)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_010116-6b790f67/logs\n",
      "2023-07-19 01:03:32,577\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.967 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:03:32,579\tWARNING util.py:315 -- The `process_trial_result` operation took 1.971 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:03:32,580\tWARNING util.py:315 -- Processing trial results took 1.972 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:03:32,582\tWARNING util.py:315 -- The `process_trial_result` operation took 1.973 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_56a35d41_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-02-59/wandb/run-20230719_010334-56a35d41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: Syncing run FSR_Trainable_56a35d41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/56a35d41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb:                      mae █▅▄▄▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb:                     mape ▅▁▆██▇▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb:                     rmse █▄▃▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb:         time_this_iter_s ▅▁▁█▆▅▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb:                      mae 129.68789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb:                     mape 1.1272925069007602e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb:                     rmse 241.6805\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb:       time_since_restore 29.21805\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb:         time_this_iter_s 3.33413\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb:             time_total_s 29.21805\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb:                timestamp 1689696238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: 🚀 View run FSR_Trainable_56a35d41 at: https://wandb.ai/seokjin/FSR-prediction/runs/56a35d41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337905)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_010334-56a35d41/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb:                      mae █▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb:                     mape ▅▁▄▇███▇▇▆▆▅▅▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb:                     rmse █▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▄▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb:         time_this_iter_s ▅▃▁▁▁▂▁▂▃▃▂█▆▄▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▄▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb:                timestamp ▁▂▂▃▃▃▄▄▄▅▅▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "2023-07-19 01:04:13,892\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.633 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:04:13,896\tWARNING util.py:315 -- The `process_trial_result` operation took 2.638 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:04:13,898\tWARNING util.py:315 -- Processing trial results took 2.640 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:04:13,900\tWARNING util.py:315 -- The `process_trial_result` operation took 2.642 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_f6c987f1_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_01-03-27/wandb/run-20230719_010417-f6c987f1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: Syncing run FSR_Trainable_f6c987f1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f6c987f1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb:                      mae 418.48882\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb:                     mape 7.734409626606829e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb:                     rmse 563.50239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb:       time_since_restore 1.54014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb:         time_this_iter_s 1.54014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb:             time_total_s 1.54014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb:                timestamp 1689696251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: 🚀 View run FSR_Trainable_f6c987f1 at: https://wandb.ai/seokjin/FSR-prediction/runs/f6c987f1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338156)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_010417-f6c987f1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 01:04:26,999\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.884 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:04:27,003\tWARNING util.py:315 -- The `process_trial_result` operation took 1.889 s, which may be a performance bottleneck.\n",
      "2023-07-19 01:04:27,005\tWARNING util.py:315 -- Processing trial results took 1.891 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 01:04:27,011\tWARNING util.py:315 -- The `process_trial_result` operation took 1.897 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb:                      mae █▄▄▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb:                     mape █▂▄▅▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb:                     rmse █▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb:         time_this_iter_s ▆▃▂▃▅█▃▄▂▃▄▃▁▃▃▃▃▂▃▃▃▅▃▄▃▃▂▃▃▃▃▃▃▂█▇▄▂▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005925-ab5b2886/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_005925-ab5b2886/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=335799)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_00-13-04/FSR_Trainable_c68eded0_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y_2023-07-19_01-04-09/wandb/run-20230719_010429-c68eded0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: Syncing run FSR_Trainable_c68eded0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c68eded0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb:                      mae 483.97656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb:                     mape 6.374175401324214e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb:                     rmse 739.79465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb:       time_since_restore 1.67645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb:         time_this_iter_s 1.67645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb:             time_total_s 1.67645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb:                timestamp 1689696265\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: 🚀 View run FSR_Trainable_c68eded0 at: https://wandb.ai/seokjin/FSR-prediction/runs/c68eded0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_010429-c68eded0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=338383)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb:                      mae █▆▆▅▅▅▅▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb:                     mape █▆▇▇▇▇▇▆▆▅▅▅▅▅▄▄▄▃▃▃▃▃▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb:                     rmse █▆▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇█████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb:         time_this_iter_s ▆▅▄▄▄▄▄▄▅▄▅▆▄▅▄▅▅█▆▅▄▄▃▅▃▄▂▁▁▂▁▁▁▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇█████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇█████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb:                      mae 104.00892\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb:                     mape 6.8015823664808744e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb:                     rmse 207.21354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb:       time_since_restore 169.31199\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb:         time_this_iter_s 1.6194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb:             time_total_s 169.31199\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb:                timestamp 1689696309\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: 🚀 View run FSR_Trainable_172e5cf3 at: https://wandb.ai/seokjin/FSR-prediction/runs/172e5cf3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=337415)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_010217-172e5cf3/logs\n",
      "2023-07-19 01:05:14,631\tINFO tune.py:1111 -- Total run time: 3126.43 seconds (3121.24 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
