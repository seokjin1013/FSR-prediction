{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task6\n",
    "\n",
    "Index_X = FSR_for_coord\n",
    "\n",
    "Index_y = x_coord, y_coord\n",
    "\n",
    "Data = Splited by Subject\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-07-19_13-03-12/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-07-19_13-03-12\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "0.6782"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.CNN_LSTM'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    imputer = trial.suggest_categorical('imputer', ['sklearn.impute.SimpleImputer'])\n",
    "    if imputer == 'sklearn.impute.SimpleImputer':\n",
    "        trial.suggest_categorical('imputer_args/strategy', [\n",
    "            'mean',\n",
    "            'median',\n",
    "        ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': 'FSR_for_coord',\n",
    "        'index_y': ['x_coord', 'y_coord'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_subject'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-19 13:03:12,252] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 13:03:14,623\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8268 \u001b[39m\u001b[22m\n",
      "2023-07-19 13:03:16,679\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-19 13:42:12</td></tr>\n",
       "<tr><td>Running for: </td><td>00:38:55.36        </td></tr>\n",
       "<tr><td>Memory:      </td><td>4.5/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=100<br>Bracket: Iter 64.000: -0.6944833369639758 | Iter 32.000: -0.6828532556024751 | Iter 16.000: -0.6830677113315693 | Iter 8.000: -0.6831276638568653 | Iter 4.000: -0.6846248710427547 | Iter 2.000: -0.6865369259177267 | Iter 1.000: -0.6885016520368619<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>imputer             </th><th>imputer_args/strateg\n",
       "y       </th><th>index_X      </th><th>index_y             </th><th>model             </th><th style=\"text-align: right;\">    model_args/cnn_hidde\n",
       "n_size</th><th style=\"text-align: right;\">  model_args/cnn_num_l\n",
       "ayer</th><th style=\"text-align: right;\">    model_args/lstm_hidd\n",
       "en_size</th><th style=\"text-align: right;\">  model_args/lstm_num_\n",
       "layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">     mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_1be8c882</td><td>TERMINATED</td><td>172.26.215.93:609734</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f440</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0408592  </td><td>sklearn.preproc_4510</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       174.216  </td><td style=\"text-align: right;\">1.4078  </td><td style=\"text-align: right;\">1.10446 </td><td style=\"text-align: right;\">0.248382 </td></tr>\n",
       "<tr><td>FSR_Trainable_6c1193b9</td><td>TERMINATED</td><td>172.26.215.93:609807</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1dc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.04675e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        71.9059 </td><td style=\"text-align: right;\">0.722471</td><td style=\"text-align: right;\">0.371832</td><td style=\"text-align: right;\">0.0941027</td></tr>\n",
       "<tr><td>FSR_Trainable_e1f18e57</td><td>TERMINATED</td><td>172.26.215.93:609985</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6a80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00177633 </td><td>sklearn.preproc_4510</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.23355</td><td style=\"text-align: right;\">2.49637 </td><td style=\"text-align: right;\">2.20098 </td><td style=\"text-align: right;\">0.329819 </td></tr>\n",
       "<tr><td>FSR_Trainable_2216a047</td><td>TERMINATED</td><td>172.26.215.93:610161</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8140</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000755354</td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       256.564  </td><td style=\"text-align: right;\">0.705195</td><td style=\"text-align: right;\">0.378961</td><td style=\"text-align: right;\">0.103032 </td></tr>\n",
       "<tr><td>FSR_Trainable_72fcdddb</td><td>TERMINATED</td><td>172.26.215.93:610458</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2440</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000511965</td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       156.559  </td><td style=\"text-align: right;\">0.704366</td><td style=\"text-align: right;\">0.388828</td><td style=\"text-align: right;\">0.102754 </td></tr>\n",
       "<tr><td>FSR_Trainable_6f8be931</td><td>TERMINATED</td><td>172.26.215.93:610789</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5480</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.078429   </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.04952</td><td style=\"text-align: right;\">0.973249</td><td style=\"text-align: right;\">0.723687</td><td style=\"text-align: right;\">0.173355 </td></tr>\n",
       "<tr><td>FSR_Trainable_40a99b6a</td><td>TERMINATED</td><td>172.26.215.93:610998</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__aa00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00311469 </td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        27.2351 </td><td style=\"text-align: right;\">0.712842</td><td style=\"text-align: right;\">0.417157</td><td style=\"text-align: right;\">0.105536 </td></tr>\n",
       "<tr><td>FSR_Trainable_7031dca4</td><td>TERMINATED</td><td>172.26.215.93:611310</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__aa00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">7</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0234534  </td><td>sklearn.preproc_4510</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.30136</td><td style=\"text-align: right;\">0.860102</td><td style=\"text-align: right;\">0.583829</td><td style=\"text-align: right;\">0.126196 </td></tr>\n",
       "<tr><td>FSR_Trainable_201cb996</td><td>TERMINATED</td><td>172.26.215.93:611525</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__a240</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.063066   </td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         5.81885</td><td style=\"text-align: right;\">0.769074</td><td style=\"text-align: right;\">0.495759</td><td style=\"text-align: right;\">0.124988 </td></tr>\n",
       "<tr><td>FSR_Trainable_d12a5617</td><td>TERMINATED</td><td>172.26.215.93:611789</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__55c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00154923 </td><td>sklearn.preproc_4510</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.8002 </td><td style=\"text-align: right;\">0.762207</td><td style=\"text-align: right;\">0.486168</td><td style=\"text-align: right;\">0.121928 </td></tr>\n",
       "<tr><td>FSR_Trainable_cacdc1e0</td><td>TERMINATED</td><td>172.26.215.93:611983</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6080</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0842409  </td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.46733</td><td style=\"text-align: right;\">0.84685 </td><td style=\"text-align: right;\">0.562504</td><td style=\"text-align: right;\">0.134783 </td></tr>\n",
       "<tr><td>FSR_Trainable_91564664</td><td>TERMINATED</td><td>172.26.215.93:612229</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0300</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0139294  </td><td>sklearn.preproc_4510</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.6666 </td><td style=\"text-align: right;\">0.769684</td><td style=\"text-align: right;\">0.46243 </td><td style=\"text-align: right;\">0.126266 </td></tr>\n",
       "<tr><td>FSR_Trainable_b861e4d8</td><td>TERMINATED</td><td>172.26.215.93:612451</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__a680</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000116034</td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.98632</td><td style=\"text-align: right;\">0.704997</td><td style=\"text-align: right;\">0.369218</td><td style=\"text-align: right;\">0.100835 </td></tr>\n",
       "<tr><td>FSR_Trainable_56a0d87a</td><td>TERMINATED</td><td>172.26.215.93:612664</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9440</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000138727</td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.21215</td><td style=\"text-align: right;\">0.72016 </td><td style=\"text-align: right;\">0.393843</td><td style=\"text-align: right;\">0.102189 </td></tr>\n",
       "<tr><td>FSR_Trainable_df9a5432</td><td>TERMINATED</td><td>172.26.215.93:612938</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3c00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000186129</td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       136.132  </td><td style=\"text-align: right;\">0.715756</td><td style=\"text-align: right;\">0.397334</td><td style=\"text-align: right;\">0.103529 </td></tr>\n",
       "<tr><td>FSR_Trainable_17f5cc0f</td><td>TERMINATED</td><td>172.26.215.93:613153</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__fb80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000283794</td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       133.422  </td><td style=\"text-align: right;\">0.719107</td><td style=\"text-align: right;\">0.40942 </td><td style=\"text-align: right;\">0.104005 </td></tr>\n",
       "<tr><td>FSR_Trainable_b7290c01</td><td>TERMINATED</td><td>172.26.215.93:613370</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2640</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000189747</td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         6.16707</td><td style=\"text-align: right;\">0.696397</td><td style=\"text-align: right;\">0.346283</td><td style=\"text-align: right;\">0.097646 </td></tr>\n",
       "<tr><td>FSR_Trainable_23e8fc85</td><td>TERMINATED</td><td>172.26.215.93:613653</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8d00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000273985</td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       136.03   </td><td style=\"text-align: right;\">0.711553</td><td style=\"text-align: right;\">0.401411</td><td style=\"text-align: right;\">0.102956 </td></tr>\n",
       "<tr><td>FSR_Trainable_ee29d0c7</td><td>TERMINATED</td><td>172.26.215.93:613906</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__52c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000411769</td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       125.787  </td><td style=\"text-align: right;\">0.721667</td><td style=\"text-align: right;\">0.418753</td><td style=\"text-align: right;\">0.107084 </td></tr>\n",
       "<tr><td>FSR_Trainable_9bf4d6f5</td><td>TERMINATED</td><td>172.26.215.93:614255</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7e80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000343122</td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.01144</td><td style=\"text-align: right;\">0.695342</td><td style=\"text-align: right;\">0.352984</td><td style=\"text-align: right;\">0.0968374</td></tr>\n",
       "<tr><td>FSR_Trainable_e248c83f</td><td>TERMINATED</td><td>172.26.215.93:614464</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4300</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        3.57139e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        10.7055 </td><td style=\"text-align: right;\">0.685834</td><td style=\"text-align: right;\">0.326482</td><td style=\"text-align: right;\">0.0954243</td></tr>\n",
       "<tr><td>FSR_Trainable_8a8d86ae</td><td>TERMINATED</td><td>172.26.215.93:614683</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__cac0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        4.80257e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         4.75465</td><td style=\"text-align: right;\">0.693819</td><td style=\"text-align: right;\">0.341042</td><td style=\"text-align: right;\">0.0968842</td></tr>\n",
       "<tr><td>FSR_Trainable_cb005763</td><td>TERMINATED</td><td>172.26.215.93:614910</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c340</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        5.19558e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       167.586  </td><td style=\"text-align: right;\">0.68067 </td><td style=\"text-align: right;\">0.325147</td><td style=\"text-align: right;\">0.0933304</td></tr>\n",
       "<tr><td>FSR_Trainable_4dc69380</td><td>TERMINATED</td><td>172.26.215.93:615167</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b640</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        4.35493e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         7.8028 </td><td style=\"text-align: right;\">0.687445</td><td style=\"text-align: right;\">0.342421</td><td style=\"text-align: right;\">0.099452 </td></tr>\n",
       "<tr><td>FSR_Trainable_53e5695a</td><td>TERMINATED</td><td>172.26.215.93:615392</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0880</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        4.21593e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.82354</td><td style=\"text-align: right;\">0.695951</td><td style=\"text-align: right;\">0.356929</td><td style=\"text-align: right;\">0.104922 </td></tr>\n",
       "<tr><td>FSR_Trainable_b642f7cd</td><td>TERMINATED</td><td>172.26.215.93:615618</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f7c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        2.967e-05  </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         7.16573</td><td style=\"text-align: right;\">0.686713</td><td style=\"text-align: right;\">0.336978</td><td style=\"text-align: right;\">0.0965921</td></tr>\n",
       "<tr><td>FSR_Trainable_27b00b7b</td><td>TERMINATED</td><td>172.26.215.93:615854</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9e00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.67395e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        21.3562 </td><td style=\"text-align: right;\">0.685259</td><td style=\"text-align: right;\">0.321025</td><td style=\"text-align: right;\">0.0944174</td></tr>\n",
       "<tr><td>FSR_Trainable_d82e72f0</td><td>TERMINATED</td><td>172.26.215.93:616083</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7200</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.33267e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.65916</td><td style=\"text-align: right;\">0.699165</td><td style=\"text-align: right;\">0.37913 </td><td style=\"text-align: right;\">0.104066 </td></tr>\n",
       "<tr><td>FSR_Trainable_888f5e28</td><td>TERMINATED</td><td>172.26.215.93:616307</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ae80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.37037e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         5.78847</td><td style=\"text-align: right;\">0.693203</td><td style=\"text-align: right;\">0.34801 </td><td style=\"text-align: right;\">0.0958057</td></tr>\n",
       "<tr><td>FSR_Trainable_f1e9ab93</td><td>TERMINATED</td><td>172.26.215.93:616537</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ab40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.00244e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.84093</td><td style=\"text-align: right;\">0.721391</td><td style=\"text-align: right;\">0.355831</td><td style=\"text-align: right;\">0.0886361</td></tr>\n",
       "<tr><td>FSR_Trainable_648cb2e4</td><td>TERMINATED</td><td>172.26.215.93:616777</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4640</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        2.67039e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.73161</td><td style=\"text-align: right;\">0.735292</td><td style=\"text-align: right;\">0.358426</td><td style=\"text-align: right;\">0.0846428</td></tr>\n",
       "<tr><td>FSR_Trainable_febdd3c6</td><td>TERMINATED</td><td>172.26.215.93:617004</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0340</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        2.52714e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        11.9135 </td><td style=\"text-align: right;\">0.690255</td><td style=\"text-align: right;\">0.333953</td><td style=\"text-align: right;\">0.0951635</td></tr>\n",
       "<tr><td>FSR_Trainable_e0453ec9</td><td>TERMINATED</td><td>172.26.215.93:617237</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b540</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        9.25359e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        12.5651 </td><td style=\"text-align: right;\">0.687832</td><td style=\"text-align: right;\">0.328992</td><td style=\"text-align: right;\">0.0931072</td></tr>\n",
       "<tr><td>FSR_Trainable_5459c42c</td><td>TERMINATED</td><td>172.26.215.93:617463</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b9c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        7.8577e-05 </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         6.46179</td><td style=\"text-align: right;\">0.69382 </td><td style=\"text-align: right;\">0.344138</td><td style=\"text-align: right;\">0.0938128</td></tr>\n",
       "<tr><td>FSR_Trainable_7079193b</td><td>TERMINATED</td><td>172.26.215.93:617693</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__cf80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        6.87027e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        14.7206 </td><td style=\"text-align: right;\">0.685601</td><td style=\"text-align: right;\">0.342439</td><td style=\"text-align: right;\">0.0947091</td></tr>\n",
       "<tr><td>FSR_Trainable_7525721c</td><td>TERMINATED</td><td>172.26.215.93:617929</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4140</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        3.93382e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         9.0377 </td><td style=\"text-align: right;\">0.686098</td><td style=\"text-align: right;\">0.33757 </td><td style=\"text-align: right;\">0.0948475</td></tr>\n",
       "<tr><td>FSR_Trainable_ac099330</td><td>TERMINATED</td><td>172.26.215.93:618156</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b1c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        4.01816e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       268.857  </td><td style=\"text-align: right;\">0.678305</td><td style=\"text-align: right;\">0.347321</td><td style=\"text-align: right;\">0.0963954</td></tr>\n",
       "<tr><td>FSR_Trainable_a66aa590</td><td>TERMINATED</td><td>172.26.215.93:618393</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c100</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.76969e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        47.9275 </td><td style=\"text-align: right;\">0.686011</td><td style=\"text-align: right;\">0.319823</td><td style=\"text-align: right;\">0.0944652</td></tr>\n",
       "<tr><td>FSR_Trainable_0641d810</td><td>TERMINATED</td><td>172.26.215.93:618616</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0380</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        6.40486e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        89.3207 </td><td style=\"text-align: right;\">0.689952</td><td style=\"text-align: right;\">0.362263</td><td style=\"text-align: right;\">0.100818 </td></tr>\n",
       "<tr><td>FSR_Trainable_74b5da82</td><td>TERMINATED</td><td>172.26.215.93:618911</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c980</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.91888e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        45.1268 </td><td style=\"text-align: right;\">0.686334</td><td style=\"text-align: right;\">0.315903</td><td style=\"text-align: right;\">0.0933294</td></tr>\n",
       "<tr><td>FSR_Trainable_986747c2</td><td>TERMINATED</td><td>172.26.215.93:619102</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__68c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        1.96172e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      1019.8    </td><td style=\"text-align: right;\">0.682384</td><td style=\"text-align: right;\">0.309915</td><td style=\"text-align: right;\">0.0935208</td></tr>\n",
       "<tr><td>FSR_Trainable_768e63a9</td><td>TERMINATED</td><td>172.26.215.93:619409</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0e80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        5.70121e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       446.547  </td><td style=\"text-align: right;\">0.692906</td><td style=\"text-align: right;\">0.367735</td><td style=\"text-align: right;\">0.0982551</td></tr>\n",
       "<tr><td>FSR_Trainable_a278091f</td><td>TERMINATED</td><td>172.26.215.93:619593</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8380</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">8</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        6.61087e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.3222 </td><td style=\"text-align: right;\">0.726559</td><td style=\"text-align: right;\">0.340509</td><td style=\"text-align: right;\">0.0859608</td></tr>\n",
       "<tr><td>FSR_Trainable_60a505df</td><td>TERMINATED</td><td>172.26.215.93:619890</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b140</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">8</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        7.3247e-05 </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        12.3537 </td><td style=\"text-align: right;\">0.727383</td><td style=\"text-align: right;\">0.337492</td><td style=\"text-align: right;\">0.0852541</td></tr>\n",
       "<tr><td>FSR_Trainable_0d0bcd1f</td><td>TERMINATED</td><td>172.26.215.93:620143</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6380</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        2.53697e-05</td><td>sklearn.preproc_4510</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.31789</td><td style=\"text-align: right;\">2.56655 </td><td style=\"text-align: right;\">2.21178 </td><td style=\"text-align: right;\">0.345873 </td></tr>\n",
       "<tr><td>FSR_Trainable_17fee470</td><td>TERMINATED</td><td>172.26.215.93:620363</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__58c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.77396e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       180.788  </td><td style=\"text-align: right;\">0.684364</td><td style=\"text-align: right;\">0.347674</td><td style=\"text-align: right;\">0.0969211</td></tr>\n",
       "<tr><td>FSR_Trainable_37ff4b86</td><td>TERMINATED</td><td>172.26.215.93:620686</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__64c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.74091e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       183.803  </td><td style=\"text-align: right;\">0.685941</td><td style=\"text-align: right;\">0.337039</td><td style=\"text-align: right;\">0.100295 </td></tr>\n",
       "<tr><td>FSR_Trainable_abdb1be5</td><td>TERMINATED</td><td>172.26.215.93:621021</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9e00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        3.41028e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         5.754  </td><td style=\"text-align: right;\">0.690955</td><td style=\"text-align: right;\">0.341758</td><td style=\"text-align: right;\">0.0949512</td></tr>\n",
       "<tr><td>FSR_Trainable_ab865ecb</td><td>TERMINATED</td><td>172.26.215.93:621384</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__97c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.0796e-05 </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.45051</td><td style=\"text-align: right;\">0.697897</td><td style=\"text-align: right;\">0.366152</td><td style=\"text-align: right;\">0.102704 </td></tr>\n",
       "<tr><td>FSR_Trainable_f631e741</td><td>TERMINATED</td><td>172.26.215.93:621602</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__36c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.14134e-05</td><td>sklearn.preproc_4510</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.26602</td><td style=\"text-align: right;\">2.32981 </td><td style=\"text-align: right;\">2.12281 </td><td style=\"text-align: right;\">0.319948 </td></tr>\n",
       "<tr><td>FSR_Trainable_592fc33b</td><td>TERMINATED</td><td>172.26.215.93:621867</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1700</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.5962e-05 </td><td>sklearn.preproc_4510</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.09212</td><td style=\"text-align: right;\">2.38597 </td><td style=\"text-align: right;\">2.16757 </td><td style=\"text-align: right;\">0.34595  </td></tr>\n",
       "<tr><td>FSR_Trainable_a3ca3991</td><td>TERMINATED</td><td>172.26.215.93:622116</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4880</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.8768e-05 </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.39193</td><td style=\"text-align: right;\">0.697025</td><td style=\"text-align: right;\">0.347075</td><td style=\"text-align: right;\">0.094197 </td></tr>\n",
       "<tr><td>FSR_Trainable_e917eab5</td><td>TERMINATED</td><td>172.26.215.93:622303</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ae80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000116419</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         6.90926</td><td style=\"text-align: right;\">0.686912</td><td style=\"text-align: right;\">0.351333</td><td style=\"text-align: right;\">0.102933 </td></tr>\n",
       "<tr><td>FSR_Trainable_725ab5d7</td><td>TERMINATED</td><td>172.26.215.93:622535</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__aac0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000118204</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.2413 </td><td style=\"text-align: right;\">0.739904</td><td style=\"text-align: right;\">0.376641</td><td style=\"text-align: right;\">0.0894273</td></tr>\n",
       "<tr><td>FSR_Trainable_cb758a07</td><td>TERMINATED</td><td>172.26.215.93:622767</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9b00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        5.6288e-05 </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         6.08518</td><td style=\"text-align: right;\">0.688514</td><td style=\"text-align: right;\">0.332248</td><td style=\"text-align: right;\">0.0945664</td></tr>\n",
       "<tr><td>FSR_Trainable_856936f7</td><td>TERMINATED</td><td>172.26.215.93:623006</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2c40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        5.74293e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        21.9375 </td><td style=\"text-align: right;\">0.683751</td><td style=\"text-align: right;\">0.332031</td><td style=\"text-align: right;\">0.0997745</td></tr>\n",
       "<tr><td>FSR_Trainable_d37d81b2</td><td>TERMINATED</td><td>172.26.215.93:623232</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2c40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        3.20659e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         6.30424</td><td style=\"text-align: right;\">0.688116</td><td style=\"text-align: right;\">0.32823 </td><td style=\"text-align: right;\">0.0935595</td></tr>\n",
       "<tr><td>FSR_Trainable_7429e81f</td><td>TERMINATED</td><td>172.26.215.93:623527</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0340</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        4.03955e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         4.99896</td><td style=\"text-align: right;\">0.687883</td><td style=\"text-align: right;\">0.327901</td><td style=\"text-align: right;\">0.0938405</td></tr>\n",
       "<tr><td>FSR_Trainable_dc7bd9d7</td><td>TERMINATED</td><td>172.26.215.93:623730</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5900</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000164351</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        40.2003 </td><td style=\"text-align: right;\">0.684127</td><td style=\"text-align: right;\">0.330719</td><td style=\"text-align: right;\">0.0969166</td></tr>\n",
       "<tr><td>FSR_Trainable_b057f1a8</td><td>TERMINATED</td><td>172.26.215.93:623955</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e640</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        2.34766e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         5.546  </td><td style=\"text-align: right;\">0.688485</td><td style=\"text-align: right;\">0.335987</td><td style=\"text-align: right;\">0.100311 </td></tr>\n",
       "<tr><td>FSR_Trainable_a18bd0d6</td><td>TERMINATED</td><td>172.26.215.93:624174</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3840</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000163925</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         5.49573</td><td style=\"text-align: right;\">0.689752</td><td style=\"text-align: right;\">0.348307</td><td style=\"text-align: right;\">0.101762 </td></tr>\n",
       "<tr><td>FSR_Trainable_5e7eb88f</td><td>TERMINATED</td><td>172.26.215.93:624409</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d5c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000182906</td><td>sklearn.preproc_4510</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.48541</td><td style=\"text-align: right;\">1.27125 </td><td style=\"text-align: right;\">0.962702</td><td style=\"text-align: right;\">0.160904 </td></tr>\n",
       "<tr><td>FSR_Trainable_eb9d41b3</td><td>TERMINATED</td><td>172.26.215.93:624641</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3600</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        9.44411e-05</td><td>sklearn.preproc_4510</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.1124 </td><td style=\"text-align: right;\">2.47156 </td><td style=\"text-align: right;\">2.15707 </td><td style=\"text-align: right;\">0.344564 </td></tr>\n",
       "<tr><td>FSR_Trainable_575aa300</td><td>TERMINATED</td><td>172.26.215.93:624881</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__44c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        5.05568e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.92396</td><td style=\"text-align: right;\">0.687879</td><td style=\"text-align: right;\">0.324515</td><td style=\"text-align: right;\">0.0923356</td></tr>\n",
       "<tr><td>FSR_Trainable_4e0c0db9</td><td>TERMINATED</td><td>172.26.215.93:625107</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__bc00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        4.66407e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        36.3765 </td><td style=\"text-align: right;\">0.683643</td><td style=\"text-align: right;\">0.321144</td><td style=\"text-align: right;\">0.0959986</td></tr>\n",
       "<tr><td>FSR_Trainable_60592dfe</td><td>TERMINATED</td><td>172.26.215.93:625341</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3740</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        8.8852e-05 </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        41.7992 </td><td style=\"text-align: right;\">0.684192</td><td style=\"text-align: right;\">0.328102</td><td style=\"text-align: right;\">0.0942859</td></tr>\n",
       "<tr><td>FSR_Trainable_458c4585</td><td>TERMINATED</td><td>172.26.215.93:625565</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7dc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        9.61709e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        21.3584 </td><td style=\"text-align: right;\">0.68532 </td><td style=\"text-align: right;\">0.32636 </td><td style=\"text-align: right;\">0.0947872</td></tr>\n",
       "<tr><td>FSR_Trainable_ef498393</td><td>TERMINATED</td><td>172.26.215.93:625853</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7980</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        8.64315e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        22.61   </td><td style=\"text-align: right;\">0.686479</td><td style=\"text-align: right;\">0.318909</td><td style=\"text-align: right;\">0.0941739</td></tr>\n",
       "<tr><td>FSR_Trainable_e81f61fa</td><td>TERMINATED</td><td>172.26.215.93:626043</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0980</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        3.12231e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       614.27   </td><td style=\"text-align: right;\">0.682766</td><td style=\"text-align: right;\">0.320327</td><td style=\"text-align: right;\">0.0935075</td></tr>\n",
       "<tr><td>FSR_Trainable_425399e5</td><td>TERMINATED</td><td>172.26.215.93:626277</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1780</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000146616</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         9.57745</td><td style=\"text-align: right;\">0.688256</td><td style=\"text-align: right;\">0.324463</td><td style=\"text-align: right;\">0.0924096</td></tr>\n",
       "<tr><td>FSR_Trainable_977aa183</td><td>TERMINATED</td><td>172.26.215.93:626507</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8fc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000251959</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        47.7525 </td><td style=\"text-align: right;\">0.684295</td><td style=\"text-align: right;\">0.320103</td><td style=\"text-align: right;\">0.0924822</td></tr>\n",
       "<tr><td>FSR_Trainable_deb61d9b</td><td>TERMINATED</td><td>172.26.215.93:626734</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6400</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        4.41337e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.0376 </td><td style=\"text-align: right;\">0.72242 </td><td style=\"text-align: right;\">0.345781</td><td style=\"text-align: right;\">0.0884825</td></tr>\n",
       "<tr><td>FSR_Trainable_2281f368</td><td>TERMINATED</td><td>172.26.215.93:627027</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__dd00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        2.91782e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.77948</td><td style=\"text-align: right;\">0.727515</td><td style=\"text-align: right;\">0.340818</td><td style=\"text-align: right;\">0.0840529</td></tr>\n",
       "<tr><td>FSR_Trainable_9a936a86</td><td>TERMINATED</td><td>172.26.215.93:627267</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b440</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        2.97702e-05</td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       446.615  </td><td style=\"text-align: right;\">0.682861</td><td style=\"text-align: right;\">0.319151</td><td style=\"text-align: right;\">0.095336 </td></tr>\n",
       "<tr><td>FSR_Trainable_794cf0e5</td><td>TERMINATED</td><td>172.26.215.93:627456</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ed80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000223911</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        13.7245 </td><td style=\"text-align: right;\">0.68677 </td><td style=\"text-align: right;\">0.320919</td><td style=\"text-align: right;\">0.0942211</td></tr>\n",
       "<tr><td>FSR_Trainable_3388440b</td><td>TERMINATED</td><td>172.26.215.93:627750</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__af00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000285523</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        12.8109 </td><td style=\"text-align: right;\">0.688483</td><td style=\"text-align: right;\">0.317848</td><td style=\"text-align: right;\">0.0924888</td></tr>\n",
       "<tr><td>FSR_Trainable_137d9c29</td><td>TERMINATED</td><td>172.26.215.93:627995</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__95c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000613495</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.15569</td><td style=\"text-align: right;\">0.690376</td><td style=\"text-align: right;\">0.326013</td><td style=\"text-align: right;\">0.0941137</td></tr>\n",
       "<tr><td>FSR_Trainable_28d1fd52</td><td>TERMINATED</td><td>172.26.215.93:628233</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8f00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000591969</td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         9.51969</td><td style=\"text-align: right;\">0.68676 </td><td style=\"text-align: right;\">0.331174</td><td style=\"text-align: right;\">0.0939025</td></tr>\n",
       "<tr><td>FSR_Trainable_f262fb70</td><td>TERMINATED</td><td>172.26.215.93:628482</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__bdc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        5.6252e-05 </td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.75502</td><td style=\"text-align: right;\">0.690649</td><td style=\"text-align: right;\">0.3223  </td><td style=\"text-align: right;\">0.093493 </td></tr>\n",
       "<tr><td>FSR_Trainable_38591770</td><td>TERMINATED</td><td>172.26.215.93:628720</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5dc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000132209</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.11209</td><td style=\"text-align: right;\">0.688701</td><td style=\"text-align: right;\">0.318857</td><td style=\"text-align: right;\">0.0921088</td></tr>\n",
       "<tr><td>FSR_Trainable_e56982cb</td><td>TERMINATED</td><td>172.26.215.93:628964</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0740</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000124211</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        37.8789 </td><td style=\"text-align: right;\">0.687   </td><td style=\"text-align: right;\">0.314065</td><td style=\"text-align: right;\">0.0921886</td></tr>\n",
       "<tr><td>FSR_Trainable_f0ef8d30</td><td>TERMINATED</td><td>172.26.215.93:629240</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__20c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00021096 </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        37.9231 </td><td style=\"text-align: right;\">0.685405</td><td style=\"text-align: right;\">0.317388</td><td style=\"text-align: right;\">0.0963785</td></tr>\n",
       "<tr><td>FSR_Trainable_35af815c</td><td>TERMINATED</td><td>172.26.215.93:629508</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2c00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        8.15844e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.37894</td><td style=\"text-align: right;\">0.699815</td><td style=\"text-align: right;\">0.381221</td><td style=\"text-align: right;\">0.108345 </td></tr>\n",
       "<tr><td>FSR_Trainable_f4215bc9</td><td>TERMINATED</td><td>172.26.215.93:629691</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8c40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        7.2221e-05 </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.90923</td><td style=\"text-align: right;\">0.708416</td><td style=\"text-align: right;\">0.381714</td><td style=\"text-align: right;\">0.0997527</td></tr>\n",
       "<tr><td>FSR_Trainable_697170b1</td><td>TERMINATED</td><td>172.26.215.93:629926</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c340</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        2.2765e-05 </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        85.3806 </td><td style=\"text-align: right;\">0.684604</td><td style=\"text-align: right;\">0.313642</td><td style=\"text-align: right;\">0.0920914</td></tr>\n",
       "<tr><td>FSR_Trainable_c80030cb</td><td>TERMINATED</td><td>172.26.215.93:630163</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3e00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        3.37093e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        22.6805 </td><td style=\"text-align: right;\">0.686945</td><td style=\"text-align: right;\">0.31674 </td><td style=\"text-align: right;\">0.0929626</td></tr>\n",
       "<tr><td>FSR_Trainable_4956b0dc</td><td>TERMINATED</td><td>172.26.215.93:630468</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3680</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        3.31699e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        21.3754 </td><td style=\"text-align: right;\">0.687138</td><td style=\"text-align: right;\">0.318785</td><td style=\"text-align: right;\">0.0923038</td></tr>\n",
       "<tr><td>FSR_Trainable_6e00c170</td><td>TERMINATED</td><td>172.26.215.93:630718</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7b80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        2.35921e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        30.1619 </td><td style=\"text-align: right;\">0.686188</td><td style=\"text-align: right;\">0.316731</td><td style=\"text-align: right;\">0.0920898</td></tr>\n",
       "<tr><td>FSR_Trainable_eb715b45</td><td>TERMINATED</td><td>172.26.215.93:630906</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3e00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        4.35841e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        50.9533 </td><td style=\"text-align: right;\">0.688796</td><td style=\"text-align: right;\">0.35278 </td><td style=\"text-align: right;\">0.100124 </td></tr>\n",
       "<tr><td>FSR_Trainable_0bf13ef3</td><td>TERMINATED</td><td>172.26.215.93:631202</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2280</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        5.18845e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.67563</td><td style=\"text-align: right;\">0.689953</td><td style=\"text-align: right;\">0.338163</td><td style=\"text-align: right;\">0.0991457</td></tr>\n",
       "<tr><td>FSR_Trainable_af01c727</td><td>TERMINATED</td><td>172.26.215.93:631411</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5940</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        4.38084e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        18.2704 </td><td style=\"text-align: right;\">0.685932</td><td style=\"text-align: right;\">0.322267</td><td style=\"text-align: right;\">0.0941558</td></tr>\n",
       "<tr><td>FSR_Trainable_82a622a9</td><td>TERMINATED</td><td>172.26.215.93:631624</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4dc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        4.5236e-05 </td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         9.31897</td><td style=\"text-align: right;\">0.686822</td><td style=\"text-align: right;\">0.328776</td><td style=\"text-align: right;\">0.0943288</td></tr>\n",
       "<tr><td>FSR_Trainable_e9a64d47</td><td>TERMINATED</td><td>172.26.215.93:631864</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9c80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000103415</td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       345.148  </td><td style=\"text-align: right;\">0.681033</td><td style=\"text-align: right;\">0.319221</td><td style=\"text-align: right;\">0.0976849</td></tr>\n",
       "<tr><td>FSR_Trainable_22dd73b2</td><td>TERMINATED</td><td>172.26.215.93:632102</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2ac0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        2.03401e-05</td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.95286</td><td style=\"text-align: right;\">0.690911</td><td style=\"text-align: right;\">0.327167</td><td style=\"text-align: right;\">0.0967041</td></tr>\n",
       "<tr><td>FSR_Trainable_ce92bf0a</td><td>TERMINATED</td><td>172.26.215.93:632325</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8f00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.31999e-05</td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       340.247  </td><td style=\"text-align: right;\">0.678153</td><td style=\"text-align: right;\">0.320553</td><td style=\"text-align: right;\">0.0944053</td></tr>\n",
       "<tr><td>FSR_Trainable_6f50045c</td><td>TERMINATED</td><td>172.26.215.93:632556</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__92c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.5097e-05 </td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.49726</td><td style=\"text-align: right;\">0.704061</td><td style=\"text-align: right;\">0.365   </td><td style=\"text-align: right;\">0.0964959</td></tr>\n",
       "<tr><td>FSR_Trainable_631d5474</td><td>TERMINATED</td><td>172.26.215.93:632820</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__52c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        6.20554e-05</td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         8.5789 </td><td style=\"text-align: right;\">0.685791</td><td style=\"text-align: right;\">0.35117 </td><td style=\"text-align: right;\">0.104767 </td></tr>\n",
       "<tr><td>FSR_Trainable_3e55d9a3</td><td>TERMINATED</td><td>172.26.215.93:633032</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d680</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">8</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        6.29637e-05</td><td>sklearn.preproc_46f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.38134</td><td style=\"text-align: right;\">0.690154</td><td style=\"text-align: right;\">0.334548</td><td style=\"text-align: right;\">0.0949157</td></tr>\n",
       "<tr><td>FSR_Trainable_4636353a</td><td>TERMINATED</td><td>172.26.215.93:633269</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8240</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        2.83232e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.26863</td><td style=\"text-align: right;\">0.693808</td><td style=\"text-align: right;\">0.34417 </td><td style=\"text-align: right;\">0.0952394</td></tr>\n",
       "<tr><td>FSR_Trainable_4fecccad</td><td>TERMINATED</td><td>172.26.215.93:633505</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4990</td><td>sklearn.impute._be60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3200</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">7</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        2.86857e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.15044</td><td style=\"text-align: right;\">0.692387</td><td style=\"text-align: right;\">0.336969</td><td style=\"text-align: right;\">0.0956486</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 13:03:16,746\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">     mape</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_0641d810</td><td>2023-07-19_13-15-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.362263</td><td style=\"text-align: right;\">0.100818 </td><td>172.26.215.93</td><td style=\"text-align: right;\">618616</td><td style=\"text-align: right;\">0.689952</td><td style=\"text-align: right;\">            89.3207 </td><td style=\"text-align: right;\">          9.70925 </td><td style=\"text-align: right;\">      89.3207 </td><td style=\"text-align: right;\"> 1689740124</td><td style=\"text-align: right;\">                   8</td><td>0641d810  </td></tr>\n",
       "<tr><td>FSR_Trainable_0bf13ef3</td><td>2023-07-19_13-35-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.338163</td><td style=\"text-align: right;\">0.0991457</td><td>172.26.215.93</td><td style=\"text-align: right;\">631202</td><td style=\"text-align: right;\">0.689953</td><td style=\"text-align: right;\">             3.67563</td><td style=\"text-align: right;\">          3.67563 </td><td style=\"text-align: right;\">       3.67563</td><td style=\"text-align: right;\"> 1689741308</td><td style=\"text-align: right;\">                   1</td><td>0bf13ef3  </td></tr>\n",
       "<tr><td>FSR_Trainable_0d0bcd1f</td><td>2023-07-19_13-16-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.21178 </td><td style=\"text-align: right;\">0.345873 </td><td>172.26.215.93</td><td style=\"text-align: right;\">620143</td><td style=\"text-align: right;\">2.56655 </td><td style=\"text-align: right;\">             5.31789</td><td style=\"text-align: right;\">          5.31789 </td><td style=\"text-align: right;\">       5.31789</td><td style=\"text-align: right;\"> 1689740206</td><td style=\"text-align: right;\">                   1</td><td>0d0bcd1f  </td></tr>\n",
       "<tr><td>FSR_Trainable_137d9c29</td><td>2023-07-19_13-28-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.326013</td><td style=\"text-align: right;\">0.0941137</td><td>172.26.215.93</td><td style=\"text-align: right;\">627995</td><td style=\"text-align: right;\">0.690376</td><td style=\"text-align: right;\">             6.15569</td><td style=\"text-align: right;\">          6.15569 </td><td style=\"text-align: right;\">       6.15569</td><td style=\"text-align: right;\"> 1689740929</td><td style=\"text-align: right;\">                   1</td><td>137d9c29  </td></tr>\n",
       "<tr><td>FSR_Trainable_17f5cc0f</td><td>2023-07-19_13-10-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.40942 </td><td style=\"text-align: right;\">0.104005 </td><td>172.26.215.93</td><td style=\"text-align: right;\">613153</td><td style=\"text-align: right;\">0.719107</td><td style=\"text-align: right;\">           133.422  </td><td style=\"text-align: right;\">          1.3634  </td><td style=\"text-align: right;\">     133.422  </td><td style=\"text-align: right;\"> 1689739807</td><td style=\"text-align: right;\">                 100</td><td>17f5cc0f  </td></tr>\n",
       "<tr><td>FSR_Trainable_17fee470</td><td>2023-07-19_13-20-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.347674</td><td style=\"text-align: right;\">0.0969211</td><td>172.26.215.93</td><td style=\"text-align: right;\">620363</td><td style=\"text-align: right;\">0.684364</td><td style=\"text-align: right;\">           180.788  </td><td style=\"text-align: right;\">         10.8598  </td><td style=\"text-align: right;\">     180.788  </td><td style=\"text-align: right;\"> 1689740411</td><td style=\"text-align: right;\">                  16</td><td>17fee470  </td></tr>\n",
       "<tr><td>FSR_Trainable_1be8c882</td><td>2023-07-19_13-06-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">1.10446 </td><td style=\"text-align: right;\">0.248382 </td><td>172.26.215.93</td><td style=\"text-align: right;\">609734</td><td style=\"text-align: right;\">1.4078  </td><td style=\"text-align: right;\">           174.216  </td><td style=\"text-align: right;\">          2.73778 </td><td style=\"text-align: right;\">     174.216  </td><td style=\"text-align: right;\"> 1689739589</td><td style=\"text-align: right;\">                  64</td><td>1be8c882  </td></tr>\n",
       "<tr><td>FSR_Trainable_201cb996</td><td>2023-07-19_13-06-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.495759</td><td style=\"text-align: right;\">0.124988 </td><td>172.26.215.93</td><td style=\"text-align: right;\">611525</td><td style=\"text-align: right;\">0.769074</td><td style=\"text-align: right;\">             5.81885</td><td style=\"text-align: right;\">          2.7286  </td><td style=\"text-align: right;\">       5.81885</td><td style=\"text-align: right;\"> 1689739583</td><td style=\"text-align: right;\">                   2</td><td>201cb996  </td></tr>\n",
       "<tr><td>FSR_Trainable_2216a047</td><td>2023-07-19_13-08-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.378961</td><td style=\"text-align: right;\">0.103032 </td><td>172.26.215.93</td><td style=\"text-align: right;\">610161</td><td style=\"text-align: right;\">0.705195</td><td style=\"text-align: right;\">           256.564  </td><td style=\"text-align: right;\">          2.55739 </td><td style=\"text-align: right;\">     256.564  </td><td style=\"text-align: right;\"> 1689739708</td><td style=\"text-align: right;\">                 100</td><td>2216a047  </td></tr>\n",
       "<tr><td>FSR_Trainable_2281f368</td><td>2023-07-19_13-27-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.340818</td><td style=\"text-align: right;\">0.0840529</td><td>172.26.215.93</td><td style=\"text-align: right;\">627027</td><td style=\"text-align: right;\">0.727515</td><td style=\"text-align: right;\">             6.77948</td><td style=\"text-align: right;\">          6.77948 </td><td style=\"text-align: right;\">       6.77948</td><td style=\"text-align: right;\"> 1689740834</td><td style=\"text-align: right;\">                   1</td><td>2281f368  </td></tr>\n",
       "<tr><td>FSR_Trainable_22dd73b2</td><td>2023-07-19_13-36-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.327167</td><td style=\"text-align: right;\">0.0967041</td><td>172.26.215.93</td><td style=\"text-align: right;\">632102</td><td style=\"text-align: right;\">0.690911</td><td style=\"text-align: right;\">             5.95286</td><td style=\"text-align: right;\">          5.95286 </td><td style=\"text-align: right;\">       5.95286</td><td style=\"text-align: right;\"> 1689741370</td><td style=\"text-align: right;\">                   1</td><td>22dd73b2  </td></tr>\n",
       "<tr><td>FSR_Trainable_23e8fc85</td><td>2023-07-19_13-10-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.401411</td><td style=\"text-align: right;\">0.102956 </td><td>172.26.215.93</td><td style=\"text-align: right;\">613653</td><td style=\"text-align: right;\">0.711553</td><td style=\"text-align: right;\">           136.03   </td><td style=\"text-align: right;\">          2.10582 </td><td style=\"text-align: right;\">     136.03   </td><td style=\"text-align: right;\"> 1689739841</td><td style=\"text-align: right;\">                 100</td><td>23e8fc85  </td></tr>\n",
       "<tr><td>FSR_Trainable_27b00b7b</td><td>2023-07-19_13-11-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.321025</td><td style=\"text-align: right;\">0.0944174</td><td>172.26.215.93</td><td style=\"text-align: right;\">615854</td><td style=\"text-align: right;\">0.685259</td><td style=\"text-align: right;\">            21.3562 </td><td style=\"text-align: right;\">          2.79301 </td><td style=\"text-align: right;\">      21.3562 </td><td style=\"text-align: right;\"> 1689739919</td><td style=\"text-align: right;\">                   8</td><td>27b00b7b  </td></tr>\n",
       "<tr><td>FSR_Trainable_28d1fd52</td><td>2023-07-19_13-29-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.331174</td><td style=\"text-align: right;\">0.0939025</td><td>172.26.215.93</td><td style=\"text-align: right;\">628233</td><td style=\"text-align: right;\">0.68676 </td><td style=\"text-align: right;\">             9.51969</td><td style=\"text-align: right;\">          4.5498  </td><td style=\"text-align: right;\">       9.51969</td><td style=\"text-align: right;\"> 1689740959</td><td style=\"text-align: right;\">                   2</td><td>28d1fd52  </td></tr>\n",
       "<tr><td>FSR_Trainable_3388440b</td><td>2023-07-19_13-28-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.317848</td><td style=\"text-align: right;\">0.0924888</td><td>172.26.215.93</td><td style=\"text-align: right;\">627750</td><td style=\"text-align: right;\">0.688483</td><td style=\"text-align: right;\">            12.8109 </td><td style=\"text-align: right;\">          5.92661 </td><td style=\"text-align: right;\">      12.8109 </td><td style=\"text-align: right;\"> 1689740908</td><td style=\"text-align: right;\">                   2</td><td>3388440b  </td></tr>\n",
       "<tr><td>FSR_Trainable_35af815c</td><td>2023-07-19_13-32-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.381221</td><td style=\"text-align: right;\">0.108345 </td><td>172.26.215.93</td><td style=\"text-align: right;\">629508</td><td style=\"text-align: right;\">0.699815</td><td style=\"text-align: right;\">             2.37894</td><td style=\"text-align: right;\">          2.37894 </td><td style=\"text-align: right;\">       2.37894</td><td style=\"text-align: right;\"> 1689741137</td><td style=\"text-align: right;\">                   1</td><td>35af815c  </td></tr>\n",
       "<tr><td>FSR_Trainable_37ff4b86</td><td>2023-07-19_13-21-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.337039</td><td style=\"text-align: right;\">0.100295 </td><td>172.26.215.93</td><td style=\"text-align: right;\">620686</td><td style=\"text-align: right;\">0.685941</td><td style=\"text-align: right;\">           183.803  </td><td style=\"text-align: right;\">         11.8159  </td><td style=\"text-align: right;\">     183.803  </td><td style=\"text-align: right;\"> 1689740505</td><td style=\"text-align: right;\">                  16</td><td>37ff4b86  </td></tr>\n",
       "<tr><td>FSR_Trainable_38591770</td><td>2023-07-19_13-30-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.318857</td><td style=\"text-align: right;\">0.0921088</td><td>172.26.215.93</td><td style=\"text-align: right;\">628720</td><td style=\"text-align: right;\">0.688701</td><td style=\"text-align: right;\">             7.11209</td><td style=\"text-align: right;\">          7.11209 </td><td style=\"text-align: right;\">       7.11209</td><td style=\"text-align: right;\"> 1689741005</td><td style=\"text-align: right;\">                   1</td><td>38591770  </td></tr>\n",
       "<tr><td>FSR_Trainable_3e55d9a3</td><td>2023-07-19_13-37-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.334548</td><td style=\"text-align: right;\">0.0949157</td><td>172.26.215.93</td><td style=\"text-align: right;\">633032</td><td style=\"text-align: right;\">0.690154</td><td style=\"text-align: right;\">             8.38134</td><td style=\"text-align: right;\">          8.38134 </td><td style=\"text-align: right;\">       8.38134</td><td style=\"text-align: right;\"> 1689741436</td><td style=\"text-align: right;\">                   1</td><td>3e55d9a3  </td></tr>\n",
       "<tr><td>FSR_Trainable_40a99b6a</td><td>2023-07-19_13-05-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.417157</td><td style=\"text-align: right;\">0.105536 </td><td>172.26.215.93</td><td style=\"text-align: right;\">610998</td><td style=\"text-align: right;\">0.712842</td><td style=\"text-align: right;\">            27.2351 </td><td style=\"text-align: right;\">          0.901052</td><td style=\"text-align: right;\">      27.2351 </td><td style=\"text-align: right;\"> 1689739545</td><td style=\"text-align: right;\">                  32</td><td>40a99b6a  </td></tr>\n",
       "<tr><td>FSR_Trainable_425399e5</td><td>2023-07-19_13-26-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.324463</td><td style=\"text-align: right;\">0.0924096</td><td>172.26.215.93</td><td style=\"text-align: right;\">626277</td><td style=\"text-align: right;\">0.688256</td><td style=\"text-align: right;\">             9.57745</td><td style=\"text-align: right;\">          4.35064 </td><td style=\"text-align: right;\">       9.57745</td><td style=\"text-align: right;\"> 1689740788</td><td style=\"text-align: right;\">                   2</td><td>425399e5  </td></tr>\n",
       "<tr><td>FSR_Trainable_458c4585</td><td>2023-07-19_13-25-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.32636 </td><td style=\"text-align: right;\">0.0947872</td><td>172.26.215.93</td><td style=\"text-align: right;\">625565</td><td style=\"text-align: right;\">0.68532 </td><td style=\"text-align: right;\">            21.3584 </td><td style=\"text-align: right;\">          4.72171 </td><td style=\"text-align: right;\">      21.3584 </td><td style=\"text-align: right;\"> 1689740743</td><td style=\"text-align: right;\">                   4</td><td>458c4585  </td></tr>\n",
       "<tr><td>FSR_Trainable_4636353a</td><td>2023-07-19_13-37-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.34417 </td><td style=\"text-align: right;\">0.0952394</td><td>172.26.215.93</td><td style=\"text-align: right;\">633269</td><td style=\"text-align: right;\">0.693808</td><td style=\"text-align: right;\">             7.26863</td><td style=\"text-align: right;\">          7.26863 </td><td style=\"text-align: right;\">       7.26863</td><td style=\"text-align: right;\"> 1689741450</td><td style=\"text-align: right;\">                   1</td><td>4636353a  </td></tr>\n",
       "<tr><td>FSR_Trainable_4956b0dc</td><td>2023-07-19_13-34-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.318785</td><td style=\"text-align: right;\">0.0923038</td><td>172.26.215.93</td><td style=\"text-align: right;\">630468</td><td style=\"text-align: right;\">0.687138</td><td style=\"text-align: right;\">            21.3754 </td><td style=\"text-align: right;\">          9.7436  </td><td style=\"text-align: right;\">      21.3754 </td><td style=\"text-align: right;\"> 1689741241</td><td style=\"text-align: right;\">                   2</td><td>4956b0dc  </td></tr>\n",
       "<tr><td>FSR_Trainable_4dc69380</td><td>2023-07-19_13-11-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.342421</td><td style=\"text-align: right;\">0.099452 </td><td>172.26.215.93</td><td style=\"text-align: right;\">615167</td><td style=\"text-align: right;\">0.687445</td><td style=\"text-align: right;\">             7.8028 </td><td style=\"text-align: right;\">          1.7744  </td><td style=\"text-align: right;\">       7.8028 </td><td style=\"text-align: right;\"> 1689739872</td><td style=\"text-align: right;\">                   4</td><td>4dc69380  </td></tr>\n",
       "<tr><td>FSR_Trainable_4e0c0db9</td><td>2023-07-19_13-25-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.321144</td><td style=\"text-align: right;\">0.0959986</td><td>172.26.215.93</td><td style=\"text-align: right;\">625107</td><td style=\"text-align: right;\">0.683643</td><td style=\"text-align: right;\">            36.3765 </td><td style=\"text-align: right;\">          4.38065 </td><td style=\"text-align: right;\">      36.3765 </td><td style=\"text-align: right;\"> 1689740733</td><td style=\"text-align: right;\">                   8</td><td>4e0c0db9  </td></tr>\n",
       "<tr><td>FSR_Trainable_4fecccad</td><td>2023-07-19_13-37-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.336969</td><td style=\"text-align: right;\">0.0956486</td><td>172.26.215.93</td><td style=\"text-align: right;\">633505</td><td style=\"text-align: right;\">0.692387</td><td style=\"text-align: right;\">             6.15044</td><td style=\"text-align: right;\">          6.15044 </td><td style=\"text-align: right;\">       6.15044</td><td style=\"text-align: right;\"> 1689741463</td><td style=\"text-align: right;\">                   1</td><td>4fecccad  </td></tr>\n",
       "<tr><td>FSR_Trainable_53e5695a</td><td>2023-07-19_13-11-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.356929</td><td style=\"text-align: right;\">0.104922 </td><td>172.26.215.93</td><td style=\"text-align: right;\">615392</td><td style=\"text-align: right;\">0.695951</td><td style=\"text-align: right;\">             3.82354</td><td style=\"text-align: right;\">          1.66774 </td><td style=\"text-align: right;\">       3.82354</td><td style=\"text-align: right;\"> 1689739880</td><td style=\"text-align: right;\">                   2</td><td>53e5695a  </td></tr>\n",
       "<tr><td>FSR_Trainable_5459c42c</td><td>2023-07-19_13-13-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.344138</td><td style=\"text-align: right;\">0.0938128</td><td>172.26.215.93</td><td style=\"text-align: right;\">617463</td><td style=\"text-align: right;\">0.69382 </td><td style=\"text-align: right;\">             6.46179</td><td style=\"text-align: right;\">          2.7     </td><td style=\"text-align: right;\">       6.46179</td><td style=\"text-align: right;\"> 1689739980</td><td style=\"text-align: right;\">                   2</td><td>5459c42c  </td></tr>\n",
       "<tr><td>FSR_Trainable_56a0d87a</td><td>2023-07-19_13-07-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.393843</td><td style=\"text-align: right;\">0.102189 </td><td>172.26.215.93</td><td style=\"text-align: right;\">612664</td><td style=\"text-align: right;\">0.72016 </td><td style=\"text-align: right;\">             3.21215</td><td style=\"text-align: right;\">          1.4052  </td><td style=\"text-align: right;\">       3.21215</td><td style=\"text-align: right;\"> 1689739646</td><td style=\"text-align: right;\">                   2</td><td>56a0d87a  </td></tr>\n",
       "<tr><td>FSR_Trainable_575aa300</td><td>2023-07-19_13-24-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.324515</td><td style=\"text-align: right;\">0.0923356</td><td>172.26.215.93</td><td style=\"text-align: right;\">624881</td><td style=\"text-align: right;\">0.687879</td><td style=\"text-align: right;\">             7.92396</td><td style=\"text-align: right;\">          3.74272 </td><td style=\"text-align: right;\">       7.92396</td><td style=\"text-align: right;\"> 1689740691</td><td style=\"text-align: right;\">                   2</td><td>575aa300  </td></tr>\n",
       "<tr><td>FSR_Trainable_592fc33b</td><td>2023-07-19_13-21-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.16757 </td><td style=\"text-align: right;\">0.34595  </td><td>172.26.215.93</td><td style=\"text-align: right;\">621867</td><td style=\"text-align: right;\">2.38597 </td><td style=\"text-align: right;\">             6.09212</td><td style=\"text-align: right;\">          6.09212 </td><td style=\"text-align: right;\">       6.09212</td><td style=\"text-align: right;\"> 1689740503</td><td style=\"text-align: right;\">                   1</td><td>592fc33b  </td></tr>\n",
       "<tr><td>FSR_Trainable_5e7eb88f</td><td>2023-07-19_13-24-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.962702</td><td style=\"text-align: right;\">0.160904 </td><td>172.26.215.93</td><td style=\"text-align: right;\">624409</td><td style=\"text-align: right;\">1.27125 </td><td style=\"text-align: right;\">             4.48541</td><td style=\"text-align: right;\">          4.48541 </td><td style=\"text-align: right;\">       4.48541</td><td style=\"text-align: right;\"> 1689740661</td><td style=\"text-align: right;\">                   1</td><td>5e7eb88f  </td></tr>\n",
       "<tr><td>FSR_Trainable_60592dfe</td><td>2023-07-19_13-25-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.328102</td><td style=\"text-align: right;\">0.0942859</td><td>172.26.215.93</td><td style=\"text-align: right;\">625341</td><td style=\"text-align: right;\">0.684192</td><td style=\"text-align: right;\">            41.7992 </td><td style=\"text-align: right;\">          5.10008 </td><td style=\"text-align: right;\">      41.7992 </td><td style=\"text-align: right;\"> 1689740750</td><td style=\"text-align: right;\">                   8</td><td>60592dfe  </td></tr>\n",
       "<tr><td>FSR_Trainable_60a505df</td><td>2023-07-19_13-16-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.337492</td><td style=\"text-align: right;\">0.0852541</td><td>172.26.215.93</td><td style=\"text-align: right;\">619890</td><td style=\"text-align: right;\">0.727383</td><td style=\"text-align: right;\">            12.3537 </td><td style=\"text-align: right;\">         12.3537  </td><td style=\"text-align: right;\">      12.3537 </td><td style=\"text-align: right;\"> 1689740181</td><td style=\"text-align: right;\">                   1</td><td>60a505df  </td></tr>\n",
       "<tr><td>FSR_Trainable_631d5474</td><td>2023-07-19_13-37-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.35117 </td><td style=\"text-align: right;\">0.104767 </td><td>172.26.215.93</td><td style=\"text-align: right;\">632820</td><td style=\"text-align: right;\">0.685791</td><td style=\"text-align: right;\">             8.5789 </td><td style=\"text-align: right;\">          1.95633 </td><td style=\"text-align: right;\">       8.5789 </td><td style=\"text-align: right;\"> 1689741425</td><td style=\"text-align: right;\">                   4</td><td>631d5474  </td></tr>\n",
       "<tr><td>FSR_Trainable_648cb2e4</td><td>2023-07-19_13-12-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.358426</td><td style=\"text-align: right;\">0.0846428</td><td>172.26.215.93</td><td style=\"text-align: right;\">616777</td><td style=\"text-align: right;\">0.735292</td><td style=\"text-align: right;\">             2.73161</td><td style=\"text-align: right;\">          2.73161 </td><td style=\"text-align: right;\">       2.73161</td><td style=\"text-align: right;\"> 1689739940</td><td style=\"text-align: right;\">                   1</td><td>648cb2e4  </td></tr>\n",
       "<tr><td>FSR_Trainable_697170b1</td><td>2023-07-19_13-34-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.313642</td><td style=\"text-align: right;\">0.0920914</td><td>172.26.215.93</td><td style=\"text-align: right;\">629926</td><td style=\"text-align: right;\">0.684604</td><td style=\"text-align: right;\">            85.3806 </td><td style=\"text-align: right;\">         10.2151  </td><td style=\"text-align: right;\">      85.3806 </td><td style=\"text-align: right;\"> 1689741252</td><td style=\"text-align: right;\">                   8</td><td>697170b1  </td></tr>\n",
       "<tr><td>FSR_Trainable_6c1193b9</td><td>2023-07-19_13-04-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.371832</td><td style=\"text-align: right;\">0.0941027</td><td>172.26.215.93</td><td style=\"text-align: right;\">609807</td><td style=\"text-align: right;\">0.722471</td><td style=\"text-align: right;\">            71.9059 </td><td style=\"text-align: right;\">          4.21242 </td><td style=\"text-align: right;\">      71.9059 </td><td style=\"text-align: right;\"> 1689739485</td><td style=\"text-align: right;\">                  16</td><td>6c1193b9  </td></tr>\n",
       "<tr><td>FSR_Trainable_6e00c170</td><td>2023-07-19_13-34-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.316731</td><td style=\"text-align: right;\">0.0920898</td><td>172.26.215.93</td><td style=\"text-align: right;\">630718</td><td style=\"text-align: right;\">0.686188</td><td style=\"text-align: right;\">            30.1619 </td><td style=\"text-align: right;\">          6.98437 </td><td style=\"text-align: right;\">      30.1619 </td><td style=\"text-align: right;\"> 1689741291</td><td style=\"text-align: right;\">                   4</td><td>6e00c170  </td></tr>\n",
       "<tr><td>FSR_Trainable_6f50045c</td><td>2023-07-19_13-36-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.365   </td><td style=\"text-align: right;\">0.0964959</td><td>172.26.215.93</td><td style=\"text-align: right;\">632556</td><td style=\"text-align: right;\">0.704061</td><td style=\"text-align: right;\">             2.49726</td><td style=\"text-align: right;\">          2.49726 </td><td style=\"text-align: right;\">       2.49726</td><td style=\"text-align: right;\"> 1689741397</td><td style=\"text-align: right;\">                   1</td><td>6f50045c  </td></tr>\n",
       "<tr><td>FSR_Trainable_6f8be931</td><td>2023-07-19_13-05-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.723687</td><td style=\"text-align: right;\">0.173355 </td><td>172.26.215.93</td><td style=\"text-align: right;\">610789</td><td style=\"text-align: right;\">0.973249</td><td style=\"text-align: right;\">             2.04952</td><td style=\"text-align: right;\">          2.04952 </td><td style=\"text-align: right;\">       2.04952</td><td style=\"text-align: right;\"> 1689739500</td><td style=\"text-align: right;\">                   1</td><td>6f8be931  </td></tr>\n",
       "<tr><td>FSR_Trainable_7031dca4</td><td>2023-07-19_13-06-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.583829</td><td style=\"text-align: right;\">0.126196 </td><td>172.26.215.93</td><td style=\"text-align: right;\">611310</td><td style=\"text-align: right;\">0.860102</td><td style=\"text-align: right;\">             3.30136</td><td style=\"text-align: right;\">          3.30136 </td><td style=\"text-align: right;\">       3.30136</td><td style=\"text-align: right;\"> 1689739560</td><td style=\"text-align: right;\">                   1</td><td>7031dca4  </td></tr>\n",
       "<tr><td>FSR_Trainable_7079193b</td><td>2023-07-19_13-13-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.342439</td><td style=\"text-align: right;\">0.0947091</td><td>172.26.215.93</td><td style=\"text-align: right;\">617693</td><td style=\"text-align: right;\">0.685601</td><td style=\"text-align: right;\">            14.7206 </td><td style=\"text-align: right;\">          1.83304 </td><td style=\"text-align: right;\">      14.7206 </td><td style=\"text-align: right;\"> 1689740000</td><td style=\"text-align: right;\">                   8</td><td>7079193b  </td></tr>\n",
       "<tr><td>FSR_Trainable_725ab5d7</td><td>2023-07-19_13-22-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.376641</td><td style=\"text-align: right;\">0.0894273</td><td>172.26.215.93</td><td style=\"text-align: right;\">622535</td><td style=\"text-align: right;\">0.739904</td><td style=\"text-align: right;\">             3.2413 </td><td style=\"text-align: right;\">          3.2413  </td><td style=\"text-align: right;\">       3.2413 </td><td style=\"text-align: right;\"> 1689740550</td><td style=\"text-align: right;\">                   1</td><td>725ab5d7  </td></tr>\n",
       "<tr><td>FSR_Trainable_72fcdddb</td><td>2023-07-19_13-06-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.388828</td><td style=\"text-align: right;\">0.102754 </td><td>172.26.215.93</td><td style=\"text-align: right;\">610458</td><td style=\"text-align: right;\">0.704366</td><td style=\"text-align: right;\">           156.559  </td><td style=\"text-align: right;\">          1.38034 </td><td style=\"text-align: right;\">     156.559  </td><td style=\"text-align: right;\"> 1689739615</td><td style=\"text-align: right;\">                 100</td><td>72fcdddb  </td></tr>\n",
       "<tr><td>FSR_Trainable_7429e81f</td><td>2023-07-19_13-23-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.327901</td><td style=\"text-align: right;\">0.0938405</td><td>172.26.215.93</td><td style=\"text-align: right;\">623527</td><td style=\"text-align: right;\">0.687883</td><td style=\"text-align: right;\">             4.99896</td><td style=\"text-align: right;\">          2.32457 </td><td style=\"text-align: right;\">       4.99896</td><td style=\"text-align: right;\"> 1689740616</td><td style=\"text-align: right;\">                   2</td><td>7429e81f  </td></tr>\n",
       "<tr><td>FSR_Trainable_74b5da82</td><td>2023-07-19_13-15-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.315903</td><td style=\"text-align: right;\">0.0933294</td><td>172.26.215.93</td><td style=\"text-align: right;\">618911</td><td style=\"text-align: right;\">0.686334</td><td style=\"text-align: right;\">            45.1268 </td><td style=\"text-align: right;\">         10.7694  </td><td style=\"text-align: right;\">      45.1268 </td><td style=\"text-align: right;\"> 1689740112</td><td style=\"text-align: right;\">                   4</td><td>74b5da82  </td></tr>\n",
       "<tr><td>FSR_Trainable_7525721c</td><td>2023-07-19_13-13-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.33757 </td><td style=\"text-align: right;\">0.0948475</td><td>172.26.215.93</td><td style=\"text-align: right;\">617929</td><td style=\"text-align: right;\">0.686098</td><td style=\"text-align: right;\">             9.0377 </td><td style=\"text-align: right;\">          1.89487 </td><td style=\"text-align: right;\">       9.0377 </td><td style=\"text-align: right;\"> 1689740004</td><td style=\"text-align: right;\">                   4</td><td>7525721c  </td></tr>\n",
       "<tr><td>FSR_Trainable_768e63a9</td><td>2023-07-19_13-23-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.367735</td><td style=\"text-align: right;\">0.0982551</td><td>172.26.215.93</td><td style=\"text-align: right;\">619409</td><td style=\"text-align: right;\">0.692906</td><td style=\"text-align: right;\">           446.547  </td><td style=\"text-align: right;\">          4.14688 </td><td style=\"text-align: right;\">     446.547  </td><td style=\"text-align: right;\"> 1689740599</td><td style=\"text-align: right;\">                 100</td><td>768e63a9  </td></tr>\n",
       "<tr><td>FSR_Trainable_794cf0e5</td><td>2023-07-19_13-28-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.320919</td><td style=\"text-align: right;\">0.0942211</td><td>172.26.215.93</td><td style=\"text-align: right;\">627456</td><td style=\"text-align: right;\">0.68677 </td><td style=\"text-align: right;\">            13.7245 </td><td style=\"text-align: right;\">          6.34672 </td><td style=\"text-align: right;\">      13.7245 </td><td style=\"text-align: right;\"> 1689740880</td><td style=\"text-align: right;\">                   2</td><td>794cf0e5  </td></tr>\n",
       "<tr><td>FSR_Trainable_82a622a9</td><td>2023-07-19_13-35-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.328776</td><td style=\"text-align: right;\">0.0943288</td><td>172.26.215.93</td><td style=\"text-align: right;\">631624</td><td style=\"text-align: right;\">0.686822</td><td style=\"text-align: right;\">             9.31897</td><td style=\"text-align: right;\">          3.81329 </td><td style=\"text-align: right;\">       9.31897</td><td style=\"text-align: right;\"> 1689741350</td><td style=\"text-align: right;\">                   2</td><td>82a622a9  </td></tr>\n",
       "<tr><td>FSR_Trainable_856936f7</td><td>2023-07-19_13-23-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.332031</td><td style=\"text-align: right;\">0.0997745</td><td>172.26.215.93</td><td style=\"text-align: right;\">623006</td><td style=\"text-align: right;\">0.683751</td><td style=\"text-align: right;\">            21.9375 </td><td style=\"text-align: right;\">          2.40322 </td><td style=\"text-align: right;\">      21.9375 </td><td style=\"text-align: right;\"> 1689740602</td><td style=\"text-align: right;\">                   8</td><td>856936f7  </td></tr>\n",
       "<tr><td>FSR_Trainable_888f5e28</td><td>2023-07-19_13-12-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.34801 </td><td style=\"text-align: right;\">0.0958057</td><td>172.26.215.93</td><td style=\"text-align: right;\">616307</td><td style=\"text-align: right;\">0.693203</td><td style=\"text-align: right;\">             5.78847</td><td style=\"text-align: right;\">          2.47127 </td><td style=\"text-align: right;\">       5.78847</td><td style=\"text-align: right;\"> 1689739925</td><td style=\"text-align: right;\">                   2</td><td>888f5e28  </td></tr>\n",
       "<tr><td>FSR_Trainable_8a8d86ae</td><td>2023-07-19_13-10-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.341042</td><td style=\"text-align: right;\">0.0968842</td><td>172.26.215.93</td><td style=\"text-align: right;\">614683</td><td style=\"text-align: right;\">0.693819</td><td style=\"text-align: right;\">             4.75465</td><td style=\"text-align: right;\">          1.86385 </td><td style=\"text-align: right;\">       4.75465</td><td style=\"text-align: right;\"> 1689739846</td><td style=\"text-align: right;\">                   2</td><td>8a8d86ae  </td></tr>\n",
       "<tr><td>FSR_Trainable_91564664</td><td>2023-07-19_13-07-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.46243 </td><td style=\"text-align: right;\">0.126266 </td><td>172.26.215.93</td><td style=\"text-align: right;\">612229</td><td style=\"text-align: right;\">0.769684</td><td style=\"text-align: right;\">            13.6666 </td><td style=\"text-align: right;\">         13.6666  </td><td style=\"text-align: right;\">      13.6666 </td><td style=\"text-align: right;\"> 1689739631</td><td style=\"text-align: right;\">                   1</td><td>91564664  </td></tr>\n",
       "<tr><td>FSR_Trainable_977aa183</td><td>2023-07-19_13-27-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.320103</td><td style=\"text-align: right;\">0.0924822</td><td>172.26.215.93</td><td style=\"text-align: right;\">626507</td><td style=\"text-align: right;\">0.684295</td><td style=\"text-align: right;\">            47.7525 </td><td style=\"text-align: right;\">          5.82092 </td><td style=\"text-align: right;\">      47.7525 </td><td style=\"text-align: right;\"> 1689740843</td><td style=\"text-align: right;\">                   8</td><td>977aa183  </td></tr>\n",
       "<tr><td>FSR_Trainable_986747c2</td><td>2023-07-19_13-32-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.309915</td><td style=\"text-align: right;\">0.0935208</td><td>172.26.215.93</td><td style=\"text-align: right;\">619102</td><td style=\"text-align: right;\">0.682384</td><td style=\"text-align: right;\">          1019.8    </td><td style=\"text-align: right;\">         10.4493  </td><td style=\"text-align: right;\">    1019.8    </td><td style=\"text-align: right;\"> 1689741132</td><td style=\"text-align: right;\">                 100</td><td>986747c2  </td></tr>\n",
       "<tr><td>FSR_Trainable_9a936a86</td><td>2023-07-19_13-35-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.319151</td><td style=\"text-align: right;\">0.095336 </td><td>172.26.215.93</td><td style=\"text-align: right;\">627267</td><td style=\"text-align: right;\">0.682861</td><td style=\"text-align: right;\">           446.615  </td><td style=\"text-align: right;\">          4.18083 </td><td style=\"text-align: right;\">     446.615  </td><td style=\"text-align: right;\"> 1689741324</td><td style=\"text-align: right;\">                 100</td><td>9a936a86  </td></tr>\n",
       "<tr><td>FSR_Trainable_9bf4d6f5</td><td>2023-07-19_13-10-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.352984</td><td style=\"text-align: right;\">0.0968374</td><td>172.26.215.93</td><td style=\"text-align: right;\">614255</td><td style=\"text-align: right;\">0.695342</td><td style=\"text-align: right;\">             3.01144</td><td style=\"text-align: right;\">          1.2871  </td><td style=\"text-align: right;\">       3.01144</td><td style=\"text-align: right;\"> 1689739817</td><td style=\"text-align: right;\">                   2</td><td>9bf4d6f5  </td></tr>\n",
       "<tr><td>FSR_Trainable_a18bd0d6</td><td>2023-07-19_13-24-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.348307</td><td style=\"text-align: right;\">0.101762 </td><td>172.26.215.93</td><td style=\"text-align: right;\">624174</td><td style=\"text-align: right;\">0.689752</td><td style=\"text-align: right;\">             5.49573</td><td style=\"text-align: right;\">          2.43698 </td><td style=\"text-align: right;\">       5.49573</td><td style=\"text-align: right;\"> 1689740652</td><td style=\"text-align: right;\">                   2</td><td>a18bd0d6  </td></tr>\n",
       "<tr><td>FSR_Trainable_a278091f</td><td>2023-07-19_13-15-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.340509</td><td style=\"text-align: right;\">0.0859608</td><td>172.26.215.93</td><td style=\"text-align: right;\">619593</td><td style=\"text-align: right;\">0.726559</td><td style=\"text-align: right;\">            10.3222 </td><td style=\"text-align: right;\">         10.3222  </td><td style=\"text-align: right;\">      10.3222 </td><td style=\"text-align: right;\"> 1689740151</td><td style=\"text-align: right;\">                   1</td><td>a278091f  </td></tr>\n",
       "<tr><td>FSR_Trainable_a3ca3991</td><td>2023-07-19_13-22-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.347075</td><td style=\"text-align: right;\">0.094197 </td><td>172.26.215.93</td><td style=\"text-align: right;\">622116</td><td style=\"text-align: right;\">0.697025</td><td style=\"text-align: right;\">             3.39193</td><td style=\"text-align: right;\">          3.39193 </td><td style=\"text-align: right;\">       3.39193</td><td style=\"text-align: right;\"> 1689740523</td><td style=\"text-align: right;\">                   1</td><td>a3ca3991  </td></tr>\n",
       "<tr><td>FSR_Trainable_a66aa590</td><td>2023-07-19_13-14-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.319823</td><td style=\"text-align: right;\">0.0944652</td><td>172.26.215.93</td><td style=\"text-align: right;\">618393</td><td style=\"text-align: right;\">0.686011</td><td style=\"text-align: right;\">            47.9275 </td><td style=\"text-align: right;\">          9.64881 </td><td style=\"text-align: right;\">      47.9275 </td><td style=\"text-align: right;\"> 1689740066</td><td style=\"text-align: right;\">                   4</td><td>a66aa590  </td></tr>\n",
       "<tr><td>FSR_Trainable_ab865ecb</td><td>2023-07-19_13-20-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.366152</td><td style=\"text-align: right;\">0.102704 </td><td>172.26.215.93</td><td style=\"text-align: right;\">621384</td><td style=\"text-align: right;\">0.697897</td><td style=\"text-align: right;\">             4.45051</td><td style=\"text-align: right;\">          4.45051 </td><td style=\"text-align: right;\">       4.45051</td><td style=\"text-align: right;\"> 1689740454</td><td style=\"text-align: right;\">                   1</td><td>ab865ecb  </td></tr>\n",
       "<tr><td>FSR_Trainable_abdb1be5</td><td>2023-07-19_13-20-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.341758</td><td style=\"text-align: right;\">0.0949512</td><td>172.26.215.93</td><td style=\"text-align: right;\">621021</td><td style=\"text-align: right;\">0.690955</td><td style=\"text-align: right;\">             5.754  </td><td style=\"text-align: right;\">          2.86056 </td><td style=\"text-align: right;\">       5.754  </td><td style=\"text-align: right;\"> 1689740436</td><td style=\"text-align: right;\">                   2</td><td>abdb1be5  </td></tr>\n",
       "<tr><td>FSR_Trainable_ac099330</td><td>2023-07-19_13-18-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.347321</td><td style=\"text-align: right;\">0.0963954</td><td>172.26.215.93</td><td style=\"text-align: right;\">618156</td><td style=\"text-align: right;\">0.678305</td><td style=\"text-align: right;\">           268.857  </td><td style=\"text-align: right;\">          2.30054 </td><td style=\"text-align: right;\">     268.857  </td><td style=\"text-align: right;\"> 1689740293</td><td style=\"text-align: right;\">                 100</td><td>ac099330  </td></tr>\n",
       "<tr><td>FSR_Trainable_af01c727</td><td>2023-07-19_13-35-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.322267</td><td style=\"text-align: right;\">0.0941558</td><td>172.26.215.93</td><td style=\"text-align: right;\">631411</td><td style=\"text-align: right;\">0.685932</td><td style=\"text-align: right;\">            18.2704 </td><td style=\"text-align: right;\">          4.62255 </td><td style=\"text-align: right;\">      18.2704 </td><td style=\"text-align: right;\"> 1689741346</td><td style=\"text-align: right;\">                   4</td><td>af01c727  </td></tr>\n",
       "<tr><td>FSR_Trainable_b057f1a8</td><td>2023-07-19_13-23-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.335987</td><td style=\"text-align: right;\">0.100311 </td><td>172.26.215.93</td><td style=\"text-align: right;\">623955</td><td style=\"text-align: right;\">0.688485</td><td style=\"text-align: right;\">             5.546  </td><td style=\"text-align: right;\">          2.50095 </td><td style=\"text-align: right;\">       5.546  </td><td style=\"text-align: right;\"> 1689740639</td><td style=\"text-align: right;\">                   2</td><td>b057f1a8  </td></tr>\n",
       "<tr><td>FSR_Trainable_b642f7cd</td><td>2023-07-19_13-11-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.336978</td><td style=\"text-align: right;\">0.0965921</td><td>172.26.215.93</td><td style=\"text-align: right;\">615618</td><td style=\"text-align: right;\">0.686713</td><td style=\"text-align: right;\">             7.16573</td><td style=\"text-align: right;\">          1.53615 </td><td style=\"text-align: right;\">       7.16573</td><td style=\"text-align: right;\"> 1689739893</td><td style=\"text-align: right;\">                   4</td><td>b642f7cd  </td></tr>\n",
       "<tr><td>FSR_Trainable_b7290c01</td><td>2023-07-19_13-08-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.346283</td><td style=\"text-align: right;\">0.097646 </td><td>172.26.215.93</td><td style=\"text-align: right;\">613370</td><td style=\"text-align: right;\">0.696397</td><td style=\"text-align: right;\">             6.16707</td><td style=\"text-align: right;\">          1.35661 </td><td style=\"text-align: right;\">       6.16707</td><td style=\"text-align: right;\"> 1689739684</td><td style=\"text-align: right;\">                   4</td><td>b7290c01  </td></tr>\n",
       "<tr><td>FSR_Trainable_b861e4d8</td><td>2023-07-19_13-07-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.369218</td><td style=\"text-align: right;\">0.100835 </td><td>172.26.215.93</td><td style=\"text-align: right;\">612451</td><td style=\"text-align: right;\">0.704997</td><td style=\"text-align: right;\">             5.98632</td><td style=\"text-align: right;\">          1.31067 </td><td style=\"text-align: right;\">       5.98632</td><td style=\"text-align: right;\"> 1689739638</td><td style=\"text-align: right;\">                   4</td><td>b861e4d8  </td></tr>\n",
       "<tr><td>FSR_Trainable_c80030cb</td><td>2023-07-19_13-33-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.31674 </td><td style=\"text-align: right;\">0.0929626</td><td>172.26.215.93</td><td style=\"text-align: right;\">630163</td><td style=\"text-align: right;\">0.686945</td><td style=\"text-align: right;\">            22.6805 </td><td style=\"text-align: right;\">         10.7105  </td><td style=\"text-align: right;\">      22.6805 </td><td style=\"text-align: right;\"> 1689741204</td><td style=\"text-align: right;\">                   2</td><td>c80030cb  </td></tr>\n",
       "<tr><td>FSR_Trainable_cacdc1e0</td><td>2023-07-19_13-06-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.562504</td><td style=\"text-align: right;\">0.134783 </td><td>172.26.215.93</td><td style=\"text-align: right;\">611983</td><td style=\"text-align: right;\">0.84685 </td><td style=\"text-align: right;\">             1.46733</td><td style=\"text-align: right;\">          1.46733 </td><td style=\"text-align: right;\">       1.46733</td><td style=\"text-align: right;\"> 1689739607</td><td style=\"text-align: right;\">                   1</td><td>cacdc1e0  </td></tr>\n",
       "<tr><td>FSR_Trainable_cb005763</td><td>2023-07-19_13-14-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.325147</td><td style=\"text-align: right;\">0.0933304</td><td>172.26.215.93</td><td style=\"text-align: right;\">614910</td><td style=\"text-align: right;\">0.68067 </td><td style=\"text-align: right;\">           167.586  </td><td style=\"text-align: right;\">          2.85518 </td><td style=\"text-align: right;\">     167.586  </td><td style=\"text-align: right;\"> 1689740047</td><td style=\"text-align: right;\">                 100</td><td>cb005763  </td></tr>\n",
       "<tr><td>FSR_Trainable_cb758a07</td><td>2023-07-19_13-22-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.332248</td><td style=\"text-align: right;\">0.0945664</td><td>172.26.215.93</td><td style=\"text-align: right;\">622767</td><td style=\"text-align: right;\">0.688514</td><td style=\"text-align: right;\">             6.08518</td><td style=\"text-align: right;\">          2.86471 </td><td style=\"text-align: right;\">       6.08518</td><td style=\"text-align: right;\"> 1689740570</td><td style=\"text-align: right;\">                   2</td><td>cb758a07  </td></tr>\n",
       "<tr><td>FSR_Trainable_ce92bf0a</td><td>2023-07-19_13-42-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.320553</td><td style=\"text-align: right;\">0.0944053</td><td>172.26.215.93</td><td style=\"text-align: right;\">632325</td><td style=\"text-align: right;\">0.678153</td><td style=\"text-align: right;\">           340.247  </td><td style=\"text-align: right;\">          2.58578 </td><td style=\"text-align: right;\">     340.247  </td><td style=\"text-align: right;\"> 1689741731</td><td style=\"text-align: right;\">                 100</td><td>ce92bf0a  </td></tr>\n",
       "<tr><td>FSR_Trainable_d12a5617</td><td>2023-07-19_13-06-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.486168</td><td style=\"text-align: right;\">0.121928 </td><td>172.26.215.93</td><td style=\"text-align: right;\">611789</td><td style=\"text-align: right;\">0.762207</td><td style=\"text-align: right;\">             6.8002 </td><td style=\"text-align: right;\">          6.8002  </td><td style=\"text-align: right;\">       6.8002 </td><td style=\"text-align: right;\"> 1689739601</td><td style=\"text-align: right;\">                   1</td><td>d12a5617  </td></tr>\n",
       "<tr><td>FSR_Trainable_d37d81b2</td><td>2023-07-19_13-23-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.32823 </td><td style=\"text-align: right;\">0.0935595</td><td>172.26.215.93</td><td style=\"text-align: right;\">623232</td><td style=\"text-align: right;\">0.688116</td><td style=\"text-align: right;\">             6.30424</td><td style=\"text-align: right;\">          2.98402 </td><td style=\"text-align: right;\">       6.30424</td><td style=\"text-align: right;\"> 1689740597</td><td style=\"text-align: right;\">                   2</td><td>d37d81b2  </td></tr>\n",
       "<tr><td>FSR_Trainable_d82e72f0</td><td>2023-07-19_13-11-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.37913 </td><td style=\"text-align: right;\">0.104066 </td><td>172.26.215.93</td><td style=\"text-align: right;\">616083</td><td style=\"text-align: right;\">0.699165</td><td style=\"text-align: right;\">             2.65916</td><td style=\"text-align: right;\">          2.65916 </td><td style=\"text-align: right;\">       2.65916</td><td style=\"text-align: right;\"> 1689739908</td><td style=\"text-align: right;\">                   1</td><td>d82e72f0  </td></tr>\n",
       "<tr><td>FSR_Trainable_dc7bd9d7</td><td>2023-07-19_13-24-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.330719</td><td style=\"text-align: right;\">0.0969166</td><td>172.26.215.93</td><td style=\"text-align: right;\">623730</td><td style=\"text-align: right;\">0.684127</td><td style=\"text-align: right;\">            40.2003 </td><td style=\"text-align: right;\">          5.345   </td><td style=\"text-align: right;\">      40.2003 </td><td style=\"text-align: right;\"> 1689740665</td><td style=\"text-align: right;\">                   8</td><td>dc7bd9d7  </td></tr>\n",
       "<tr><td>FSR_Trainable_deb61d9b</td><td>2023-07-19_13-26-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.345781</td><td style=\"text-align: right;\">0.0884825</td><td>172.26.215.93</td><td style=\"text-align: right;\">626734</td><td style=\"text-align: right;\">0.72242 </td><td style=\"text-align: right;\">             7.0376 </td><td style=\"text-align: right;\">          7.0376  </td><td style=\"text-align: right;\">       7.0376 </td><td style=\"text-align: right;\"> 1689740810</td><td style=\"text-align: right;\">                   1</td><td>deb61d9b  </td></tr>\n",
       "<tr><td>FSR_Trainable_df9a5432</td><td>2023-07-19_13-10-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.397334</td><td style=\"text-align: right;\">0.103529 </td><td>172.26.215.93</td><td style=\"text-align: right;\">612938</td><td style=\"text-align: right;\">0.715756</td><td style=\"text-align: right;\">           136.132  </td><td style=\"text-align: right;\">          1.42293 </td><td style=\"text-align: right;\">     136.132  </td><td style=\"text-align: right;\"> 1689739801</td><td style=\"text-align: right;\">                 100</td><td>df9a5432  </td></tr>\n",
       "<tr><td>FSR_Trainable_e0453ec9</td><td>2023-07-19_13-12-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.328992</td><td style=\"text-align: right;\">0.0931072</td><td>172.26.215.93</td><td style=\"text-align: right;\">617237</td><td style=\"text-align: right;\">0.687832</td><td style=\"text-align: right;\">            12.5651 </td><td style=\"text-align: right;\">          3.51961 </td><td style=\"text-align: right;\">      12.5651 </td><td style=\"text-align: right;\"> 1689739974</td><td style=\"text-align: right;\">                   4</td><td>e0453ec9  </td></tr>\n",
       "<tr><td>FSR_Trainable_e1f18e57</td><td>2023-07-19_13-03-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.20098 </td><td style=\"text-align: right;\">0.329819 </td><td>172.26.215.93</td><td style=\"text-align: right;\">609985</td><td style=\"text-align: right;\">2.49637 </td><td style=\"text-align: right;\">             5.23355</td><td style=\"text-align: right;\">          5.23355 </td><td style=\"text-align: right;\">       5.23355</td><td style=\"text-align: right;\"> 1689739425</td><td style=\"text-align: right;\">                   1</td><td>e1f18e57  </td></tr>\n",
       "<tr><td>FSR_Trainable_e248c83f</td><td>2023-07-19_13-10-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.326482</td><td style=\"text-align: right;\">0.0954243</td><td>172.26.215.93</td><td style=\"text-align: right;\">614464</td><td style=\"text-align: right;\">0.685834</td><td style=\"text-align: right;\">            10.7055 </td><td style=\"text-align: right;\">          2.48878 </td><td style=\"text-align: right;\">      10.7055 </td><td style=\"text-align: right;\"> 1689739839</td><td style=\"text-align: right;\">                   4</td><td>e248c83f  </td></tr>\n",
       "<tr><td>FSR_Trainable_e56982cb</td><td>2023-07-19_13-31-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.314065</td><td style=\"text-align: right;\">0.0921886</td><td>172.26.215.93</td><td style=\"text-align: right;\">628964</td><td style=\"text-align: right;\">0.687   </td><td style=\"text-align: right;\">            37.8789 </td><td style=\"text-align: right;\">         18.2351  </td><td style=\"text-align: right;\">      37.8789 </td><td style=\"text-align: right;\"> 1689741063</td><td style=\"text-align: right;\">                   2</td><td>e56982cb  </td></tr>\n",
       "<tr><td>FSR_Trainable_e81f61fa</td><td>2023-07-19_13-36-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.320327</td><td style=\"text-align: right;\">0.0935075</td><td>172.26.215.93</td><td style=\"text-align: right;\">626043</td><td style=\"text-align: right;\">0.682766</td><td style=\"text-align: right;\">           614.27   </td><td style=\"text-align: right;\">          6.4647  </td><td style=\"text-align: right;\">     614.27   </td><td style=\"text-align: right;\"> 1689741407</td><td style=\"text-align: right;\">                 100</td><td>e81f61fa  </td></tr>\n",
       "<tr><td>FSR_Trainable_e917eab5</td><td>2023-07-19_13-22-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.351333</td><td style=\"text-align: right;\">0.102933 </td><td>172.26.215.93</td><td style=\"text-align: right;\">622303</td><td style=\"text-align: right;\">0.686912</td><td style=\"text-align: right;\">             6.90926</td><td style=\"text-align: right;\">          3.10342 </td><td style=\"text-align: right;\">       6.90926</td><td style=\"text-align: right;\"> 1689740543</td><td style=\"text-align: right;\">                   2</td><td>e917eab5  </td></tr>\n",
       "<tr><td>FSR_Trainable_e9a64d47</td><td>2023-07-19_13-41-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.319221</td><td style=\"text-align: right;\">0.0976849</td><td>172.26.215.93</td><td style=\"text-align: right;\">631864</td><td style=\"text-align: right;\">0.681033</td><td style=\"text-align: right;\">           345.148  </td><td style=\"text-align: right;\">          3.56547 </td><td style=\"text-align: right;\">     345.148  </td><td style=\"text-align: right;\"> 1689741712</td><td style=\"text-align: right;\">                 100</td><td>e9a64d47  </td></tr>\n",
       "<tr><td>FSR_Trainable_eb715b45</td><td>2023-07-19_13-35-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.35278 </td><td style=\"text-align: right;\">0.100124 </td><td>172.26.215.93</td><td style=\"text-align: right;\">630906</td><td style=\"text-align: right;\">0.688796</td><td style=\"text-align: right;\">            50.9533 </td><td style=\"text-align: right;\">          3.4248  </td><td style=\"text-align: right;\">      50.9533 </td><td style=\"text-align: right;\"> 1689741327</td><td style=\"text-align: right;\">                  16</td><td>eb715b45  </td></tr>\n",
       "<tr><td>FSR_Trainable_eb9d41b3</td><td>2023-07-19_13-24-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.15707 </td><td style=\"text-align: right;\">0.344564 </td><td>172.26.215.93</td><td style=\"text-align: right;\">624641</td><td style=\"text-align: right;\">2.47156 </td><td style=\"text-align: right;\">             4.1124 </td><td style=\"text-align: right;\">          4.1124  </td><td style=\"text-align: right;\">       4.1124 </td><td style=\"text-align: right;\"> 1689740673</td><td style=\"text-align: right;\">                   1</td><td>eb9d41b3  </td></tr>\n",
       "<tr><td>FSR_Trainable_ee29d0c7</td><td>2023-07-19_13-10-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.418753</td><td style=\"text-align: right;\">0.107084 </td><td>172.26.215.93</td><td style=\"text-align: right;\">613906</td><td style=\"text-align: right;\">0.721667</td><td style=\"text-align: right;\">           125.787  </td><td style=\"text-align: right;\">          1.19684 </td><td style=\"text-align: right;\">     125.787  </td><td style=\"text-align: right;\"> 1689739858</td><td style=\"text-align: right;\">                 100</td><td>ee29d0c7  </td></tr>\n",
       "<tr><td>FSR_Trainable_ef498393</td><td>2023-07-19_13-26-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.318909</td><td style=\"text-align: right;\">0.0941739</td><td>172.26.215.93</td><td style=\"text-align: right;\">625853</td><td style=\"text-align: right;\">0.686479</td><td style=\"text-align: right;\">            22.61   </td><td style=\"text-align: right;\">          5.29146 </td><td style=\"text-align: right;\">      22.61   </td><td style=\"text-align: right;\"> 1689740773</td><td style=\"text-align: right;\">                   4</td><td>ef498393  </td></tr>\n",
       "<tr><td>FSR_Trainable_f0ef8d30</td><td>2023-07-19_13-31-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.317388</td><td style=\"text-align: right;\">0.0963785</td><td>172.26.215.93</td><td style=\"text-align: right;\">629240</td><td style=\"text-align: right;\">0.685405</td><td style=\"text-align: right;\">            37.9231 </td><td style=\"text-align: right;\">          9.05884 </td><td style=\"text-align: right;\">      37.9231 </td><td style=\"text-align: right;\"> 1689741119</td><td style=\"text-align: right;\">                   4</td><td>f0ef8d30  </td></tr>\n",
       "<tr><td>FSR_Trainable_f1e9ab93</td><td>2023-07-19_13-12-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.355831</td><td style=\"text-align: right;\">0.0886361</td><td>172.26.215.93</td><td style=\"text-align: right;\">616537</td><td style=\"text-align: right;\">0.721391</td><td style=\"text-align: right;\">             2.84093</td><td style=\"text-align: right;\">          2.84093 </td><td style=\"text-align: right;\">       2.84093</td><td style=\"text-align: right;\"> 1689739930</td><td style=\"text-align: right;\">                   1</td><td>f1e9ab93  </td></tr>\n",
       "<tr><td>FSR_Trainable_f262fb70</td><td>2023-07-19_13-29-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.3223  </td><td style=\"text-align: right;\">0.093493 </td><td>172.26.215.93</td><td style=\"text-align: right;\">628482</td><td style=\"text-align: right;\">0.690649</td><td style=\"text-align: right;\">             6.75502</td><td style=\"text-align: right;\">          6.75502 </td><td style=\"text-align: right;\">       6.75502</td><td style=\"text-align: right;\"> 1689740980</td><td style=\"text-align: right;\">                   1</td><td>f262fb70  </td></tr>\n",
       "<tr><td>FSR_Trainable_f4215bc9</td><td>2023-07-19_13-32-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.381714</td><td style=\"text-align: right;\">0.0997527</td><td>172.26.215.93</td><td style=\"text-align: right;\">629691</td><td style=\"text-align: right;\">0.708416</td><td style=\"text-align: right;\">             2.90923</td><td style=\"text-align: right;\">          2.90923 </td><td style=\"text-align: right;\">       2.90923</td><td style=\"text-align: right;\"> 1689741151</td><td style=\"text-align: right;\">                   1</td><td>f4215bc9  </td></tr>\n",
       "<tr><td>FSR_Trainable_f631e741</td><td>2023-07-19_13-21-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.12281 </td><td style=\"text-align: right;\">0.319948 </td><td>172.26.215.93</td><td style=\"text-align: right;\">621602</td><td style=\"text-align: right;\">2.32981 </td><td style=\"text-align: right;\">             8.26602</td><td style=\"text-align: right;\">          8.26602 </td><td style=\"text-align: right;\">       8.26602</td><td style=\"text-align: right;\"> 1689740480</td><td style=\"text-align: right;\">                   1</td><td>f631e741  </td></tr>\n",
       "<tr><td>FSR_Trainable_febdd3c6</td><td>2023-07-19_13-12-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.333953</td><td style=\"text-align: right;\">0.0951635</td><td>172.26.215.93</td><td style=\"text-align: right;\">617004</td><td style=\"text-align: right;\">0.690255</td><td style=\"text-align: right;\">            11.9135 </td><td style=\"text-align: right;\">          3.39306 </td><td style=\"text-align: right;\">      11.9135 </td><td style=\"text-align: right;\"> 1689739963</td><td style=\"text-align: right;\">                   4</td><td>febdd3c6  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_1be8c882_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_13-03-16/wandb/run-20230719_130328-1be8c882\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Syncing run FSR_Trainable_1be8c882\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/1be8c882\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_6c1193b9_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_13-03-22/wandb/run-20230719_130337-6c1193b9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: Syncing run FSR_Trainable_6c1193b9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/6c1193b9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_e1f18e57_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_13-03-30/wandb/run-20230719_130348-e1f18e57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: Syncing run FSR_Trainable_e1f18e57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e1f18e57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:                      mae 2.20098\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:                     mape 0.32982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:                     rmse 2.49637\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:       time_since_restore 5.23355\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:         time_this_iter_s 5.23355\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:             time_total_s 5.23355\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:                timestamp 1689739425\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb:  View run FSR_Trainable_e1f18e57 at: https://wandb.ai/seokjin/FSR-prediction/runs/e1f18e57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610160)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130348-e1f18e57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_2216a047_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_13-03-39/wandb/run-20230719_130359-2216a047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: Syncing run FSR_Trainable_2216a047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2216a047\n",
      "2023-07-19 13:04:09,088\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.736 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:04:09,096\tWARNING util.py:315 -- The `process_trial_result` operation took 2.745 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:04:09,098\tWARNING util.py:315 -- Processing trial results took 2.747 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:04:09,103\tWARNING util.py:315 -- The `process_trial_result` operation took 2.752 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_72fcdddb_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_13-03-50/wandb/run-20230719_130413-72fcdddb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Syncing run FSR_Trainable_72fcdddb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/72fcdddb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:                      mae 0.37183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:                     mape 0.0941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:                     rmse 0.72247\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:       time_since_restore 71.90586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:         time_this_iter_s 4.21242\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:             time_total_s 71.90586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:                timestamp 1689739485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb:  View run FSR_Trainable_6c1193b9 at: https://wandb.ai/seokjin/FSR-prediction/runs/6c1193b9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609984)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130337-6c1193b9/logs\n",
      "2023-07-19 13:05:03,051\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.511 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:05:03,061\tWARNING util.py:315 -- The `process_trial_result` operation took 2.522 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:05:03,063\tWARNING util.py:315 -- Processing trial results took 2.524 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:05:03,066\tWARNING util.py:315 -- The `process_trial_result` operation took 2.526 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_6f8be931_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_13-04-04/wandb/run-20230719_130506-6f8be931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: Syncing run FSR_Trainable_6f8be931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/6f8be931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:                      mae 0.72369\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:                     mape 0.17336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:                     rmse 0.97325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:       time_since_restore 2.04952\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:         time_this_iter_s 2.04952\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:             time_total_s 2.04952\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:                timestamp 1689739500\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb:  View run FSR_Trainable_6f8be931 at: https://wandb.ai/seokjin/FSR-prediction/runs/6f8be931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610855)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130506-6f8be931/logs\n",
      "2023-07-19 13:05:18,899\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.494 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:05:18,903\tWARNING util.py:315 -- The `process_trial_result` operation took 2.499 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:05:18,904\tWARNING util.py:315 -- Processing trial results took 2.501 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:05:18,906\tWARNING util.py:315 -- The `process_trial_result` operation took 2.503 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_40a99b6a_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_13-04-58/wandb/run-20230719_130522-40a99b6a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb: Syncing run FSR_Trainable_40a99b6a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/40a99b6a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:                      mae 0.41716\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:                     mape 0.10554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:                     rmse 0.71284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:       time_since_restore 27.23506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:         time_this_iter_s 0.90105\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:             time_total_s 27.23506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:                timestamp 1689739545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb:  View run FSR_Trainable_40a99b6a at: https://wandb.ai/seokjin/FSR-prediction/runs/40a99b6a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611092)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130522-40a99b6a/logs\n",
      "2023-07-19 13:06:02,603\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.728 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:06:02,612\tWARNING util.py:315 -- The `process_trial_result` operation took 1.738 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:06:02,616\tWARNING util.py:315 -- Processing trial results took 1.742 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:06:02,619\tWARNING util.py:315 -- The `process_trial_result` operation took 1.746 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_7031dca4_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_13-05-15/wandb/run-20230719_130604-7031dca4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: Syncing run FSR_Trainable_7031dca4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7031dca4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:                      mae 0.58383\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:                     mape 0.1262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:                     rmse 0.8601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:       time_since_restore 3.30136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:         time_this_iter_s 3.30136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:             time_total_s 3.30136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:                timestamp 1689739560\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb:  View run FSR_Trainable_7031dca4 at: https://wandb.ai/seokjin/FSR-prediction/runs/7031dca4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611370)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130604-7031dca4/logs\n",
      "2023-07-19 13:06:20,527\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.191 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:06:20,531\tWARNING util.py:315 -- The `process_trial_result` operation took 2.196 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:06:20,533\tWARNING util.py:315 -- Processing trial results took 2.198 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:06:20,535\tWARNING util.py:315 -- The `process_trial_result` operation took 2.201 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_201cb996_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_13-05-57/wandb/run-20230719_130623-201cb996\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: Syncing run FSR_Trainable_201cb996\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/201cb996\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:                      mae 0.49576\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:                     mape 0.12499\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:                     rmse 0.76907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:       time_since_restore 5.81885\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:         time_this_iter_s 2.7286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:             time_total_s 5.81885\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:                timestamp 1689739583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb:  View run FSR_Trainable_201cb996 at: https://wandb.ai/seokjin/FSR-prediction/runs/201cb996\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611609)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130623-201cb996/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130623-201cb996/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130328-1be8c882/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130328-1be8c882/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=609806)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 13:06:43,211\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.871 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:06:43,215\tWARNING util.py:315 -- The `process_trial_result` operation took 1.876 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:06:43,217\tWARNING util.py:315 -- Processing trial results took 1.878 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:06:43,219\tWARNING util.py:315 -- The `process_trial_result` operation took 1.880 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_d12a5617_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-06-15/wandb/run-20230719_130642-d12a5617\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb: Syncing run FSR_Trainable_d12a5617\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d12a5617\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:                      mae 0.48617\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:                     mape 0.12193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:                     rmse 0.76221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:       time_since_restore 6.8002\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:         time_this_iter_s 6.8002\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:             time_total_s 6.8002\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:                timestamp 1689739601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb:  View run FSR_Trainable_d12a5617 at: https://wandb.ai/seokjin/FSR-prediction/runs/d12a5617\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130642-d12a5617/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=611866)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 13:06:49,725\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.110 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:06:49,730\tWARNING util.py:315 -- The `process_trial_result` operation took 2.116 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:06:49,733\tWARNING util.py:315 -- Processing trial results took 2.118 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:06:49,734\tWARNING util.py:315 -- The `process_trial_result` operation took 2.120 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_cacdc1e0_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-06-34/wandb/run-20230719_130652-cacdc1e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: Syncing run FSR_Trainable_cacdc1e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/cacdc1e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:                      mae 0.5625\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:                     mape 0.13478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:                     rmse 0.84685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:       time_since_restore 1.46733\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:         time_this_iter_s 1.46733\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:             time_total_s 1.46733\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:                timestamp 1689739607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb:  View run FSR_Trainable_cacdc1e0 at: https://wandb.ai/seokjin/FSR-prediction/runs/cacdc1e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612091)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130652-cacdc1e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130413-72fcdddb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_91564664_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-06-46/wandb/run-20230719_130705-91564664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb: Syncing run FSR_Trainable_91564664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/91564664\n",
      "2023-07-19 13:07:13,633\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.456 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:07:13,638\tWARNING util.py:315 -- The `process_trial_result` operation took 2.462 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:07:13,648\tWARNING util.py:315 -- Processing trial results took 2.472 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:07:13,651\tWARNING util.py:315 -- The `process_trial_result` operation took 2.476 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:07:15,986\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.310 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:07:15,990\tWARNING util.py:315 -- The `process_trial_result` operation took 2.315 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:07:15,992\tWARNING util.py:315 -- Processing trial results took 2.317 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:07:15,995\tWARNING util.py:315 -- The `process_trial_result` operation took 2.320 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_b861e4d8_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-06-58/wandb/run-20230719_130716-b861e4d8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Syncing run FSR_Trainable_b861e4d8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b861e4d8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:                      mae 0.46243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:                     mape 0.12627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:                     rmse 0.76968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:       time_since_restore 13.6666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:         time_this_iter_s 13.6666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:             time_total_s 13.6666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:                timestamp 1689739631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb:  View run FSR_Trainable_91564664 at: https://wandb.ai/seokjin/FSR-prediction/runs/91564664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612329)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130705-91564664/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130716-b861e4d8/logs\n",
      "2023-07-19 13:07:24,967\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.090 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:07:24,973\tWARNING util.py:315 -- The `process_trial_result` operation took 2.097 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:07:24,975\tWARNING util.py:315 -- Processing trial results took 2.099 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:07:24,978\tWARNING util.py:315 -- The `process_trial_result` operation took 2.102 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612548)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_56a0d87a_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-07-09/wandb/run-20230719_130728-56a0d87a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb: Syncing run FSR_Trainable_56a0d87a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/56a0d87a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:                      mae 0.39384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:                     mape 0.10219\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:                     rmse 0.72016\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:       time_since_restore 3.21215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:         time_this_iter_s 1.4052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:             time_total_s 3.21215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:                timestamp 1689739646\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb:  View run FSR_Trainable_56a0d87a at: https://wandb.ai/seokjin/FSR-prediction/runs/56a0d87a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130728-56a0d87a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=612797)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 13:07:37,486\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.242 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:07:37,489\tWARNING util.py:315 -- The `process_trial_result` operation took 2.245 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:07:37,492\tWARNING util.py:315 -- Processing trial results took 2.248 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:07:37,496\tWARNING util.py:315 -- The `process_trial_result` operation took 2.252 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_df9a5432_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-07-21/wandb/run-20230719_130740-df9a5432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: Syncing run FSR_Trainable_df9a5432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/df9a5432\n",
      "2023-07-19 13:07:48,467\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.266 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:07:48,472\tWARNING util.py:315 -- The `process_trial_result` operation took 2.271 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:07:48,474\tWARNING util.py:315 -- Processing trial results took 2.273 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:07:48,476\tWARNING util.py:315 -- The `process_trial_result` operation took 2.275 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_17f5cc0f_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-07-33/wandb/run-20230719_130752-17f5cc0f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: Syncing run FSR_Trainable_17f5cc0f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/17f5cc0f\n",
      "2023-07-19 13:08:00,555\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.898 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:08:00,561\tWARNING util.py:315 -- The `process_trial_result` operation took 1.905 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:08:00,563\tWARNING util.py:315 -- Processing trial results took 1.907 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:08:00,567\tWARNING util.py:315 -- The `process_trial_result` operation took 1.911 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_b7290c01_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-07-44/wandb/run-20230719_130803-b7290c01\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb: Syncing run FSR_Trainable_b7290c01\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b7290c01\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:                      mae 0.34628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:                     mape 0.09765\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:                     rmse 0.6964\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:       time_since_restore 6.16707\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:         time_this_iter_s 1.35661\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:             time_total_s 6.16707\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:                timestamp 1689739684\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb:  View run FSR_Trainable_b7290c01 at: https://wandb.ai/seokjin/FSR-prediction/runs/b7290c01\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613474)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130803-b7290c01/logs\n",
      "2023-07-19 13:08:20,871\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.945 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:08:20,875\tWARNING util.py:315 -- The `process_trial_result` operation took 1.950 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:08:20,877\tWARNING util.py:315 -- Processing trial results took 1.952 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:08:20,881\tWARNING util.py:315 -- The `process_trial_result` operation took 1.957 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_23e8fc85_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-07-56/wandb/run-20230719_130824-23e8fc85\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: Syncing run FSR_Trainable_23e8fc85\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/23e8fc85\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:                      mae 0.37896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:                     mape 0.10303\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:                     rmse 0.7052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:       time_since_restore 256.56417\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:         time_this_iter_s 2.55739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:             time_total_s 256.56417\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:                timestamp 1689739708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb:  View run FSR_Trainable_2216a047 at: https://wandb.ai/seokjin/FSR-prediction/runs/2216a047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=610343)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130359-2216a047/logs\n",
      "2023-07-19 13:08:44,740\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.051 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:08:44,743\tWARNING util.py:315 -- The `process_trial_result` operation took 2.055 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:08:44,748\tWARNING util.py:315 -- Processing trial results took 2.060 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:08:44,751\tWARNING util.py:315 -- The `process_trial_result` operation took 2.062 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_ee29d0c7_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-08-17/wandb/run-20230719_130848-ee29d0c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb: Syncing run FSR_Trainable_ee29d0c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ee29d0c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:                      mae 0.39733\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:                     mape 0.10353\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:                     rmse 0.71576\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:       time_since_restore 136.13159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:         time_this_iter_s 1.42293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:             time_total_s 136.13159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:                timestamp 1689739801\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb:  View run FSR_Trainable_df9a5432 at: https://wandb.ai/seokjin/FSR-prediction/runs/df9a5432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613030)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130740-df9a5432/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:                      mae 0.40942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:                     mape 0.104\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:                     rmse 0.71911\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:       time_since_restore 133.42236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:         time_this_iter_s 1.3634\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:             time_total_s 133.42236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:                timestamp 1689739807\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb:  View run FSR_Trainable_17f5cc0f at: https://wandb.ai/seokjin/FSR-prediction/runs/17f5cc0f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613253)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130752-17f5cc0f/logs\n",
      "2023-07-19 13:10:16,651\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.796 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:10:16,656\tWARNING util.py:315 -- The `process_trial_result` operation took 1.802 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:10:16,658\tWARNING util.py:315 -- Processing trial results took 1.804 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:10:16,659\tWARNING util.py:315 -- The `process_trial_result` operation took 1.806 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_9bf4d6f5_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-08-40/wandb/run-20230719_131020-9bf4d6f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: Syncing run FSR_Trainable_9bf4d6f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9bf4d6f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:                      mae 0.35298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:                     mape 0.09684\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:                     rmse 0.69534\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:       time_since_restore 3.01144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:         time_this_iter_s 1.2871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:             time_total_s 3.01144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:                timestamp 1689739817\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb:  View run FSR_Trainable_9bf4d6f5 at: https://wandb.ai/seokjin/FSR-prediction/runs/9bf4d6f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614332)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131020-9bf4d6f5/logs\n",
      "2023-07-19 13:10:31,216\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.230 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:10:31,221\tWARNING util.py:315 -- The `process_trial_result` operation took 2.235 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:10:31,222\tWARNING util.py:315 -- Processing trial results took 2.237 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:10:31,225\tWARNING util.py:315 -- The `process_trial_result` operation took 2.240 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_e248c83f_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-10-13/wandb/run-20230719_131033-e248c83f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: Syncing run FSR_Trainable_e248c83f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e248c83f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:                      mae 0.32648\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:                     mape 0.09542\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:                     rmse 0.68583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:       time_since_restore 10.70545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:         time_this_iter_s 2.48878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:             time_total_s 10.70545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:                timestamp 1689739839\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb:  View run FSR_Trainable_e248c83f at: https://wandb.ai/seokjin/FSR-prediction/runs/e248c83f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614565)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131033-e248c83f/logs\n",
      "2023-07-19 13:10:44,279\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.480 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:10:44,283\tWARNING util.py:315 -- The `process_trial_result` operation took 2.485 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:10:44,285\tWARNING util.py:315 -- Processing trial results took 2.486 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:10:44,286\tWARNING util.py:315 -- The `process_trial_result` operation took 2.488 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130824-23e8fc85/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_8a8d86ae_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-10-26/wandb/run-20230719_131047-8a8d86ae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb: Syncing run FSR_Trainable_8a8d86ae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8a8d86ae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613717)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:                      mae 0.34104\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:                     mape 0.09688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:                     rmse 0.69382\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:       time_since_restore 4.75465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:         time_this_iter_s 1.86385\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:             time_total_s 4.75465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:                timestamp 1689739846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb:  View run FSR_Trainable_8a8d86ae at: https://wandb.ai/seokjin/FSR-prediction/runs/8a8d86ae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131047-8a8d86ae/logs\n",
      "2023-07-19 13:10:55,340\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.350 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:10:55,344\tWARNING util.py:315 -- The `process_trial_result` operation took 2.355 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:10:55,346\tWARNING util.py:315 -- Processing trial results took 2.357 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:10:55,349\tWARNING util.py:315 -- The `process_trial_result` operation took 2.359 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=614787)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_cb005763_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-10-39/wandb/run-20230719_131058-cb005763\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: Syncing run FSR_Trainable_cb005763\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/cb005763\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:                      mae 0.41875\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:                     mape 0.10708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:                     rmse 0.72167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:       time_since_restore 125.78712\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:         time_this_iter_s 1.19684\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:             time_total_s 125.78712\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:                timestamp 1689739858\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb:  View run FSR_Trainable_ee29d0c7 at: https://wandb.ai/seokjin/FSR-prediction/runs/ee29d0c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_130848-ee29d0c7/logs\n",
      "2023-07-19 13:11:06,563\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.176 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:11:06,567\tWARNING util.py:315 -- The `process_trial_result` operation took 2.181 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:11:06,569\tWARNING util.py:315 -- Processing trial results took 2.182 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:11:06,571\tWARNING util.py:315 -- The `process_trial_result` operation took 2.185 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=613972)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_4dc69380_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-10-51/wandb/run-20230719_131109-4dc69380\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: Syncing run FSR_Trainable_4dc69380\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4dc69380\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:                      mae 0.34242\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:                     mape 0.09945\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:                     rmse 0.68745\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:       time_since_restore 7.8028\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:         time_this_iter_s 1.7744\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:             time_total_s 7.8028\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:                timestamp 1689739872\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb:  View run FSR_Trainable_4dc69380 at: https://wandb.ai/seokjin/FSR-prediction/runs/4dc69380\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615274)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131109-4dc69380/logs\n",
      "2023-07-19 13:11:18,391\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.063 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:11:18,396\tWARNING util.py:315 -- The `process_trial_result` operation took 3.069 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:11:18,398\tWARNING util.py:315 -- Processing trial results took 3.071 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:11:18,406\tWARNING util.py:315 -- The `process_trial_result` operation took 3.080 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_53e5695a_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-11-02/wandb/run-20230719_131121-53e5695a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: Syncing run FSR_Trainable_53e5695a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/53e5695a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:                      mae 0.35693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:                     mape 0.10492\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:                     rmse 0.69595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:       time_since_restore 3.82354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:         time_this_iter_s 1.66774\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:             time_total_s 3.82354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:                timestamp 1689739880\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb:  View run FSR_Trainable_53e5695a at: https://wandb.ai/seokjin/FSR-prediction/runs/53e5695a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615501)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131121-53e5695a/logs\n",
      "2023-07-19 13:11:28,844\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.236 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:11:28,849\tWARNING util.py:315 -- The `process_trial_result` operation took 2.242 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:11:28,852\tWARNING util.py:315 -- Processing trial results took 2.245 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:11:28,855\tWARNING util.py:315 -- The `process_trial_result` operation took 2.248 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_b642f7cd_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-11-13/wandb/run-20230719_131131-b642f7cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb: Syncing run FSR_Trainable_b642f7cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b642f7cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:                      mae 0.33698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:                     mape 0.09659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:                     rmse 0.68671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:       time_since_restore 7.16573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:         time_this_iter_s 1.53615\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:             time_total_s 7.16573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:                timestamp 1689739893\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb:  View run FSR_Trainable_b642f7cd at: https://wandb.ai/seokjin/FSR-prediction/runs/b642f7cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615733)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131131-b642f7cd/logs\n",
      "2023-07-19 13:11:39,914\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.187 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:11:39,917\tWARNING util.py:315 -- The `process_trial_result` operation took 2.192 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:11:39,919\tWARNING util.py:315 -- Processing trial results took 2.194 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:11:39,921\tWARNING util.py:315 -- The `process_trial_result` operation took 2.196 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_27b00b7b_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-11-24/wandb/run-20230719_131141-27b00b7b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Syncing run FSR_Trainable_27b00b7b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/27b00b7b\n",
      "2023-07-19 13:11:50,736\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.559 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:11:50,740\tWARNING util.py:315 -- The `process_trial_result` operation took 2.564 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:11:50,742\tWARNING util.py:315 -- Processing trial results took 2.566 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:11:50,744\tWARNING util.py:315 -- The `process_trial_result` operation took 2.568 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_d82e72f0_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-11-34/wandb/run-20230719_131153-d82e72f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: Syncing run FSR_Trainable_d82e72f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d82e72f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:                      mae 0.37913\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:                     mape 0.10407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:                     rmse 0.69916\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:       time_since_restore 2.65916\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:         time_this_iter_s 2.65916\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:             time_total_s 2.65916\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:                timestamp 1689739908\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb:  View run FSR_Trainable_d82e72f0 at: https://wandb.ai/seokjin/FSR-prediction/runs/d82e72f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616191)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131153-d82e72f0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 13:12:02,789\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.154 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:12:02,795\tWARNING util.py:315 -- The `process_trial_result` operation took 2.161 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:12:02,797\tWARNING util.py:315 -- Processing trial results took 2.163 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:12:02,800\tWARNING util.py:315 -- The `process_trial_result` operation took 2.165 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615962)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_888f5e28_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-11-45/wandb/run-20230719_131204-888f5e28\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb: Syncing run FSR_Trainable_888f5e28\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/888f5e28\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:                      mae 0.34801\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:                     mape 0.09581\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:                     rmse 0.6932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:       time_since_restore 5.78847\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:         time_this_iter_s 2.47127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:             time_total_s 5.78847\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:                timestamp 1689739925\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb:  View run FSR_Trainable_888f5e28 at: https://wandb.ai/seokjin/FSR-prediction/runs/888f5e28\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131204-888f5e28/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616418)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 13:12:12,964\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.141 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:12:12,971\tWARNING util.py:315 -- The `process_trial_result` operation took 2.149 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:12:12,974\tWARNING util.py:315 -- Processing trial results took 2.152 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:12:12,976\tWARNING util.py:315 -- The `process_trial_result` operation took 2.154 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_f1e9ab93_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-11-57/wandb/run-20230719_131215-f1e9ab93\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb: Syncing run FSR_Trainable_f1e9ab93\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f1e9ab93\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:                      mae 0.35583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:                     mape 0.08864\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:                     rmse 0.72139\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:       time_since_restore 2.84093\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:         time_this_iter_s 2.84093\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:             time_total_s 2.84093\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:                timestamp 1689739930\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb:  View run FSR_Trainable_f1e9ab93 at: https://wandb.ai/seokjin/FSR-prediction/runs/f1e9ab93\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616661)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131215-f1e9ab93/logs\n",
      "2023-07-19 13:12:23,379\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.451 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:12:23,384\tWARNING util.py:315 -- The `process_trial_result` operation took 2.458 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:12:23,388\tWARNING util.py:315 -- Processing trial results took 2.461 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:12:23,389\tWARNING util.py:315 -- The `process_trial_result` operation took 2.463 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...889)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_648cb2e4_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-12-08/wandb/run-20230719_131225-648cb2e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: Syncing run FSR_Trainable_648cb2e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/648cb2e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:                      mae 0.35843\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:                     mape 0.08464\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:                     rmse 0.73529\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:       time_since_restore 2.73161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:         time_this_iter_s 2.73161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:             time_total_s 2.73161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:                timestamp 1689739940\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb:  View run FSR_Trainable_648cb2e4 at: https://wandb.ai/seokjin/FSR-prediction/runs/648cb2e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=616889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131225-648cb2e4/logs\n",
      "2023-07-19 13:12:34,549\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.147 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:12:34,554\tWARNING util.py:315 -- The `process_trial_result` operation took 2.152 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:12:34,556\tWARNING util.py:315 -- Processing trial results took 2.154 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:12:34,558\tWARNING util.py:315 -- The `process_trial_result` operation took 2.157 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_febdd3c6_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-12-18/wandb/run-20230719_131236-febdd3c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb: Syncing run FSR_Trainable_febdd3c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/febdd3c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 13:12:45,814\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.291 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:12:45,819\tWARNING util.py:315 -- The `process_trial_result` operation took 2.297 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:12:45,821\tWARNING util.py:315 -- Processing trial results took 2.298 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:12:45,822\tWARNING util.py:315 -- The `process_trial_result` operation took 2.300 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:                      mae 0.33395\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:                     mape 0.09516\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:                     rmse 0.69025\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:       time_since_restore 11.91354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:         time_this_iter_s 3.39306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:             time_total_s 11.91354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:                timestamp 1689739963\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb:  View run FSR_Trainable_febdd3c6 at: https://wandb.ai/seokjin/FSR-prediction/runs/febdd3c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617117)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131236-febdd3c6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_e0453ec9_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-12-29/wandb/run-20230719_131247-e0453ec9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb: Syncing run FSR_Trainable_e0453ec9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e0453ec9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 13:12:57,474\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.379 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:12:57,487\tWARNING util.py:315 -- The `process_trial_result` operation took 2.393 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:12:57,490\tWARNING util.py:315 -- Processing trial results took 2.396 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:12:57,493\tWARNING util.py:315 -- The `process_trial_result` operation took 2.399 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:                      mae 0.32899\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:                     mape 0.09311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:                     rmse 0.68783\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:       time_since_restore 12.56515\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:         time_this_iter_s 3.51961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:             time_total_s 12.56515\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:                timestamp 1689739974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb:  View run FSR_Trainable_e0453ec9 at: https://wandb.ai/seokjin/FSR-prediction/runs/e0453ec9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617346)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131247-e0453ec9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_5459c42c_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-12-39/wandb/run-20230719_131258-5459c42c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Syncing run FSR_Trainable_5459c42c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5459c42c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131258-5459c42c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131258-5459c42c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131258-5459c42c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131258-5459c42c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131258-5459c42c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131258-5459c42c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131258-5459c42c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131258-5459c42c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131258-5459c42c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131258-5459c42c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131258-5459c42c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131258-5459c42c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131258-5459c42c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617579)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131258-5459c42c/logs\n",
      "2023-07-19 13:13:06,631\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.222 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:13:06,636\tWARNING util.py:315 -- The `process_trial_result` operation took 2.227 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:13:06,637\tWARNING util.py:315 -- Processing trial results took 2.228 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:13:06,639\tWARNING util.py:315 -- The `process_trial_result` operation took 2.231 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_7079193b_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-12-51/wandb/run-20230719_131309-7079193b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb: Syncing run FSR_Trainable_7079193b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7079193b\n",
      "2023-07-19 13:13:18,179\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.352 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:13:18,182\tWARNING util.py:315 -- The `process_trial_result` operation took 2.356 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:13:18,185\tWARNING util.py:315 -- Processing trial results took 2.359 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:13:18,187\tWARNING util.py:315 -- The `process_trial_result` operation took 2.361 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_7525721c_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-13-02/wandb/run-20230719_131320-7525721c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Syncing run FSR_Trainable_7525721c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7525721c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:                      mae 0.34244\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:                     mape 0.09471\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:                     rmse 0.6856\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:       time_since_restore 14.72057\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:         time_this_iter_s 1.83304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:             time_total_s 14.72057\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:                timestamp 1689740000\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb:  View run FSR_Trainable_7079193b at: https://wandb.ai/seokjin/FSR-prediction/runs/7079193b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=617814)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131309-7079193b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131320-7525721c/logs\n",
      "2023-07-19 13:13:29,507\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.229 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:13:29,510\tWARNING util.py:315 -- The `process_trial_result` operation took 2.234 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:13:29,512\tWARNING util.py:315 -- Processing trial results took 2.236 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:13:29,515\tWARNING util.py:315 -- The `process_trial_result` operation took 2.239 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618039)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_ac099330_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-13-13/wandb/run-20230719_131332-ac099330\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: Syncing run FSR_Trainable_ac099330\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ac099330\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_a66aa590_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-13-25/wandb/run-20230719_131347-a66aa590\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb: Syncing run FSR_Trainable_a66aa590\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a66aa590\n",
      "2023-07-19 13:13:51,575\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.938 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:13:51,576\tWARNING util.py:315 -- The `process_trial_result` operation took 1.941 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:13:51,577\tWARNING util.py:315 -- Processing trial results took 1.942 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:13:51,578\tWARNING util.py:315 -- The `process_trial_result` operation took 1.943 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_0641d810_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-13-36/wandb/run-20230719_131404-0641d810\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: Syncing run FSR_Trainable_0641d810\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0641d810\n",
      "2023-07-19 13:14:10,020\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.927 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:14:10,027\tWARNING util.py:315 -- The `process_trial_result` operation took 2.935 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:14:10,031\tWARNING util.py:315 -- Processing trial results took 2.940 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:14:10,038\tWARNING util.py:315 -- The `process_trial_result` operation took 2.946 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:                      mae 0.32515\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:                     mape 0.09333\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:                     rmse 0.68067\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:       time_since_restore 167.5857\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:         time_this_iter_s 2.85518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:             time_total_s 167.5857\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:                timestamp 1689740047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb:  View run FSR_Trainable_cb005763 at: https://wandb.ai/seokjin/FSR-prediction/runs/cb005763\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=615043)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131058-cb005763/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:                      mae 0.31982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:                     mape 0.09447\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:                     rmse 0.68601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:       time_since_restore 47.9275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:         time_this_iter_s 9.64881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:             time_total_s 47.9275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:                timestamp 1689740066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb:  View run FSR_Trainable_a66aa590 at: https://wandb.ai/seokjin/FSR-prediction/runs/a66aa590\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618502)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131347-a66aa590/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_74b5da82_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-13-51/wandb/run-20230719_131433-74b5da82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb: Syncing run FSR_Trainable_74b5da82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/74b5da82\n",
      "2023-07-19 13:14:38,316\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.456 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:14:38,329\tWARNING util.py:315 -- The `process_trial_result` operation took 2.469 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:14:38,336\tWARNING util.py:315 -- Processing trial results took 2.476 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:14:38,338\tWARNING util.py:315 -- The `process_trial_result` operation took 2.478 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_986747c2_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-14-24/wandb/run-20230719_131451-986747c2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: Syncing run FSR_Trainable_986747c2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/986747c2\n",
      "2023-07-19 13:14:57,047\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.089 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:14:57,049\tWARNING util.py:315 -- The `process_trial_result` operation took 3.092 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:14:57,053\tWARNING util.py:315 -- Processing trial results took 3.096 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:14:57,057\tWARNING util.py:315 -- The `process_trial_result` operation took 3.099 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:                      mae 0.3159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:                     mape 0.09333\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:                     rmse 0.68633\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:       time_since_restore 45.12683\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:         time_this_iter_s 10.76936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:             time_total_s 45.12683\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:                timestamp 1689740112\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb:  View run FSR_Trainable_74b5da82 at: https://wandb.ai/seokjin/FSR-prediction/runs/74b5da82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618972)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131433-74b5da82/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:                      mae 0.36226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:                     mape 0.10082\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:                     rmse 0.68995\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:       time_since_restore 89.32071\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:         time_this_iter_s 9.70925\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:             time_total_s 89.32071\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:                timestamp 1689740124\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb:  View run FSR_Trainable_0641d810 at: https://wandb.ai/seokjin/FSR-prediction/runs/0641d810\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618723)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131404-0641d810/logs\n",
      "2023-07-19 13:15:34,115\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.702 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:15:34,119\tWARNING util.py:315 -- The `process_trial_result` operation took 2.707 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:15:34,121\tWARNING util.py:315 -- Processing trial results took 2.710 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:15:34,127\tWARNING util.py:315 -- The `process_trial_result` operation took 2.715 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619477)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...477)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619477)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619477)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619477)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619477)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_768e63a9_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-14-40/wandb/run-20230719_131535-768e63a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619477)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619477)\u001b[0m wandb: Syncing run FSR_Trainable_768e63a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619477)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619477)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/768e63a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_a278091f_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-15-26/wandb/run-20230719_131550-a278091f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb: Syncing run FSR_Trainable_a278091f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a278091f\n",
      "2023-07-19 13:15:53,910\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.447 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:15:53,914\tWARNING util.py:315 -- The `process_trial_result` operation took 2.452 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:15:53,916\tWARNING util.py:315 -- Processing trial results took 2.453 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:15:53,919\tWARNING util.py:315 -- The `process_trial_result` operation took 2.456 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:                      mae 0.34051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:                     mape 0.08596\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:                     rmse 0.72656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:       time_since_restore 10.32221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:         time_this_iter_s 10.32221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:             time_total_s 10.32221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:                timestamp 1689740151\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb:  View run FSR_Trainable_a278091f at: https://wandb.ai/seokjin/FSR-prediction/runs/a278091f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619706)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131550-a278091f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_60a505df_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-15-41/wandb/run-20230719_131620-60a505df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb: Syncing run FSR_Trainable_60a505df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/60a505df\n",
      "2023-07-19 13:16:24,618\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.943 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:16:24,620\tWARNING util.py:315 -- The `process_trial_result` operation took 2.948 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:16:24,623\tWARNING util.py:315 -- Processing trial results took 2.951 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:16:24,637\tWARNING util.py:315 -- The `process_trial_result` operation took 2.964 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:                      mae 0.33749\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:                     mape 0.08525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:                     rmse 0.72738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:       time_since_restore 12.35371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:         time_this_iter_s 12.35371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:             time_total_s 12.35371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:                timestamp 1689740181\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb:  View run FSR_Trainable_60a505df at: https://wandb.ai/seokjin/FSR-prediction/runs/60a505df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619952)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131620-60a505df/logs\n",
      "2023-07-19 13:16:49,970\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.779 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:16:49,975\tWARNING util.py:315 -- The `process_trial_result` operation took 3.784 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:16:49,977\tWARNING util.py:315 -- Processing trial results took 3.787 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:16:49,981\tWARNING util.py:315 -- The `process_trial_result` operation took 3.791 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_0d0bcd1f_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-16-09/wandb/run-20230719_131651-0d0bcd1f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: Syncing run FSR_Trainable_0d0bcd1f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0d0bcd1f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:                      mae 2.21178\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:                     mape 0.34587\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:                     rmse 2.56655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:       time_since_restore 5.31789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:         time_this_iter_s 5.31789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:             time_total_s 5.31789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:                timestamp 1689740206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb:  View run FSR_Trainable_0d0bcd1f at: https://wandb.ai/seokjin/FSR-prediction/runs/0d0bcd1f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620202)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131651-0d0bcd1f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_17fee470_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-16-40/wandb/run-20230719_131719-17fee470\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb: Syncing run FSR_Trainable_17fee470\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/17fee470\n",
      "2023-07-19 13:17:25,081\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.375 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:17:25,086\tWARNING util.py:315 -- The `process_trial_result` operation took 2.380 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:17:25,087\tWARNING util.py:315 -- Processing trial results took 2.382 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:17:25,089\tWARNING util.py:315 -- The `process_trial_result` operation took 2.383 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:                      mae 0.34732\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:                     mape 0.0964\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:                     rmse 0.67831\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:       time_since_restore 268.85722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:         time_this_iter_s 2.30054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:             time_total_s 268.85722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:                timestamp 1689740293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb:  View run FSR_Trainable_ac099330 at: https://wandb.ai/seokjin/FSR-prediction/runs/ac099330\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=618274)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131332-ac099330/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620754)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620754)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620754)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620754)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620754)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_37ff4b86_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-17-07/wandb/run-20230719_131838-37ff4b86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620754)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620754)\u001b[0m wandb: Syncing run FSR_Trainable_37ff4b86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620754)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620754)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/37ff4b86\n",
      "2023-07-19 13:18:43,566\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.568 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:18:43,571\tWARNING util.py:315 -- The `process_trial_result` operation took 2.574 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:18:43,577\tWARNING util.py:315 -- Processing trial results took 2.580 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:18:43,580\tWARNING util.py:315 -- The `process_trial_result` operation took 2.584 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-07-19 13:19:14,665 E 609381 609381] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: aefd3402ebd2fc51e5dbdd1e08bca8d13488b5714e207568084755de, IP: 172.26.215.93) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.26.215.93`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:                      mae 0.34767\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:                     mape 0.09692\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:                     rmse 0.68436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:       time_since_restore 180.78781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:         time_this_iter_s 10.85985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:             time_total_s 180.78781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:                timestamp 1689740411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb:  View run FSR_Trainable_17fee470 at: https://wandb.ai/seokjin/FSR-prediction/runs/17fee470\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=620453)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131719-17fee470/logs\n",
      "2023-07-19 13:20:20,140\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 5.480 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:20:20,148\tWARNING util.py:315 -- The `process_trial_result` operation took 5.489 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:20:20,150\tWARNING util.py:315 -- Processing trial results took 5.491 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:20:20,156\tWARNING util.py:315 -- The `process_trial_result` operation took 5.497 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_37ff4b86_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-17-07/wandb/run-20230719_132024-37ff4b86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: Syncing run FSR_Trainable_37ff4b86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/37ff4b86\n",
      "2023-07-19 13:20:33,358\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.950 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:20:33,362\tWARNING util.py:315 -- The `process_trial_result` operation took 2.956 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:20:33,364\tWARNING util.py:315 -- Processing trial results took 2.957 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:20:33,366\tWARNING util.py:315 -- The `process_trial_result` operation took 2.959 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_abdb1be5_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-18-27/wandb/run-20230719_132036-abdb1be5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: Syncing run FSR_Trainable_abdb1be5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/abdb1be5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:                      mae 0.34176\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:                     mape 0.09495\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:                     rmse 0.69096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:       time_since_restore 5.754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:         time_this_iter_s 2.86056\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:             time_total_s 5.754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:                timestamp 1689740436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb:  View run FSR_Trainable_abdb1be5 at: https://wandb.ai/seokjin/FSR-prediction/runs/abdb1be5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621208)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132036-abdb1be5/logs\n",
      "2023-07-19 13:20:57,036\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.453 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:20:57,048\tWARNING util.py:315 -- The `process_trial_result` operation took 2.466 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:20:57,051\tWARNING util.py:315 -- Processing trial results took 2.469 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:20:57,053\tWARNING util.py:315 -- The `process_trial_result` operation took 2.471 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...447)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_ab865ecb_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-20-27/wandb/run-20230719_132059-ab865ecb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: Syncing run FSR_Trainable_ab865ecb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ab865ecb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:                      mae 0.36615\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:                     mape 0.1027\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:                     rmse 0.6979\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:       time_since_restore 4.45051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:         time_this_iter_s 4.45051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:             time_total_s 4.45051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:                timestamp 1689740454\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb:  View run FSR_Trainable_ab865ecb at: https://wandb.ai/seokjin/FSR-prediction/runs/ab865ecb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621447)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132059-ab865ecb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 13:21:22,821\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.204 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:21:22,826\tWARNING util.py:315 -- The `process_trial_result` operation took 2.210 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:21:22,829\tWARNING util.py:315 -- Processing trial results took 2.213 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:21:22,831\tWARNING util.py:315 -- The `process_trial_result` operation took 2.215 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_f631e741_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-20-50/wandb/run-20230719_132122-f631e741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: Syncing run FSR_Trainable_f631e741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f631e741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:                      mae 2.12281\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:                     mape 0.31995\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:                     rmse 2.32981\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:       time_since_restore 8.26602\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:         time_this_iter_s 8.26602\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:             time_total_s 8.26602\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:                timestamp 1689740480\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb:  View run FSR_Trainable_f631e741 at: https://wandb.ai/seokjin/FSR-prediction/runs/f631e741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621689)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132122-f631e741/logs\n",
      "2023-07-19 13:21:46,610\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.777 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:21:46,617\tWARNING util.py:315 -- The `process_trial_result` operation took 2.785 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:21:46,620\tWARNING util.py:315 -- Processing trial results took 2.787 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:21:46,623\tWARNING util.py:315 -- The `process_trial_result` operation took 2.790 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_592fc33b_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-21-12/wandb/run-20230719_132146-592fc33b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Syncing run FSR_Trainable_592fc33b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/592fc33b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:                      mae 0.33704\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:                     mape 0.1003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:                     rmse 0.68594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:       time_since_restore 183.80263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:         time_this_iter_s 11.81592\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:             time_total_s 183.80263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:                timestamp 1689740505\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb:  View run FSR_Trainable_37ff4b86 at: https://wandb.ai/seokjin/FSR-prediction/runs/37ff4b86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621023)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132024-37ff4b86/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132024-37ff4b86/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb:       training_iteration \n",
      "2023-07-19 13:22:06,529\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.997 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:22:06,533\tWARNING util.py:315 -- The `process_trial_result` operation took 3.003 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:22:06,538\tWARNING util.py:315 -- Processing trial results took 3.007 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:22:06,540\tWARNING util.py:315 -- The `process_trial_result` operation took 3.009 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=621934)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_a3ca3991_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-21-37/wandb/run-20230719_132209-a3ca3991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb: Syncing run FSR_Trainable_a3ca3991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a3ca3991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:                      mae 0.34708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:                     mape 0.0942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:                     rmse 0.69702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:       time_since_restore 3.39193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:         time_this_iter_s 3.39193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:             time_total_s 3.39193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:                timestamp 1689740523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb:  View run FSR_Trainable_a3ca3991 at: https://wandb.ai/seokjin/FSR-prediction/runs/a3ca3991\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622187)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132209-a3ca3991/logs\n",
      "2023-07-19 13:22:20,037\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.755 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:22:20,041\tWARNING util.py:315 -- The `process_trial_result` operation took 2.761 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:22:20,044\tWARNING util.py:315 -- Processing trial results took 2.763 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:22:20,046\tWARNING util.py:315 -- The `process_trial_result` operation took 2.765 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_e917eab5_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-22-00/wandb/run-20230719_132222-e917eab5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: Syncing run FSR_Trainable_e917eab5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e917eab5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:                      mae 0.35133\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:                     mape 0.10293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:                     rmse 0.68691\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:       time_since_restore 6.90926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:         time_this_iter_s 3.10342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:             time_total_s 6.90926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:                timestamp 1689740543\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb:  View run FSR_Trainable_e917eab5 at: https://wandb.ai/seokjin/FSR-prediction/runs/e917eab5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622413)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132222-e917eab5/logs\n",
      "2023-07-19 13:22:33,944\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.128 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:22:33,964\tWARNING util.py:315 -- The `process_trial_result` operation took 3.149 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:22:33,966\tWARNING util.py:315 -- Processing trial results took 3.151 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:22:33,969\tWARNING util.py:315 -- The `process_trial_result` operation took 3.154 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_725ab5d7_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-22-13/wandb/run-20230719_132236-725ab5d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: Syncing run FSR_Trainable_725ab5d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/725ab5d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:                      mae 0.37664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:                     mape 0.08943\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:                     rmse 0.7399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:       time_since_restore 3.2413\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:         time_this_iter_s 3.2413\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:             time_total_s 3.2413\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:                timestamp 1689740550\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb:  View run FSR_Trainable_725ab5d7 at: https://wandb.ai/seokjin/FSR-prediction/runs/725ab5d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622647)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132236-725ab5d7/logs\n",
      "2023-07-19 13:22:47,674\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.038 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:22:47,680\tWARNING util.py:315 -- The `process_trial_result` operation took 3.046 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:22:47,683\tWARNING util.py:315 -- Processing trial results took 3.049 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:22:47,687\tWARNING util.py:315 -- The `process_trial_result` operation took 3.052 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_cb758a07_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-22-27/wandb/run-20230719_132250-cb758a07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: Syncing run FSR_Trainable_cb758a07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/cb758a07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:                      mae 0.33225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:                     mape 0.09457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:                     rmse 0.68851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:       time_since_restore 6.08518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:         time_this_iter_s 2.86471\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:             time_total_s 6.08518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:                timestamp 1689740570\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb:  View run FSR_Trainable_cb758a07 at: https://wandb.ai/seokjin/FSR-prediction/runs/cb758a07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=622881)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132250-cb758a07/logs\n",
      "2023-07-19 13:23:01,221\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.690 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:23:01,224\tWARNING util.py:315 -- The `process_trial_result` operation took 2.694 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:23:01,229\tWARNING util.py:315 -- Processing trial results took 2.699 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:23:01,231\tWARNING util.py:315 -- The `process_trial_result` operation took 2.701 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_856936f7_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-22-41/wandb/run-20230719_132303-856936f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb: Syncing run FSR_Trainable_856936f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/856936f7\n",
      "2023-07-19 13:23:14,562\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.611 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:23:14,565\tWARNING util.py:315 -- The `process_trial_result` operation took 2.615 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:23:14,568\tWARNING util.py:315 -- Processing trial results took 2.618 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:23:14,570\tWARNING util.py:315 -- The `process_trial_result` operation took 2.620 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:23:15,436\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 0.700 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:23:15,442\tWARNING util.py:315 -- The `process_trial_result` operation took 0.706 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:23:15,445\tWARNING util.py:315 -- Processing trial results took 0.708 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:23:15,447\tWARNING util.py:315 -- The `process_trial_result` operation took 0.711 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_d37d81b2_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-22-55/wandb/run-20230719_132317-d37d81b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: Syncing run FSR_Trainable_d37d81b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d37d81b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:                      mae 0.32823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:                     mape 0.09356\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:                     rmse 0.68812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:       time_since_restore 6.30424\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:         time_this_iter_s 2.98402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:             time_total_s 6.30424\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:                timestamp 1689740597\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb:  View run FSR_Trainable_d37d81b2 at: https://wandb.ai/seokjin/FSR-prediction/runs/d37d81b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623342)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132317-d37d81b2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619477)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619477)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619477)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619477)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619477)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619477)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619477)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619477)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619477)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb: Waiting for W&B process to finish... (success).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2023-07-19 13:23:33,801\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.616 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:23:33,806\tWARNING util.py:315 -- The `process_trial_result` operation took 2.624 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:23:33,808\tWARNING util.py:315 -- Processing trial results took 2.625 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:23:33,810\tWARNING util.py:315 -- The `process_trial_result` operation took 2.627 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb: iterations_since_restore 8\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:                      mae 0.33203\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:                     mape 0.09977\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:                     rmse 0.68375\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:       time_since_restore 21.93754\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:         time_this_iter_s 2.40322\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:             time_total_s 21.93754\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:                timestamp 1689740602\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:       training_iteration 8\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb:  View run FSR_Trainable_856936f7 at: https://wandb.ai/seokjin/FSR-prediction/runs/856936f7\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623115)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132303-856936f7/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_7429e81f_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-23-08/wandb/run-20230719_132336-7429e81f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: Syncing run FSR_Trainable_7429e81f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7429e81f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb:  View run FSR_Trainable_7429e81f at: https://wandb.ai/seokjin/FSR-prediction/runs/7429e81f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623617)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132336-7429e81f/logs\n",
      "2023-07-19 13:23:46,814\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.126 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:23:46,818\tWARNING util.py:315 -- The `process_trial_result` operation took 2.130 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:23:46,819\tWARNING util.py:315 -- Processing trial results took 2.132 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:23:46,821\tWARNING util.py:315 -- The `process_trial_result` operation took 2.134 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_dc7bd9d7_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-23-28/wandb/run-20230719_132347-dc7bd9d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb: Syncing run FSR_Trainable_dc7bd9d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/dc7bd9d7\n",
      "2023-07-19 13:23:56,899\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.879 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:23:56,902\tWARNING util.py:315 -- The `process_trial_result` operation took 2.884 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:23:56,905\tWARNING util.py:315 -- Processing trial results took 2.886 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:23:56,911\tWARNING util.py:315 -- The `process_trial_result` operation took 2.892 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_b057f1a8_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-23-39/wandb/run-20230719_132359-b057f1a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: Syncing run FSR_Trainable_b057f1a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b057f1a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:                      mae 0.33599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:                     mape 0.10031\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:                     rmse 0.68849\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:       time_since_restore 5.546\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:         time_this_iter_s 2.50095\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:             time_total_s 5.546\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:                timestamp 1689740639\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb:  View run FSR_Trainable_b057f1a8 at: https://wandb.ai/seokjin/FSR-prediction/runs/b057f1a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624061)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132359-b057f1a8/logs\n",
      "2023-07-19 13:24:09,821\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.638 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:24:09,825\tWARNING util.py:315 -- The `process_trial_result` operation took 2.643 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:24:09,827\tWARNING util.py:315 -- Processing trial results took 2.645 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:24:09,829\tWARNING util.py:315 -- The `process_trial_result` operation took 2.647 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_a18bd0d6_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-23-51/wandb/run-20230719_132412-a18bd0d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: Syncing run FSR_Trainable_a18bd0d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a18bd0d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:                      mae 0.34831\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:                     mape 0.10176\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:                     rmse 0.68975\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:       time_since_restore 5.49573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:         time_this_iter_s 2.43698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:             time_total_s 5.49573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:                timestamp 1689740652\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb:  View run FSR_Trainable_a18bd0d6 at: https://wandb.ai/seokjin/FSR-prediction/runs/a18bd0d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624289)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132412-a18bd0d6/logs\n",
      "2023-07-19 13:24:24,984\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.188 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:24:24,989\tWARNING util.py:315 -- The `process_trial_result` operation took 3.194 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:24:24,992\tWARNING util.py:315 -- Processing trial results took 3.197 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:24:24,996\tWARNING util.py:315 -- The `process_trial_result` operation took 3.202 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_5e7eb88f_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-24-04/wandb/run-20230719_132426-5e7eb88f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Syncing run FSR_Trainable_5e7eb88f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5e7eb88f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:                      mae 0.33072\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:                     mape 0.09692\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:                     rmse 0.68413\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:       time_since_restore 40.20033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:         time_this_iter_s 5.345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:             time_total_s 40.20033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:                timestamp 1689740665\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb:  View run FSR_Trainable_dc7bd9d7 at: https://wandb.ai/seokjin/FSR-prediction/runs/dc7bd9d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=623839)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132347-dc7bd9d7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb:       training_iteration \n",
      "2023-07-19 13:24:36,112\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.259 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:24:36,118\tWARNING util.py:315 -- The `process_trial_result` operation took 2.265 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:24:36,119\tWARNING util.py:315 -- Processing trial results took 2.267 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:24:36,121\tWARNING util.py:315 -- The `process_trial_result` operation took 2.269 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624521)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_eb9d41b3_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-24-17/wandb/run-20230719_132437-eb9d41b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb: Syncing run FSR_Trainable_eb9d41b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/eb9d41b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:                      mae 2.15707\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:                     mape 0.34456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:                     rmse 2.47156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:       time_since_restore 4.1124\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:         time_this_iter_s 4.1124\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:             time_total_s 4.1124\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:                timestamp 1689740673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb:  View run FSR_Trainable_eb9d41b3 at: https://wandb.ai/seokjin/FSR-prediction/runs/eb9d41b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132437-eb9d41b3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624765)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 13:24:47,423\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.564 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:24:47,447\tWARNING util.py:315 -- The `process_trial_result` operation took 2.589 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:24:47,454\tWARNING util.py:315 -- Processing trial results took 2.596 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:24:47,462\tWARNING util.py:315 -- The `process_trial_result` operation took 2.604 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_575aa300_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-24-29/wandb/run-20230719_132448-575aa300\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb: Syncing run FSR_Trainable_575aa300\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/575aa300\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:                      mae 0.32452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:                     mape 0.09234\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:                     rmse 0.68788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:       time_since_restore 7.92396\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:         time_this_iter_s 3.74272\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:             time_total_s 7.92396\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:                timestamp 1689740691\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb:  View run FSR_Trainable_575aa300 at: https://wandb.ai/seokjin/FSR-prediction/runs/575aa300\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=624992)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132448-575aa300/logs\n",
      "2023-07-19 13:25:00,650\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.840 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:25:00,654\tWARNING util.py:315 -- The `process_trial_result` operation took 2.845 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:25:00,659\tWARNING util.py:315 -- Processing trial results took 2.850 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:25:00,662\tWARNING util.py:315 -- The `process_trial_result` operation took 2.853 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_4e0c0db9_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-24-40/wandb/run-20230719_132501-4e0c0db9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb: Syncing run FSR_Trainable_4e0c0db9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4e0c0db9\n",
      "2023-07-19 13:25:13,788\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.389 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:25:13,790\tWARNING util.py:315 -- The `process_trial_result` operation took 2.393 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:25:13,794\tWARNING util.py:315 -- Processing trial results took 2.397 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:25:13,797\tWARNING util.py:315 -- The `process_trial_result` operation took 2.400 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_60592dfe_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-24-52/wandb/run-20230719_132513-60592dfe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb: Syncing run FSR_Trainable_60592dfe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/60592dfe\n",
      "2023-07-19 13:25:28,755\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.882 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:25:28,760\tWARNING util.py:315 -- The `process_trial_result` operation took 2.888 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:25:28,763\tWARNING util.py:315 -- Processing trial results took 2.890 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:25:28,767\tWARNING util.py:315 -- The `process_trial_result` operation took 2.894 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_458c4585_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-25-05/wandb/run-20230719_132529-458c4585\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb: Syncing run FSR_Trainable_458c4585\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/458c4585\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:                      mae 0.32114\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:                     mape 0.096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:                     rmse 0.68364\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:       time_since_restore 36.3765\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:         time_this_iter_s 4.38065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:             time_total_s 36.3765\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:                timestamp 1689740733\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb:  View run FSR_Trainable_4e0c0db9 at: https://wandb.ai/seokjin/FSR-prediction/runs/4e0c0db9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625225)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132501-4e0c0db9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:                      mae 0.32636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:                     mape 0.09479\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:                     rmse 0.68532\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:       time_since_restore 21.35839\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:         time_this_iter_s 4.72171\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:             time_total_s 21.35839\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:                timestamp 1689740743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb:  View run FSR_Trainable_458c4585 at: https://wandb.ai/seokjin/FSR-prediction/runs/458c4585\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625670)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132529-458c4585/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:                      mae 0.3281\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:                     mape 0.09429\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:                     rmse 0.68419\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:       time_since_restore 41.79924\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:         time_this_iter_s 5.10008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:             time_total_s 41.79924\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:                timestamp 1689740750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb:  View run FSR_Trainable_60592dfe at: https://wandb.ai/seokjin/FSR-prediction/runs/60592dfe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625449)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132513-60592dfe/logs\n",
      "2023-07-19 13:25:57,165\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.850 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:25:57,169\tWARNING util.py:315 -- The `process_trial_result` operation took 2.855 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:25:57,171\tWARNING util.py:315 -- Processing trial results took 2.857 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:25:57,173\tWARNING util.py:315 -- The `process_trial_result` operation took 2.859 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_ef498393_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-25-19/wandb/run-20230719_132557-ef498393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb: Syncing run FSR_Trainable_ef498393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ef498393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 13:26:11,746\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.955 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:26:11,751\tWARNING util.py:315 -- The `process_trial_result` operation took 2.961 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:26:11,753\tWARNING util.py:315 -- Processing trial results took 2.963 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:26:11,756\tWARNING util.py:315 -- The `process_trial_result` operation took 2.966 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_e81f61fa_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-25-47/wandb/run-20230719_132610-e81f61fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Syncing run FSR_Trainable_e81f61fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e81f61fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:                      mae 0.31891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:                     mape 0.09417\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:                     rmse 0.68648\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:       time_since_restore 22.60997\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:         time_this_iter_s 5.29146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:             time_total_s 22.60997\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:                timestamp 1689740773\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb:  View run FSR_Trainable_ef498393 at: https://wandb.ai/seokjin/FSR-prediction/runs/ef498393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=625922)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132557-ef498393/logs\n",
      "2023-07-19 13:26:24,164\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.069 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:26:24,171\tWARNING util.py:315 -- The `process_trial_result` operation took 3.077 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:26:24,175\tWARNING util.py:315 -- Processing trial results took 3.081 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:26:24,177\tWARNING util.py:315 -- The `process_trial_result` operation took 3.083 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_425399e5_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-26-01/wandb/run-20230719_132624-425399e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: Syncing run FSR_Trainable_425399e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/425399e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:                      mae 0.32446\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:                     mape 0.09241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:                     rmse 0.68826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:       time_since_restore 9.57745\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:         time_this_iter_s 4.35064\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:             time_total_s 9.57745\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:                timestamp 1689740788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb:  View run FSR_Trainable_425399e5 at: https://wandb.ai/seokjin/FSR-prediction/runs/425399e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626384)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132624-425399e5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 13:26:39,944\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.521 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:26:39,950\tWARNING util.py:315 -- The `process_trial_result` operation took 2.528 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:26:39,952\tWARNING util.py:315 -- Processing trial results took 2.529 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:26:39,954\tWARNING util.py:315 -- The `process_trial_result` operation took 2.532 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_977aa183_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-26-15/wandb/run-20230719_132639-977aa183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Syncing run FSR_Trainable_977aa183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/977aa183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 13:26:53,558\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.592 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:26:53,564\tWARNING util.py:315 -- The `process_trial_result` operation took 2.599 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:26:53,566\tWARNING util.py:315 -- Processing trial results took 2.600 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:26:53,570\tWARNING util.py:315 -- The `process_trial_result` operation took 2.604 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_deb61d9b_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-26-30/wandb/run-20230719_132653-deb61d9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: Syncing run FSR_Trainable_deb61d9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/deb61d9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:                      mae 0.34578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:                     mape 0.08848\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:                     rmse 0.72242\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:       time_since_restore 7.0376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:         time_this_iter_s 7.0376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:             time_total_s 7.0376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:                timestamp 1689740810\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb:  View run FSR_Trainable_deb61d9b at: https://wandb.ai/seokjin/FSR-prediction/runs/deb61d9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626848)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132653-deb61d9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 13:27:17,440\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.933 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:27:17,446\tWARNING util.py:315 -- The `process_trial_result` operation took 2.940 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:27:17,455\tWARNING util.py:315 -- Processing trial results took 2.950 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:27:17,466\tWARNING util.py:315 -- The `process_trial_result` operation took 2.961 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_2281f368_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-26-44/wandb/run-20230719_132716-2281f368\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: Syncing run FSR_Trainable_2281f368\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2281f368\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:                      mae 0.34082\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:                     mape 0.08405\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:                     rmse 0.72752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:       time_since_restore 6.77948\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:         time_this_iter_s 6.77948\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:             time_total_s 6.77948\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:                timestamp 1689740834\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb:  View run FSR_Trainable_2281f368 at: https://wandb.ai/seokjin/FSR-prediction/runs/2281f368\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627090)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132716-2281f368/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132639-977aa183/logs\n",
      "2023-07-19 13:27:37,966\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.648 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:27:37,979\tWARNING util.py:315 -- The `process_trial_result` operation took 2.661 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:27:37,982\tWARNING util.py:315 -- Processing trial results took 2.664 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:27:37,993\tWARNING util.py:315 -- The `process_trial_result` operation took 2.675 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626620)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_9a936a86_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-27-07/wandb/run-20230719_132738-9a936a86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: Syncing run FSR_Trainable_9a936a86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9a936a86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 13:27:54,079\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.643 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:27:54,086\tWARNING util.py:315 -- The `process_trial_result` operation took 2.651 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:27:54,097\tWARNING util.py:315 -- Processing trial results took 2.662 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:27:54,132\tWARNING util.py:315 -- The `process_trial_result` operation took 2.697 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_794cf0e5_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-27-30/wandb/run-20230719_132753-794cf0e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb: Syncing run FSR_Trainable_794cf0e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/794cf0e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:                      mae 0.32092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:                     mape 0.09422\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:                     rmse 0.68677\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:       time_since_restore 13.72453\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:         time_this_iter_s 6.34672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:             time_total_s 13.72453\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:                timestamp 1689740880\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb:  View run FSR_Trainable_794cf0e5 at: https://wandb.ai/seokjin/FSR-prediction/runs/794cf0e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627565)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132753-794cf0e5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 13:28:23,010\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.260 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:28:23,015\tWARNING util.py:315 -- The `process_trial_result` operation took 2.266 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:28:23,017\tWARNING util.py:315 -- Processing trial results took 2.268 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:28:23,018\tWARNING util.py:315 -- The `process_trial_result` operation took 2.269 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_3388440b_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-27-44/wandb/run-20230719_132822-3388440b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb: Syncing run FSR_Trainable_3388440b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/3388440b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:                      mae 0.31785\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:                     mape 0.09249\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:                     rmse 0.68848\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:       time_since_restore 12.81088\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:         time_this_iter_s 5.92661\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:             time_total_s 12.81088\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:                timestamp 1689740908\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb:  View run FSR_Trainable_3388440b at: https://wandb.ai/seokjin/FSR-prediction/runs/3388440b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627813)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132822-3388440b/logs\n",
      "2023-07-19 13:28:51,819\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.541 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:28:51,825\tWARNING util.py:315 -- The `process_trial_result` operation took 2.548 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:28:51,827\tWARNING util.py:315 -- Processing trial results took 2.551 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:28:51,830\tWARNING util.py:315 -- The `process_trial_result` operation took 2.553 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_137d9c29_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-28-13/wandb/run-20230719_132852-137d9c29\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: Syncing run FSR_Trainable_137d9c29\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/137d9c29\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:                      mae 0.32601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:                     mape 0.09411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:                     rmse 0.69038\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:       time_since_restore 6.15569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:         time_this_iter_s 6.15569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:             time_total_s 6.15569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:                timestamp 1689740929\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb:  View run FSR_Trainable_137d9c29 at: https://wandb.ai/seokjin/FSR-prediction/runs/137d9c29\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628056)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132852-137d9c29/logs\n",
      "2023-07-19 13:29:14,801\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.543 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:29:14,805\tWARNING util.py:315 -- The `process_trial_result` operation took 2.549 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:29:14,807\tWARNING util.py:315 -- Processing trial results took 2.550 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:29:14,809\tWARNING util.py:315 -- The `process_trial_result` operation took 2.552 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_28d1fd52_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-28-43/wandb/run-20230719_132915-28d1fd52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb: Syncing run FSR_Trainable_28d1fd52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/28d1fd52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:                      mae 0.33117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:                     mape 0.0939\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:                     rmse 0.68676\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:       time_since_restore 9.51969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:         time_this_iter_s 4.5498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:             time_total_s 9.51969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:                timestamp 1689740959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb:  View run FSR_Trainable_28d1fd52 at: https://wandb.ai/seokjin/FSR-prediction/runs/28d1fd52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628297)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132915-28d1fd52/logs\n",
      "2023-07-19 13:29:43,090\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.002 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:29:43,097\tWARNING util.py:315 -- The `process_trial_result` operation took 3.010 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:29:43,098\tWARNING util.py:315 -- Processing trial results took 3.012 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:29:43,101\tWARNING util.py:315 -- The `process_trial_result` operation took 3.014 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_f262fb70_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-29-07/wandb/run-20230719_132943-f262fb70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: Syncing run FSR_Trainable_f262fb70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f262fb70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:                      mae 0.3223\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:                     mape 0.09349\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:                     rmse 0.69065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:       time_since_restore 6.75502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:         time_this_iter_s 6.75502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:             time_total_s 6.75502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:                timestamp 1689740980\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb:  View run FSR_Trainable_f262fb70 at: https://wandb.ai/seokjin/FSR-prediction/runs/f262fb70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628542)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132943-f262fb70/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 13:30:08,300\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.010 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:30:08,307\tWARNING util.py:315 -- The `process_trial_result` operation took 3.018 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:30:08,308\tWARNING util.py:315 -- Processing trial results took 3.020 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:30:08,310\tWARNING util.py:315 -- The `process_trial_result` operation took 3.022 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_38591770_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-29-33/wandb/run-20230719_133008-38591770\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: Syncing run FSR_Trainable_38591770\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/38591770\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: / 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:                      mae 0.31886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:                     mape 0.09211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:                     rmse 0.6887\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:       time_since_restore 7.11209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:         time_this_iter_s 7.11209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:             time_total_s 7.11209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:                timestamp 1689741005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb:  View run FSR_Trainable_38591770 at: https://wandb.ai/seokjin/FSR-prediction/runs/38591770\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=628785)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133008-38591770/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629031)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629031)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629031)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629031)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629031)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_e56982cb_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-29-58/wandb/run-20230719_133032-e56982cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629031)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629031)\u001b[0m wandb: Syncing run FSR_Trainable_e56982cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629031)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629031)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e56982cb\n",
      "2023-07-19 13:30:45,456\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.701 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:30:45,458\tWARNING util.py:315 -- The `process_trial_result` operation took 2.704 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:30:45,461\tWARNING util.py:315 -- Processing trial results took 2.707 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:30:45,463\tWARNING util.py:315 -- The `process_trial_result` operation took 2.709 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629031)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-07-19 13:31:14,690 E 609381 609381] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: aefd3402ebd2fc51e5dbdd1e08bca8d13488b5714e207568084755de, IP: 172.26.215.93) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.26.215.93`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_f0ef8d30_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-30-23/wandb/run-20230719_133128-f0ef8d30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb: Syncing run FSR_Trainable_f0ef8d30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0ef8d30\n",
      "2023-07-19 13:31:31,653\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.625 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:31:31,654\tWARNING util.py:315 -- The `process_trial_result` operation took 2.628 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:31:31,656\tWARNING util.py:315 -- Processing trial results took 2.629 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:31:31,657\tWARNING util.py:315 -- The `process_trial_result` operation took 2.631 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:                      mae 0.31739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:                     mape 0.09638\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:                     rmse 0.68541\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:       time_since_restore 37.92315\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:         time_this_iter_s 9.05884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:             time_total_s 37.92315\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:                timestamp 1689741119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb:  View run FSR_Trainable_f0ef8d30 at: https://wandb.ai/seokjin/FSR-prediction/runs/f0ef8d30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629296)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133128-f0ef8d30/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:                      mae 0.30992\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:                     mape 0.09352\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:                     rmse 0.68238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:       time_since_restore 1019.79647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:         time_this_iter_s 10.44934\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:             time_total_s 1019.79647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:                timestamp 1689741132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb:  View run FSR_Trainable_986747c2 at: https://wandb.ai/seokjin/FSR-prediction/runs/986747c2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=619207)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_131451-986747c2/logs\n",
      "2023-07-19 13:32:19,858\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.845 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:32:19,868\tWARNING util.py:315 -- The `process_trial_result` operation took 2.857 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:32:19,870\tWARNING util.py:315 -- Processing trial results took 2.859 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:32:19,872\tWARNING util.py:315 -- The `process_trial_result` operation took 2.861 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_35af815c_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-31-18/wandb/run-20230719_133223-35af815c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: Syncing run FSR_Trainable_35af815c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/35af815c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:                      mae 0.38122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:                     mape 0.10835\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:                     rmse 0.69981\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:       time_since_restore 2.37894\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:         time_this_iter_s 2.37894\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:             time_total_s 2.37894\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:                timestamp 1689741137\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb:  View run FSR_Trainable_35af815c at: https://wandb.ai/seokjin/FSR-prediction/runs/35af815c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629576)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133223-35af815c/logs\n",
      "2023-07-19 13:32:34,344\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.076 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:32:34,349\tWARNING util.py:315 -- The `process_trial_result` operation took 3.082 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:32:34,352\tWARNING util.py:315 -- Processing trial results took 3.085 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:32:34,354\tWARNING util.py:315 -- The `process_trial_result` operation took 3.087 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_f4215bc9_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-32-14/wandb/run-20230719_133237-f4215bc9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: Syncing run FSR_Trainable_f4215bc9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f4215bc9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:                      mae 0.38171\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:                     mape 0.09975\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:                     rmse 0.70842\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:       time_since_restore 2.90923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:         time_this_iter_s 2.90923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:             time_total_s 2.90923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:                timestamp 1689741151\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb:  View run FSR_Trainable_f4215bc9 at: https://wandb.ai/seokjin/FSR-prediction/runs/f4215bc9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=629808)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133237-f4215bc9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_697170b1_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-32-28/wandb/run-20230719_133252-697170b1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: Syncing run FSR_Trainable_697170b1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/697170b1\n",
      "2023-07-19 13:32:57,670\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.142 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:32:57,674\tWARNING util.py:315 -- The `process_trial_result` operation took 3.147 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:32:57,677\tWARNING util.py:315 -- Processing trial results took 3.150 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:32:57,679\tWARNING util.py:315 -- The `process_trial_result` operation took 3.152 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630266)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630266)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630266)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630266)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630266)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_c80030cb_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-32-42/wandb/run-20230719_133308-c80030cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630266)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630266)\u001b[0m wandb: Syncing run FSR_Trainable_c80030cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630266)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630266)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c80030cb\n",
      "2023-07-19 13:33:13,683\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.415 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:33:13,686\tWARNING util.py:315 -- The `process_trial_result` operation took 3.419 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:33:13,688\tWARNING util.py:315 -- Processing trial results took 3.421 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:33:13,689\tWARNING util.py:315 -- The `process_trial_result` operation took 3.422 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-07-19 13:33:14,698 E 609381 609381] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: aefd3402ebd2fc51e5dbdd1e08bca8d13488b5714e207568084755de, IP: 172.26.215.93) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.26.215.93`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_4956b0dc_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-32-58/wandb/run-20230719_133347-4956b0dc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb: Syncing run FSR_Trainable_4956b0dc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4956b0dc\n",
      "2023-07-19 13:33:51,886\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.139 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:33:51,887\tWARNING util.py:315 -- The `process_trial_result` operation took 2.142 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:33:51,889\tWARNING util.py:315 -- Processing trial results took 2.143 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:33:51,890\tWARNING util.py:315 -- The `process_trial_result` operation took 2.144 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:                      mae 0.31878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:                     mape 0.0923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:                     rmse 0.68714\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:       time_since_restore 21.37538\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:         time_this_iter_s 9.7436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:             time_total_s 21.37538\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:                timestamp 1689741241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb:  View run FSR_Trainable_4956b0dc at: https://wandb.ai/seokjin/FSR-prediction/runs/4956b0dc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630523)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133347-4956b0dc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:                      mae 0.31364\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:                     mape 0.09209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:                     rmse 0.6846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:       time_since_restore 85.3806\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:         time_this_iter_s 10.21514\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:             time_total_s 85.3806\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:                timestamp 1689741252\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb:  View run FSR_Trainable_697170b1 at: https://wandb.ai/seokjin/FSR-prediction/runs/697170b1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630038)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133252-697170b1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_6e00c170_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-33-38/wandb/run-20230719_133425-6e00c170\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb: Syncing run FSR_Trainable_6e00c170\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/6e00c170\n",
      "2023-07-19 13:34:26,861\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.037 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:34:26,867\tWARNING util.py:315 -- The `process_trial_result` operation took 3.044 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:34:26,869\tWARNING util.py:315 -- Processing trial results took 3.045 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:34:26,871\tWARNING util.py:315 -- The `process_trial_result` operation took 3.048 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:34:36,759\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.449 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:34:36,763\tWARNING util.py:315 -- The `process_trial_result` operation took 2.454 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:34:36,765\tWARNING util.py:315 -- Processing trial results took 2.456 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:34:36,766\tWARNING util.py:315 -- The `process_trial_result` operation took 2.458 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_eb715b45_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-34-15/wandb/run-20230719_133438-eb715b45\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Syncing run FSR_Trainable_eb715b45\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/eb715b45\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:                      mae 0.31673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:                     mape 0.09209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:                     rmse 0.68619\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:       time_since_restore 30.16193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:         time_this_iter_s 6.98437\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:             time_total_s 30.16193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:                timestamp 1689741291\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb:  View run FSR_Trainable_6e00c170 at: https://wandb.ai/seokjin/FSR-prediction/runs/6e00c170\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=630786)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133425-6e00c170/logs\n",
      "2023-07-19 13:35:11,396\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.661 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:35:11,401\tWARNING util.py:315 -- The `process_trial_result` operation took 2.667 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:35:11,403\tWARNING util.py:315 -- Processing trial results took 2.669 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:35:11,404\tWARNING util.py:315 -- The `process_trial_result` operation took 2.670 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_0bf13ef3_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-34-30/wandb/run-20230719_133513-0bf13ef3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: Syncing run FSR_Trainable_0bf13ef3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0bf13ef3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:                      mae 0.33816\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:                     mape 0.09915\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:                     rmse 0.68995\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:       time_since_restore 3.67563\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:         time_this_iter_s 3.67563\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:             time_total_s 3.67563\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:                timestamp 1689741308\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb:  View run FSR_Trainable_0bf13ef3 at: https://wandb.ai/seokjin/FSR-prediction/runs/0bf13ef3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631265)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133513-0bf13ef3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:                      mae 0.31915\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:                     mape 0.09534\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:                     rmse 0.68286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:       time_since_restore 446.61473\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:         time_this_iter_s 4.18083\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:             time_total_s 446.61473\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:                timestamp 1689741324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb:  View run FSR_Trainable_9a936a86 at: https://wandb.ai/seokjin/FSR-prediction/runs/9a936a86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=627342)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_132738-9a936a86/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133438-eb715b45/logs\n",
      "2023-07-19 13:35:33,056\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.524 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:35:33,060\tWARNING util.py:315 -- The `process_trial_result` operation took 2.529 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:35:33,061\tWARNING util.py:315 -- Processing trial results took 2.530 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:35:33,063\tWARNING util.py:315 -- The `process_trial_result` operation took 2.533 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_af01c727_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-35-05/wandb/run-20230719_133533-af01c727\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb: Syncing run FSR_Trainable_af01c727\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/af01c727\n",
      "2023-07-19 13:35:46,539\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.947 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:35:46,541\tWARNING util.py:315 -- The `process_trial_result` operation took 2.950 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:35:46,543\tWARNING util.py:315 -- Processing trial results took 2.952 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:35:46,544\tWARNING util.py:315 -- The `process_trial_result` operation took 2.953 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631011)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_82a622a9_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-35-25/wandb/run-20230719_133547-82a622a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Syncing run FSR_Trainable_82a622a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/82a622a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:                      mae 0.32227\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:                     mape 0.09416\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:                     rmse 0.68593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:       time_since_restore 18.27037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:         time_this_iter_s 4.62255\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:             time_total_s 18.27037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:                timestamp 1689741346\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb:  View run FSR_Trainable_af01c727 at: https://wandb.ai/seokjin/FSR-prediction/runs/af01c727\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133533-af01c727/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631508)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133547-82a622a9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133547-82a622a9/logs\n",
      "2023-07-19 13:35:58,843\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.487 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:35:58,847\tWARNING util.py:315 -- The `process_trial_result` operation took 2.492 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:35:58,849\tWARNING util.py:315 -- Processing trial results took 2.494 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:35:58,850\tWARNING util.py:315 -- The `process_trial_result` operation took 2.495 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631747)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_e9a64d47_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-35-38/wandb/run-20230719_133559-e9a64d47\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb: Syncing run FSR_Trainable_e9a64d47\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e9a64d47\n",
      "2023-07-19 13:36:13,312\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.064 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:36:13,318\tWARNING util.py:315 -- The `process_trial_result` operation took 3.071 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:36:13,320\tWARNING util.py:315 -- Processing trial results took 3.073 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:36:13,322\tWARNING util.py:315 -- The `process_trial_result` operation took 3.075 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_22dd73b2_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-35-51/wandb/run-20230719_133614-22dd73b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: Syncing run FSR_Trainable_22dd73b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/22dd73b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:                      mae 0.32717\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:                     mape 0.0967\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:                     rmse 0.69091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:       time_since_restore 5.95286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:         time_this_iter_s 5.95286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:             time_total_s 5.95286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:                timestamp 1689741370\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb:  View run FSR_Trainable_22dd73b2 at: https://wandb.ai/seokjin/FSR-prediction/runs/22dd73b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632210)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133614-22dd73b2/logs\n",
      "2023-07-19 13:36:28,688\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.788 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:36:28,692\tWARNING util.py:315 -- The `process_trial_result` operation took 2.793 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:36:28,694\tWARNING util.py:315 -- Processing trial results took 2.794 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:36:28,695\tWARNING util.py:315 -- The `process_trial_result` operation took 2.796 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...437)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_ce92bf0a_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-36-04/wandb/run-20230719_133629-ce92bf0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb: Syncing run FSR_Trainable_ce92bf0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ce92bf0a\n",
      "2023-07-19 13:36:39,623\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.333 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:36:39,629\tWARNING util.py:315 -- The `process_trial_result` operation took 2.339 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:36:39,631\tWARNING util.py:315 -- Processing trial results took 2.341 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:36:39,634\tWARNING util.py:315 -- The `process_trial_result` operation took 2.345 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_6f50045c_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-36-19/wandb/run-20230719_133643-6f50045c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: Syncing run FSR_Trainable_6f50045c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/6f50045c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:                      mae 0.365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:                     mape 0.0965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:                     rmse 0.70406\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:       time_since_restore 2.49726\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:         time_this_iter_s 2.49726\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:             time_total_s 2.49726\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:                timestamp 1689741397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb:  View run FSR_Trainable_6f50045c at: https://wandb.ai/seokjin/FSR-prediction/runs/6f50045c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632666)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133643-6f50045c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb:       training_iteration \n",
      "2023-07-19 13:36:59,610\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.355 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:36:59,624\tWARNING util.py:315 -- The `process_trial_result` operation took 2.370 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:36:59,625\tWARNING util.py:315 -- Processing trial results took 2.372 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:36:59,627\tWARNING util.py:315 -- The `process_trial_result` operation took 2.374 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=626159)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_631d5474_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-36-34/wandb/run-20230719_133703-631d5474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb: Syncing run FSR_Trainable_631d5474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/631d5474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:                      mae 0.35117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:                     mape 0.10477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:                     rmse 0.68579\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:       time_since_restore 8.5789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:         time_this_iter_s 1.95633\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:             time_total_s 8.5789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:                timestamp 1689741425\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb:  View run FSR_Trainable_631d5474 at: https://wandb.ai/seokjin/FSR-prediction/runs/631d5474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133703-631d5474/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632917)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_3e55d9a3_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-36-54/wandb/run-20230719_133717-3e55d9a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb: Syncing run FSR_Trainable_3e55d9a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/3e55d9a3\n",
      "2023-07-19 13:37:19,366\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.915 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:37:19,371\tWARNING util.py:315 -- The `process_trial_result` operation took 2.921 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:37:19,374\tWARNING util.py:315 -- Processing trial results took 2.924 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:37:19,379\tWARNING util.py:315 -- The `process_trial_result` operation took 2.928 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)04 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:                      mae 0.33455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:                     mape 0.09492\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:                     rmse 0.69015\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:       time_since_restore 8.38134\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:         time_this_iter_s 8.38134\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:             time_total_s 8.38134\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:                timestamp 1689741436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb:  View run FSR_Trainable_3e55d9a3 at: https://wandb.ai/seokjin/FSR-prediction/runs/3e55d9a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633146)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133717-3e55d9a3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 13:37:33,622\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.084 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:37:33,632\tWARNING util.py:315 -- The `process_trial_result` operation took 3.095 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:37:33,635\tWARNING util.py:315 -- Processing trial results took 3.098 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:37:33,637\tWARNING util.py:315 -- The `process_trial_result` operation took 3.100 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_4636353a_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_13-37-08/wandb/run-20230719_133732-4636353a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: Syncing run FSR_Trainable_4636353a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4636353a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:                      mae 0.34417\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:                     mape 0.09524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:                     rmse 0.69381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:       time_since_restore 7.26863\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:         time_this_iter_s 7.26863\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:             time_total_s 7.26863\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:                timestamp 1689741450\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb:  View run FSR_Trainable_4636353a at: https://wandb.ai/seokjin/FSR-prediction/runs/4636353a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633382)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133732-4636353a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 13:37:46,296\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.559 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:37:46,300\tWARNING util.py:315 -- The `process_trial_result` operation took 2.564 s, which may be a performance bottleneck.\n",
      "2023-07-19 13:37:46,302\tWARNING util.py:315 -- Processing trial results took 2.566 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 13:37:46,304\tWARNING util.py:315 -- The `process_trial_result` operation took 2.568 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_13-03-12/FSR_Trainable_4fecccad_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Sim_2023-07-19_13-37-23/wandb/run-20230719_133745-4fecccad\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: Syncing run FSR_Trainable_4fecccad\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4fecccad\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:                      mae 0.33697\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:                     mape 0.09565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:                     rmse 0.69239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:       time_since_restore 6.15044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:         time_this_iter_s 6.15044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:             time_total_s 6.15044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:                timestamp 1689741463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb:  View run FSR_Trainable_4fecccad at: https://wandb.ai/seokjin/FSR-prediction/runs/4fecccad\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=633618)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133745-4fecccad/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:                      mae 0.31922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:                     mape 0.09768\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:                     rmse 0.68103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:       time_since_restore 345.14841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:         time_this_iter_s 3.56547\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:             time_total_s 345.14841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:                timestamp 1689741712\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb:  View run FSR_Trainable_e9a64d47 at: https://wandb.ai/seokjin/FSR-prediction/runs/e9a64d47\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=631986)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133559-e9a64d47/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:                      mae 0.32055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:                     mape 0.09441\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:                     rmse 0.67815\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:       time_since_restore 340.2473\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:         time_this_iter_s 2.58578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:             time_total_s 340.2473\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:                timestamp 1689741731\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb:  View run FSR_Trainable_ce92bf0a at: https://wandb.ai/seokjin/FSR-prediction/runs/ce92bf0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=632437)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_133629-ce92bf0a/logs\n",
      "2023-07-19 13:42:18,586\tINFO tune.py:1111 -- Total run time: 2341.90 seconds (2335.32 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
